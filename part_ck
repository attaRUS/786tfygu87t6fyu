PECT_TRUE(response->headers().get(Http::LowerCaseString("test-protocol")).empty());

    // verify scheme
    EXPECT_EQ("http", getHeader(response->headers(), "test-scheme"));

    // verify method
    EXPECT_EQ("POST", getHeader(response->headers(), "test-method"));

    // verify path
    EXPECT_EQ(path, getHeader(response->headers(), "test-path"));

    // verify host
    EXPECT_EQ("test.com", getHeader(response->headers(), "test-host"));

    // verify log level
    EXPECT_EQ("error", getHeader(response->headers(), "test-log-level"));

    // upper("goodbye")
    EXPECT_EQ("GOODBYE", response->body());

    cleanup();
  }

  void testMetric(std::string path) {
    initializeBasicFilter(METRIC);

    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{
        {":method", "POST"}, {":path", path}, {":scheme", "http"}, {":authority", "test.com"}};

    auto encoder_decoder = codec_client_->startRequest(request_headers, true);
    auto response = std::move(encoder_decoder.second);

    waitForNextUpstreamRequest();

    EXPECT_EQ("2", getHeader(upstream_request_->headers(), "go-metric-counter-test-header-key"));

    EXPECT_EQ("3", getHeader(upstream_request_->headers(), "go-metric-gauge-test-header-key"));

    EXPECT_EQ("3",
              getHeader(upstream_request_->headers(), "go-metric-counter-record-test-header-key"));

    EXPECT_EQ("1",
              getHeader(upstream_request_->headers(), "go-metric-gauge-record-test-header-key"));

    Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
    upstream_request_->encodeHeaders(response_headers, true);

    ASSERT_TRUE(response->waitForEndStream());

    cleanup();
  }

  void testRouteConfig(std::string domain, std::string path, bool header_0_existing,
                       std::string set_header) {
    initializeRouteConfig(ROUTECONFIG);

    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{
        {":method", "GET"}, {":path", path}, {":scheme", "http"}, {":authority", domain}};

    auto encoder_decoder = codec_client_->startRequest(request_headers, true);
    auto response = std::move(encoder_decoder.second);

    waitForNextUpstreamRequest();

    Http::TestResponseHeaderMapImpl response_headers{
        {":status", "200"}, {"x-test-header-0", "foo"}, {"x-test-header-1", "bar"}};
    upstream_request_->encodeHeaders(response_headers, true);

    ASSERT_TRUE(response->waitForEndStream());

    EXPECT_EQ(header_0_existing,
              !response->headers().get(Http::LowerCaseString("x-test-header-0")).empty());

    if (!set_header.empty()) {
      EXPECT_EQ("test-value", getHeader(response->headers(), set_header));
    }
    cleanup();
  }

  void testSendLocalReply(std::string path, std::string phase) {
    initializeBasicFilter(BASIC);

    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{
        {":method", "POST"}, {":path", path}, {":scheme", "http"}, {":authority", "test.com"}};

    auto encoder_decoder = codec_client_->startRequest(request_headers);
    Http::RequestEncoder& request_encoder = encoder_decoder.first;
    auto response = std::move(encoder_decoder.second);

    // do not sendData when phase is decode-header,
    // since the request may be terminated before sendData.
    if (phase != "decode-header") {
      codec_client_->sendData(request_encoder, "hello", true);
    }

    // need upstream request when send local reply in encode phases.
    if (phase == "encode-header" || phase == "encode-data") {
      waitForNextUpstreamRequest();
      Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
      upstream_request_->encodeHeaders(response_headers, false);

      // do not sendData when phase is encode-header
      if (phase == "encode-data") {
        Buffer::OwnedImpl response_data("bye");
        upstream_request_->encodeData(response_data, true);
      }
    }

    ASSERT_TRUE(response->waitForEndStream());

    // check resp status
    EXPECT_EQ("403", response->headers().getStatusValue());

    // forbidden from go in %s\r\n
    auto body = StringUtil::toUpper(absl::StrFormat("forbidden from go in %s\r\n", phase));
    EXPECT_EQ(body, StringUtil::toUpper(response->body()));

    // verify phase
    EXPECT_EQ(phase, getHeader(response->headers(), "test-phase"));

    // verify content-type
    EXPECT_EQ("text/html", getHeader(response->headers(), "content-type"));

    // verify two values
    auto values = response->headers().get(Http::LowerCaseString("x-two-values"));
    if (values.size() == 2) {
      EXPECT_EQ("foo", values[0]->value().getStringView());
      EXPECT_EQ("bar", values[1]->value().getStringView());
    } else {
      EXPECT_EQ(values.size(), 2);
    }

    cleanup();
  }

  void testBufferExceedLimit(std::string path) {
    config_helper_.setBufferLimits(1024, 150);
    initializeBasicFilter(BASIC);

    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{
        {":method", "POST"}, {":path", path}, {":scheme", "http"}, {":authority", "test.com"}};

    auto encoder_decoder = codec_client_->startRequest(request_headers);
    Http::RequestEncoder& request_encoder = encoder_decoder.first;
    auto response = std::move(encoder_decoder.second);
    // 100 + 200 > 150, exceed buffer limit.
    codec_client_->sendData(request_encoder, std::string(100, '-'), false);
    codec_client_->sendData(request_encoder, std::string(200, '-'), true);

    ASSERT_TRUE(response->waitForEndStream());

    // check resp status
    EXPECT_EQ("413", response->headers().getStatusValue());

    auto body = StringUtil::toUpper("payload too large");
    EXPECT_EQ(body, response->body());

    cleanup();
  }

  void testPanicRecover(std::string path, std::string phase) {
    initializeBasicFilter(BASIC);

    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{
        {":method", "POST"}, {":path", path}, {":scheme", "http"}, {":authority", "test.com"}};

    auto encoder_decoder = codec_client_->startRequest(request_headers);
    Http::RequestEncoder& request_encoder = encoder_decoder.first;
    auto response = std::move(encoder_decoder.second);

    // do not sendData when phase is decode-header,
    // since the request may be terminated before sendData.
    if (phase != "decode-header") {
      codec_client_->sendData(request_encoder, "hello", true);
    }

    // need upstream request when send local reply in encode phases.
    if (phase == "encode-header" || phase == "encode-data") {
      waitForNextUpstreamRequest();
      Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
      upstream_request_->encodeHeaders(response_headers, false);

      // do not sendData when phase is encode-header
      if (phase == "encode-data") {
        Buffer::OwnedImpl response_data("bye");
        upstream_request_->encodeData(response_data, true);
      }
    }

    ASSERT_TRUE(response->waitForEndStream());

    // check resp status
    EXPECT_EQ("500", response->headers().getStatusValue());

    // error happened in filter\r\n
    auto body = StringUtil::toUpper("error happened in filter\r\n");
    EXPECT_EQ(body, StringUtil::toUpper(response->body()));

    EXPECT_EQ(1, test_server_->counter("http.config_test.golang.panic_error")->value());

    cleanup();
  }

  void cleanup() {
    codec_client_->close();

    if (fake_upstream_connection_ != nullptr) {
      AssertionResult result = fake_upstream_connection_->close();
      RELEASE_ASSERT(result, result.message());
      result = fake_upstream_connection_->waitForDisconnect();
      RELEASE_ASSERT(result, result.message());
    }
  }

  void testDynamicMetadata(std::string path) {
    initializeBasicFilter(BASIC, "*", true);

    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{
        {":method", "POST"},        {":path", path},
        {":scheme", "http"},        {":authority", "test.com"},
        {"x-set-metadata", "true"},
    };

    auto encoder_decoder = codec_client_->startRequest(request_headers);
    Http::RequestEncoder& request_encoder = encoder_decoder.first;
    auto response = std::move(encoder_decoder.second);
    codec_client_->sendData(request_encoder, "helloworld", true);

    waitForNextUpstreamRequest();

    Http::TestResponseHeaderMapImpl response_headers{
        {":status", "200"},
    };
    upstream_request_->encodeHeaders(response_headers, true);

    ASSERT_TRUE(response->waitForEndStream());
    EXPECT_THAT(response->headers(), Http::HttpStatusIs("200"));
    cleanup();
  }

  void testActionWithoutData(std::string query) {
    initializeBasicFilter(ACTION, "test.com");
    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{{":method", "GET"},
                                                   {":path", "/test?" + query},
                                                   {":scheme", "http"},
                                                   {":authority", "test.com"}};

    auto encoder_decoder = codec_client_->startRequest(request_headers, true);
    auto response = std::move(encoder_decoder.second);

    if (query.find("encodeHeadersRet") != std::string::npos) {
      waitForNextUpstreamRequest();

      Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
      upstream_request_->encodeHeaders(response_headers, true);
    }

    ASSERT_TRUE(response->waitForEndStream());
    EXPECT_EQ("500", response->headers().getStatusValue());

    EXPECT_EQ(1, test_server_->counter("http.config_test.golang.panic_error")->value());

    cleanup();
  }

  void testBufferApi(std::string query) {
    initializeBasicFilter(BUFFER, "test.com");

    auto path = std::string("/test?") + query;
    codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
    Http::TestRequestHeaderMapImpl request_headers{
        {":method", "POST"},
        {":path", path},
        {":scheme", "http"},
        {":authority", "test.com"},
    };

    auto encoder_decoder = codec_client_->startRequest(request_headers);
    Http::RequestEncoder& request_encoder = encoder_decoder.first;
    auto response = std::move(encoder_decoder.second);
    std::string data = "";
    for (int i = 0; i < 10; i++) {
      data += "12345";
    }
    codec_client_->sendData(request_encoder, data, true);

    waitForNextUpstreamRequest();

    Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
    upstream_request_->encodeHeaders(response_headers, false);
    Buffer::OwnedImpl response_data("goodbye");
    upstream_request_->encodeData(response_data, true);

    ASSERT_TRUE(response->waitForEndStream());
    EXPECT_EQ("200", response->headers().getStatusValue());
    cleanup();
  }

  const std::string ECHO{"echo"};
  const std::string BASIC{"basic"};
  const std::string PASSTHROUGH{"passthrough"};
  const std::string BUFFER{"buffer"};
  const std::string ROUTECONFIG{"routeconfig"};
  const std::string PROPERTY{"property"};
  const std::string ACCESSLOG{"access_log"};
  const std::string METRIC{"metric"};
  const std::string ACTION{"action"};
};

INSTANTIATE_TEST_SUITE_P(IpVersions, GolangIntegrationTest,
                         testing::ValuesIn(TestEnvironment::getIpVersionsForTest()),
                         TestUtility::ipTestParamsToString);

TEST_P(GolangIntegrationTest, Echo) {
  initializeConfig(ECHO, genSoPath(ECHO), ECHO);
  initialize();
  registerTestServerPorts({"http"});

  auto path = "/echo";
  auto echo_body = "echo from go";
  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "POST"}, {":path", path}, {":scheme", "http"}, {":authority", "test.com"}};

  auto encoder_decoder = codec_client_->startRequest(request_headers);
  auto response = std::move(encoder_decoder.second);

  ASSERT_TRUE(response->waitForEndStream());

  // check status for echo
  EXPECT_EQ("403", response->headers().getStatusValue());

  // check body for echo
  auto body = absl::StrFormat("%s, path: %s\r\n", echo_body, path);
  EXPECT_EQ(body, response->body());

  cleanup();
}

TEST_P(GolangIntegrationTest, Passthrough) {
  initializeConfig(PASSTHROUGH, genSoPath(PASSTHROUGH), PASSTHROUGH);
  initialize();
  registerTestServerPorts({"http"});

  auto path = "/";
  auto good = "good";
  auto bye = "bye";
  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "POST"}, {":path", path}, {":scheme", "http"}, {":authority", "test.com"}};

  auto encoder_decoder = codec_client_->startRequest(request_headers);
  Http::RequestEncoder& request_encoder = encoder_decoder.first;
  auto response = std::move(encoder_decoder.second);
  codec_client_->sendData(request_encoder, "helloworld", true);

  waitForNextUpstreamRequest();
  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  upstream_request_->encodeHeaders(response_headers, false);
  Buffer::OwnedImpl response_data1(good);
  upstream_request_->encodeData(response_data1, false);
  Buffer::OwnedImpl response_data2(bye);
  upstream_request_->encodeData(response_data2, true);

  ASSERT_TRUE(response->waitForEndStream());

  // check status for passthrough
  EXPECT_EQ("200", response->headers().getStatusValue());

  // check body for passthrough
  auto body = absl::StrFormat("%s%s", good, bye);
  EXPECT_EQ(body, response->body());

  cleanup();
}

TEST_P(GolangIntegrationTest, PluginNotFound) {
  initializeConfig(ECHO, genSoPath(ECHO), PASSTHROUGH);
  initialize();
  registerTestServerPorts({"http"});

  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "POST"}, {":path", "/"}, {":scheme", "http"}, {":authority", "test.com"}};

  auto response = sendRequestAndWaitForResponse(request_headers, 0, default_response_headers_, 0);
  ASSERT_TRUE(response->waitForEndStream());

  EXPECT_EQ("200", response->headers().getStatusValue());
  cleanup();
}

TEST_P(GolangIntegrationTest, BufferDrain) { testBufferApi("Drain"); }

TEST_P(GolangIntegrationTest, BufferReset) { testBufferApi("Reset"); }

TEST_P(GolangIntegrationTest, BufferResetAfterDrain) { testBufferApi("ResetAfterDrain"); }

TEST_P(GolangIntegrationTest, BufferLen) { testBufferApi("Len"); }

TEST_P(GolangIntegrationTest, Property) {
  initializePropertyConfig(PROPERTY, genSoPath(PROPERTY), PROPERTY);
  initialize();
  registerTestServerPorts({"http"});

  auto path = "/property?a=1";
  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "POST"},  {":path", path},  {":scheme", "http"},     {":authority", "test.com"},
      {"User-Agent", "ua"}, {"Referer", "r"}, {"X-Request-Id", "xri"},
  };

  auto encoder_decoder = codec_client_->startRequest(request_headers);
  Http::RequestEncoder& request_encoder = encoder_decoder.first;
  auto response = std::move(encoder_decoder.second);
  codec_client_->sendData(request_encoder, "helloworld", true);

  waitForNextUpstreamRequest();

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  upstream_request_->encodeHeaders(response_headers, false);
  Buffer::OwnedImpl response_data("goodbye");
  upstream_request_->encodeData(response_data, true);

  ASSERT_TRUE(response->waitForEndStream());
  EXPECT_EQ("200", response->headers().getStatusValue());
  cleanup();
}

TEST_P(GolangIntegrationTest, AccessLog) {
  initializeBasicFilter(ACCESSLOG, "test.com");

  auto path = "/test";
  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "POST"},
      {":path", path},
      {":scheme", "http"},
      {":authority", "test.com"},
  };

  auto encoder_decoder = codec_client_->startRequest(request_headers);
  Http::RequestEncoder& request_encoder = encoder_decoder.first;
  auto response = std::move(encoder_decoder.second);
  codec_client_->sendData(request_encoder, "helloworld", true);

  waitForNextUpstreamRequest();

  Http::TestResponseHeaderMapImpl response_headers{
      {":status", "206"},
  };
  upstream_request_->encodeHeaders(response_headers, false);
  Buffer::OwnedImpl response_data1("good");
  upstream_request_->encodeData(response_data1, false);
  Buffer::OwnedImpl response_data2("bye");
  upstream_request_->encodeData(response_data2, true);

  ASSERT_TRUE(response->waitForEndStream());
  codec_client_->close();

  // use the second request to get the logged data
  codec_client_ = makeHttpConnection(makeClientConnection((lookupPort("http"))));
  response = sendRequestAndWaitForResponse(request_headers, 0, default_response_headers_, 0);

  EXPECT_TRUE(upstream_request_->complete());
  EXPECT_TRUE(response->complete());
  EXPECT_EQ("206", getHeader(upstream_request_->headers(), "respCode"));
  EXPECT_EQ("7", getHeader(upstream_request_->headers(), "respSize"));
  EXPECT_EQ("true", getHeader(upstream_request_->headers(), "canRunAsyncly"));

  cleanup();
}

TEST_P(GolangIntegrationTest, AccessLogDownstreamStart) {
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager&
              hcm) {
        hcm.mutable_access_log_options()->set_flush_access_log_on_new_request(true);
      });
  initializeBasicFilter(ACCESSLOG, "test.com");

  auto path = "/test";
  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "POST"},        {":path", path},  {":scheme", "http"},
      {":authority", "test.com"}, {"Referer", "r"},
  };

  auto response = sendRequestAndWaitForResponse(request_headers, 0, default_response_headers_, 0);
  EXPECT_TRUE(response->complete());
  codec_client_->close();

  // use the second request to get the logged data
  codec_client_ = makeHttpConnection(makeClientConnection((lookupPort("http"))));
  Http::TestRequestHeaderMapImpl request_headers2{
      {":method", "POST"},        {":path", path},   {":scheme", "http"},
      {":authority", "test.com"}, {"Referer", "r2"},
  };

  response = sendRequestAndWaitForResponse(request_headers2, 0, default_response_headers_, 0);

  EXPECT_TRUE(response->complete());
  EXPECT_EQ("r;r2", getHeader(upstream_request_->headers(), "referers"));
  EXPECT_EQ("true", getHeader(upstream_request_->headers(), "canRunAsynclyForDownstreamStart"));

  cleanup();
}

TEST_P(GolangIntegrationTest, AccessLogDownstreamPeriodic) {
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager&
              hcm) {
        hcm.mutable_access_log_options()->mutable_access_log_flush_interval()->set_nanos(
            100000000); // 0.1 seconds
      });
  initializeBasicFilter(ACCESSLOG, "test.com");

  auto path = "/test?periodic=1";
  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "POST"},        {":path", path},  {":scheme", "http"},
      {":authority", "test.com"}, {"Referer", "r"},
  };

  auto response = sendRequestAndWaitForResponse(request_headers, 0, default_response_headers_, 0);
  EXPECT_TRUE(response->complete());
  codec_client_->close();

  // use the second request to get the logged data
  codec_client_ = makeHttpConnection(makeClientConnection((lookupPort("http"))));
  response = sendRequestAndWaitForResponse(request_headers, 0, default_response_headers_, 0);

  EXPECT_TRUE(response->complete());
  EXPECT_EQ("r", getHeader(upstream_request_->headers(), "referers"));
  EXPECT_EQ("true", getHeader(upstream_request_->headers(), "canRunAsynclyForDownstreamPeriodic"));

  cleanup();
}

// Metric API testing
TEST_P(GolangIntegrationTest, Metric) { testMetric("/test"); }

// Metric API testing in async mode.
TEST_P(GolangIntegrationTest, AsyncMetric) { testMetric("/test?async=1"); }

// Basic API testing, i.e. add/remove/set Headers & data rewrite.
TEST_P(GolangIntegrationTest, Basic) { testBasic("/test"); }

// Basic API testing in async mode.
TEST_P(GolangIntegrationTest, Async) { testBasic("/test?async=1"); }

// buffer all data in decode header phase.
TEST_P(GolangIntegrationTest, DataBuffer_DecodeHeader) {
  testBasic("/test?databuffer=decode-header");
}

// Go sleep in sync mode.
TEST_P(GolangIntegrationTest, Sleep) { testBasic("/test?sleep=1"); }

// Go sleep in decode/encode data phase.
TEST_P(GolangIntegrationTest, DataSleep) { testBasic("/test?data_sleep=1"); }

// Go sleep in async mode.
TEST_P(GolangIntegrationTest, Async_Sleep) { testBasic("/test?async=1&sleep=1"); }

// Go sleep in decode/encode data phase with async mode.
TEST_P(GolangIntegrationTest, Async_DataSleep) { testBasic("/test?async=1&data_sleep=1"); }

// buffer all data in decode header phase with async mode.
TEST_P(GolangIntegrationTest, Async_DataBuffer_DecodeHeader) {
  testBasic("/test?async=1&databuffer=decode-header");
}

// buffer all data in decode data phase with sync mode.
TEST_P(GolangIntegrationTest, DataBuffer_DecodeData) { testBasic("/test?databuffer=decode-data"); }

// buffer all data in decode data phase with async mode.
TEST_P(GolangIntegrationTest, Async_DataBuffer_DecodeData) {
  testBasic("/test?async=1&databuffer=decode-data");
}

// Go send local reply in decode header phase.
TEST_P(GolangIntegrationTest, LocalReply_DecodeHeader) {
  testSendLocalReply("/test?localreply=decode-header", "decode-header");
}

// Go send local reply in decode header phase with async mode.
TEST_P(GolangIntegrationTest, LocalReply_DecodeHeader_Async) {
  testSendLocalReply("/test?async=1&localreply=decode-header", "decode-header");
}

// Go send local reply in decode data phase.
TEST_P(GolangIntegrationTest, LocalReply_DecodeData) {
  testSendLocalReply("/test?localreply=decode-data", "decode-data");
}

// Go send local reply in decode data phase with async mode.
TEST_P(GolangIntegrationTest, LocalReply_DecodeData_Async) {
  testSendLocalReply("/test?async=1&sleep=1&localreply=decode-data", "decode-data");
}

// The next filter(lua filter) send local reply after Go filter continue in decode header phase.
// Go filter will terminate when lua filter send local reply.
TEST_P(GolangIntegrationTest, LuaRespondAfterGoHeaderContinue) {
  // put lua filter after golang filter
  // golang filter => lua filter.

  const std::string LUA_RESPOND =
      R"EOF(
name: envoy.filters.http.lua
typed_config:
  "@type": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua
  default_source_code:
    inline_string: |
        function envoy_on_request(handle)
        local orig_header = handle:headers():get('x-test-header-0')
        local go_header = handle:headers():get('test-x-set-header-0')
        handle:respond({[":status"] = "403"}, "forbidden from lua, orig header: "
            .. (orig_header or "nil")
            .. ", go header: "
            .. (go_header or "nil"))
        end
)EOF";
  config_helper_.prependFilter(LUA_RESPOND);

  initializeBasicFilter(BASIC);

  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort("http")));
  Http::TestRequestHeaderMapImpl request_headers{{":method", "POST"},
                                                 {":path", "/test"},
                                                 {":scheme", "http"},
                                                 {":authority", "test.com"},
                                                 {"x-test-header-0", "foo"}};

  auto encoder_decoder = codec_client_->startRequest(request_headers);
  auto response = std::move(encoder_decoder.second);

  ASSERT_TRUE(response->waitForEndStream());

  // check resp status
  EXPECT_EQ("403", response->headers().getStatusValue());

  // forbidden from lua, orig header: foo, go header: foo
  auto body = StringUtil::toUpper("forbidden from lua, orig header: foo, go header: foo");
  EXPECT_EQ(body, response->body());

  cleanup();
}

// Buffer exceed limit in decode header phase.
TEST_P(GolangIntegrationTest, BufferExceedLimit_DecodeHeader) {
  testBufferExceedLimit("/test?databuffer=decode-header");
}

// Using the original config in http filter: (no per route config)
// remove: x-test-header-0
// set: foo
TEST_P(GolangIntegrationTest, RouteConfig_Filter) {
  testRouteConfig("filter-level.com", "/test", false, "foo");
}

// Using the merged config from http filter & virtualhost level per route config:
// remove: x-test-header-1
// set: bar
TEST_P(GolangIntegrationTest, RouteConfig_VirtualHost) {
  testRouteConfig("test.com", "/test", true, "bar");
}

// Using the merged config from route level & virtualhost level & http filter:
// remove: x-test-header-0
// set: baz
TEST_P(GolangIntegrationTest, RouteConfig_Route) {
  testRouteConfig("test.com", "/route-config-test", false, "baz");
}

// Out of range in decode header phase
TEST_P(GolangIntegrationTest, PanicRecover_DecodeHeader) {
  testPanicRecover("/test?panic=decode-header", "decode-header");
}

// Out of range in decode header phase with async mode
TEST_P(GolangIntegrationTest, PanicRecover_DecodeHeader_Async) {
  testPanicRecover("/test?async=1&panic=decode-header", "decode-header");
}

// Out of range in decode data phase
TEST_P(GolangIntegrationTest, PanicRecover_DecodeData) {
  testPanicRecover("/test?panic=decode-data", "decode-data");
}

// Out of range in decode data phase with async mode & sleep
TEST_P(GolangIntegrationTest, PanicRecover_DecodeData_Async) {
  testPanicRecover("/test?async=1&sleep=1&panic=decode-data", "decode-data");
}

// Out of range in encode data phase with async mode & sleep
TEST_P(GolangIntegrationTest, PanicRecover_EncodeData_Async) {
  testPanicRecover("/test?async=1&sleep=1&panic=encode-data", "encode-data");
}

// Panic ErrInvalidPhase
TEST_P(GolangIntegrationTest, PanicRecover_BadAPI) {
  testPanicRecover("/test?badapi=decode-data", "decode-data");
}

TEST_P(GolangIntegrationTest, DynamicMetadata) { testDynamicMetadata("/test?dymeta=1"); }

TEST_P(GolangIntegrationTest, DynamicMetadata_Async) {
  testDynamicMetadata("/test?dymeta=1&async=1");
}

TEST_P(GolangIntegrationTest, DynamicMetadata_Async_Sleep) {
  testDynamicMetadata("/test?dymeta=1&async=1&sleep=1");
}

TEST_P(GolangIntegrationTest, DecodeHeadersWithoutData_StopAndBuffer) {
  testActionWithoutData("decodeHeadersRet=StopAndBuffer");
}

TEST_P(GolangIntegrationTest, DecodeHeadersWithoutData_StopAndBufferWatermark) {
  testActionWithoutData("decodeHeadersRet=StopAndBufferWatermark");
}

TEST_P(GolangIntegrationTest, DecodeHeadersWithoutData_StopAndBuffer_Async) {
  testActionWithoutData("decodeHeadersRet=StopAndBuffer&aysnc=1");
}

TEST_P(GolangIntegrationTest, DecodeHeadersWithoutData_StopAndBufferWatermark_Async) {
  testActionWithoutData("decodeHeadersRet=StopAndBufferWatermark&aysnc=1");
}

TEST_P(GolangIntegrationTest, EncodeHeadersWithoutData_StopAndBuffer) {
  testActionWithoutData("encodeHeadersRet=StopAndBuffer");
}

TEST_P(GolangIntegrationTest, EncodeHeadersWithoutData_StopAndBufferWatermark) {
  testActionWithoutData("encodeHeadersRet=StopAndBufferWatermark");
}

TEST_P(GolangIntegrationTest, EncodeHeadersWithoutData_StopAndBuffer_Async) {
  testActionWithoutData("encodeHeadersRet=StopAndBuffer&aysnc=1");
}

TEST_P(GolangIntegrationTest, EncodeHeadersWithoutData_StopAndBufferWatermark_Async) {
  testActionWithoutData("encodeHeadersRet=StopAndBufferWatermark&aysnc=1");
}

} // namespace Envoy
#include <string>

#include "envoy/registry/registry.h"

#include "test/mocks/server/factory_context.h"
#include "test/test_common/environment.h"
#include "test/test_common/utility.h"

#include "absl/strings/str_format.h"
#include "contrib/golang/filters/http/source/config.h"
#include "contrib/golang/filters/http/source/golang_filter.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {
namespace {

std::string genSoPath(std::string name) {
  return TestEnvironment::substitute(
      "{{ test_rundir }}/contrib/golang/filters/http/test/test_data/" + name + "/filter.so");
}

TEST(GolangFilterConfigTest, InvalidateEmptyConfig) {
  NiceMock<Server::Configuration::MockFactoryContext> context;
  EXPECT_THROW_WITH_REGEX(
      GolangFilterConfig()
          .createFilterFactoryFromProto(envoy::extensions::filters::http::golang::v3alpha::Config(),
                                        "stats", context)
          .status()
          .IgnoreError(),
      Envoy::ProtoValidationException,
      "ConfigValidationError.LibraryId: value length must be at least 1 characters");
}

TEST(GolangFilterConfigTest, GolangFilterWithValidConfig) {
  const auto yaml_fmt = R"EOF(
  library_id: %s
  library_path: %s
  plugin_name: xxx
  merge_policy: MERGE_VIRTUALHOST_ROUTER_FILTER
  plugin_config:
    "@type": type.googleapis.com/udpa.type.v1.TypedStruct
    type_url: typexx
    value:
        key: value
        int: 10
  )EOF";

  const std::string PASSTHROUGH{"passthrough"};
  auto yaml_string = absl::StrFormat(yaml_fmt, PASSTHROUGH, genSoPath(PASSTHROUGH));
  envoy::extensions::filters::http::golang::v3alpha::Config proto_config;
  TestUtility::loadFromYaml(yaml_string, proto_config);
  NiceMock<Server::Configuration::MockFactoryContext> context;
  GolangFilterConfig factory;
  Http::FilterFactoryCb cb =
      factory.createFilterFactoryFromProto(proto_config, "stats", context).value();
  Http::MockFilterChainFactoryCallbacks filter_callback;
  EXPECT_CALL(filter_callback, addStreamFilter(_));
  EXPECT_CALL(filter_callback, addAccessLogHandler(_));
  auto plugin_config = proto_config.plugin_config();
  std::string str;
  EXPECT_TRUE(plugin_config.SerializeToString(&str));
  cb(filter_callback);
}

TEST(GolangFilterConfigTest, GolangFilterWithNilPluginConfig) {
  const auto yaml_fmt = R"EOF(
  library_id: %s
  library_path: %s
  plugin_name: xxx
  )EOF";

  const std::string PASSTHROUGH{"passthrough"};
  auto yaml_string = absl::StrFormat(yaml_fmt, PASSTHROUGH, genSoPath(PASSTHROUGH));
  envoy::extensions::filters::http::golang::v3alpha::Config proto_config;
  TestUtility::loadFromYaml(yaml_string, proto_config);
  NiceMock<Server::Configuration::MockFactoryContext> context;
  GolangFilterConfig factory;
  Http::FilterFactoryCb cb =
      factory.createFilterFactoryFromProto(proto_config, "stats", context).value();
  Http::MockFilterChainFactoryCallbacks filter_callback;
  EXPECT_CALL(filter_callback, addStreamFilter(_));
  EXPECT_CALL(filter_callback, addAccessLogHandler(_));
  auto plugin_config = proto_config.plugin_config();
  std::string str;
  EXPECT_TRUE(plugin_config.SerializeToString(&str));
  cb(filter_callback);
}

} // namespace
} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
module example.com/dummy

go 1.20

require github.com/envoyproxy/envoy v1.24.0

require google.golang.org/protobuf v1.30.0 // indirect

replace github.com/envoyproxy/envoy => ../../../../../../../
// This is a NOOP plugin for the purpose of configuration testing.

package main

import (
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser("", http.PassThroughFactory, nil)
}

func main() {
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "my_plugin.so",
    srcs = ["plugin.go"],
    out = "my_plugin.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/dummy",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/filters/http/source/go/pkg/http",
    ],
)

go_binary(
    name = "my_other_plugin.so",
    srcs = ["plugin.go"],
    out = "my_other_plugin.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/dummy",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/filters/http/source/go/pkg/http",
    ],
)

go_binary(
    name = "my_configurable_plugin.so",
    srcs = ["plugin.go"],
    out = "my_configurable_plugin.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/dummy",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/filters/http/source/go/pkg/http",
    ],
)

go_binary(
    name = "simple.so",
    srcs = ["plugin.go"],
    out = "simple.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/dummy",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/filters/http/source/go/pkg/http",
    ],
)

filegroup(
    name = "testing_shared_objects",
    srcs = [
        ":my_configurable_plugin.so",
        ":my_other_plugin.so",
        ":my_plugin.so",
        ":simple.so",
    ],
    visibility = ["//visibility:public"],
)
package main

import (
	xds "github.com/cncf/xds/go/xds/type/v3"
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "echo"

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser(Name, ConfigFactory, &parser{})
}

type config struct {
	echoBody  string
	matchPath string
}

type parser struct {
}

func (p *parser) Parse(any *anypb.Any, callbacks api.ConfigCallbackHandler) (interface{}, error) {
	configStruct := &xds.TypedStruct{}
	if err := any.UnmarshalTo(configStruct); err != nil {
		return nil, err
	}

	v := configStruct.Value
	conf := &config{}
	if str, ok := v.AsMap()["echo_body"].(string); ok {
		conf.echoBody = str
	}
	if str, ok := v.AsMap()["match_path"].(string); ok {
		conf.matchPath = str
	}
	return conf, nil
}

func (p *parser) Merge(parent interface{}, child interface{}) interface{} {
	panic("TODO")
}

func ConfigFactory(c interface{}) api.StreamFilterFactory {
	conf, ok := c.(*config)
	if !ok {
		panic("unexpected config type")
	}
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			callbacks: callbacks,
			config:    conf,
		}
	}
}

func main() {}
module example.com/echo

go 1.20

require (
	github.com/cncf/xds/go v0.0.0-20231128003011-0fa0005c9caa
	github.com/envoyproxy/envoy v1.24.0
)

require (
	google.golang.org/genproto/googleapis/api v0.0.0-20240102182953-50ed04b92917 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20240102182953-50ed04b92917 // indirect
)

require (
	github.com/envoyproxy/protoc-gen-validate v1.0.2 // indirect
	github.com/golang/protobuf v1.5.3 // indirect
	google.golang.org/protobuf v1.32.0
)

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"fmt"
	"runtime"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type filter struct {
	api.PassThroughStreamFilter

	callbacks api.FilterCallbackHandler
	path      string
	config    *config
}

func (f *filter) sendLocalReply() api.StatusType {
	echoBody := f.config.echoBody
	{
		body := fmt.Sprintf("%s, path: %s\r\n", echoBody, f.path)
		f.callbacks.SendLocalReply(403, body, nil, 0, "")
	}
	// Force GC to free the body string.
	// For the case that C++ shouldn't touch the memory of the body string,
	// after the sendLocalReply function returns.
	runtime.GC()
	return api.LocalReply
}

func (f *filter) DecodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	header.Del("x-test-header-1")
	f.path, _ = header.Get(":path")
	header.Set("rsp-header-from-go", "foo-test")
	// For the convenience of testing, it's better to in the config parse phase
	matchPath := f.config.matchPath
	if matchPath != "" && f.path == matchPath {
		return f.sendLocalReply()
	}
	return api.Continue
}

func (f *filter) EncodeHeaders(header api.ResponseHeaderMap, endStream bool) api.StatusType {
	header.Set("Rsp-Header-From-Go", "bar-test")
	return api.Continue
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/echo",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@com_github_cncf_xds_go//xds/type/v3:type",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
package main

import (
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "access_log"

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser(Name, ConfigFactory, &parser{})
}

type config struct {
}

type parser struct {
}

func (p *parser) Parse(any *anypb.Any, callbacks api.ConfigCallbackHandler) (interface{}, error) {
	conf := &config{}
	return conf, nil
}

func (p *parser) Merge(parent interface{}, child interface{}) interface{} {
	return child
}

func ConfigFactory(c interface{}) api.StreamFilterFactory {
	conf, ok := c.(*config)
	if !ok {
		panic("unexpected config type")
	}
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			callbacks: callbacks,
			config:    conf,
		}
	}
}

func main() {}
module example.com/access_log

go 1.20

require github.com/envoyproxy/envoy v1.24.0

require google.golang.org/protobuf v1.30.0 // indirect

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

var (
	counter = 0
	wg      = &sync.WaitGroup{}

	respCode      string
	respSize      string
	canRunAsyncly bool

	canRunAsynclyForDownstreamStart bool

	canRunAsynclyForDownstreamPeriodic bool

	referers = []string{}
)

type filter struct {
	api.PassThroughStreamFilter

	callbacks api.FilterCallbackHandler
	config    *config
}

func (f *filter) DecodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	if counter == 0 {
		query, _ := f.callbacks.GetProperty("request.query")
		if query == "periodic=1" {
			go func() {
				defer f.callbacks.RecoverPanic()

				// trigger AccessLogDownstreamPeriodic
				time.Sleep(110 * time.Millisecond)
				f.callbacks.Continue(api.Continue)
			}()
			return api.Running
		}
	}

	if counter > 0 {
		wg.Wait()
		header.Set("respCode", respCode)
		header.Set("respSize", respSize)
		header.Set("canRunAsyncly", strconv.FormatBool(canRunAsyncly))
		header.Set("canRunAsynclyForDownstreamStart", strconv.FormatBool(canRunAsynclyForDownstreamStart))
		header.Set("canRunAsynclyForDownstreamPeriodic", strconv.FormatBool(canRunAsynclyForDownstreamPeriodic))

		header.Set("referers", strings.Join(referers, ";"))

		// reset for the next test
		referers = []string{}
		// the counter will be 0 when this request is ended
		counter = -1

	}

	return api.Continue
}

func (f *filter) OnLogDownstreamStart() {
	referer, err := f.callbacks.GetProperty("request.referer")
	if err != nil {
		api.LogErrorf("err: %s", err)
		return
	}

	referers = append(referers, referer)

	wg.Add(1)
	go func() {
		time.Sleep(1 * time.Millisecond)
		canRunAsynclyForDownstreamStart = true
		wg.Done()
	}()
}

func (f *filter) OnLogDownstreamPeriodic() {
	referer, err := f.callbacks.GetProperty("request.referer")
	if err != nil {
		api.LogErrorf("err: %s", err)
		return
	}

	referers = append(referers, referer)

	wg.Add(1)
	go func() {
		time.Sleep(1 * time.Millisecond)
		canRunAsynclyForDownstreamPeriodic = true
		wg.Done()
	}()
}

func (f *filter) OnLog() {
	code, ok := f.callbacks.StreamInfo().ResponseCode()
	if !ok {
		return
	}
	respCode = strconv.Itoa(int(code))
	api.LogCritical(respCode)
	size, err := f.callbacks.GetProperty("response.size")
	if err != nil {
		api.LogErrorf("err: %s", err)
		return
	}
	respSize = size

	wg.Add(1)
	go func() {
		time.Sleep(1 * time.Millisecond)
		canRunAsyncly = true
		wg.Done()
	}()

	counter++
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/access_log",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@com_github_cncf_xds_go//xds/type/v3:type",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
module example.com/passthrough

go 1.20

require github.com/envoyproxy/envoy v1.24.0

require google.golang.org/protobuf v1.30.0 // indirect

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser("passthrough", http.PassThroughFactory, nil)
}

func main() {
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/passthrough",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@com_github_cncf_xds_go//udpa/type/v1:type",
        "@org_golang_google_protobuf//types/known/anypb",
    ],
)
package main

import (
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "property"

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser(Name, ConfigFactory, &parser{})
}

type config struct {
}

type parser struct {
}

func (p *parser) Parse(any *anypb.Any, callbacks api.ConfigCallbackHandler) (interface{}, error) {
	conf := &config{}
	return conf, nil
}

func (p *parser) Merge(parent interface{}, child interface{}) interface{} {
	return child
}

func ConfigFactory(c interface{}) api.StreamFilterFactory {
	conf, ok := c.(*config)
	if !ok {
		panic("unexpected config type")
	}
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			callbacks: callbacks,
			config:    conf,
		}
	}
}

func main() {}
module example.com/property

go 1.20

require (
	github.com/envoyproxy/envoy v1.24.0
	google.golang.org/protobuf v1.32.0
)

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"strconv"
	"time"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type filter struct {
	api.PassThroughStreamFilter

	callbacks api.FilterCallbackHandler
	path      string
	config    *config

	failed bool
}

func (f *filter) assertProperty(name, exp string) {
	act, err := f.callbacks.GetProperty(name)
	if err != nil {
		act = err.Error()
	}
	if exp != act {
		f.callbacks.Log(api.Critical, name+" expect "+exp+" got "+act)
		f.failed = true
	}
}

func (f *filter) panicIfFailed() {
	if f.failed {
		panic("Check the critical log for the failed cases")
	}
}

func (f *filter) DecodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	ts, _ := f.callbacks.GetProperty("request.time")
	ymd := ts[:len("2023-07-31T00:00:00")]
	startTime, _ := time.Parse("2006-01-02T15:04:05", ymd)
	if time.Now().UTC().Sub(startTime) > 1*time.Minute {
		f.callbacks.Log(api.Critical, "got request.time "+ts)
		f.failed = true
	}

	f.assertProperty("request.protocol", "HTTP/1.1")
	f.assertProperty("request.path", "/property?a=1")
	f.assertProperty("request.url_path", "/property")
	f.assertProperty("request.query", "a=1")
	f.assertProperty("request.host", "test.com")
	f.assertProperty("request.scheme", "http")
	f.assertProperty("request.method", "POST")
	f.assertProperty("request.referer", "r")
	f.assertProperty("request.useragent", "ua")
	f.assertProperty("request.id", "xri")

	f.assertProperty("request.duration", api.ErrValueNotFound.Error()) // available only when the request is finished

	f.assertProperty("source.address", f.callbacks.StreamInfo().DownstreamRemoteAddress())
	f.assertProperty("destination.address", f.callbacks.StreamInfo().DownstreamLocalAddress())
	f.assertProperty("connection.mtls", "false")
	// route name can be determinated in the decode phase
	f.assertProperty("xds.route_name", "test-route-name")

	// non-existed attribute
	f.assertProperty("request.user_agent", api.ErrValueNotFound.Error())

	// access response attribute in the decode phase
	f.assertProperty("response.total_size", "0")

	// bad case
	// strange input
	for _, attr := range []string{
		".",
		".total_size",
	} {
		f.assertProperty(attr, api.ErrValueNotFound.Error())
	}
	// unsupported value type
	for _, attr := range []string{
		// unknown type
		"",
		// map type
		"request",
		"request.",
	} {
		f.assertProperty(attr, api.ErrSerializationFailure.Error())
	}

	// error handling
	_, err := f.callbacks.GetProperty(".not_found")
	if err != api.ErrValueNotFound {
		f.callbacks.Log(api.Critical, "unexpected error "+err.Error())
		f.failed = true
	}
	return api.Continue
}

func (f *filter) EncodeHeaders(header api.ResponseHeaderMap, endStream bool) api.StatusType {
	f.assertProperty("xds.route_name", "test-route-name")
	f.assertProperty("xds.cluster_name", "cluster_0")
	f.assertProperty("xds.cluster_metadata", "")

	code, _ := f.callbacks.StreamInfo().ResponseCode()
	exp := ""
	if code != 0 {
		exp = strconv.Itoa(int(code))
	}
	f.assertProperty("response.code", exp)
	f.assertProperty("response.code_details", "via_upstream")

	f.assertProperty("request.size", "10") // "helloworld"
	size, _ := f.callbacks.GetProperty("request.total_size")
	intSize, _ := strconv.Atoi(size)
	if intSize <= 10 {
		f.callbacks.Log(api.Critical, "got request.total_size "+size)
		f.failed = true
	}
	f.assertProperty("request.referer", "r")

	return api.Continue
}

func (f *filter) EncodeData(buffer api.BufferInstance, endStream bool) api.StatusType {
	f.assertProperty("response.code", "200")

	// panic if any condition is not met
	f.panicIfFailed()
	return api.Continue
}

func (f *filter) OnLog() {
	f.assertProperty("response.size", "7") // "goodbye"

	// panic if any condition is not met
	f.panicIfFailed()
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/property",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@com_github_cncf_xds_go//xds/type/v3:type",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
package main

import (
	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "basic"

func init() {
	api.LogCritical("init")
	api.LogCritical(api.GetLogLevel().String())

	http.RegisterHttpFilterConfigFactoryAndParser(Name, ConfigFactory, nil)
}

func ConfigFactory(interface{}) api.StreamFilterFactory {
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			callbacks: callbacks,
		}
	}
}

func main() {
}
module example.com/basic

go 1.20

require github.com/envoyproxy/envoy v1.24.0

require google.golang.org/protobuf v1.30.0 // indirect

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"fmt"
	"net/url"
	"strconv"
	"strings"
	"time"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type filter struct {
	api.PassThroughStreamFilter

	callbacks       api.FilterCallbackHandler
	req_body_length uint64
	query_params    url.Values
	scheme          string
	method          string
	path            string
	host            string

	// for bad api call testing
	header api.RequestHeaderMap

	// test mode, from query parameters
	async       bool
	sleep       bool   // all sleep
	data_sleep  bool   // only sleep in data phase
	localreplay string // send local reply
	databuffer  string // return api.Stop
	panic       string // hit panic in which phase
	badapi      bool   // bad api call
}

func parseQuery(path string) url.Values {
	if idx := strings.Index(path, "?"); idx >= 0 {
		query := path[idx+1:]
		values, _ := url.ParseQuery(query)
		return values
	}
	return make(url.Values)
}

func badcode() {
	// panic index out of range
	s := []int{1}
	s[1] = s[5]
}

func (f *filter) initRequest(header api.RequestHeaderMap) {
	f.header = header

	f.req_body_length = 0

	f.scheme = header.Scheme()
	f.method = header.Method()
	f.path = header.Path()
	f.host = header.Host()

	f.query_params = parseQuery(f.path)
	if f.query_params.Get("async") != "" {
		f.async = true
	}
	if f.query_params.Get("sleep") != "" {
		f.sleep = true
	}
	if f.query_params.Get("data_sleep") != "" {
		f.data_sleep = true
	}
	if f.query_params.Get("decode_localrepaly") != "" {
		f.data_sleep = true
	}
	f.databuffer = f.query_params.Get("databuffer")
	f.localreplay = f.query_params.Get("localreply")
	f.panic = f.query_params.Get("panic")
	f.badapi = f.query_params.Get("badapi") != ""
}

func (f *filter) fail(msg string, a ...any) api.StatusType {
	body := fmt.Sprintf(msg, a...)
	f.callbacks.Log(api.Error, fmt.Sprintf("test failed: %s", body))
	f.callbacks.SendLocalReply(500, body, nil, 0, "")
	return api.LocalReply
}

func (f *filter) sendLocalReply(phase string) api.StatusType {
	headers := map[string][]string{
		"Content-type": {"text/html"},
		"test-phase":   {phase},
		"x-two-values": {"foo", "bar"},
	}
	body := fmt.Sprintf("forbidden from go in %s\r\n", phase)
	f.callbacks.SendLocalReply(403, body, headers, 0, "")
	return api.LocalReply
}

// test: get, set, remove, values, add
func (f *filter) decodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	// test logging
	f.callbacks.Log(api.Trace, "log test")
	f.callbacks.Log(api.Debug, "log test")
	f.callbacks.Log(api.Info, "log test")
	f.callbacks.Log(api.Warn, "log test")
	f.callbacks.Log(api.Error, "log test")
	f.callbacks.Log(api.Critical, "log test")

	api.LogTrace("log test")
	api.LogDebug("log test")
	api.LogInfo("log test")
	api.LogWarn("log test")
	api.LogError("log test")
	api.LogCritical("log test")

	api.LogTracef("log test %v", endStream)
	api.LogDebugf("log test %v", endStream)
	api.LogInfof("log test %v", endStream)
	api.LogWarnf("log test %v", endStream)
	api.LogErrorf("log test %v", endStream)
	api.LogCriticalf("log test %v", endStream)

	if f.callbacks.LogLevel() != api.GetLogLevel() {
		return f.fail("log level mismatch")
	}

	if f.sleep {
		time.Sleep(time.Millisecond * 100) // sleep 100 ms
	}

	_, found := header.Get("x-set-metadata")
	if found {
		md := f.callbacks.StreamInfo().DynamicMetadata()
		empty_metadata := md.Get("filter.go")
		if len(empty_metadata) != 0 {
			return f.fail("Metadata should be empty")
		}
		md.Set("filter.go", "foo", "bar")
		metadata := md.Get("filter.go")
		if len(metadata) == 0 {
			return f.fail("Metadata should not be empty")
		}

		k, ok := metadata["foo"]
		if !ok {
			return f.fail("Metadata foo should be found")
		}

		if fmt.Sprint(k) != "bar" {
			return f.fail("Metadata foo has unexpected value %v", k)
		}
	}

	fs := f.callbacks.StreamInfo().FilterState()
	fs.SetString("go_state_test_key", "go_state_test_value", api.StateTypeReadOnly, api.LifeSpanRequest, api.SharedWithUpstreamConnection)

	val := fs.GetString("go_state_test_key")
	header.Add("go-state-test-header-key", val)

	if strings.Contains(f.localreplay, "decode-header") {
		return f.sendLocalReply("decode-header")
	}

	header.Range(func(key, value string) bool {
		if key == ":path" && value != f.path {
			f.fail("path not match in Range")
			return false
		}
		return true
	})

	header.RangeWithCopy(func(key, value string) bool {
		if key == ":path" && value != f.path {
			f.fail("path not match in RangeWithCopy")
			return false
		}
		return true
	})

	origin, found := header.Get("x-test-header-0")
	hdrs := header.Values("x-test-header-0")
	if found {
		if origin != hdrs[0] {
			return f.fail("Values return incorrect data %v", hdrs)
		}
	} else if hdrs != nil {
		return f.fail("Values return unexpected data %v", hdrs)
	}

	if found {
		upperCase, _ := header.Get("X-Test-Header-0")
		if upperCase != origin {
			return f.fail("Get should be case-insensitive")
		}
		upperCaseHdrs := header.Values("X-Test-Header-0")
		if hdrs[0] != upperCaseHdrs[0] {
			return f.fail("Values should be case-insensitive")
		}
	}

	header.Add("UpperCase", "header")
	if hdr, _ := header.Get("uppercase"); hdr != "header" {
		return f.fail("Add should be case-insensitive")
	}
	header.Set("UpperCase", "header")
	if hdr, _ := header.Get("uppercase"); hdr != "header" {
		return f.fail("Set should be case-insensitive")
	}
	header.Del("UpperCase")
	if hdr, _ := header.Get("uppercase"); hdr != "" {
		return f.fail("Del should be case-insensitive")
	}

	header.Add("existed-header", "bar")
	header.Add("newly-added-header", "foo")
	header.Add("newly-added-header", "bar")

	header.Set("test-x-set-header-0", origin)
	header.Del("x-test-header-1")
	header.Set("req-route-name", f.callbacks.StreamInfo().GetRouteName())
	header.Set("req-downstream-local-address", f.callbacks.StreamInfo().DownstreamLocalAddress())
	header.Set("req-downstream-remote-address", f.callbacks.StreamInfo().DownstreamRemoteAddress())
	if !endStream && strings.Contains(f.databuffer, "decode-header") {
		return api.StopAndBuffer
	}

	if f.panic == "decode-header" {
		badcode()
	}
	return api.Continue
}

// test: get, set, append, prepend
func (f *filter) decodeData(buffer api.BufferInstance, endStream bool) api.StatusType {
	if f.sleep || f.data_sleep {
		time.Sleep(time.Millisecond * 100) // sleep 100 ms
	}
	if strings.Contains(f.localreplay, "decode-data") {
		return f.sendLocalReply("decode-data")
	}
	f.req_body_length += uint64(buffer.Len())
	if buffer.Len() != 0 {
		data := buffer.String()
		if string(buffer.Bytes()) != data {
			return f.sendLocalReply(fmt.Sprintf("data in bytes: %s vs data in string: %s",
				string(buffer.Bytes()), data))
		}

		buffer.SetString(strings.ToUpper(data))
		buffer.AppendString("_append")
		buffer.PrependString("prepend_")
	}
	if !endStream && strings.Contains(f.databuffer, "decode-data") {
		return api.StopAndBuffer
	}

	if f.panic == "decode-data" {
		badcode()
	}
	if f.badapi {
		// set header after header continued will panic with the ErrInvalidPhase error message.
		f.header.Set("foo", "bar")
	}
	return api.Continue
}

func (f *filter) decodeTrailers(trailers api.RequestTrailerMap) api.StatusType {
	if f.sleep {
		time.Sleep(time.Millisecond * 100) // sleep 100 ms
	}
	if strings.Contains(f.localreplay, "decode-trailer") {
		return f.sendLocalReply("decode-trailer")
	}

	trailers.Add("existed-trailer", "bar")
	trailers.Set("x-test-trailer-0", "bar")
	trailers.Del("x-test-trailer-1")

	if trailers.GetRaw("existed-trailer") == "foo" {
		trailers.Add("x-test-trailer-2", "bar")
	}

	upperCase, _ := trailers.Get("X-Test-Trailer-0")
	if upperCase != "bar" {
		return f.fail("Get should be case-insensitive")
	}
	upperCaseHdrs := trailers.Values("X-Test-Trailer-0")
	if upperCaseHdrs[0] != "bar" {
		return f.fail("Values should be case-insensitive")
	}

	trailers.Add("UpperCase", "trailers")
	if hdr, _ := trailers.Get("uppercase"); hdr != "trailers" {
		return f.fail("Add should be case-insensitive")
	}
	trailers.Set("UpperCase", "trailers")
	if hdr, _ := trailers.Get("uppercase"); hdr != "trailers" {
		return f.fail("Set should be case-insensitive")
	}
	trailers.Del("UpperCase")
	if hdr, _ := trailers.Get("uppercase"); hdr != "" {
		return f.fail("Del should be case-insensitive")
	}

	if f.panic == "decode-trailer" {
		badcode()
	}
	return api.Continue
}

func (f *filter) encodeHeaders(header api.ResponseHeaderMap, endStream bool) api.StatusType {
	if f.sleep {
		time.Sleep(time.Millisecond * 100) // sleep 100 ms
	}
	if strings.Contains(f.localreplay, "encode-header") {
		return f.sendLocalReply("encode-header")
	}

	if protocol, ok := f.callbacks.StreamInfo().Protocol(); ok {
		header.Set("rsp-protocol", protocol)
	}
	if code, ok := f.callbacks.StreamInfo().ResponseCode(); ok {
		header.Set("rsp-response-code", strconv.Itoa(int(code)))
	}
	if details, ok := f.callbacks.StreamInfo().ResponseCodeDetails(); ok {
		header.Set("rsp-response-code-details", details)
	}
	if upstream_host_address, ok := f.callbacks.StreamInfo().UpstreamRemoteAddress(); ok {
		header.Set("rsp-upstream-host", upstream_host_address)
	}
	if upstream_cluster_name, ok := f.callbacks.StreamInfo().UpstreamClusterName(); ok {
		header.Set("rsp-upstream-cluster", upstream_cluster_name)
	}

	origin, found := header.Get("x-test-header-0")
	hdrs := header.Values("x-test-header-0")
	if found {
		if origin != hdrs[0] {
			return f.fail("Values return incorrect data %v", hdrs)
		}
	} else if hdrs != nil {
		return f.fail("Values return unexpected data %v", hdrs)
	}

	if status, ok := header.Status(); ok {
		header.Add("rsp-status", strconv.Itoa(status))
	}

	header.Add("existed-header", "bar")
	header.Add("newly-added-header", "foo")
	header.Add("newly-added-header", "bar")

	header.Set("test-x-set-header-0", origin)
	header.Del("x-test-header-1")
	header.Set("test-req-body-length", strconv.Itoa(int(f.req_body_length)))
	header.Set("test-query-param-foo", f.query_params.Get("foo"))
	header.Set("test-scheme", f.scheme)
	header.Set("test-method", f.method)
	header.Set("test-path", f.path)
	header.Set("test-host", f.host)
	header.Set("test-log-level", f.callbacks.LogLevel().String())
	header.Set("rsp-route-name", f.callbacks.StreamInfo().GetRouteName())
	header.Set("rsp-filter-chain-name", f.callbacks.StreamInfo().FilterChainName())
	header.Set("rsp-attempt-count", strconv.Itoa(int(f.callbacks.StreamInfo().AttemptCount())))
	if name, ok := f.callbacks.StreamInfo().VirtualClusterName(); ok {
		header.Set("rsp-virtual-cluster-name", name)
	} else {
		header.Set("rsp-virtual-cluster-name", "not found")
	}

	if f.panic == "encode-header" {
		badcode()
	}
	return api.Continue
}

func (f *filter) encodeData(buffer api.BufferInstance, endStream bool) api.StatusType {
	if f.sleep || f.data_sleep {
		time.Sleep(time.Millisecond * 100) // sleep 100 ms
	}
	if strings.Contains(f.localreplay, "encode-data") {
		return f.sendLocalReply("encode-data")
	}
	data := buffer.String()
	buffer.SetString(strings.ToUpper(data))

	if f.panic == "encode-data" {
		badcode()
	}
	return api.Continue
}

func (f *filter) encodeTrailers(trailers api.ResponseTrailerMap) api.StatusType {
	if f.sleep {
		time.Sleep(time.Millisecond * 100) // sleep 100 ms
	}
	if strings.Contains(f.localreplay, "encode-trailer") {
		return f.sendLocalReply("encode-trailer")
	}

	if f.panic == "encode-trailer" {
		badcode()
	}
	return api.Continue
}

func (f *filter) DecodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	f.initRequest(header)
	if f.async {
		go func() {
			defer f.callbacks.RecoverPanic()

			status := f.decodeHeaders(header, endStream)
			if status != api.LocalReply {
				f.callbacks.Continue(status)
			}
		}()
		return api.Running
	} else {
		status := f.decodeHeaders(header, endStream)
		return status
	}
}

func (f *filter) DecodeData(buffer api.BufferInstance, endStream bool) api.StatusType {
	if f.async {
		go func() {
			defer f.callbacks.RecoverPanic()

			status := f.decodeData(buffer, endStream)
			if status != api.LocalReply {
				f.callbacks.Continue(status)
			}
		}()
		return api.Running
	} else {
		status := f.decodeData(buffer, endStream)
		return status
	}
}

func (f *filter) DecodeTrailers(trailers api.RequestTrailerMap) api.StatusType {
	if f.async {
		go func() {
			defer f.callbacks.RecoverPanic()

			status := f.decodeTrailers(trailers)
			if status != api.LocalReply {
				f.callbacks.Continue(status)
			}
		}()
		return api.Running
	} else {
		status := f.decodeTrailers(trailers)
		return status
	}
}

func (f *filter) EncodeHeaders(header api.ResponseHeaderMap, endStream bool) api.StatusType {
	if f.async {
		go func() {
			defer f.callbacks.RecoverPanic()

			status := f.encodeHeaders(header, endStream)
			if status != api.LocalReply {
				f.callbacks.Continue(status)
			}
		}()
		return api.Running
	} else {
		status := f.encodeHeaders(header, endStream)
		return status
	}
}

func (f *filter) EncodeData(buffer api.BufferInstance, endStream bool) api.StatusType {
	if f.async {
		go func() {
			defer f.callbacks.RecoverPanic()

			status := f.encodeData(buffer, endStream)
			if status != api.LocalReply {
				f.callbacks.Continue(status)
			}
		}()
		return api.Running
	} else {
		status := f.encodeData(buffer, endStream)
		return status
	}
}

func (f *filter) EncodeTrailers(trailers api.ResponseTrailerMap) api.StatusType {
	if f.async {
		go func() {
			defer f.callbacks.RecoverPanic()

			status := f.encodeTrailers(trailers)
			if status != api.LocalReply {
				f.callbacks.Continue(status)
			}
		}()
		return api.Running
	} else {
		status := f.encodeTrailers(trailers)
		return status
	}
}

func (f *filter) OnLog() {
	api.LogError("call log in OnLog")
}

func (f *filter) OnDestroy(reason api.DestroyReason) {
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/basic",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
package main

import (
	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "action"

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser(Name, ConfigFactory, nil)
}

type config struct {
	decodeHeadersRet api.StatusType
}

func ConfigFactory(c interface{}) api.StreamFilterFactory {
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			callbacks: callbacks,
		}
	}
}

func main() {}
module example.com/action

go 1.20

require github.com/envoyproxy/envoy v1.24.0

require github.com/google/go-cmp v0.5.9 // indirect

require google.golang.org/protobuf v1.32.0 // indirect

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"net/url"
	"strings"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type filter struct {
	api.PassThroughStreamFilter

	callbacks    api.FilterCallbackHandler
	query_params url.Values
}

func parseQuery(path string) url.Values {
	if idx := strings.Index(path, "?"); idx >= 0 {
		query := path[idx+1:]
		values, _ := url.ParseQuery(query)
		return values
	}
	return make(url.Values)
}

func getStatus(status string) api.StatusType {
	switch status {
	case "StopAndBuffer":
		return api.StopAndBuffer
	case "StopAndBufferWatermark":
		return api.StopAndBufferWatermark
	}
	return api.Continue
}

func (f *filter) DecodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	f.query_params = parseQuery(header.Path())

	decodeHeadersRet := f.query_params.Get("decodeHeadersRet")
	async := f.query_params.Get("async")
	if decodeHeadersRet != "" {
		if async != "" {
			go func() {
				defer f.callbacks.RecoverPanic()
				f.callbacks.Continue(getStatus(decodeHeadersRet))
			}()
			return api.Running
		}

		return getStatus(decodeHeadersRet)
	}

	return api.Continue
}

func (f *filter) EncodeHeaders(header api.ResponseHeaderMap, endStream bool) api.StatusType {
	encodeHeadersRet := f.query_params.Get("encodeHeadersRet")
	async := f.query_params.Get("async")
	if encodeHeadersRet != "" {
		if async != "" {
			go func() {
				defer f.callbacks.RecoverPanic()
				f.callbacks.Continue(getStatus(encodeHeadersRet))
			}()
			return api.Running
		}

		return getStatus(encodeHeadersRet)
	}

	return api.Continue
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/action",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@com_github_cncf_xds_go//xds/type/v3:type",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
package main

import (
	"errors"

	xds "github.com/cncf/xds/go/xds/type/v3"
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "routeconfig"

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser(Name, configFactory, &parser{})
}

func configFactory(c interface{}) api.StreamFilterFactory {
	conf, ok := c.(*config)
	if !ok {
		panic("unexpected config type")
	}
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			config:    conf,
			callbacks: callbacks,
		}
	}
}

type config struct {
	removeHeader string
	setHeader    string
}

type parser struct {
}

func (p *parser) Parse(any *anypb.Any, callbacks api.ConfigCallbackHandler) (interface{}, error) {
	configStruct := &xds.TypedStruct{}
	if err := any.UnmarshalTo(configStruct); err != nil {
		return nil, err
	}

	conf := &config{}
	m := configStruct.Value.AsMap()
	if _, ok := m["invalid"].(string); ok {
		return nil, errors.New("testing invalid config")
	}
	if remove, ok := m["remove"].(string); ok {
		conf.removeHeader = remove
	}
	if set, ok := m["set"].(string); ok {
		conf.setHeader = set
	}
	return conf, nil
}

func (p *parser) Merge(parent interface{}, child interface{}) interface{} {
	parentConfig := parent.(*config)
	childConfig := child.(*config)

	// copy one
	newConfig := *parentConfig
	if childConfig.removeHeader != "" {
		newConfig.removeHeader = childConfig.removeHeader
	}
	if childConfig.setHeader != "" {
		newConfig.setHeader = childConfig.setHeader
	}
	return &newConfig
}

func main() {
}
module example.com/routeconfig

go 1.20

require (
	github.com/cncf/xds/go v0.0.0-20231128003011-0fa0005c9caa
	github.com/envoyproxy/envoy v1.24.0
)

require (
	google.golang.org/genproto/googleapis/api v0.0.0-20240102182953-50ed04b92917 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20240102182953-50ed04b92917 // indirect
)

require (
	github.com/envoyproxy/protoc-gen-validate v1.0.2 // indirect
	github.com/golang/protobuf v1.5.3 // indirect
	google.golang.org/protobuf v1.32.0
)

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type filter struct {
	api.PassThroughStreamFilter

	config    *config
	callbacks api.FilterCallbackHandler
}

func (f *filter) EncodeHeaders(header api.ResponseHeaderMap, endStream bool) api.StatusType {
	if f.config.removeHeader != "" {
		header.Del(f.config.removeHeader)
	}
	if f.config.setHeader != "" {
		header.Set(f.config.setHeader, "test-value")
	}
	return api.Continue
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/routeconfig",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@com_github_cncf_xds_go//xds/type/v3:type",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
package main

import (
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "metric"

func init() {
	api.LogCritical("init")
	api.LogCritical(api.GetLogLevel().String())

	http.RegisterHttpFilterConfigFactoryAndParser(Name, ConfigFactory, &parser{})
}

type config struct {
	counter api.CounterMetric
	gauge   api.GaugeMetric
}

type parser struct {
}

func (p *parser) Parse(any *anypb.Any, callbacks api.ConfigCallbackHandler) (interface{}, error) {
	conf := &config{}
	if callbacks != nil {
		conf.counter = callbacks.DefineCounterMetric("test-counter")
		conf.gauge = callbacks.DefineGaugeMetric("test-gauge")
	}
	return conf, nil
}

func (p *parser) Merge(parent interface{}, child interface{}) interface{} {
	panic("TODO")
}

func ConfigFactory(c interface{}) api.StreamFilterFactory {
	conf, ok := c.(*config)
	if !ok {
		panic("unexpected config type")
	}
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			callbacks: callbacks,
			config:    conf,
		}
	}
}

func main() {
}
module example.com/basic

go 1.20

require github.com/envoyproxy/envoy v1.24.0

require google.golang.org/protobuf v1.30.0 // indirect

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"net/url"
	"strconv"
	"strings"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type filter struct {
	api.PassThroughStreamFilter

	callbacks    api.FilterCallbackHandler
	config       *config
	query_params url.Values
	path         string

	// test mode, from query parameters
	async bool
}

func parseQuery(path string) url.Values {
	if idx := strings.Index(path, "?"); idx >= 0 {
		query := path[idx+1:]
		values, _ := url.ParseQuery(query)
		return values
	}
	return make(url.Values)
}

func (f *filter) initRequest(header api.RequestHeaderMap) {
	f.path = header.Path()
	f.query_params = parseQuery(f.path)
	if f.query_params.Get("async") != "" {
		f.async = true
	}
}

func (f *filter) decodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	f.config.counter.Increment(2)
	value := f.config.counter.Get()
	header.Add("go-metric-counter-test-header-key", strconv.FormatUint(value, 10))

	f.config.counter.Record(1)
	value = f.config.counter.Get()
	header.Add("go-metric-counter-record-test-header-key", strconv.FormatUint(value, 10))

	f.config.gauge.Increment(3)
	value = f.config.gauge.Get()
	header.Add("go-metric-gauge-test-header-key", strconv.FormatUint(value, 10))

	f.config.gauge.Record(1)
	value = f.config.gauge.Get()
	header.Add("go-metric-gauge-record-test-header-key", strconv.FormatUint(value, 10))

	return api.Continue
}

func (f *filter) DecodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {
	f.initRequest(header)
	if f.async {
		go func() {
			defer f.callbacks.RecoverPanic()

			status := f.decodeHeaders(header, endStream)
			if status != api.LocalReply {
				f.callbacks.Continue(status)
			}
		}()
		return api.Running
	} else {
		status := f.decodeHeaders(header, endStream)
		return status
	}
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/metric",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
package main

import (
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http"
)

const Name = "buffer"

func init() {
	http.RegisterHttpFilterConfigFactoryAndParser(Name, ConfigFactory, &parser{})
}

type config struct {
}

type parser struct {
}

func (p *parser) Parse(any *anypb.Any, callbacks api.ConfigCallbackHandler) (interface{}, error) {
	conf := &config{}
	return conf, nil
}

func (p *parser) Merge(parent interface{}, child interface{}) interface{} {
	return child
}

func ConfigFactory(c interface{}) api.StreamFilterFactory {
	conf, ok := c.(*config)
	if !ok {
		panic("unexpected config type")
	}
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &filter{
			callbacks: callbacks,
			config:    conf,
		}
	}
}

func main() {}
module example.com/buffer

go 1.20

require (
	github.com/envoyproxy/envoy v1.24.0
	google.golang.org/protobuf v1.32.0
)

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"fmt"
	"reflect"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type filter struct {
	api.PassThroughStreamFilter

	callbacks api.FilterCallbackHandler
	path      string
	config    *config

	failed bool
}

func testReset(b api.BufferInstance) {
	b.Reset()

	bs := b.Bytes()
	if len(bs) > 0 {
		panic(fmt.Sprintf("unexpected data: %s", string(bs)))
	}
}

func testDrain(b api.BufferInstance) {
	b.Drain(40)
	bs := b.Bytes()
	if string(bs) != "1234512345" {
		panic(fmt.Sprintf("unexpected data: %s", string(bs)))
	}

	b.Drain(5)
	bs = b.Bytes()
	if string(bs) != "12345" {
		panic(fmt.Sprintf("unexpected data: %s", string(bs)))
	}

	b.Drain(10)
	bs = b.Bytes()
	if string(bs) != "" {
		panic(fmt.Sprintf("unexpected data: %s", string(bs)))
	}

	// drain when all data are drained
	b.Drain(10)
	bs = b.Bytes()
	if string(bs) != "" {
		panic(fmt.Sprintf("unexpected data: %s", string(bs)))
	}

	// bad offset
	for _, n := range []int{-1, 0} {
		b.Drain(n)
	}
}

func testResetAfterDrain(b api.BufferInstance) {
	b.Drain(40)
	b.Reset()
	bs := b.Bytes()
	if string(bs) != "" {
		panic(fmt.Sprintf("unexpected data: %s", string(bs)))
	}
}

func panicIfNotEqual(a, b any) {
	if !reflect.DeepEqual(a, b) {
		panic(fmt.Sprintf("expected %v, got %v", a, b))
	}
}

func panicIfLenMismatch(b api.BufferInstance, size int) {
	panicIfNotEqual(size, b.Len())
	panicIfNotEqual(len(b.Bytes()), b.Len())
}

func testLen(b api.BufferInstance) {
	b.Set([]byte("12"))
	panicIfLenMismatch(b, 2)
	b.SetString("123")
	panicIfLenMismatch(b, 3)

	b.Write([]byte("45"))
	panicIfLenMismatch(b, 5)
	b.WriteString("67")
	panicIfLenMismatch(b, 7)
	b.WriteByte('8')
	panicIfLenMismatch(b, 8)
	b.WriteUint16(90)
	panicIfLenMismatch(b, 10)
	b.WriteUint32(12)
	panicIfLenMismatch(b, 12)
	b.WriteUint64(12)
	panicIfLenMismatch(b, 14)

	b.Drain(2)
	panicIfLenMismatch(b, 12)
	b.Write([]byte("45"))
	panicIfLenMismatch(b, 14)

	b.Reset()
	panicIfLenMismatch(b, 0)

	b.Append([]byte("12"))
	panicIfLenMismatch(b, 2)
	b.Prepend([]byte("0"))
	panicIfLenMismatch(b, 3)
	b.AppendString("345")
	panicIfLenMismatch(b, 6)
	b.PrependString("00")
	panicIfLenMismatch(b, 8)
}

func (f *filter) DecodeData(buffer api.BufferInstance, endStream bool) api.StatusType {
	if endStream {
		return api.Continue
	}
	// run once

	query, _ := f.callbacks.GetProperty("request.query")
	switch query {
	case "Reset":
		testReset(buffer)
	case "ResetAfterDrain":
		testResetAfterDrain(buffer)
	case "Drain":
		testDrain(buffer)
	case "Len":
		testLen(buffer)
	default:
		panic(fmt.Sprintf("unknown case %s", query))
	}
	return api.Continue
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "filter.so",
    srcs = [
        "config.go",
        "filter.go",
    ],
    out = "filter.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/test/test_data/buffer",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/filters/http/source/go/pkg/http",
        "@com_github_cncf_xds_go//xds/type/v3:type",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
#include <cstdint>
#include <memory>

#include "envoy/config/core/v3/base.pb.h"

#include "source/common/buffer/buffer_impl.h"
#include "source/common/http/message_impl.h"
#include "source/common/stream_info/stream_info_impl.h"

#include "test/common/stats/stat_test_utility.h"
#include "test/mocks/api/mocks.h"
#include "test/mocks/http/mocks.h"
#include "test/mocks/network/mocks.h"
#include "test/mocks/server/factory_context.h"
#include "test/mocks/ssl/mocks.h"
#include "test/mocks/thread_local/mocks.h"
#include "test/mocks/upstream/cluster_manager.h"
#include "test/test_common/environment.h"
#include "test/test_common/logging.h"
#include "test/test_common/printers.h"
#include "test/test_common/utility.h"

#include "absl/strings/str_format.h"
#include "contrib/golang/filters/http/source/golang_filter.h"
#include "gmock/gmock.h"

using testing::_;
using testing::AtLeast;
using testing::InSequence;
using testing::Invoke;

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {
namespace {

class TestFilter : public Filter {
public:
  using Filter::Filter;
};

class GolangHttpFilterTest : public testing::Test {
public:
  GolangHttpFilterTest() {
    cluster_manager_.initializeThreadLocalClusters({"cluster"});

    // Avoid strict mock failures for the following calls. We want strict for other calls.
    EXPECT_CALL(decoder_callbacks_, addDecodedData(_, _))
        .Times(AtLeast(0))
        .WillRepeatedly(Invoke([this](Buffer::Instance& data, bool) {
          if (decoder_callbacks_.buffer_ == nullptr) {
            decoder_callbacks_.buffer_ = std::make_unique<Buffer::OwnedImpl>();
          }
          decoder_callbacks_.buffer_->move(data);
        }));

    EXPECT_CALL(decoder_callbacks_, activeSpan()).Times(AtLeast(0));
    EXPECT_CALL(decoder_callbacks_, decodingBuffer()).Times(AtLeast(0));
    EXPECT_CALL(decoder_callbacks_, route()).Times(AtLeast(0));

    EXPECT_CALL(encoder_callbacks_, addEncodedData(_, _))
        .Times(AtLeast(0))
        .WillRepeatedly(Invoke([this](Buffer::Instance& data, bool) {
          if (encoder_callbacks_.buffer_ == nullptr) {
            encoder_callbacks_.buffer_ = std::make_unique<Buffer::OwnedImpl>();
          }
          encoder_callbacks_.buffer_->move(data);
        }));
    EXPECT_CALL(encoder_callbacks_, activeSpan()).Times(AtLeast(0));
    EXPECT_CALL(encoder_callbacks_, encodingBuffer()).Times(AtLeast(0));
    EXPECT_CALL(decoder_callbacks_, streamInfo()).Times(testing::AnyNumber());
  }

  ~GolangHttpFilterTest() override {
    if (filter_ != nullptr) {
      filter_->onDestroy();
    }
  }

  void setup(const std::string& lib_id, const std::string& lib_path,
             const std::string& plugin_name) {
    const auto yaml_fmt = R"EOF(
    library_id: %s
    library_path: %s
    plugin_name: %s
    plugin_config:
      "@type": type.googleapis.com/xds.type.v3.TypedStruct
      type_url: typexx
      value:
          key: value
          int: 10
          invalid: "invalid"
    )EOF";

    auto yaml_string = absl::StrFormat(yaml_fmt, lib_id, lib_path, plugin_name);
    envoy::extensions::filters::http::golang::v3alpha::Config proto_config;
    TestUtility::loadFromYaml(yaml_string, proto_config);

    envoy::extensions::filters::http::golang::v3alpha::ConfigsPerRoute per_route_proto_config;
    setupDso(lib_id, lib_path, plugin_name);
    setupConfig(proto_config, per_route_proto_config, plugin_name);
    setupFilter(plugin_name);
  }

  std::string genSoPath(std::string name) {
    return TestEnvironment::substitute(
        "{{ test_rundir }}/contrib/golang/filters/http/test/test_data/" + name + "/filter.so");
  }

  void setupDso(std::string id, std::string path, std::string plugin_name) {
    Dso::DsoManager<Dso::HttpFilterDsoImpl>::load(id, path, plugin_name);
  }

  void setupConfig(
      envoy::extensions::filters::http::golang::v3alpha::Config& proto_config,
      envoy::extensions::filters::http::golang::v3alpha::ConfigsPerRoute& per_route_proto_config,
      std::string plugin_name) {
    // Setup filter config for Golang filter.
    config_ = std::make_shared<FilterConfig>(
        proto_config, Dso::DsoManager<Dso::HttpFilterDsoImpl>::getDsoByPluginName(plugin_name), "",
        context_);
    config_->newGoPluginConfig();
    // Setup per route config for Golang filter.
    per_route_config_ =
        std::make_shared<FilterConfigPerRoute>(per_route_proto_config, server_factory_context_);
  }

  void setupFilter(const std::string& plugin_name) {
    Event::SimulatedTimeSystem test_time;
    test_time.setSystemTime(std::chrono::microseconds(1583879145572237));

    filter_ = std::make_unique<TestFilter>(
        config_, Dso::DsoManager<Dso::HttpFilterDsoImpl>::getDsoByPluginName(plugin_name));
    filter_->setDecoderFilterCallbacks(decoder_callbacks_);
    filter_->setEncoderFilterCallbacks(encoder_callbacks_);
  }

  void setupMetadata(const std::string& yaml) {
    TestUtility::loadFromYaml(yaml, metadata_);
    ON_CALL(*decoder_callbacks_.route_, metadata()).WillByDefault(testing::ReturnRef(metadata_));
  }

  NiceMock<Server::Configuration::MockFactoryContext> context_;
  NiceMock<Server::Configuration::MockServerFactoryContext> server_factory_context_;
  NiceMock<ThreadLocal::MockInstance> tls_;
  NiceMock<Api::MockApi> api_;
  Upstream::MockClusterManager cluster_manager_;
  std::shared_ptr<FilterConfig> config_;
  std::shared_ptr<FilterConfigPerRoute> per_route_config_;
  std::unique_ptr<TestFilter> filter_;
  NiceMock<Http::MockStreamDecoderFilterCallbacks> decoder_callbacks_;
  NiceMock<Http::MockStreamEncoderFilterCallbacks> encoder_callbacks_;
  envoy::config::core::v3::Metadata metadata_;
  std::shared_ptr<NiceMock<Envoy::Ssl::MockConnectionInfo>> ssl_;
  NiceMock<Envoy::Network::MockConnection> connection_;
  NiceMock<Envoy::StreamInfo::MockStreamInfo> stream_info_;
  Tracing::MockSpan child_span_;
  Stats::TestUtil::TestStore stats_store_;

  const std::string PASSTHROUGH{"passthrough"};
  const std::string ROUTECONFIG{"routeconfig"};
};

// request that is headers only.
TEST_F(GolangHttpFilterTest, ScriptHeadersOnlyRequestHeadersOnly) {
  InSequence s;
  setup(PASSTHROUGH, genSoPath(PASSTHROUGH), PASSTHROUGH);

  Http::TestRequestHeaderMapImpl request_headers{{":path", "/"}};
  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->decodeHeaders(request_headers, true));
  EXPECT_EQ(0, stats_store_.counter("test.golang.errors").value());
}

// setHeader at wrong stage
TEST_F(GolangHttpFilterTest, SetHeaderAtWrongStage) {
  InSequence s;
  setup(PASSTHROUGH, genSoPath(PASSTHROUGH), PASSTHROUGH);

  EXPECT_EQ(CAPINotInGo, filter_->setHeader("foo", "bar", HeaderSet));
}

// invalid config for routeconfig filter
TEST_F(GolangHttpFilterTest, InvalidConfigForRouteConfigFilter) {
  InSequence s;
  EXPECT_THROW_WITH_REGEX(setup(ROUTECONFIG, genSoPath(ROUTECONFIG), ROUTECONFIG), EnvoyException,
                          "golang filter failed to parse plugin config");
}

} // namespace
} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "source/common/network/address_impl.h"

#include "test/extensions/filters/http/common/fuzz/http_filter_fuzzer.h"
#include "test/fuzz/fuzz_runner.h"
#include "test/mocks/http/mocks.h"
#include "test/mocks/network/mocks.h"
#include "test/mocks/runtime/mocks.h"
#include "test/mocks/server/factory_context.h"

#include "contrib/envoy/extensions/filters/http/golang/v3alpha/golang.pb.validate.h"
#include "contrib/golang/common/dso/test/mocks.h"
#include "contrib/golang/filters/http/source/golang_filter.h"
#include "contrib/golang/filters/http/test/golang_filter_fuzz.pb.validate.h"
#include "gmock/gmock.h"

using testing::Return;

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {
namespace {

class FuzzerMocks {
public:
  FuzzerMocks() : addr_(std::make_shared<Network::Address::PipeInstance>("/test/test.sock")) {

    ON_CALL(decoder_callbacks_, connection())
        .WillByDefault(Return(OptRef<const Network::Connection>{connection_}));
    connection_.stream_info_.downstream_connection_info_provider_->setRemoteAddress(addr_);
    connection_.stream_info_.downstream_connection_info_provider_->setLocalAddress(addr_);
  }

  NiceMock<Runtime::MockLoader> runtime_;
  NiceMock<Http::MockStreamDecoderFilterCallbacks> decoder_callbacks_;
  NiceMock<Http::MockStreamEncoderFilterCallbacks> encoder_callbacks_;
  Network::Address::InstanceConstSharedPtr addr_;
  NiceMock<Envoy::Network::MockConnection> connection_;
};

DEFINE_PROTO_FUZZER(const envoy::extensions::filters::http::golang::GolangFilterTestCase& input) {
  try {
    TestUtility::validate(input);
  } catch (const EnvoyException& e) {
    ENVOY_LOG_MISC(debug, "EnvoyException during validation: {}", e.what());
    return;
  }

  auto dso_lib = std::make_shared<Dso::MockHttpFilterDsoImpl>();

  // hard code the return config_id to 1 since the default 0 is invalid.
  ON_CALL(*dso_lib.get(), envoyGoFilterNewHttpPluginConfig(_)).WillByDefault(Return(1));
  ON_CALL(*dso_lib.get(), envoyGoFilterOnHttpHeader(_, _, _, _))
      .WillByDefault(Return(static_cast<uint64_t>(GolangStatus::Continue)));
  ON_CALL(*dso_lib.get(), envoyGoFilterOnHttpData(_, _, _, _))
      .WillByDefault(Return(static_cast<uint64_t>(GolangStatus::Continue)));
  ON_CALL(*dso_lib.get(), envoyGoFilterOnHttpLog(_, _))
      .WillByDefault(Invoke([&](httpRequest*, int) -> void {}));
  ON_CALL(*dso_lib.get(), envoyGoFilterOnHttpDestroy(_, _))
      .WillByDefault(Invoke([&](httpRequest* p0, int) -> void {
        // delete the filter->req_, make LeakSanitizer happy.
        auto req = reinterpret_cast<httpRequestInternal*>(p0);
        delete req;
      }));

  static FuzzerMocks mocks;

  // Filter config is typically considered trusted (coming from a trusted domain), use a const
  // config is good enough.
  const auto yaml = R"EOF(
    library_id: test
    library_path: test
    plugin_name: test
    )EOF";

  envoy::extensions::filters::http::golang::v3alpha::Config proto_config;
  TestUtility::loadFromYaml(yaml, proto_config);

  // Prepare filter.
  NiceMock<Server::Configuration::MockFactoryContext> context;
  FilterConfigSharedPtr config = std::make_shared<FilterConfig>(proto_config, dso_lib, "", context);
  std::unique_ptr<Filter> filter = std::make_unique<Filter>(config, dso_lib);
  filter->setDecoderFilterCallbacks(mocks.decoder_callbacks_);
  filter->setEncoderFilterCallbacks(mocks.encoder_callbacks_);

  Envoy::Extensions::HttpFilters::HttpFilterFuzzer fuzzer;
  fuzzer.runData(static_cast<Envoy::Http::StreamDecoderFilter*>(filter.get()),
                 input.request_data());
  filter->onDestroy();
}

} // namespace
} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_fuzz_test",
    "envoy_cc_test",
    "envoy_contrib_package",
    "envoy_proto_library",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "config_test",
    srcs = ["config_test.cc"],
    data = [
        "//contrib/golang/filters/http/test/test_data/passthrough:filter.so",
    ],
    deps = [
        "//contrib/golang/filters/http/source:config",
        "//test/mocks/server:factory_context_mocks",
        "//test/test_common:utility_lib",
    ],
)

envoy_cc_test(
    name = "golang_filter_test",
    srcs = ["golang_filter_test.cc"],
    data = [
        "//contrib/golang/filters/http/test/test_data/passthrough:filter.so",
        "//contrib/golang/filters/http/test/test_data/routeconfig:filter.so",
    ],
    deps = [
        "//contrib/golang/filters/http/source:golang_filter_lib",
        "//source/common/stream_info:stream_info_lib",
        "//test/mocks/api:api_mocks",
        "//test/mocks/http:http_mocks",
        "//test/mocks/network:network_mocks",
        "//test/mocks/server:factory_context_mocks",
        "//test/mocks/ssl:ssl_mocks",
        "//test/mocks/thread_local:thread_local_mocks",
        "//test/mocks/upstream:cluster_manager_mocks",
        "//test/test_common:logging_lib",
        "//test/test_common:utility_lib",
        "@envoy_api//envoy/config/core/v3:pkg_cc_proto",
    ],
)

envoy_cc_test(
    name = "golang_integration_test",
    srcs = ["golang_integration_test.cc"],
    data = [
        "//contrib/golang/filters/http/test/test_data/access_log:filter.so",
        "//contrib/golang/filters/http/test/test_data/action:filter.so",
        "//contrib/golang/filters/http/test/test_data/basic:filter.so",
        "//contrib/golang/filters/http/test/test_data/buffer:filter.so",
        "//contrib/golang/filters/http/test/test_data/echo:filter.so",
        "//contrib/golang/filters/http/test/test_data/metric:filter.so",
        "//contrib/golang/filters/http/test/test_data/passthrough:filter.so",
        "//contrib/golang/filters/http/test/test_data/property:filter.so",
        "//contrib/golang/filters/http/test/test_data/routeconfig:filter.so",
    ],
    deps = [
        "//contrib/golang/filters/http/source:config",
        "//source/exe:main_common_lib",
        "//source/extensions/filters/http/lua:config",
        "//test/config:v2_link_hacks",
        "//test/integration:http_integration_lib",
        "//test/test_common:utility_lib",
        "@envoy_api//envoy/extensions/filters/network/http_connection_manager/v3:pkg_cc_proto",
    ],
)

envoy_proto_library(
    name = "golang_filter_fuzz_proto",
    srcs = ["golang_filter_fuzz.proto"],
    deps = [
        "//test/fuzz:common_proto",
        "@envoy_api//contrib/envoy/extensions/filters/http/golang/v3alpha:pkg",
        "@envoy_api//envoy/config/core/v3:pkg",
    ],
)

envoy_cc_fuzz_test(
    name = "golang_filter_fuzz_test",
    srcs = ["golang_filter_fuzz_test.cc"],
    corpus = "golang_filter_corpus",
    deps = [
        ":golang_filter_fuzz_proto_cc_proto",
        "//contrib/golang/common/dso/test:dso_mocks",
        "//contrib/golang/filters/http/source:config",
        "//source/common/network:address_lib",
        "//test/extensions/filters/http/common/fuzz:http_filter_fuzzer_lib",
        "//test/mocks/http:http_mocks",
        "//test/mocks/network:network_mocks",
        "//test/mocks/runtime:runtime_mocks",
        "//test/mocks/server:factory_context_mocks",
        "@envoy_api//contrib/envoy/extensions/filters/http/golang/v3alpha:pkg_cc_proto",
    ],
)
config {
  library_id: "test"
  library_path: "test"
  plugin_name: "test"
}
request_data {
  headers {
    headers {
      key: ":host"
      value: "example.com"
    }
    headers {
      key: ":method"
      value: "POST"
    }
    headers {
      key: ":path"
      value: "/users"
    }
    headers {
      key: ":scheme"
      value: "https"
    }
  }
  http_body {
    data: "foobarbaz"
  }
}
config {
  library_id: "test"
  library_path: "test"
  plugin_name: "test"
}
request_data {
  http_body {
    data: "\177\023"
  }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package http

/*
// ref https://github.com/golang/go/issues/25832

#cgo linux LDFLAGS: -Wl,-unresolved-symbols=ignore-all
#cgo darwin LDFLAGS: -Wl,-undefined,dynamic_lookup

#include <stdlib.h>
#include <string.h>

#include "api.h"

*/
import "C"

import (
	"fmt"
	"runtime"
	"sync"
	"sync/atomic"
	"time"

	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	"github.com/envoyproxy/envoy/contrib/golang/common/go/utils"
)

var (
	configNumGenerator uint64
	configCache        = &sync.Map{} // uint64 -> config(interface{})
	// From get a cached merged_config_id_ in getMergedConfigId on the C++ side,
	// to get the merged config by the id on the Go side, 2 seconds should be long enough.
	delayDeleteTime = time.Second * 2 // 2s
)

func configFinalize(c *httpConfig) {
	c.Finalize()
}

func createConfig(c *C.httpConfig) *httpConfig {
	config := &httpConfig{
		config: c,
	}
	// NP: make sure httpConfig will be deleted.
	runtime.SetFinalizer(config, configFinalize)

	return config
}

//export envoyGoFilterNewHttpPluginConfig
func envoyGoFilterNewHttpPluginConfig(c *C.httpConfig) uint64 {
	buf := utils.BytesToSlice(uint64(c.config_ptr), uint64(c.config_len))
	var any anypb.Any
	proto.Unmarshal(buf, &any)

	configNum := atomic.AddUint64(&configNumGenerator, 1)

	name := utils.BytesToString(uint64(c.plugin_name_ptr), uint64(c.plugin_name_len))
	configParser := getHttpFilterConfigParser(name)
	if configParser != nil {
		var parsedConfig interface{}
		var err error
		if c.is_route_config == 1 {
			parsedConfig, err = configParser.Parse(&any, nil)
		} else {
			http_config := createConfig(c)
			parsedConfig, err = configParser.Parse(&any, http_config)
		}
		if err != nil {
			cAPI.HttpLog(api.Error, fmt.Sprintf("failed to parse golang plugin config: %v", err))
			return 0
		}
		configCache.Store(configNum, parsedConfig)
	} else {
		configCache.Store(configNum, &any)
	}

	return configNum
}

//export envoyGoFilterDestroyHttpPluginConfig
func envoyGoFilterDestroyHttpPluginConfig(id uint64, needDelay int) {
	if needDelay == 1 {
		// there is a concurrency race in the c++ side:
		// 1. when A envoy worker thread is using the cached merged_config_id_ and it will call into Go after some time.
		// 2. while B envoy worker thread may update the merged_config_id_ in getMergedConfigId, that will delete the id.
		// so, we delay deleting the id in the Go side.
		time.AfterFunc(delayDeleteTime, func() {
			configCache.Delete(id)
		})
	} else {
		// there is no race for non-merged config.
		configCache.Delete(id)
	}
}

//export envoyGoFilterMergeHttpPluginConfig
func envoyGoFilterMergeHttpPluginConfig(namePtr, nameLen, parentId, childId uint64) uint64 {
	name := utils.BytesToString(namePtr, nameLen)
	configParser := getHttpFilterConfigParser(name)

	if configParser != nil {
		parent, ok := configCache.Load(parentId)
		if !ok {
			panic(fmt.Sprintf("merge config: get parentId: %d config failed", parentId))
		}
		child, ok := configCache.Load(childId)
		if !ok {
			panic(fmt.Sprintf("merge config: get childId: %d config failed", childId))
		}

		new := configParser.Merge(parent, child)
		configNum := atomic.AddUint64(&configNumGenerator, 1)
		configCache.Store(configNum, new)
		return configNum

	} else {
		// child override parent by default.
		// It's safe to reuse the childId, since the merged config have the same life time with the child config.
		return childId
	}
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package http

import (
	"strconv"
	"strings"
	"sync"
	"unsafe"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

// panic error messages when C API return not ok
const (
	errRequestFinished = "request has been finished"
	errFilterDestroyed = "golang filter has been destroyed"
	errNotInGo         = "not proccessing Go"
	errInvalidPhase    = "invalid phase, maybe headers/buffer already continued"
)

// api.HeaderMap
type headerMapImpl struct {
	request     *httpRequest
	headers     map[string][]string
	headerNum   uint64
	headerBytes uint64
	mutex       sync.Mutex
}

type requestOrResponseHeaderMapImpl struct {
	headerMapImpl
}

func (h *requestOrResponseHeaderMapImpl) initHeaders() {
	if h.headers == nil {
		h.headers = cAPI.HttpCopyHeaders(unsafe.Pointer(h.request.req), h.headerNum, h.headerBytes)
	}
}

func (h *requestOrResponseHeaderMapImpl) GetRaw(key string) string {
	// GetRaw is case-sensitive
	return cAPI.HttpGetHeader(unsafe.Pointer(h.request.req), key)
}

func (h *requestOrResponseHeaderMapImpl) Get(key string) (string, bool) {
	key = strings.ToLower(key)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initHeaders()
	value, ok := h.headers[key]
	if !ok {
		return "", false
	}
	return value[0], ok
}

func (h *requestOrResponseHeaderMapImpl) Values(key string) []string {
	key = strings.ToLower(key)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initHeaders()
	value, ok := h.headers[key]
	if !ok {
		return nil
	}
	return value
}

func (h *requestOrResponseHeaderMapImpl) Set(key, value string) {
	key = strings.ToLower(key)
	// Get all header values first before setting a value, since the set operation may not take affects immediately
	// when it's invoked in a Go thread, instead, it will post a callback to run in the envoy worker thread.
	// Otherwise, we may get outdated values in a following Get call.
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initHeaders()
	if h.headers != nil {
		h.headers[key] = []string{value}
	}
	cAPI.HttpSetHeader(unsafe.Pointer(h.request.req), key, value, false)
}

func (h *requestOrResponseHeaderMapImpl) Add(key, value string) {
	key = strings.ToLower(key)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initHeaders()
	if h.headers != nil {
		if hdrs, found := h.headers[key]; found {
			h.headers[key] = append(hdrs, value)
		} else {
			h.headers[key] = []string{value}
		}
	}
	cAPI.HttpSetHeader(unsafe.Pointer(h.request.req), key, value, true)
}

func (h *requestOrResponseHeaderMapImpl) Del(key string) {
	key = strings.ToLower(key)
	// Get all header values first before removing a key, since the del operation may not take affects immediately
	// when it's invoked in a Go thread, instead, it will post a callback to run in the envoy worker thread.
	// Otherwise, we may get outdated values in a following Get call.
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initHeaders()
	delete(h.headers, key)
	cAPI.HttpRemoveHeader(unsafe.Pointer(h.request.req), key)
}

func (h *requestOrResponseHeaderMapImpl) Range(f func(key, value string) bool) {
	// To avoid dead lock, methods with lock(Get, Values, Set, Add, Del) should not be used in func f.
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initHeaders()
	for key, values := range h.headers {
		for _, value := range values {
			if !f(key, value) {
				return
			}
		}
	}
}

func (h *requestOrResponseHeaderMapImpl) RangeWithCopy(f func(key, value string) bool) {
	// There is no dead lock risk in RangeWithCopy, but copy may introduce performance cost.
	h.mutex.Lock()
	h.initHeaders()
	copied_headers := make(map[string][]string)
	for key, values := range h.headers {
		copied_headers[key] = values
	}
	h.mutex.Unlock()
	for key, values := range copied_headers {
		for _, value := range values {
			if !f(key, value) {
				return
			}
		}
	}
}

// api.RequestHeaderMap
type requestHeaderMapImpl struct {
	requestOrResponseHeaderMapImpl
}

var _ api.RequestHeaderMap = (*requestHeaderMapImpl)(nil)

func (h *requestHeaderMapImpl) Scheme() string {
	v, _ := h.Get(":scheme")
	return v
}

func (h *requestHeaderMapImpl) Method() string {
	v, _ := h.Get(":method")
	return v
}

func (h *requestHeaderMapImpl) Path() string {
	v, _ := h.Get(":path")
	return v
}

func (h *requestHeaderMapImpl) Host() string {
	v, _ := h.Get(":authority")
	return v
}

// api.ResponseHeaderMap
type responseHeaderMapImpl struct {
	requestOrResponseHeaderMapImpl
}

var _ api.ResponseHeaderMap = (*responseHeaderMapImpl)(nil)

func (h *responseHeaderMapImpl) Status() (int, bool) {
	if str, ok := h.Get(":status"); ok {
		v, _ := strconv.Atoi(str)
		return v, true
	}
	return 0, false
}

type requestOrResponseTrailerMapImpl struct {
	headerMapImpl
}

func (h *requestOrResponseTrailerMapImpl) initTrailers() {
	if h.headers == nil {
		h.headers = cAPI.HttpCopyTrailers(unsafe.Pointer(h.request.req), h.headerNum, h.headerBytes)
	}
}

func (h *requestOrResponseTrailerMapImpl) GetRaw(key string) string {
	return cAPI.HttpGetHeader(unsafe.Pointer(h.request.req), key)
}

func (h *requestOrResponseTrailerMapImpl) Get(key string) (string, bool) {
	key = strings.ToLower(key)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initTrailers()
	value, ok := h.headers[key]
	if !ok {
		return "", false
	}
	return value[0], ok
}

func (h *requestOrResponseTrailerMapImpl) Values(key string) []string {
	key = strings.ToLower(key)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initTrailers()
	value, ok := h.headers[key]
	if !ok {
		return nil
	}
	return value
}

func (h *requestOrResponseTrailerMapImpl) Set(key, value string) {
	key = strings.ToLower(key)
	// Get all header values first before setting a value, since the set operation may not take affects immediately
	// when it's invoked in a Go thread, instead, it will post a callback to run in the envoy worker thread.
	// Otherwise, we may get outdated values in a following Get call.
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initTrailers()
	if h.headers != nil {
		h.headers[key] = []string{value}
	}

	cAPI.HttpSetTrailer(unsafe.Pointer(h.request.req), key, value, false)
}

func (h *requestOrResponseTrailerMapImpl) Add(key, value string) {
	key = strings.ToLower(key)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initTrailers()
	if h.headers != nil {
		if trailers, found := h.headers[key]; found {
			h.headers[key] = append(trailers, value)
		} else {
			h.headers[key] = []string{value}
		}
	}
	cAPI.HttpSetTrailer(unsafe.Pointer(h.request.req), key, value, true)
}

func (h *requestOrResponseTrailerMapImpl) Del(key string) {
	key = strings.ToLower(key)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initTrailers()
	delete(h.headers, key)
	cAPI.HttpRemoveTrailer(unsafe.Pointer(h.request.req), key)
}

func (h *requestOrResponseTrailerMapImpl) Range(f func(key, value string) bool) {
	// To avoid dead lock, methods with lock(Get, Values, Set, Add, Del) should not be used in func f.
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.initTrailers()
	for key, values := range h.headers {
		for _, value := range values {
			if !f(key, value) {
				return
			}
		}
	}
}

func (h *requestOrResponseTrailerMapImpl) RangeWithCopy(f func(key, value string) bool) {
	// There is no dead lock risk in RangeWithCopy, but copy may introduce performance cost.
	h.mutex.Lock()
	h.initTrailers()
	copied_headers := make(map[string][]string)
	for key, values := range h.headers {
		copied_headers[key] = values
	}
	h.mutex.Unlock()
	for key, values := range copied_headers {
		for _, value := range values {
			if !f(key, value) {
				return
			}
		}
	}
}

// api.RequestTrailerMap
type requestTrailerMapImpl struct {
	requestOrResponseTrailerMapImpl
}

var _ api.RequestTrailerMap = (*requestTrailerMapImpl)(nil)

// api.ResponseTrailerMap
type responseTrailerMapImpl struct {
	requestOrResponseTrailerMapImpl
}

var _ api.ResponseTrailerMap = (*responseTrailerMapImpl)(nil)

// api.BufferInstance
type httpBuffer struct {
	request             *httpRequest
	envoyBufferInstance uint64
	length              uint64
	value               []byte
}

var _ api.BufferInstance = (*httpBuffer)(nil)

func (b *httpBuffer) Write(p []byte) (n int, err error) {
	cAPI.HttpSetBytesBufferHelper(unsafe.Pointer(b.request.req), b.envoyBufferInstance, p, api.AppendBuffer)
	n = len(p)
	b.length += uint64(n)
	return n, nil
}

func (b *httpBuffer) WriteString(s string) (n int, err error) {
	cAPI.HttpSetBufferHelper(unsafe.Pointer(b.request.req), b.envoyBufferInstance, s, api.AppendBuffer)
	n = len(s)
	b.length += uint64(n)
	return n, nil
}

func (b *httpBuffer) WriteByte(p byte) error {
	cAPI.HttpSetBufferHelper(unsafe.Pointer(b.request.req), b.envoyBufferInstance, string(p), api.AppendBuffer)
	b.length++
	return nil
}

func (b *httpBuffer) WriteUint16(p uint16) error {
	s := strconv.FormatUint(uint64(p), 10)
	_, err := b.WriteString(s)
	return err
}

func (b *httpBuffer) WriteUint32(p uint32) error {
	s := strconv.FormatUint(uint64(p), 10)
	_, err := b.WriteString(s)
	return err
}

func (b *httpBuffer) WriteUint64(p uint64) error {
	s := strconv.FormatUint(p, 10)
	_, err := b.WriteString(s)
	return err
}

func (b *httpBuffer) Bytes() []byte {
	if b.length == 0 {
		return nil
	}
	b.value = cAPI.HttpGetBuffer(unsafe.Pointer(b.request.req), b.envoyBufferInstance, b.length)
	return b.value
}

func (b *httpBuffer) Drain(offset int) {
	if offset <= 0 || b.length == 0 {
		return
	}

	size := uint64(offset)
	if size > b.length {
		size = b.length
	}

	cAPI.HttpDrainBuffer(unsafe.Pointer(b.request.req), b.envoyBufferInstance, size)

	b.length -= size
}

func (b *httpBuffer) Len() int {
	return int(b.length)
}

func (b *httpBuffer) Reset() {
	b.Drain(b.Len())
}

func (b *httpBuffer) String() string {
	if b.length == 0 {
		return ""
	}
	b.value = cAPI.HttpGetBuffer(unsafe.Pointer(b.request.req), b.envoyBufferInstance, b.length)
	return string(b.value)
}

func (b *httpBuffer) Append(data []byte) error {
	_, err := b.Write(data)
	return err
}

func (b *httpBuffer) Prepend(data []byte) error {
	cAPI.HttpSetBytesBufferHelper(unsafe.Pointer(b.request.req), b.envoyBufferInstance, data, api.PrependBuffer)
	b.length += uint64(len(data))
	return nil
}

func (b *httpBuffer) AppendString(s string) error {
	_, err := b.WriteString(s)
	return err
}

func (b *httpBuffer) PrependString(s string) error {
	cAPI.HttpSetBufferHelper(unsafe.Pointer(b.request.req), b.envoyBufferInstance, s, api.PrependBuffer)
	b.length += uint64(len(s))
	return nil
}

func (b *httpBuffer) Set(data []byte) error {
	cAPI.HttpSetBytesBufferHelper(unsafe.Pointer(b.request.req), b.envoyBufferInstance, data, api.SetBuffer)
	b.length = uint64(len(data))
	return nil
}

func (b *httpBuffer) SetString(s string) error {
	cAPI.HttpSetBufferHelper(unsafe.Pointer(b.request.req), b.envoyBufferInstance, s, api.SetBuffer)
	b.length = uint64(len(s))
	return nil
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package http

/*
// ref https://github.com/golang/go/issues/25832

#cgo CFLAGS: -I../../../../../../common/go/api -I../api
#cgo linux LDFLAGS: -Wl,-unresolved-symbols=ignore-all
#cgo darwin LDFLAGS: -Wl,-undefined,dynamic_lookup

#include <stdlib.h>
#include <string.h>

#include "api.h"

*/
import "C"
import (
	"errors"
	"runtime"
	"strings"
	"sync/atomic"
	"unsafe"

	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/structpb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
	_ "github.com/envoyproxy/envoy/contrib/golang/common/go/api_impl"
)

const (
	ValueRouteName               = 1
	ValueFilterChainName         = 2
	ValueProtocol                = 3
	ValueResponseCode            = 4
	ValueResponseCodeDetails     = 5
	ValueAttemptCount            = 6
	ValueDownstreamLocalAddress  = 7
	ValueDownstreamRemoteAddress = 8
	ValueUpstreamLocalAddress    = 9
	ValueUpstreamRemoteAddress   = 10
	ValueUpstreamClusterName     = 11
	ValueVirtualClusterName      = 12

	// NOTE: this is a trade-off value.
	// When the number of header is less this value, we could use the slice on the stack,
	// otherwise, we have to allocate a new slice on the heap,
	// and the slice on the stack will be wasted.
	// So, we choose a value that many requests' number of header is less than this value.
	// But also, it should not be too large, otherwise it might be waste stack memory.
	maxStackAllocedHeaderSize = 16
	maxStackAllocedSliceLen   = maxStackAllocedHeaderSize * 2
)

type httpCApiImpl struct{}

// When the status means unexpected stage when invoke C API,
// panic here and it will be recover in the Go entry function.
func handleCApiStatus(status C.CAPIStatus) {
	switch status {
	case C.CAPIFilterIsGone,
		C.CAPIFilterIsDestroy,
		C.CAPINotInGo,
		C.CAPIInvalidPhase:
		panic(capiStatusToStr(status))
	}
}

func capiStatusToStr(status C.CAPIStatus) string {
	switch status {
	case C.CAPIFilterIsGone:
		return errRequestFinished
	case C.CAPIFilterIsDestroy:
		return errFilterDestroyed
	case C.CAPINotInGo:
		return errNotInGo
	case C.CAPIInvalidPhase:
		return errInvalidPhase
	}

	return "unknown status"
}

func capiStatusToErr(status C.CAPIStatus) error {
	switch status {
	case C.CAPIValueNotFound:
		return api.ErrValueNotFound
	case C.CAPIInternalFailure:
		return api.ErrInternalFailure
	case C.CAPISerializationFailure:
		return api.ErrSerializationFailure
	}

	return errors.New("unknown status")
}

func (c *httpCApiImpl) HttpContinue(r unsafe.Pointer, status uint64) {
	res := C.envoyGoFilterHttpContinue(r, C.int(status))
	handleCApiStatus(res)
}

// Only may panic with errRequestFinished, errFilterDestroyed or errNotInGo,
// won't panic with errInvalidPhase and others, otherwise will cause deadloop, see RecoverPanic for the details.
func (c *httpCApiImpl) HttpSendLocalReply(r unsafe.Pointer, responseCode int, bodyText string, headers map[string][]string, grpcStatus int64, details string) {
	hLen := len(headers)
	strs := make([]*C.char, 0, hLen*2)
	defer func() {
		for _, s := range strs {
			C.free(unsafe.Pointer(s))
		}
	}()
	// TODO: use runtime.Pinner after go1.22 release for better performance.
	for k, h := range headers {
		for _, v := range h {
			keyStr := C.CString(k)
			valueStr := C.CString(v)
			strs = append(strs, keyStr, valueStr)
		}
	}
	res := C.envoyGoFilterHttpSendLocalReply(r, C.int(responseCode),
		unsafe.Pointer(unsafe.StringData(bodyText)), C.int(len(bodyText)),
		unsafe.Pointer(unsafe.SliceData(strs)), C.int(len(strs)),
		C.longlong(grpcStatus), unsafe.Pointer(unsafe.StringData(details)), C.int(len(details)))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpSendPanicReply(r unsafe.Pointer, details string) {
	res := C.envoyGoFilterHttpSendPanicReply(r, unsafe.Pointer(unsafe.StringData(details)), C.int(len(details)))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpGetHeader(r unsafe.Pointer, key string) string {
	var valueData C.uint64_t
	var valueLen C.int
	res := C.envoyGoFilterHttpGetHeader(r, unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)), &valueData, &valueLen)
	handleCApiStatus(res)
	return unsafe.String((*byte)(unsafe.Pointer(uintptr(valueData))), int(valueLen))
}

func (c *httpCApiImpl) HttpCopyHeaders(r unsafe.Pointer, num uint64, bytes uint64) map[string][]string {
	var strs []string
	if num <= maxStackAllocedHeaderSize {
		// NOTE: only const length slice may be allocated on stack.
		strs = make([]string, maxStackAllocedSliceLen)
	} else {
		// TODO: maybe we could use a memory pool for better performance,
		// since these go strings in strs, will be copied into the following map.
		strs = make([]string, num*2)
	}
	// NOTE: this buffer can not be reused safely,
	// since strings may refer to this buffer as string data, and string is const in go.
	// we have to make sure the all strings is not using before reusing,
	// but strings may be alive beyond the request life.
	buf := make([]byte, bytes)
	res := C.envoyGoFilterHttpCopyHeaders(r, unsafe.Pointer(unsafe.SliceData(strs)), unsafe.Pointer(unsafe.SliceData(buf)))
	handleCApiStatus(res)

	m := make(map[string][]string, num)
	for i := uint64(0); i < num*2; i += 2 {
		key := strs[i]
		value := strs[i+1]

		if v, found := m[key]; !found {
			m[key] = []string{value}
		} else {
			m[key] = append(v, value)
		}
	}
	runtime.KeepAlive(buf)
	return m
}

func (c *httpCApiImpl) HttpSetHeader(r unsafe.Pointer, key string, value string, add bool) {
	var act C.headerAction
	if add {
		act = C.HeaderAdd
	} else {
		act = C.HeaderSet
	}
	res := C.envoyGoFilterHttpSetHeaderHelper(r, unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)),
		unsafe.Pointer(unsafe.StringData(value)), C.int(len(value)), act)
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpRemoveHeader(r unsafe.Pointer, key string) {
	res := C.envoyGoFilterHttpRemoveHeader(r, unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpGetBuffer(r unsafe.Pointer, bufferPtr uint64, length uint64) []byte {
	buf := make([]byte, length)
	res := C.envoyGoFilterHttpGetBuffer(r, C.uint64_t(bufferPtr), unsafe.Pointer(unsafe.SliceData(buf)))
	handleCApiStatus(res)
	return unsafe.Slice(unsafe.SliceData(buf), length)
}

func (c *httpCApiImpl) HttpDrainBuffer(r unsafe.Pointer, bufferPtr uint64, length uint64) {
	res := C.envoyGoFilterHttpDrainBuffer(r, C.uint64_t(bufferPtr), C.uint64_t(length))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpSetBufferHelper(r unsafe.Pointer, bufferPtr uint64, value string, action api.BufferAction) {
	c.httpSetBufferHelper(r, bufferPtr, unsafe.Pointer(unsafe.StringData(value)), C.int(len(value)), action)
}

func (c *httpCApiImpl) HttpSetBytesBufferHelper(r unsafe.Pointer, bufferPtr uint64, value []byte, action api.BufferAction) {
	c.httpSetBufferHelper(r, bufferPtr, unsafe.Pointer(unsafe.SliceData(value)), C.int(len(value)), action)
}

func (c *httpCApiImpl) httpSetBufferHelper(r unsafe.Pointer, bufferPtr uint64, data unsafe.Pointer, length C.int, action api.BufferAction) {
	var act C.bufferAction
	switch action {
	case api.SetBuffer:
		act = C.Set
	case api.AppendBuffer:
		act = C.Append
	case api.PrependBuffer:
		act = C.Prepend
	}
	res := C.envoyGoFilterHttpSetBufferHelper(r, C.uint64_t(bufferPtr), data, length, act)
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpCopyTrailers(r unsafe.Pointer, num uint64, bytes uint64) map[string][]string {
	var strs []string
	if num <= maxStackAllocedHeaderSize {
		// NOTE: only const length slice may be allocated on stack.
		strs = make([]string, maxStackAllocedSliceLen)
	} else {
		// TODO: maybe we could use a memory pool for better performance,
		// since these go strings in strs, will be copied into the following map.
		strs = make([]string, num*2)
	}
	// NOTE: this buffer can not be reused safely,
	// since strings may refer to this buffer as string data, and string is const in go.
	// we have to make sure the all strings is not using before reusing,
	// but strings may be alive beyond the request life.
	buf := make([]byte, bytes)
	res := C.envoyGoFilterHttpCopyTrailers(r, unsafe.Pointer(unsafe.SliceData(strs)), unsafe.Pointer(unsafe.SliceData(buf)))
	handleCApiStatus(res)

	m := make(map[string][]string, num)
	for i := uint64(0); i < num*2; i += 2 {
		key := strs[i]
		value := strs[i+1]

		if v, found := m[key]; !found {
			m[key] = []string{value}
		} else {
			m[key] = append(v, value)
		}
	}
	runtime.KeepAlive(buf)
	return m
}

func (c *httpCApiImpl) HttpSetTrailer(r unsafe.Pointer, key string, value string, add bool) {
	var act C.headerAction
	if add {
		act = C.HeaderAdd
	} else {
		act = C.HeaderSet
	}
	res := C.envoyGoFilterHttpSetTrailer(r, unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)),
		unsafe.Pointer(unsafe.StringData(value)), C.int(len(value)), act)
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpRemoveTrailer(r unsafe.Pointer, key string) {
	res := C.envoyGoFilterHttpRemoveTrailer(r, unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpGetStringValue(rr unsafe.Pointer, id int) (string, bool) {
	r := (*httpRequest)(rr)
	// add a lock to protect filter->req_->strValue field in the Envoy side, from being writing concurrency,
	// since there might be multiple concurrency goroutines invoking this API on the Go side.
	r.mutex.Lock()
	defer r.mutex.Unlock()

	var valueData C.uint64_t
	var valueLen C.int
	res := C.envoyGoFilterHttpGetStringValue(unsafe.Pointer(r.req), C.int(id), &valueData, &valueLen)
	if res == C.CAPIValueNotFound {
		return "", false
	}
	handleCApiStatus(res)
	value := unsafe.String((*byte)(unsafe.Pointer(uintptr(valueData))), int(valueLen))
	// copy the memory from c to Go.
	return strings.Clone(value), true
}

func (c *httpCApiImpl) HttpGetIntegerValue(r unsafe.Pointer, id int) (uint64, bool) {
	var value C.uint64_t
	res := C.envoyGoFilterHttpGetIntegerValue(r, C.int(id), &value)
	if res == C.CAPIValueNotFound {
		return 0, false
	}
	handleCApiStatus(res)
	return uint64(value), true
}

func (c *httpCApiImpl) HttpGetDynamicMetadata(rr unsafe.Pointer, filterName string) map[string]interface{} {
	r := (*httpRequest)(rr)
	r.mutex.Lock()
	defer r.mutex.Unlock()
	r.sema.Add(1)

	var valueData C.uint64_t
	var valueLen C.int
	res := C.envoyGoFilterHttpGetDynamicMetadata(unsafe.Pointer(r.req),
		unsafe.Pointer(unsafe.StringData(filterName)), C.int(len(filterName)), &valueData, &valueLen)
	if res == C.CAPIYield {
		atomic.AddInt32(&r.waitingOnEnvoy, 1)
		r.sema.Wait()
	} else {
		r.sema.Done()
		handleCApiStatus(res)
	}
	buf := unsafe.Slice((*byte)(unsafe.Pointer(uintptr(valueData))), int(valueLen))
	// copy the memory from c to Go.
	var meta structpb.Struct
	proto.Unmarshal(buf, &meta)
	return meta.AsMap()
}

func (c *httpCApiImpl) HttpSetDynamicMetadata(r unsafe.Pointer, filterName string, key string, value interface{}) {
	v, err := structpb.NewValue(value)
	if err != nil {
		panic(err)
	}
	buf, err := proto.Marshal(v)
	if err != nil {
		panic(err)
	}
	res := C.envoyGoFilterHttpSetDynamicMetadata(r,
		unsafe.Pointer(unsafe.StringData(filterName)), C.int(len(filterName)),
		unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)),
		unsafe.Pointer(unsafe.SliceData(buf)), C.int(len(buf)))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpLog(level api.LogType, message string) {
	C.envoyGoFilterLog(C.uint32_t(level), unsafe.Pointer(unsafe.StringData(message)), C.int(len(message)))
}

func (c *httpCApiImpl) HttpLogLevel() api.LogType {
	return api.GetLogLevel()
}

func (c *httpCApiImpl) HttpFinalize(r unsafe.Pointer, reason int) {
	C.envoyGoFilterHttpFinalize(r, C.int(reason))
}

func (c *httpCApiImpl) HttpConfigFinalize(cfg unsafe.Pointer) {
	C.envoyGoConfigHttpFinalize(cfg)
}

var cAPI api.HttpCAPI = &httpCApiImpl{}

// SetHttpCAPI for mock cAPI
func SetHttpCAPI(api api.HttpCAPI) {
	cAPI = api
}

func (c *httpCApiImpl) HttpSetStringFilterState(r unsafe.Pointer, key string, value string, stateType api.StateType, lifeSpan api.LifeSpan, streamSharing api.StreamSharing) {
	res := C.envoyGoFilterHttpSetStringFilterState(r,
		unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)),
		unsafe.Pointer(unsafe.StringData(value)), C.int(len(value)),
		C.int(stateType), C.int(lifeSpan), C.int(streamSharing))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpGetStringFilterState(rr unsafe.Pointer, key string) string {
	r := (*httpRequest)(rr)
	var valueData C.uint64_t
	var valueLen C.int
	r.mutex.Lock()
	defer r.mutex.Unlock()
	r.sema.Add(1)
	res := C.envoyGoFilterHttpGetStringFilterState(unsafe.Pointer(r.req),
		unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)), &valueData, &valueLen)
	if res == C.CAPIYield {
		atomic.AddInt32(&r.waitingOnEnvoy, 1)
		r.sema.Wait()
	} else {
		r.sema.Done()
		handleCApiStatus(res)
	}

	value := unsafe.String((*byte)(unsafe.Pointer(uintptr(valueData))), int(valueLen))
	return strings.Clone(value)
}

func (c *httpCApiImpl) HttpGetStringProperty(rr unsafe.Pointer, key string) (string, error) {
	r := (*httpRequest)(rr)
	var valueData C.uint64_t
	var valueLen C.int
	var rc C.int
	r.mutex.Lock()
	defer r.mutex.Unlock()
	r.sema.Add(1)
	res := C.envoyGoFilterHttpGetStringProperty(unsafe.Pointer(r.req),
		unsafe.Pointer(unsafe.StringData(key)), C.int(len(key)), &valueData, &valueLen, &rc)
	if res == C.CAPIYield {
		atomic.AddInt32(&r.waitingOnEnvoy, 1)
		r.sema.Wait()
		res = C.CAPIStatus(rc)
	} else {
		r.sema.Done()
		handleCApiStatus(res)
	}

	if res == C.CAPIOK {
		value := unsafe.String((*byte)(unsafe.Pointer(uintptr(valueData))), int(valueLen))
		return strings.Clone(value), nil
	}

	return "", capiStatusToErr(res)
}

func (c *httpCApiImpl) HttpDefineMetric(cfg unsafe.Pointer, metricType api.MetricType, name string) uint32 {
	var value C.uint32_t
	res := C.envoyGoFilterHttpDefineMetric(cfg, C.uint32_t(metricType), unsafe.Pointer(unsafe.StringData(name)), C.int(len(name)), &value)
	handleCApiStatus(res)
	return uint32(value)
}

func (c *httpCApiImpl) HttpIncrementMetric(cc unsafe.Pointer, metricId uint32, offset int64) {
	cfg := (*httpConfig)(cc)
	res := C.envoyGoFilterHttpIncrementMetric(unsafe.Pointer(cfg.config), C.uint32_t(metricId), C.int64_t(offset))
	handleCApiStatus(res)
}

func (c *httpCApiImpl) HttpGetMetric(cc unsafe.Pointer, metricId uint32) uint64 {
	cfg := (*httpConfig)(cc)
	var value C.uint64_t
	res := C.envoyGoFilterHttpGetMetric(unsafe.Pointer(cfg.config), C.uint32_t(metricId), &value)
	handleCApiStatus(res)
	return uint64(value)
}

func (c *httpCApiImpl) HttpRecordMetric(cc unsafe.Pointer, metricId uint32, value uint64) {
	cfg := (*httpConfig)(cc)
	res := C.envoyGoFilterHttpRecordMetric(unsafe.Pointer(cfg.config), C.uint32_t(metricId), C.uint64_t(value))
	handleCApiStatus(res)
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package http

/*
// ref https://github.com/golang/go/issues/25832

#cgo CFLAGS: -I../../../../../../common/go/api -I../api
#cgo linux LDFLAGS: -Wl,-unresolved-symbols=ignore-all
#cgo darwin LDFLAGS: -Wl,-undefined,dynamic_lookup

#include <stdlib.h>
#include <string.h>

#include "api.h"

*/
import "C"
import (
	"fmt"
	"runtime"
	"sync"
	"unsafe"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

const (
	HTTP10 = "HTTP/1.0"
	HTTP11 = "HTTP/1.1"
	HTTP20 = "HTTP/2.0"
	HTTP30 = "HTTP/3.0"
)

var protocolsIdToName = map[uint64]string{
	0: HTTP10,
	1: HTTP11,
	2: HTTP20,
	3: HTTP30,
}

type panicInfo struct {
	paniced bool
	details string
}
type httpRequest struct {
	req            *C.httpRequest
	httpFilter     api.StreamFilter
	pInfo          panicInfo
	sema           sync.WaitGroup
	waitingOnEnvoy int32
	mutex          sync.Mutex
}

func (r *httpRequest) pluginName() string {
	return C.GoStringN(r.req.plugin_name.data, C.int(r.req.plugin_name.len))
}

func (r *httpRequest) sendPanicReply(details string) {
	defer r.RecoverPanic()
	cAPI.HttpSendPanicReply(unsafe.Pointer(r.req), details)
}

func (r *httpRequest) RecoverPanic() {
	if e := recover(); e != nil {
		const size = 64 << 10
		buf := make([]byte, size)
		buf = buf[:runtime.Stack(buf, false)]
		api.LogErrorf("http: panic serving: %v\n%s", e, buf)

		switch e {
		case errRequestFinished, errFilterDestroyed:
			// do nothing

		case errNotInGo:
			// We can not send local reply now, since not in go now,
			// will delay to the next time entering Go.
			r.pInfo = panicInfo{
				paniced: true,
				details: fmt.Sprint(e),
			}

		default:
			// The following safeReplyPanic should only may get errRequestFinished,
			// errFilterDestroyed or errNotInGo, won't hit this branch, so, won't dead loop here.

			// errInvalidPhase, or other panic, not from not-ok C return status.
			// It's safe to try send a local reply with 500 status.
			r.sendPanicReply(fmt.Sprint(e))
		}
	}
}

func (r *httpRequest) Continue(status api.StatusType) {
	if status == api.LocalReply {
		fmt.Printf("warning: LocalReply status is useless after sendLocalReply, ignoring")
		return
	}
	cAPI.HttpContinue(unsafe.Pointer(r.req), uint64(status))
}

func (r *httpRequest) SendLocalReply(responseCode int, bodyText string, headers map[string][]string, grpcStatus int64, details string) {
	cAPI.HttpSendLocalReply(unsafe.Pointer(r.req), responseCode, bodyText, headers, grpcStatus, details)
}

func (r *httpRequest) Log(level api.LogType, message string) {
	// TODO performance optimization points:
	// Add a new goroutine to write logs asynchronously and avoid frequent cgo calls
	cAPI.HttpLog(level, fmt.Sprintf("[http][%v] %v", r.pluginName(), message))
	// The default log format is:
	// [2023-08-09 03:04:16.179][1390][error][golang] [contrib/golang/common/log/cgo.cc:24] [http][plugin_name] msg
}

func (r *httpRequest) LogLevel() api.LogType {
	return cAPI.HttpLogLevel()
}

func (r *httpRequest) GetProperty(key string) (string, error) {
	return cAPI.HttpGetStringProperty(unsafe.Pointer(r), key)
}

func (r *httpRequest) StreamInfo() api.StreamInfo {
	return &streamInfo{
		request: r,
	}
}

func (r *httpRequest) Finalize(reason int) {
	cAPI.HttpFinalize(unsafe.Pointer(r.req), reason)
}

type streamInfo struct {
	request *httpRequest
}

func (s *streamInfo) GetRouteName() string {
	name, _ := cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueRouteName)
	return name
}

func (s *streamInfo) FilterChainName() string {
	name, _ := cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueFilterChainName)
	return name
}

func (s *streamInfo) Protocol() (string, bool) {
	if protocol, ok := cAPI.HttpGetIntegerValue(unsafe.Pointer(s.request.req), ValueProtocol); ok {
		if name, ok := protocolsIdToName[protocol]; ok {
			return name, true
		}
		panic(fmt.Sprintf("invalid protocol id: %d", protocol))
	}
	return "", false
}

func (s *streamInfo) ResponseCode() (uint32, bool) {
	if code, ok := cAPI.HttpGetIntegerValue(unsafe.Pointer(s.request.req), ValueResponseCode); ok {
		return uint32(code), true
	}
	return 0, false
}

func (s *streamInfo) ResponseCodeDetails() (string, bool) {
	return cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueResponseCodeDetails)
}

func (s *streamInfo) AttemptCount() uint32 {
	count, _ := cAPI.HttpGetIntegerValue(unsafe.Pointer(s.request.req), ValueAttemptCount)
	return uint32(count)
}

type dynamicMetadata struct {
	request *httpRequest
}

func (s *streamInfo) DynamicMetadata() api.DynamicMetadata {
	return &dynamicMetadata{
		request: s.request,
	}
}

func (d *dynamicMetadata) Get(filterName string) map[string]interface{} {
	return cAPI.HttpGetDynamicMetadata(unsafe.Pointer(d.request), filterName)
}

func (d *dynamicMetadata) Set(filterName string, key string, value interface{}) {
	cAPI.HttpSetDynamicMetadata(unsafe.Pointer(d.request.req), filterName, key, value)
}

func (s *streamInfo) DownstreamLocalAddress() string {
	address, _ := cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueDownstreamLocalAddress)
	return address
}

func (s *streamInfo) DownstreamRemoteAddress() string {
	address, _ := cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueDownstreamRemoteAddress)
	return address
}

// UpstreamLocalAddress return the upstream local address.
func (s *streamInfo) UpstreamLocalAddress() (string, bool) {
	return cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueUpstreamLocalAddress)
}

// UpstreamRemoteAddress return the upstream remote address.
func (s *streamInfo) UpstreamRemoteAddress() (string, bool) {
	return cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueUpstreamRemoteAddress)
}

func (s *streamInfo) UpstreamClusterName() (string, bool) {
	return cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueUpstreamClusterName)
}

func (s *streamInfo) VirtualClusterName() (string, bool) {
	return cAPI.HttpGetStringValue(unsafe.Pointer(s.request), ValueVirtualClusterName)
}

type filterState struct {
	request *httpRequest
}

func (s *streamInfo) FilterState() api.FilterState {
	return &filterState{
		request: s.request,
	}
}

func (f *filterState) SetString(key, value string, stateType api.StateType, lifeSpan api.LifeSpan, streamSharing api.StreamSharing) {
	cAPI.HttpSetStringFilterState(unsafe.Pointer(f.request.req), key, value, stateType, lifeSpan, streamSharing)
}

func (f *filterState) GetString(key string) string {
	return cAPI.HttpGetStringFilterState(unsafe.Pointer(f.request), key)
}

type httpConfig struct {
	config *C.httpConfig
}

func (c *httpConfig) DefineCounterMetric(name string) api.CounterMetric {
	id := cAPI.HttpDefineMetric(unsafe.Pointer(c.config), api.Counter, name)
	return &counterMetric{
		config:   c,
		metricId: id,
	}
}

func (c *httpConfig) DefineGaugeMetric(name string) api.GaugeMetric {
	id := cAPI.HttpDefineMetric(unsafe.Pointer(c.config), api.Gauge, name)
	return &gaugeMetric{
		config:   c,
		metricId: id,
	}
}

func (c *httpConfig) Finalize() {
	cAPI.HttpConfigFinalize(unsafe.Pointer(c.config))
}

type counterMetric struct {
	config   *httpConfig
	metricId uint32
}

func (m *counterMetric) Increment(offset int64) {
	cAPI.HttpIncrementMetric(unsafe.Pointer(m.config), m.metricId, offset)
}

func (m *counterMetric) Get() uint64 {
	return cAPI.HttpGetMetric(unsafe.Pointer(m.config), m.metricId)
}

func (m *counterMetric) Record(value uint64) {
	cAPI.HttpRecordMetric(unsafe.Pointer(m.config), m.metricId, value)
}

type gaugeMetric struct {
	config   *httpConfig
	metricId uint32
}

func (m *gaugeMetric) Increment(offset int64) {
	cAPI.HttpIncrementMetric(unsafe.Pointer(m.config), m.metricId, offset)
}

func (m *gaugeMetric) Get() uint64 {
	return cAPI.HttpGetMetric(unsafe.Pointer(m.config), m.metricId)
}

func (m *gaugeMetric) Record(value uint64) {
	cAPI.HttpRecordMetric(unsafe.Pointer(m.config), m.metricId, value)
}
load("@io_bazel_rules_go//go:def.bzl", "go_library")

licenses(["notice"])  # Apache 2

go_library(
    name = "http",
    srcs = [
        "api.h",
        "capi_impl.go",
        "config.go",
        "filter.go",
        "filtermanager.go",
        "passthrough.go",
        "shim.go",
        "type.go",
    ],
    cgo = True,
    clinkopts = select({
        "@io_bazel_rules_go//go/platform:android": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "@io_bazel_rules_go//go/platform:darwin": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:ios": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:linux": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "//conditions:default": [],
    }),
    importpath = "github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
        "//contrib/golang/common/go/api_impl",
        "//contrib/golang/common/go/utils",
        "@org_golang_google_protobuf//proto",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package http

/*
// ref https://github.com/golang/go/issues/25832

#cgo CFLAGS: -I../../../../../../common/go/api -I../api
#cgo linux LDFLAGS: -Wl,-unresolved-symbols=ignore-all
#cgo darwin LDFLAGS: -Wl,-undefined,dynamic_lookup

#include <stdlib.h>
#include <string.h>

#include "api.h"

*/
import "C"

import (
	"errors"
	"fmt"
	"runtime"
	"sync"
	"sync/atomic"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

var ErrDupRequestKey = errors.New("dup request key")

var Requests = &requestMap{}

type requestMap struct {
	m sync.Map // *C.httpRequest -> *httpRequest
}

func (f *requestMap) StoreReq(key *C.httpRequest, req *httpRequest) error {
	if _, loaded := f.m.LoadOrStore(key, req); loaded {
		return ErrDupRequestKey
	}
	return nil
}

func (f *requestMap) GetReq(key *C.httpRequest) *httpRequest {
	if v, ok := f.m.Load(key); ok {
		return v.(*httpRequest)
	}
	return nil
}

func (f *requestMap) DeleteReq(key *C.httpRequest) {
	f.m.Delete(key)
}

func (f *requestMap) Clear() {
	f.m.Range(func(key, _ interface{}) bool {
		f.m.Delete(key)
		return true
	})
}

func requestFinalize(r *httpRequest) {
	r.Finalize(api.NormalFinalize)
}

func createRequest(r *C.httpRequest) *httpRequest {
	req := &httpRequest{
		req: r,
	}
	// NP: make sure filter will be deleted.
	runtime.SetFinalizer(req, requestFinalize)

	err := Requests.StoreReq(r, req)
	if err != nil {
		panic(fmt.Sprintf("createRequest failed, err: %s", err.Error()))
	}

	configId := uint64(r.configId)
	filterFactory := getOrCreateHttpFilterFactory(req.pluginName(), configId)
	f := filterFactory(req)
	req.httpFilter = f

	return req
}

func getRequest(r *C.httpRequest) *httpRequest {
	return Requests.GetReq(r)
}

//export envoyGoFilterOnHttpHeader
func envoyGoFilterOnHttpHeader(r *C.httpRequest, endStream, headerNum, headerBytes uint64) uint64 {
	var req *httpRequest
	phase := api.EnvoyRequestPhase(r.phase)
	// early SendLocalReply or OnLogDownstreamStart may run before the header handling
	req = getRequest(r)
	if req == nil {
		req = createRequest(r)
	}

	if req.pInfo.paniced {
		// goroutine panic in the previous state that could not sendLocalReply, delay terminating the request here,
		// to prevent error from spreading.
		req.sendPanicReply(req.pInfo.details)
		return uint64(api.LocalReply)
	}
	defer req.RecoverPanic()
	f := req.httpFilter

	var status api.StatusType
	switch phase {
	case api.DecodeHeaderPhase:
		header := &requestHeaderMapImpl{
			requestOrResponseHeaderMapImpl{
				headerMapImpl{
					request:     req,
					headerNum:   headerNum,
					headerBytes: headerBytes,
				},
			},
		}
		status = f.DecodeHeaders(header, endStream == 1)
	case api.DecodeTrailerPhase:
		header := &requestTrailerMapImpl{
			requestOrResponseTrailerMapImpl{
				headerMapImpl{
					request:     req,
					headerNum:   headerNum,
					headerBytes: headerBytes,
				},
			},
		}
		status = f.DecodeTrailers(header)
	case api.EncodeHeaderPhase:
		header := &responseHeaderMapImpl{
			requestOrResponseHeaderMapImpl{
				headerMapImpl{
					request:     req,
					headerNum:   headerNum,
					headerBytes: headerBytes,
				},
			},
		}
		status = f.EncodeHeaders(header, endStream == 1)
	case api.EncodeTrailerPhase:
		header := &responseTrailerMapImpl{
			requestOrResponseTrailerMapImpl{
				headerMapImpl{
					request:     req,
					headerNum:   headerNum,
					headerBytes: headerBytes,
				},
			},
		}
		status = f.EncodeTrailers(header)
	}

	if endStream == 1 && (status == api.StopAndBuffer || status == api.StopAndBufferWatermark) {
		panic("received wait data status when there is no data, please fix the returned status")
	}

	return uint64(status)
}

//export envoyGoFilterOnHttpData
func envoyGoFilterOnHttpData(r *C.httpRequest, endStream, buffer, length uint64) uint64 {
	req := getRequest(r)
	if req.pInfo.paniced {
		// goroutine panic in the previous state that could not sendLocalReply, delay terminating the request here,
		// to prevent error from spreading.
		req.sendPanicReply(req.pInfo.details)
		return uint64(api.LocalReply)
	}
	defer req.RecoverPanic()

	f := req.httpFilter
	isDecode := api.EnvoyRequestPhase(r.phase) == api.DecodeDataPhase

	buf := &httpBuffer{
		request:             req,
		envoyBufferInstance: buffer,
		length:              length,
	}

	var status api.StatusType
	if isDecode {
		status = f.DecodeData(buf, endStream == 1)
	} else {
		status = f.EncodeData(buf, endStream == 1)
	}
	return uint64(status)
}

//export envoyGoFilterOnHttpLog
func envoyGoFilterOnHttpLog(r *C.httpRequest, logType uint64) {
	req := getRequest(r)
	if req == nil {
		req = createRequest(r)
	}

	defer req.RecoverPanic()
	if atomic.CompareAndSwapInt32(&req.waitingOnEnvoy, 1, 0) {
		req.sema.Done()
	}

	v := api.AccessLogType(logType)

	f := req.httpFilter
	switch v {
	case api.AccessLogDownstreamStart:
		f.OnLogDownstreamStart()
	case api.AccessLogDownstreamPeriodic:
		f.OnLogDownstreamPeriodic()
	case api.AccessLogDownstreamEnd:
		f.OnLog()
	default:
		api.LogErrorf("access log type %d is not supported yet", logType)
	}
}

//export envoyGoFilterOnHttpDestroy
func envoyGoFilterOnHttpDestroy(r *C.httpRequest, reason uint64) {
	req := getRequest(r)
	// do nothing even when req.panic is true, since filter is already destroying.
	defer req.RecoverPanic()
	if atomic.CompareAndSwapInt32(&req.waitingOnEnvoy, 1, 0) {
		req.sema.Done()
	}

	v := api.DestroyReason(reason)

	f := req.httpFilter
	f.OnDestroy(v)

	// Break circular references between httpRequest and StreamFilter,
	// since Finalizers don't work with circular references,
	// otherwise, it will leads to memory leaking.
	req.httpFilter = nil

	Requests.DeleteReq(r)
}

//export envoyGoRequestSemaDec
func envoyGoRequestSemaDec(r *C.httpRequest) {
	req := getRequest(r)
	defer req.RecoverPanic()
	if atomic.CompareAndSwapInt32(&req.waitingOnEnvoy, 1, 0) {
		req.sema.Done()
	}
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package http

import (
	"fmt"
	"sync"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

var httpFilterConfigFactoryAndParser = sync.Map{}

type filterConfigFactoryAndParser struct {
	configFactory api.StreamFilterConfigFactory
	configParser  api.StreamFilterConfigParser
}

// Register config factory and config parser for the specified plugin.
// The "factory" parameter is required, should not be nil,
// and the "parser" parameter is optional, could be nil.
func RegisterHttpFilterConfigFactoryAndParser(name string, factory api.StreamFilterConfigFactory, parser api.StreamFilterConfigParser) {
	if factory == nil {
		panic("config factory should not be nil")
	}
	httpFilterConfigFactoryAndParser.Store(name, &filterConfigFactoryAndParser{factory, parser})
}

func getOrCreateHttpFilterFactory(name string, configId uint64) api.StreamFilterFactory {
	config, ok := configCache.Load(configId)
	if !ok {
		panic(fmt.Sprintf("config not found, plugin: %s, configId: %d", name, configId))
	}

	if v, ok := httpFilterConfigFactoryAndParser.Load(name); ok {
		return (v.(*filterConfigFactoryAndParser)).configFactory(config)
	}

	api.LogErrorf("plugin %s not found, pass through by default", name)

	// pass through by default
	return PassThroughFactory(config)
}

func getHttpFilterConfigParser(name string) api.StreamFilterConfigParser {
	if v, ok := httpFilterConfigFactoryAndParser.Load(name); ok {
		return (v.(*filterConfigFactoryAndParser)).configParser
	}
	return nil
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package http

import (
	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type passThroughFilter struct {
	api.PassThroughStreamFilter
	callbacks api.FilterCallbackHandler
}

func PassThroughFactory(interface{}) api.StreamFilterFactory {
	return func(callbacks api.FilterCallbackHandler) api.StreamFilter {
		return &passThroughFilter{
			callbacks: callbacks,
		}
	}
}
#pragma once

#include "envoy/server/lifecycle_notifier.h"

#include "source/extensions/filters/http/common/factory_base.h"

#include "contrib/envoy/extensions/filters/http/golang/v3alpha/golang.pb.h"
#include "contrib/envoy/extensions/filters/http/golang/v3alpha/golang.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

constexpr char CanonicalName[] = "envoy.filters.http.golang";

/**
 * Config registration for the golang extentions  filter. @see
 * NamedHttpFilterConfigFactory.
 */
class GolangFilterConfig : public Common::FactoryBase<
                               envoy::extensions::filters::http::golang::v3alpha::Config,
                               envoy::extensions::filters::http::golang::v3alpha::ConfigsPerRoute> {
public:
  GolangFilterConfig() : FactoryBase(CanonicalName) {}

private:
  Server::ServerLifecycleNotifier::HandlePtr handler_{};

  Http::FilterFactoryCb createFilterFactoryFromProtoTyped(
      const envoy::extensions::filters::http::golang::v3alpha::Config& proto_config,
      const std::string& stats_prefix,
      Server::Configuration::FactoryContext& factory_context) override;

  Router::RouteSpecificFilterConfigConstSharedPtr createRouteSpecificFilterConfigTyped(
      const envoy::extensions::filters::http::golang::v3alpha::ConfigsPerRoute& proto_config,
      Server::Configuration::ServerFactoryContext& context,
      ProtobufMessage::ValidationVisitor& validator) override;
};

} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <atomic>

#include "envoy/access_log/access_log.h"
#include "envoy/http/filter.h"
#include "envoy/upstream/cluster_manager.h"

#include "source/common/buffer/watermark_buffer.h"
#include "source/common/common/linked_object.h"
#include "source/common/common/thread.h"
#include "source/common/grpc/context_impl.h"
#include "source/common/http/utility.h"
#include "source/extensions/filters/common/expr/evaluator.h"

#include "contrib/envoy/extensions/filters/http/golang/v3alpha/golang.pb.h"
#include "contrib/golang/common/dso/dso.h"
#include "contrib/golang/filters/http/source/processor_state.h"
#include "contrib/golang/filters/http/source/stats.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

enum class MetricType {
  Counter = 0,
  Gauge = 1,
  Histogram = 2,
  Max = 2,
};

class MetricStore {
public:
  MetricStore(Stats::ScopeSharedPtr scope) : scope_(scope) {}

  static constexpr uint32_t kMetricTypeMask = 0x3;
  static constexpr uint32_t kMetricIdIncrement = 0x4;

  uint32_t nextCounterMetricId() { return next_counter_metric_id_ += kMetricIdIncrement; }
  uint32_t nextGaugeMetricId() { return next_gauge_metric_id_ += kMetricIdIncrement; }
  uint32_t nextHistogramMetricId() { return next_histogram_metric_id_ += kMetricIdIncrement; }

  absl::flat_hash_map<uint32_t, Stats::Counter*> counters_;
  absl::flat_hash_map<uint32_t, Stats::Gauge*> gauges_;
  absl::flat_hash_map<uint32_t, Stats::Histogram*> histograms_;

  Stats::ScopeSharedPtr scope_;

private:
  uint32_t next_counter_metric_id_ = static_cast<uint32_t>(MetricType::Counter);
  uint32_t next_gauge_metric_id_ = static_cast<uint32_t>(MetricType::Gauge);
  uint32_t next_histogram_metric_id_ = static_cast<uint32_t>(MetricType::Histogram);
};

using MetricStoreSharedPtr = std::shared_ptr<MetricStore>;

struct httpConfigInternal;

/**
 * Configuration for the HTTP golang extension filter.
 */
class FilterConfig : public std::enable_shared_from_this<FilterConfig>,
                     Logger::Loggable<Logger::Id::http> {
public:
  FilterConfig(const envoy::extensions::filters::http::golang::v3alpha::Config& proto_config,
               Dso::HttpFilterDsoPtr dso_lib, const std::string& stats_prefix,
               Server::Configuration::FactoryContext& context);
  ~FilterConfig();

  const std::string& soId() const { return so_id_; }
  const std::string& soPath() const { return so_path_; }
  const std::string& pluginName() const { return plugin_name_; }
  uint64_t getConfigId();
  GolangFilterStats& stats() { return stats_; }

  void newGoPluginConfig();
  CAPIStatus defineMetric(uint32_t metric_type, absl::string_view name, uint32_t* metric_id);
  CAPIStatus incrementMetric(uint32_t metric_id, int64_t offset);
  CAPIStatus getMetric(uint32_t metric_id, uint64_t* value);
  CAPIStatus recordMetric(uint32_t metric_id, uint64_t value);

private:
  const std::string plugin_name_;
  const std::string so_id_;
  const std::string so_path_;
  const ProtobufWkt::Any plugin_config_;

  GolangFilterStats stats_;

  Dso::HttpFilterDsoPtr dso_lib_;
  uint64_t config_id_{0};
  // TODO(StarryVae): use rwlock.
  Thread::MutexBasicLockable mutex_{};
  MetricStoreSharedPtr metric_store_ ABSL_GUARDED_BY(mutex_);
  httpConfigInternal* config_{nullptr};
};

using FilterConfigSharedPtr = std::shared_ptr<FilterConfig>;

class RoutePluginConfig : public std::enable_shared_from_this<RoutePluginConfig>,
                          Logger::Loggable<Logger::Id::http> {
public:
  RoutePluginConfig(const std::string plugin_name,
                    const envoy::extensions::filters::http::golang::v3alpha::RouterPlugin& config);
  ~RoutePluginConfig();
  uint64_t getConfigId();
  uint64_t getMergedConfigId(uint64_t parent_id);

private:
  const std::string plugin_name_;
  const ProtobufWkt::Any plugin_config_;

  Dso::HttpFilterDsoPtr dso_lib_;
  uint64_t config_id_{0};
  // since these two fields are updated in worker threads, we need to protect them with a mutex.
  uint64_t merged_config_id_ ABSL_GUARDED_BY(mutex_){0};
  uint64_t cached_parent_id_ ABSL_GUARDED_BY(mutex_){0};

  absl::Mutex mutex_;
  httpConfig* config_{nullptr};
};

using RoutePluginConfigPtr = std::shared_ptr<RoutePluginConfig>;

/**
 * Route configuration for the filter.
 */
class FilterConfigPerRoute : public Router::RouteSpecificFilterConfig,
                             Logger::Loggable<Logger::Id::http> {
public:
  FilterConfigPerRoute(const envoy::extensions::filters::http::golang::v3alpha::ConfigsPerRoute&,
                       Server::Configuration::ServerFactoryContext&);
  uint64_t getPluginConfigId(uint64_t parent_id, std::string plugin_name) const;

  ~FilterConfigPerRoute() override { plugins_config_.clear(); }

private:
  std::map<std::string, RoutePluginConfigPtr> plugins_config_;
};

enum class DestroyReason {
  Normal,
  Terminate,
};

enum class EnvoyValue {
  RouteName = 1,
  FilterChainName,
  Protocol,
  ResponseCode,
  ResponseCodeDetails,
  AttemptCount,
  DownstreamLocalAddress,
  DownstreamRemoteAddress,
  UpstreamLocalAddress,
  UpstreamRemoteAddress,
  UpstreamClusterName,
  VirtualClusterName,
};

struct httpRequestInternal;

/**
 * See docs/configuration/http_filters/golang_extension_filter.rst
 */
class Filter : public Http::StreamFilter,
               public std::enable_shared_from_this<Filter>,
               public Filters::Common::Expr::StreamActivation,
               Logger::Loggable<Logger::Id::http>,
               public AccessLog::Instance {
public:
  explicit Filter(FilterConfigSharedPtr config, Dso::HttpFilterDsoPtr dynamic_lib)
      : config_(config), dynamic_lib_(dynamic_lib), decoding_state_(*this), encoding_state_(*this) {
  }

  // Http::StreamFilterBase
  void onDestroy() ABSL_LOCKS_EXCLUDED(mutex_) override;
  Http::LocalErrorStatus onLocalReply(const LocalReplyData&) override;

  // Http::StreamDecoderFilter
  Http::FilterHeadersStatus decodeHeaders(Http::RequestHeaderMap& headers,
                                          bool end_stream) override;
  Http::FilterDataStatus decodeData(Buffer::Instance&, bool) override;
  Http::FilterTrailersStatus decodeTrailers(Http::RequestTrailerMap&) override;
  void setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks& callbacks) override {
    decoding_state_.setDecoderFilterCallbacks(callbacks);
  }

  // Http::StreamEncoderFilter
  Http::Filter1xxHeadersStatus encode1xxHeaders(Http::ResponseHeaderMap&) override {
    return Http::Filter1xxHeadersStatus::Continue;
  }
  Http::FilterHeadersStatus encodeHeaders(Http::ResponseHeaderMap& headers,
                                          bool end_stream) override;
  Http::FilterDataStatus encodeData(Buffer::Instance& data, bool end_stream) override;
  Http::FilterTrailersStatus encodeTrailers(Http::ResponseTrailerMap& trailers) override;
  Http::FilterMetadataStatus encodeMetadata(Http::MetadataMap&) override {
    return Http::FilterMetadataStatus::Continue;
  }

  void setEncoderFilterCallbacks(Http::StreamEncoderFilterCallbacks& callbacks) override {
    encoding_state_.setEncoderFilterCallbacks(callbacks);
  }

  // AccessLog::Instance
  void log(const Formatter::HttpFormatterContext& log_context,
           const StreamInfo::StreamInfo& info) override;

  void onStreamComplete() override {}

  CAPIStatus continueStatus(GolangStatus status);

  CAPIStatus sendLocalReply(Http::Code response_code, std::string body_text,
                            std::function<void(Http::ResponseHeaderMap& headers)> modify_headers,
                            Grpc::Status::GrpcStatus grpc_status, std::string details);

  CAPIStatus sendPanicReply(absl::string_view details);

  CAPIStatus getHeader(absl::string_view key, uint64_t* value_data, int* value_len);
  CAPIStatus copyHeaders(GoString* go_strs, char* go_buf);
  CAPIStatus setHeader(absl::string_view key, absl::string_view value, headerAction act);
  CAPIStatus removeHeader(absl::string_view key);
  CAPIStatus copyBuffer(Buffer::Instance* buffer, char* data);
  CAPIStatus drainBuffer(Buffer::Instance* buffer, uint64_t length);
  CAPIStatus setBufferHelper(Buffer::Instance* buffer, absl::string_view& value,
                             bufferAction action);
  CAPIStatus copyTrailers(GoString* go_strs, char* go_buf);
  CAPIStatus setTrailer(absl::string_view key, absl::string_view value, headerAction act);
  CAPIStatus removeTrailer(absl::string_view key);
  CAPIStatus getStringValue(int id, uint64_t* value_data, int* value_len);
  CAPIStatus getIntegerValue(int id, uint64_t* value);

  CAPIStatus getDynamicMetadata(const std::string& filter_name, uint64_t* buf_data, int* buf_len);
  CAPIStatus setDynamicMetadata(std::string filter_name, std::string key, absl::string_view buf);
  CAPIStatus setStringFilterState(absl::string_view key, absl::string_view value, int state_type,
                                  int life_span, int stream_sharing);
  CAPIStatus getStringFilterState(absl::string_view key, uint64_t* value_data, int* value_len);
  CAPIStatus getStringProperty(absl::string_view path, uint64_t* value_data, int* value_len,
                               GoInt32* rc);

private:
  bool hasDestroyed() {
    Thread::LockGuard lock(mutex_);
    return has_destroyed_;
  };
  ProcessorState& getProcessorState();

  bool doHeaders(ProcessorState& state, Http::RequestOrResponseHeaderMap& headers, bool end_stream);
  GolangStatus doHeadersGo(ProcessorState& state, Http::RequestOrResponseHeaderMap& headers,
                           bool end_stream);
  bool doData(ProcessorState& state, Buffer::Instance&, bool);
  bool doDataGo(ProcessorState& state, Buffer::Instance& data, bool end_stream);
  bool doTrailer(ProcessorState& state, Http::HeaderMap& trailers);
  bool doTrailerGo(ProcessorState& state, Http::HeaderMap& trailers);

  void initRequest(ProcessorState& state);

  uint64_t getMergedConfigId(ProcessorState& state);

  void continueEncodeLocalReply(ProcessorState& state);
  void continueStatusInternal(GolangStatus status);
  void continueData(ProcessorState& state);

  void onHeadersModified();

  void sendLocalReplyInternal(Http::Code response_code, absl::string_view body_text,
                              std::function<void(Http::ResponseHeaderMap& headers)> modify_headers,
                              Grpc::Status::GrpcStatus grpc_status, absl::string_view details);

  void setDynamicMetadataInternal(ProcessorState& state, std::string filter_name, std::string key,
                                  const absl::string_view& buf);

  void populateSliceWithMetadata(ProcessorState& state, const std::string& filter_name,
                                 uint64_t* buf_data, int* buf_len);

  CAPIStatus getStringPropertyCommon(absl::string_view path, uint64_t* value_data, int* value_len,
                                     ProcessorState& state);
  CAPIStatus getStringPropertyInternal(absl::string_view path, std::string* result);
  absl::optional<google::api::expr::runtime::CelValue> findValue(absl::string_view name,
                                                                 Protobuf::Arena* arena);
  CAPIStatus serializeStringValue(Filters::Common::Expr::CelValue value, std::string* result);

  const FilterConfigSharedPtr config_;
  Dso::HttpFilterDsoPtr dynamic_lib_;

  Http::RequestOrResponseHeaderMap* headers_ ABSL_GUARDED_BY(mutex_){nullptr};
  Http::HeaderMap* trailers_ ABSL_GUARDED_BY(mutex_){nullptr};

  // save temp values from local reply
  Http::RequestOrResponseHeaderMap* local_headers_{nullptr};
  Http::HeaderMap* local_trailers_{nullptr};

  // save temp values for fetching request attributes in the later phase,
  // like getting request size
  Http::RequestOrResponseHeaderMap* request_headers_{nullptr};

  // The state of the filter on both the encoding and decoding side.
  DecodingProcessorState decoding_state_;
  EncodingProcessorState encoding_state_;

  httpRequestInternal* req_{nullptr};

  // lock for has_destroyed_ and the functions get/set/copy/remove/etc that operate on the
  // headers_/trailers_/etc, to avoid race between envoy c thread and go thread (when calling back
  // from go). it should also be okay without this lock in most cases, just for extreme case.
  Thread::MutexBasicLockable mutex_{};
  bool has_destroyed_ ABSL_GUARDED_BY(mutex_){false};

  // other filter trigger sendLocalReply during go processing in async.
  // will wait go return before continue.
  // this variable is read/write in safe thread, do no need lock.
  bool local_reply_waiting_go_{false};

  // the filter enter encoding phase
  bool enter_encoding_{false};
};

// Go code only touch the fields in httpRequest
struct httpRequestInternal : httpRequest {
  std::weak_ptr<Filter> filter_;
  // anchor a string temporarily, make sure it won't be freed before copied to Go.
  std::string strValue;
  httpRequestInternal(std::weak_ptr<Filter> f) { filter_ = f; }
  std::weak_ptr<Filter> weakFilter() { return filter_; }
};

struct httpConfigInternal : httpConfig {
  std::weak_ptr<FilterConfig> config_;
  httpConfigInternal(std::weak_ptr<FilterConfig> c) { config_ = c; }
  std::weak_ptr<FilterConfig> weakFilterConfig() { return config_; }
};

class GoStringFilterState : public StreamInfo::FilterState::Object {
public:
  GoStringFilterState(absl::string_view value) : value_(value) {}
  const std::string& value() const { return value_; }

private:
  const std::string value_;
};

} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/golang/filters/http/source/golang_filter.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

//
// These functions may be invoked in another go thread,
// which means may introduce race between go thread and envoy thread.
// So we use the envoy's dispatcher in the filter to post it, and make it only executes in the envoy
// thread.
//

// Deep copy Go memory into std::string,
// it's safe to use it after the current cgo call returns.
std::string copyStringFromGoPointer(void* p, int len) {
  return {static_cast<const char*>(p), static_cast<size_t>(len)};
}

// The returned absl::string_view only refer to Go memory,
// should not use it after the current cgo call returns.
absl::string_view stringViewFromGoPointer(void* p, int len) {
  return {static_cast<const char*>(p), static_cast<size_t>(len)};
}

absl::string_view stringViewFromGoSlice(void* slice) {
  if (slice == nullptr) {
    return "";
  }
  auto go_slice = reinterpret_cast<GoSlice*>(slice);
  return {static_cast<const char*>(go_slice->data), static_cast<size_t>(go_slice->len)};
}

std::vector<std::string> stringsFromGoSlice(void* slice_data, int slice_len) {
  std::vector<std::string> list;
  if (slice_len == 0) {
    return list;
  }
  auto strs = reinterpret_cast<char**>(slice_data);
  for (auto i = 0; i < slice_len; i += 2) {
    auto key = std::string(strs[i + 0]);
    auto value = std::string(strs[i + 1]);
    list.push_back(key);
    list.push_back(value);
  }
  return list;
}

#ifdef __cplusplus
extern "C" {
#endif

CAPIStatus envoyGoFilterHandlerWrapper(void* r,
                                       std::function<CAPIStatus(std::shared_ptr<Filter>&)> f) {
  auto req = reinterpret_cast<httpRequestInternal*>(r);
  auto weak_filter = req->weakFilter();
  if (auto filter = weak_filter.lock()) {
    return f(filter);
  }
  return CAPIStatus::CAPIFilterIsGone;
}

CAPIStatus
envoyGoConfigHandlerWrapper(void* c, std::function<CAPIStatus(std::shared_ptr<FilterConfig>&)> fc) {
  auto config = reinterpret_cast<httpConfigInternal*>(c);
  auto weak_filter_config = config->weakFilterConfig();
  if (auto filter_config = weak_filter_config.lock()) {
    return fc(filter_config);
  }
  return CAPIStatus::CAPIFilterIsGone;
}

CAPIStatus envoyGoFilterHttpContinue(void* r, int status) {
  return envoyGoFilterHandlerWrapper(r, [status](std::shared_ptr<Filter>& filter) -> CAPIStatus {
    return filter->continueStatus(static_cast<GolangStatus>(status));
  });
}

CAPIStatus envoyGoFilterHttpSendLocalReply(void* r, int response_code, void* body_text_data,
                                           int body_text_len, void* headers, int headers_num,
                                           long long int grpc_status, void* details_data,
                                           int details_len) {
  return envoyGoFilterHandlerWrapper(
      r,
      [response_code, body_text_data, body_text_len, headers, headers_num, grpc_status,
       details_data, details_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto header_values = stringsFromGoSlice(headers, headers_num);
        std::function<void(Http::ResponseHeaderMap&)> modify_headers =
            [header_values](Http::ResponseHeaderMap& headers) -> void {
          for (size_t i = 0; i < header_values.size(); i += 2) {
            const auto& key = header_values[i];
            const auto& value = header_values[i + 1];
            if (value.length() > 0) {
              headers.addCopy(Http::LowerCaseString(key), value);
            }
          }
        };
        auto status = static_cast<Grpc::Status::GrpcStatus>(grpc_status);

        // Deep clone the GoString into C++, since the GoString may be freed after the function
        // returns, while they may still be used in the callback.
        return filter->sendLocalReply(static_cast<Http::Code>(response_code),
                                      copyStringFromGoPointer(body_text_data, body_text_len),
                                      modify_headers, status,
                                      copyStringFromGoPointer(details_data, details_len));
      });
}

// unsafe API, without copy memory from c to go.
CAPIStatus envoyGoFilterHttpGetHeader(void* r, void* key_data, int key_len, uint64_t* value_data,
                                      int* value_len) {
  return envoyGoFilterHandlerWrapper(
      r, [key_data, key_len, value_data, value_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto key_str = stringViewFromGoPointer(key_data, key_len);
        return filter->getHeader(key_str, value_data, value_len);
      });
}

CAPIStatus envoyGoFilterHttpCopyHeaders(void* r, void* strs, void* buf) {
  return envoyGoFilterHandlerWrapper(r, [strs, buf](std::shared_ptr<Filter>& filter) -> CAPIStatus {
    auto go_strs = reinterpret_cast<GoString*>(strs);
    auto go_buf = reinterpret_cast<char*>(buf);
    return filter->copyHeaders(go_strs, go_buf);
  });
}

CAPIStatus envoyGoFilterHttpSetHeaderHelper(void* r, void* key_data, int key_len, void* value_data,
                                            int value_len, headerAction act) {
  return envoyGoFilterHandlerWrapper(r,
                                     [key_data, key_len, value_data, value_len,
                                      act](std::shared_ptr<Filter>& filter) -> CAPIStatus {
                                       auto key_str = stringViewFromGoPointer(key_data, key_len);
                                       auto value_str =
                                           stringViewFromGoPointer(value_data, value_len);
                                       return filter->setHeader(key_str, value_str, act);
                                     });
}

CAPIStatus envoyGoFilterHttpRemoveHeader(void* r, void* key_data, int key_len) {
  return envoyGoFilterHandlerWrapper(
      r, [key_data, key_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto key_str = stringViewFromGoPointer(key_data, key_len);
        return filter->removeHeader(key_str);
      });
}

CAPIStatus envoyGoFilterHttpGetBuffer(void* r, uint64_t buffer_ptr, void* data) {
  return envoyGoFilterHandlerWrapper(
      r, [buffer_ptr, data](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto buffer = reinterpret_cast<Buffer::Instance*>(buffer_ptr);
        return filter->copyBuffer(buffer, reinterpret_cast<char*>(data));
      });
}

CAPIStatus envoyGoFilterHttpDrainBuffer(void* r, uint64_t buffer_ptr, uint64_t length) {
  return envoyGoFilterHandlerWrapper(
      r, [buffer_ptr, length](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto buffer = reinterpret_cast<Buffer::Instance*>(buffer_ptr);
        return filter->drainBuffer(buffer, length);
      });
}

CAPIStatus envoyGoFilterHttpSetBufferHelper(void* r, uint64_t buffer_ptr, void* data, int length,
                                            bufferAction action) {
  return envoyGoFilterHandlerWrapper(
      r, [buffer_ptr, data, length, action](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto buffer = reinterpret_cast<Buffer::Instance*>(buffer_ptr);
        auto value = stringViewFromGoPointer(data, length);
        return filter->setBufferHelper(buffer, value, action);
      });
}

CAPIStatus envoyGoFilterHttpCopyTrailers(void* r, void* strs, void* buf) {
  return envoyGoFilterHandlerWrapper(r, [strs, buf](std::shared_ptr<Filter>& filter) -> CAPIStatus {
    auto go_strs = reinterpret_cast<GoString*>(strs);
    auto go_buf = reinterpret_cast<char*>(buf);
    return filter->copyTrailers(go_strs, go_buf);
  });
}

CAPIStatus envoyGoFilterHttpSetTrailer(void* r, void* key_data, int key_len, void* value_data,
                                       int value_len, headerAction act) {
  return envoyGoFilterHandlerWrapper(r,
                                     [key_data, key_len, value_data, value_len,
                                      act](std::shared_ptr<Filter>& filter) -> CAPIStatus {
                                       auto key_str = stringViewFromGoPointer(key_data, key_len);
                                       auto value_str =
                                           stringViewFromGoPointer(value_data, value_len);
                                       return filter->setTrailer(key_str, value_str, act);
                                     });
}

CAPIStatus envoyGoFilterHttpRemoveTrailer(void* r, void* key_data, int key_len) {
  return envoyGoFilterHandlerWrapper(
      r, [key_data, key_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto key_str = stringViewFromGoPointer(key_data, key_len);
        return filter->removeTrailer(key_str);
      });
}

CAPIStatus envoyGoFilterHttpGetStringValue(void* r, int id, uint64_t* value_data, int* value_len) {
  return envoyGoFilterHandlerWrapper(
      r, [id, value_data, value_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        return filter->getStringValue(id, value_data, value_len);
      });
}

CAPIStatus envoyGoFilterHttpGetIntegerValue(void* r, int id, uint64_t* value) {
  return envoyGoFilterHandlerWrapper(r, [id, value](std::shared_ptr<Filter>& filter) -> CAPIStatus {
    return filter->getIntegerValue(id, value);
  });
}

CAPIStatus envoyGoFilterHttpGetDynamicMetadata(void* r, void* name_data, int name_len,
                                               uint64_t* buf_data, int* buf_len) {
  return envoyGoFilterHandlerWrapper(
      r, [name_data, name_len, buf_data, buf_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto name_str = copyStringFromGoPointer(name_data, name_len);
        return filter->getDynamicMetadata(name_str, buf_data, buf_len);
      });
}

CAPIStatus envoyGoFilterHttpSetDynamicMetadata(void* r, void* name_data, int name_len,
                                               void* key_data, int key_len, void* buf_data,
                                               int buf_len) {
  return envoyGoFilterHandlerWrapper(r,
                                     [name_data, name_len, key_data, key_len, buf_data,
                                      buf_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
                                       auto name_str = copyStringFromGoPointer(name_data, name_len);
                                       auto key_str = copyStringFromGoPointer(key_data, key_len);
                                       auto buf_str = stringViewFromGoPointer(buf_data, buf_len);
                                       return filter->setDynamicMetadata(name_str, key_str,
                                                                         buf_str);
                                     });
}

void envoyGoFilterHttpFinalize(void* r, int reason) {
  UNREFERENCED_PARAMETER(reason);
  // req is used by go, so need to use raw memory and then it is safe to release at the gc finalize
  // phase of the go object.
  auto req = reinterpret_cast<httpRequestInternal*>(r);
  delete req;
}

void envoyGoConfigHttpFinalize(void* c) {
  // config is used by go, so need to use raw memory and then it is safe to release at the gc
  // finalize phase of the go object.
  auto config = reinterpret_cast<httpConfigInternal*>(c);
  delete config;
}

CAPIStatus envoyGoFilterHttpSendPanicReply(void* r, void* details_data, int details_len) {
  return envoyGoFilterHandlerWrapper(
      r, [details_data, details_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        // Since this is only used for logs we don't need to deep copy.
        auto details = stringViewFromGoPointer(details_data, details_len);
        return filter->sendPanicReply(details);
      });
}

CAPIStatus envoyGoFilterHttpSetStringFilterState(void* r, void* key_data, int key_len,
                                                 void* value_data, int value_len, int state_type,
                                                 int life_span, int stream_sharing) {
  return envoyGoFilterHandlerWrapper(
      r,
      [key_data, key_len, value_data, value_len, state_type, life_span,
       stream_sharing](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto key_str = stringViewFromGoPointer(key_data, key_len);
        auto value_str = stringViewFromGoPointer(value_data, value_len);
        return filter->setStringFilterState(key_str, value_str, state_type, life_span,
                                            stream_sharing);
      });
}

CAPIStatus envoyGoFilterHttpGetStringFilterState(void* r, void* key_data, int key_len,
                                                 uint64_t* value_data, int* value_len) {
  return envoyGoFilterHandlerWrapper(
      r, [key_data, key_len, value_data, value_len](std::shared_ptr<Filter>& filter) -> CAPIStatus {
        auto key_str = stringViewFromGoPointer(key_data, key_len);
        return filter->getStringFilterState(key_str, value_data, value_len);
      });
}

CAPIStatus envoyGoFilterHttpGetStringProperty(void* r, void* key_data, int key_len,
                                              uint64_t* value_data, int* value_len, int* rc) {
  return envoyGoFilterHandlerWrapper(r,
                                     [key_data, key_len, value_data, value_len,
                                      rc](std::shared_ptr<Filter>& filter) -> CAPIStatus {
                                       auto key_str = stringViewFromGoPointer(key_data, key_len);
                                       return filter->getStringProperty(key_str, value_data,
                                                                        value_len, rc);
                                     });
}

CAPIStatus envoyGoFilterHttpDefineMetric(void* c, uint32_t metric_type, void* name_data,
                                         int name_len, uint32_t* metric_id) {
  return envoyGoConfigHandlerWrapper(
      c,
      [metric_type, name_data, name_len,
       metric_id](std::shared_ptr<FilterConfig>& filter_config) -> CAPIStatus {
        auto name_str = stringViewFromGoPointer(name_data, name_len);
        return filter_config->defineMetric(metric_type, name_str, metric_id);
      });
}

CAPIStatus envoyGoFilterHttpIncrementMetric(void* c, uint32_t metric_id, int64_t offset) {
  return envoyGoConfigHandlerWrapper(
      c, [metric_id, offset](std::shared_ptr<FilterConfig>& filter_config) -> CAPIStatus {
        return filter_config->incrementMetric(metric_id, offset);
      });
}

CAPIStatus envoyGoFilterHttpGetMetric(void* c, uint32_t metric_id, uint64_t* value) {
  return envoyGoConfigHandlerWrapper(
      c, [metric_id, value](std::shared_ptr<FilterConfig>& filter_config) -> CAPIStatus {
        return filter_config->getMetric(metric_id, value);
      });
}

CAPIStatus envoyGoFilterHttpRecordMetric(void* c, uint32_t metric_id, uint64_t value) {
  return envoyGoConfigHandlerWrapper(
      c, [metric_id, value](std::shared_ptr<FilterConfig>& filter_config) -> CAPIStatus {
        return filter_config->recordMetric(metric_id, value);
      });
}

#ifdef __cplusplus
}
#endif

} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <deque>
#include <memory>

#include "envoy/buffer/buffer.h"
#include "envoy/http/filter.h"
#include "envoy/http/header_map.h"

#include "source/common/buffer/buffer_impl.h"
#include "source/common/common/logger.h"
#include "source/common/http/codes.h"
#include "source/common/http/utility.h"

#include "absl/status/status.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

class Filter;

class BufferList : public NonCopyable {
public:
  BufferList() = default;

  bool empty() const { return bytes_ == 0; }
  // return a new buffer instance, it will existing until moveOut or drain.
  Buffer::Instance& push(Buffer::Instance& data);
  // move all buffer into data, the list is empty then.
  void moveOut(Buffer::Instance& data);
  // clear the latest push in buffer.
  void clearLatest();
  // clear all.
  void clearAll();
  // check the buffer instance if existing
  bool checkExisting(Buffer::Instance* data);

private:
  std::deque<Buffer::InstancePtr> queue_;
  // The total size of buffers in the list.
  uint32_t bytes_{0};
};

// This describes the processor state.
enum class FilterState {
  // Waiting header
  WaitingHeader,
  // Processing header in Go
  ProcessingHeader,
  // Waiting data
  WaitingData,
  // Waiting all data
  WaitingAllData,
  // Processing data in Go
  ProcessingData,
  // Waiting trailer
  WaitingTrailer,
  // Processing trailer in Go
  ProcessingTrailer,
  // Log in Go
  Log,
  // All done
  Done,
};

/*
 * request phase
 */
enum class Phase {
  DecodeHeader = 1,
  DecodeData,
  DecodeTrailer,
  EncodeHeader,
  EncodeData,
  EncodeTrailer,
  Log,
  Done,
};

/**
 * An enum specific for Golang status.
 */
enum class GolangStatus {
  Running,
  // after called sendLocalReply
  LocalReply,
  // Continue filter chain iteration.
  Continue,
  StopAndBuffer,
  StopAndBufferWatermark,
  StopNoBuffer,
};

class ProcessorState : public Logger::Loggable<Logger::Id::http>, NonCopyable {
public:
  explicit ProcessorState(Filter& filter) : filter_(filter) {}
  virtual ~ProcessorState() = default;

  FilterState state() const { return state_; }
  std::string stateStr();

  virtual Phase phase() PURE;
  std::string phaseStr();

  bool isProcessingInGo() {
    return state_ == FilterState::ProcessingHeader || state_ == FilterState::ProcessingData ||
           state_ == FilterState::ProcessingTrailer || state_ == FilterState::Log;
  }
  bool isProcessingHeader() { return state_ == FilterState::ProcessingHeader; }
  Http::StreamFilterCallbacks* getFilterCallbacks() { return filter_callbacks_; };

  bool isThreadSafe() { return filter_callbacks_->dispatcher().isThreadSafe(); };
  Event::Dispatcher& getDispatcher() { return filter_callbacks_->dispatcher(); }

  /* data buffer */
  // add data to state buffer
  virtual void addBufferData(Buffer::Instance& data) PURE;
  // get state buffer
  Buffer::Instance& getBufferData() { return *data_buffer_.get(); };
  bool isBufferDataEmpty() { return data_buffer_ == nullptr || data_buffer_->length() == 0; };
  void drainBufferData();

  void setSeenTrailers() { seen_trailers_ = true; }
  bool isProcessingEndStream() { return do_end_stream_; }

  virtual void continueProcessing() PURE;
  virtual void injectDataToFilterChain(Buffer::Instance& data, bool end_stream) PURE;
  void continueDoData() {
    if (!end_stream_ && doDataList.empty()) {
      return;
    }
    Buffer::OwnedImpl data_to_write;
    doDataList.moveOut(data_to_write);

    injectDataToFilterChain(data_to_write, do_end_stream_);
  }

  void processHeader(bool end_stream) {
    ASSERT(state_ == FilterState::WaitingHeader);
    state_ = FilterState::ProcessingHeader;
    do_end_stream_ = end_stream;
  }

  void processData(bool end_stream) {
    ASSERT(state_ == FilterState::WaitingData ||
           (state_ == FilterState::WaitingAllData && (end_stream || seen_trailers_)));
    state_ = FilterState::ProcessingData;
    do_end_stream_ = end_stream;
  }

  void processTrailer() {
    ASSERT(state_ == FilterState::WaitingTrailer || state_ == FilterState::WaitingData ||
           state_ == FilterState::WaitingAllData);
    state_ = FilterState::ProcessingTrailer;
    do_end_stream_ = true;
  }

  void enterLog() {
    prev_state_ = state_;
    state_ = FilterState::Log;
  }
  void leaveLog() {
    state_ = prev_state_;
    prev_state_ = FilterState::Log;
  }

  bool handleHeaderGolangStatus(const GolangStatus status);
  bool handleDataGolangStatus(const GolangStatus status);
  bool handleTrailerGolangStatus(const GolangStatus status);
  bool handleGolangStatus(GolangStatus status);

  virtual void sendLocalReply(Http::Code response_code, absl::string_view body_text,
                              std::function<void(Http::ResponseHeaderMap& headers)> modify_headers,
                              Grpc::Status::GrpcStatus grpc_status, absl::string_view details) PURE;

  const StreamInfo::StreamInfo& streamInfo() const { return filter_callbacks_->streamInfo(); }
  StreamInfo::StreamInfo& streamInfo() { return filter_callbacks_->streamInfo(); }

  void setEndStream(bool end_stream) { end_stream_ = end_stream; }
  bool getEndStream() { return end_stream_; }
  // seen trailers also means stream is end
  bool isStreamEnd() { return end_stream_ || seen_trailers_; }

  BufferList doDataList;

protected:
  Phase state2Phase();
  Filter& filter_;
  Http::StreamFilterCallbacks* filter_callbacks_{nullptr};
  bool watermark_requested_{false};
  Buffer::InstancePtr data_buffer_{nullptr};
  FilterState state_{FilterState::WaitingHeader};
  FilterState prev_state_{FilterState::Done};
  bool end_stream_{false};
  bool do_end_stream_{false};
  bool seen_trailers_{false};
};

class DecodingProcessorState : public ProcessorState {
public:
  explicit DecodingProcessorState(Filter& filter) : ProcessorState(filter) {}

  void setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks& callbacks) {
    decoder_callbacks_ = &callbacks;
    filter_callbacks_ = &callbacks;
  }

  void injectDataToFilterChain(Buffer::Instance& data, bool end_stream) override {
    decoder_callbacks_->injectDecodedDataToFilterChain(data, end_stream);
  }

  Phase phase() override { return state2Phase(); };

  void addBufferData(Buffer::Instance& data) override;

  void continueProcessing() override {
    ENVOY_LOG(debug, "golang filter callback continue, continueDecoding");
    decoder_callbacks_->continueDecoding();
  }
  void sendLocalReply(Http::Code response_code, absl::string_view body_text,
                      std::function<void(Http::ResponseHeaderMap& headers)> modify_headers,
                      Grpc::Status::GrpcStatus grpc_status, absl::string_view details) override {
    // it's safe to reset state_, since it is read/write in safe thread.
    ENVOY_LOG(debug, "golang filter phase grow to EncodeHeader and state grow to WaitHeader before "
                     "sendLocalReply");
    state_ = FilterState::WaitingHeader;
    decoder_callbacks_->sendLocalReply(response_code, body_text, modify_headers, grpc_status,
                                       details);
  };

private:
  Http::StreamDecoderFilterCallbacks* decoder_callbacks_{nullptr};
};

class EncodingProcessorState : public ProcessorState {
public:
  explicit EncodingProcessorState(Filter& filter) : ProcessorState(filter) {}

  void setEncoderFilterCallbacks(Http::StreamEncoderFilterCallbacks& callbacks) {
    encoder_callbacks_ = &callbacks;
    filter_callbacks_ = &callbacks;
  }

  void injectDataToFilterChain(Buffer::Instance& data, bool end_stream) override {
    encoder_callbacks_->injectEncodedDataToFilterChain(data, end_stream);
  }

  Phase phase() override { return static_cast<Phase>(static_cast<int>(state2Phase()) + 3); };

  void addBufferData(Buffer::Instance& data) override;

  void continueProcessing() override {
    ENVOY_LOG(debug, "golang filter callback continue, continueEncoding");
    encoder_callbacks_->continueEncoding();
  }
  void sendLocalReply(Http::Code response_code, absl::string_view body_text,
                      std::function<void(Http::ResponseHeaderMap& headers)> modify_headers,
                      Grpc::Status::GrpcStatus grpc_status, absl::string_view details) override {
    encoder_callbacks_->sendLocalReply(response_code, body_text, modify_headers, grpc_status,
                                       details);
  };

private:
  Http::StreamEncoderFilterCallbacks* encoder_callbacks_{nullptr};
};

} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_contrib_extension",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

# Golang extensions filter.
# Public docs: https://envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/golang_filter

envoy_contrib_package()

envoy_cc_library(
    name = "golang_filter_lib",
    srcs = [
        "golang_filter.cc",
        "processor_state.cc",
    ],
    hdrs = [
        "golang_filter.h",
        "processor_state.h",
        "stats.h",
    ],
    deps = [
        ":cgo",
        "//contrib/golang/common/dso:dso_lib",
        "//envoy/http:codes_interface",
        "//envoy/http:filter_interface",
        "//source/common/buffer:watermark_buffer_lib",
        "//source/common/common:enum_to_int",
        "//source/common/common:linked_object",
        "//source/common/common:thread_lib",
        "//source/common/common:utility_lib",
        "//source/common/grpc:context_lib",
        "//source/common/http:headers_lib",
        "//source/common/http:utility_lib",
        "//source/common/http/http1:codec_lib",
        "//source/common/protobuf:utility_lib",
        "//source/extensions/filters/common/expr:cel_state_lib",
        "//source/extensions/filters/common/expr:evaluator_lib",
        "@com_google_cel_cpp//eval/public:activation",
        "@com_google_cel_cpp//eval/public:builtin_func_registrar",
        "@com_google_cel_cpp//eval/public:cel_expr_builder_factory",
        "@com_google_cel_cpp//eval/public:cel_value",
        "@com_google_cel_cpp//eval/public:value_export_util",
        "@com_google_cel_cpp//eval/public/containers:field_access",
        "@com_google_cel_cpp//eval/public/containers:field_backed_list_impl",
        "@com_google_cel_cpp//eval/public/containers:field_backed_map_impl",
        "@com_google_cel_cpp//eval/public/structs:cel_proto_wrapper",
        "@envoy_api//contrib/envoy/extensions/filters/http/golang/v3alpha:pkg_cc_proto",
    ],
)

envoy_cc_contrib_extension(
    name = "config",
    srcs = ["config.cc"],
    hdrs = ["config.h"],
    deps = [
        ":golang_filter_lib",
        "//contrib/golang/common/dso:dso_lib",
        "//envoy/registry",
        "//envoy/server:filter_config_interface",
        "//envoy/server:lifecycle_notifier_interface",
        "//source/extensions/filters/http/common:factory_base_lib",
        "@envoy_api//contrib/envoy/extensions/filters/http/golang/v3alpha:pkg_cc_proto",
    ],
)

envoy_cc_library(
    name = "cgo",
    srcs = ["cgo.cc"],
    hdrs = [
        "golang_filter.h",
        "processor_state.h",
        "stats.h",
    ],
    deps = [
        "//contrib/golang/common/dso:dso_lib",
        "//contrib/golang/common/log:log_lib",
        "//envoy/http:codes_interface",
        "//envoy/http:filter_interface",
        "//source/common/buffer:watermark_buffer_lib",
        "//source/common/common:enum_to_int",
        "//source/common/common:linked_object",
        "//source/common/common:utility_lib",
        "//source/common/grpc:context_lib",
        "//source/common/http:headers_lib",
        "//source/common/http:utility_lib",
        "//source/common/http/http1:codec_lib",
        "//source/common/protobuf:utility_lib",
        "//source/extensions/filters/common/expr:cel_state_lib",
        "//source/extensions/filters/common/expr:evaluator_lib",
        "@envoy_api//contrib/envoy/extensions/filters/http/golang/v3alpha:pkg_cc_proto",
    ],
)
#include "contrib/golang/filters/http/source/processor_state.h"

#include "source/common/buffer/buffer_impl.h"
#include "source/common/protobuf/utility.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

Buffer::Instance& BufferList::push(Buffer::Instance& data) {
  bytes_ += data.length();

  auto ptr = std::make_unique<Buffer::OwnedImpl>();
  Buffer::Instance& buffer = *ptr.get();
  buffer.move(data);
  queue_.push_back(std::move(ptr));

  return buffer;
}

void BufferList::moveOut(Buffer::Instance& data) {
  for (auto it = queue_.begin(); it != queue_.end(); it = queue_.erase(it)) {
    data.move(**it);
  }
  bytes_ = 0;
};

void BufferList::clearLatest() {
  auto buffer = std::move(queue_.back());
  bytes_ -= buffer->length();
  queue_.pop_back();
};

void BufferList::clearAll() {
  bytes_ = 0;
  queue_.clear();
};

bool BufferList::checkExisting(Buffer::Instance* data) {
  for (auto& it : queue_) {
    if (it.get() == data) {
      return true;
    }
  }
  return false;
};

// headers_ should set to nullptr when return true.
bool ProcessorState::handleHeaderGolangStatus(GolangStatus status) {
  ENVOY_LOG(debug, "golang filter handle header status, state: {}, phase: {}, status: {}",
            stateStr(), phaseStr(), int(status));

  ASSERT(state_ == FilterState::ProcessingHeader);
  bool done = false;

  switch (status) {
  case GolangStatus::LocalReply:
    // already invoked sendLocalReply, do nothing
    break;

  case GolangStatus::Running:
    // do nothing, go side turn to async mode
    break;

  case GolangStatus::Continue:
    if (do_end_stream_) {
      state_ = FilterState::Done;
    } else {
      state_ = FilterState::WaitingData;
    }
    done = true;
    break;

  case GolangStatus::StopAndBuffer:
    state_ = FilterState::WaitingAllData;
    break;

  case GolangStatus::StopAndBufferWatermark:
    state_ = FilterState::WaitingData;
    break;

  default:
    ENVOY_LOG(error, "unexpected status: {}", int(status));
    break;
  }

  ENVOY_LOG(debug, "golang filter after handle header status, state: {}, phase: {}, status: {}",
            stateStr(), phaseStr(), int(status));

  return done;
};

bool ProcessorState::handleDataGolangStatus(const GolangStatus status) {
  ENVOY_LOG(debug, "golang filter handle data status, state: {}, phase: {}, status: {}", stateStr(),
            phaseStr(), int(status));

  ASSERT(state_ == FilterState::ProcessingData);

  bool done = false;

  switch (status) {
  case GolangStatus::LocalReply:
    // already invoked sendLocalReply, do nothing
    // return directly to skip phase grow by checking trailers
    return false;

  case GolangStatus::Running:
    // do nothing, go side turn to async mode
    // return directly to skip phase grow by checking trailers
    return false;

  case GolangStatus::Continue:
    if (do_end_stream_) {
      state_ = FilterState::Done;
    } else {
      state_ = FilterState::WaitingData;
    }
    done = true;
    break;

  case GolangStatus::StopAndBuffer:
    if (do_end_stream_) {
      ENVOY_LOG(error, "want more data while stream is end");
      // TODO: terminate the stream?
    }
    state_ = FilterState::WaitingAllData;
    break;

  case GolangStatus::StopAndBufferWatermark:
    if (do_end_stream_) {
      ENVOY_LOG(error, "want more data while stream is end");
      // TODO: terminate the stream?
    }
    state_ = FilterState::WaitingData;
    break;

  case GolangStatus::StopNoBuffer:
    if (do_end_stream_) {
      ENVOY_LOG(error, "want more data while stream is end");
      // TODO: terminate the stream?
    }
    doDataList.clearLatest();
    state_ = FilterState::WaitingData;
    break;

  default:
    // TODO: terminate the stream?
    ENVOY_LOG(error, "unexpected status: {}", int(status));
    break;
  }

  // see trailers and no buffered data
  if (seen_trailers_ && isBufferDataEmpty()) {
    ENVOY_LOG(error, "see trailers and buffer is empty");
    state_ = FilterState::WaitingTrailer;
  }

  ENVOY_LOG(debug, "golang filter after handle data status, state: {}, phase: {}, status: {}",
            int(state_), phaseStr(), int(status));

  return done;
};

// should set trailers_ to nullptr when return true.
// means we should not read/write trailers then, since trailers will pass to next fitler.
bool ProcessorState::handleTrailerGolangStatus(const GolangStatus status) {
  ENVOY_LOG(debug, "golang filter handle trailer status, state: {}, phase: {}, status: {}",
            stateStr(), phaseStr(), int(status));

  ASSERT(state_ == FilterState::ProcessingTrailer);

  auto done = false;

  switch (status) {
  case GolangStatus::LocalReply:
    // already invoked sendLocalReply, do nothing
    break;

  case GolangStatus::Running:
    // do nothing, go side turn to async mode
    break;

  case GolangStatus::Continue:
    state_ = FilterState::Done;
    done = true;
    break;

  default:
    // TODO: terminate the stream?
    ENVOY_LOG(error, "unexpected status: {}", int(status));
    break;
  }

  ENVOY_LOG(debug, "golang filter after handle trailer status, state: {}, phase: {}, status: {}",
            stateStr(), phaseStr(), int(status));

  return done;
};

// must in envoy thread.
bool ProcessorState::handleGolangStatus(GolangStatus status) {
  ASSERT(isThreadSafe());
  ASSERT(isProcessingInGo(), "unexpected state");

  ENVOY_LOG(debug,
            "before handle golang status, status: {}, state: {}, phase: {}, "
            "do_end_stream_: {}",
            int(status), stateStr(), phaseStr(), do_end_stream_);

  bool done = false;
  switch (state_) {
  case FilterState::ProcessingHeader:
    done = handleHeaderGolangStatus(status);
    break;

  case FilterState::ProcessingData:
    done = handleDataGolangStatus(status);
    break;

  case FilterState::ProcessingTrailer:
    done = handleTrailerGolangStatus(status);
    break;

  default:
    ASSERT(0, "unexpected state");
  }

  ENVOY_LOG(debug,
            "after handle golang status, status: {}, state: {}, phase: {}, "
            "do_end_stream_: {}",
            int(status), stateStr(), phaseStr(), do_end_stream_);

  return done;
}

void ProcessorState::drainBufferData() {
  if (data_buffer_ != nullptr) {
    auto len = data_buffer_->length();
    if (len > 0) {
      ENVOY_LOG(debug, "golang filter drain buffer data");
      data_buffer_->drain(len);
    }
  }
}

std::string ProcessorState::stateStr() {
  switch (state_) {
  case FilterState::WaitingHeader:
    return "WaitingHeader";
  case FilterState::ProcessingHeader:
    return "ProcessingHeader";
  case FilterState::WaitingData:
    return "WaitingData";
  case FilterState::WaitingAllData:
    return "WaitingAllData";
  case FilterState::ProcessingData:
    return "ProcessingData";
  case FilterState::WaitingTrailer:
    return "WaitingTrailer";
  case FilterState::ProcessingTrailer:
    return "ProcessingTrailer";
  case FilterState::Log:
    return "Log";
  case FilterState::Done:
    return "Done";
  default:
    return "unknown";
  }
}

Phase ProcessorState::state2Phase() {
  Phase phase;
  switch (state_) {
  case FilterState::WaitingHeader:
  case FilterState::ProcessingHeader:
    phase = Phase::DecodeHeader;
    break;
  case FilterState::WaitingData:
  case FilterState::WaitingAllData:
  case FilterState::ProcessingData:
    phase = Phase::DecodeData;
    break;
  case FilterState::WaitingTrailer:
  case FilterState::ProcessingTrailer:
    phase = Phase::DecodeTrailer;
    break;
  case FilterState::Log:
    phase = Phase::Log;
    break;
  // decode Done state means encode header phase, encode done state means done phase
  case FilterState::Done:
    phase = Phase::EncodeHeader;
    break;
  }
  return phase;
};

std::string ProcessorState::phaseStr() {
  switch (phase()) {
  case Phase::DecodeHeader:
    return "DecodeHeader";
  case Phase::DecodeData:
    return "DecodeData";
  case Phase::DecodeTrailer:
    return "DecodeTrailer";
  case Phase::EncodeHeader:
    return "EncodeHeader";
  case Phase::EncodeData:
    return "EncodeData";
  case Phase::EncodeTrailer:
    return "EncodeTrailer";
  case Phase::Log:
    return "Log";
  default:
    return "unknown";
  }
}

void DecodingProcessorState::addBufferData(Buffer::Instance& data) {
  if (data_buffer_ == nullptr) {
    data_buffer_ = decoder_callbacks_->dispatcher().getWatermarkFactory().createBuffer(
        [this]() -> void {
          if (watermark_requested_) {
            watermark_requested_ = false;
            ENVOY_LOG(debug, "golang filter decode data buffer want more data");
            decoder_callbacks_->onDecoderFilterBelowWriteBufferLowWatermark();
          }
        },
        [this]() -> void {
          if (state_ == FilterState::WaitingAllData) {
            // On the request path exceeding buffer limits will result in a 413.
            ENVOY_LOG(debug, "golang filter decode data buffer is full, reply with 413");
            decoder_callbacks_->sendLocalReply(
                Http::Code::PayloadTooLarge,
                Http::CodeUtility::toString(Http::Code::PayloadTooLarge), nullptr, absl::nullopt,
                StreamInfo::ResponseCodeDetails::get().RequestPayloadTooLarge);
            return;
          }
          if (!watermark_requested_) {
            watermark_requested_ = true;
            ENVOY_LOG(debug, "golang filter decode data buffer is full, disable reading");
            decoder_callbacks_->onDecoderFilterAboveWriteBufferHighWatermark();
          }
        },
        []() -> void { /* TODO: Handle overflow watermark */ });
    data_buffer_->setWatermarks(decoder_callbacks_->decoderBufferLimit());
  }
  data_buffer_->move(data);
}

void EncodingProcessorState::addBufferData(Buffer::Instance& data) {
  if (data_buffer_ == nullptr) {
    data_buffer_ = encoder_callbacks_->dispatcher().getWatermarkFactory().createBuffer(
        [this]() -> void {
          if (watermark_requested_) {
            watermark_requested_ = false;
            ENVOY_LOG(debug, "golang filter encode data buffer want more data");
            encoder_callbacks_->onEncoderFilterBelowWriteBufferLowWatermark();
          }
        },
        [this]() -> void {
          if (state_ == FilterState::WaitingAllData) {
            // On the request path exceeding buffer limits will result in a 413.
            ENVOY_LOG(debug, "golang filter encode data buffer is full, reply with 413");
            encoder_callbacks_->sendLocalReply(
                Http::Code::PayloadTooLarge,
                Http::CodeUtility::toString(Http::Code::PayloadTooLarge), nullptr, absl::nullopt,
                StreamInfo::ResponseCodeDetails::get().RequestPayloadTooLarge);
            return;
          }
          if (!watermark_requested_) {
            watermark_requested_ = true;
            ENVOY_LOG(debug, "golang filter encode data buffer is full, disable reading");
            encoder_callbacks_->onEncoderFilterAboveWriteBufferHighWatermark();
          }
        },
        []() -> void { /* TODO: Handle overflow watermark */ });
    data_buffer_->setWatermarks(encoder_callbacks_->encoderBufferLimit());
  }
  data_buffer_->move(data);
}

} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <string>

#include "envoy/stats/scope.h"
#include "envoy/stats/stats_macros.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

/**
 * All golang filter stats. @see stats_macros.h
 */
#define ALL_GOLANG_FILTER_STATS(COUNTER, GAUGE, HISTOGRAM) COUNTER(panic_error)

/**
 * Struct definition for all golang proxy stats. @see stats_macros.h
 */
struct GolangFilterStats {
  ALL_GOLANG_FILTER_STATS(GENERATE_COUNTER_STRUCT, GENERATE_GAUGE_STRUCT, GENERATE_HISTOGRAM_STRUCT)

  static GolangFilterStats generateStats(const std::string& prefix, Stats::Scope& scope) {
    return GolangFilterStats{ALL_GOLANG_FILTER_STATS(POOL_COUNTER_PREFIX(scope, prefix),
                                                     POOL_GAUGE_PREFIX(scope, prefix),
                                                     POOL_HISTOGRAM_PREFIX(scope, prefix))};
  }
};
} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/golang/filters/http/source/golang_filter.h"

#include <cstddef>
#include <cstdint>
#include <memory>
#include <string>
#include <vector>

#include "envoy/http/codes.h"

#include "source/common/buffer/buffer_impl.h"
#include "source/common/common/base64.h"
#include "source/common/common/enum_to_int.h"
#include "source/common/common/lock_guard.h"
#include "source/common/common/utility.h"
#include "source/common/grpc/common.h"
#include "source/common/grpc/context_impl.h"
#include "source/common/grpc/status.h"
#include "source/common/http/headers.h"
#include "source/common/http/http1/codec_impl.h"
#include "source/extensions/filters/common/expr/context.h"

#include "eval/public/cel_value.h"
#include "eval/public/containers/field_access.h"
#include "eval/public/containers/field_backed_list_impl.h"
#include "eval/public/containers/field_backed_map_impl.h"
#include "eval/public/structs/cel_proto_wrapper.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

void Filter::onHeadersModified() {
  // Any changes to request headers can affect how the request is going to be
  // routed. If we are changing the headers we also need to clear the route
  // cache.
  decoding_state_.getFilterCallbacks()->downstreamCallbacks()->clearRouteCache();
}

Http::LocalErrorStatus Filter::onLocalReply(const LocalReplyData& data) {
  auto& state = getProcessorState();
  ASSERT(state.isThreadSafe());
  ENVOY_LOG(debug, "golang filter onLocalReply, state: {}, phase: {}, code: {}", state.stateStr(),
            state.phaseStr(), int(data.code_));

  return Http::LocalErrorStatus::Continue;
}

Http::FilterHeadersStatus Filter::decodeHeaders(Http::RequestHeaderMap& headers, bool end_stream) {
  ProcessorState& state = decoding_state_;

  ENVOY_LOG(debug, "golang filter decodeHeaders, state: {}, phase: {}, end_stream: {}",
            state.stateStr(), state.phaseStr(), end_stream);

  request_headers_ = &headers;

  state.setEndStream(end_stream);

  bool done = doHeaders(state, headers, end_stream);

  return done ? Http::FilterHeadersStatus::Continue : Http::FilterHeadersStatus::StopIteration;
}

Http::FilterDataStatus Filter::decodeData(Buffer::Instance& data, bool end_stream) {
  ProcessorState& state = decoding_state_;
  ENVOY_LOG(debug,
            "golang filter decodeData, state: {}, phase: {}, data length: {}, end_stream: {}",
            state.stateStr(), state.phaseStr(), data.length(), end_stream);

  state.setEndStream(end_stream);

  bool done = doData(state, data, end_stream);

  if (done) {
    state.doDataList.moveOut(data);
    return Http::FilterDataStatus::Continue;
  }

  return Http::FilterDataStatus::StopIterationNoBuffer;
}

Http::FilterTrailersStatus Filter::decodeTrailers(Http::RequestTrailerMap& trailers) {
  ProcessorState& state = decoding_state_;
  ENVOY_LOG(debug, "golang filter decodeTrailers, state: {}, phase: {}", state.stateStr(),
            state.phaseStr());

  state.setSeenTrailers();

  bool done = doTrailer(state, trailers);

  return done ? Http::FilterTrailersStatus::Continue : Http::FilterTrailersStatus::StopIteration;
}

Http::FilterHeadersStatus Filter::encodeHeaders(Http::ResponseHeaderMap& headers, bool end_stream) {
  ProcessorState& state = getProcessorState();
  ENVOY_LOG(debug, "golang filter encodeHeaders, state: {}, phase: {}, end_stream: {}",
            state.stateStr(), state.phaseStr(), end_stream);

  encoding_state_.setEndStream(end_stream);

  // NP: may enter encodeHeaders in any phase & any state_,
  // since other filters or filtermanager could call encodeHeaders or sendLocalReply in any time.
  // eg. filtermanager may invoke sendLocalReply, when scheme is invalid,
  // with "Sending local reply with details // http1.invalid_scheme" details.
  if (state.state() != FilterState::Done) {
    ENVOY_LOG(debug,
              "golang filter enter encodeHeaders early, maybe sendLocalReply or encodeHeaders "
              "happened, current state: {}, phase: {}",
              state.stateStr(), state.phaseStr());

    ENVOY_LOG(debug, "golang filter drain data buffer since enter encodeHeaders early");
    // NP: is safe to overwrite it since go code won't read it directly
    // need drain buffer to enable read when it's high watermark
    state.drainBufferData();

    // get the state before changing it.
    bool in_go = state.isProcessingInGo();

    if (in_go) {
      // NP: wait go returns to avoid concurrency conflict in go side.
      local_reply_waiting_go_ = true;
      ENVOY_LOG(debug, "waiting go returns before handle the local reply from other filter");

      // NP: save to another local_headers_ variable to avoid conflict,
      // since the headers_ may be used in Go side.
      local_headers_ = &headers;

      // can not use "StopAllIterationAndWatermark" here, since Go decodeHeaders may return
      // stopAndBuffer, that means it need data buffer and not continue header.
      return Http::FilterHeadersStatus::StopIteration;

    } else {
      ENVOY_LOG(debug, "golang filter clear do data buffer before continue encodeHeader, "
                       "since no go code is running");
      state.doDataList.clearAll();
    }
  }

  enter_encoding_ = true;

  bool done = doHeaders(encoding_state_, headers, end_stream);

  return done ? Http::FilterHeadersStatus::Continue : Http::FilterHeadersStatus::StopIteration;
}

Http::FilterDataStatus Filter::encodeData(Buffer::Instance& data, bool end_stream) {
  ProcessorState& state = getProcessorState();
  ENVOY_LOG(debug,
            "golang filter encodeData, state: {}, phase: {}, data length: {}, end_stream: {}",
            state.stateStr(), state.phaseStr(), data.length(), end_stream);

  encoding_state_.setEndStream(end_stream);

  if (local_reply_waiting_go_) {
    ENVOY_LOG(debug, "golang filter appending data to buffer");
    encoding_state_.addBufferData(data);
    return Http::FilterDataStatus::StopIterationNoBuffer;
  }

  bool done = doData(encoding_state_, data, end_stream);

  if (done) {
    state.doDataList.moveOut(data);
    return Http::FilterDataStatus::Continue;
  }

  return Http::FilterDataStatus::StopIterationNoBuffer;
}

Http::FilterTrailersStatus Filter::encodeTrailers(Http::ResponseTrailerMap& trailers) {
  ProcessorState& state = getProcessorState();
  ENVOY_LOG(debug, "golang filter encodeTrailers, state: {}, phase: {}", state.stateStr(),
            state.phaseStr());

  encoding_state_.setSeenTrailers();

  if (local_reply_waiting_go_) {
    // NP: save to another local_trailers_ variable to avoid conflict,
    // since the trailers_ may be used in Go side.
    local_trailers_ = &trailers;
    return Http::FilterTrailersStatus::StopIteration;
  }

  bool done = doTrailer(encoding_state_, trailers);

  return done ? Http::FilterTrailersStatus::Continue : Http::FilterTrailersStatus::StopIteration;
}

void Filter::onDestroy() {
  ENVOY_LOG(debug, "golang filter on destroy");

  // do nothing, stream reset may happen before entering this filter.
  if (req_ == nullptr) {
    return;
  }

  {
    Thread::LockGuard lock(mutex_);
    if (has_destroyed_) {
      ENVOY_LOG(debug, "golang filter has been destroyed");
      return;
    }
    has_destroyed_ = true;
  }

  auto& state = getProcessorState();
  auto reason = state.isProcessingInGo() ? DestroyReason::Terminate : DestroyReason::Normal;

  dynamic_lib_->envoyGoFilterOnHttpDestroy(req_, int(reason));
}

// access_log is executed before the log of the stream filter
void Filter::log(const Formatter::HttpFormatterContext& log_context,
                 const StreamInfo::StreamInfo&) {
  // `log` may be called multiple times with different log type
  switch (log_context.accessLogType()) {
  case Envoy::AccessLog::AccessLogType::DownstreamStart:
  case Envoy::AccessLog::AccessLogType::DownstreamPeriodic:
  case Envoy::AccessLog::AccessLogType::DownstreamEnd: {
    auto& state = getProcessorState();

    if (req_ == nullptr) {
      // log called by AccessLogDownstreamStart will happen before doHeaders
      initRequest(state);

      request_headers_ = static_cast<Http::RequestOrResponseHeaderMap*>(
          const_cast<Http::RequestHeaderMap*>(&log_context.requestHeaders()));
    }

    state.enterLog();
    req_->phase = static_cast<int>(state.phase());
    dynamic_lib_->envoyGoFilterOnHttpLog(req_, int(log_context.accessLogType()));
    state.leaveLog();
  } break;
  default:
    // skip calling with unsupported log types
    break;
  }
}

/*** common APIs for filter, both decode and encode ***/

GolangStatus Filter::doHeadersGo(ProcessorState& state, Http::RequestOrResponseHeaderMap& headers,
                                 bool end_stream) {
  ENVOY_LOG(debug, "golang filter passing data to golang, state: {}, phase: {}, end_stream: {}",
            state.stateStr(), state.phaseStr(), end_stream);

  if (req_ == nullptr) {
    initRequest(state);
  }

  req_->phase = static_cast<int>(state.phase());
  {
    Thread::LockGuard lock(mutex_);
    headers_ = &headers;
  }
  auto status = dynamic_lib_->envoyGoFilterOnHttpHeader(req_, end_stream ? 1 : 0, headers.size(),
                                                        headers.byteSize());
  return static_cast<GolangStatus>(status);
}

bool Filter::doHeaders(ProcessorState& state, Http::RequestOrResponseHeaderMap& headers,
                       bool end_stream) {
  ENVOY_LOG(debug, "golang filter doHeaders, state: {}, phase: {}, end_stream: {}",
            state.stateStr(), state.phaseStr(), end_stream);

  ASSERT(state.isBufferDataEmpty());

  state.processHeader(end_stream);
  auto status = doHeadersGo(state, headers, end_stream);
  auto done = state.handleHeaderGolangStatus(status);
  if (done) {
    Thread::LockGuard lock(mutex_);
    headers_ = nullptr;
  }
  return done;
}

bool Filter::doDataGo(ProcessorState& state, Buffer::Instance& data, bool end_stream) {
  ENVOY_LOG(debug, "golang filter passing data to golang, state: {}, phase: {}, end_stream: {}",
            state.stateStr(), state.phaseStr(), end_stream);

  state.processData(end_stream);

  Buffer::Instance& buffer = state.doDataList.push(data);

  ASSERT(req_ != nullptr);
  req_->phase = static_cast<int>(state.phase());
  auto status = dynamic_lib_->envoyGoFilterOnHttpData(
      req_, end_stream ? 1 : 0, reinterpret_cast<uint64_t>(&buffer), buffer.length());

  return state.handleDataGolangStatus(static_cast<GolangStatus>(status));
}

bool Filter::doData(ProcessorState& state, Buffer::Instance& data, bool end_stream) {
  ENVOY_LOG(debug, "golang filter doData, state: {}, phase: {}, end_stream: {}", state.stateStr(),
            state.phaseStr(), end_stream);

  bool done = false;
  switch (state.state()) {
  case FilterState::WaitingData:
    done = doDataGo(state, data, end_stream);
    break;
  case FilterState::WaitingAllData:
    if (end_stream) {
      if (!state.isBufferDataEmpty()) {
        // NP: new data = data_buffer_ + data
        state.addBufferData(data);
        data.move(state.getBufferData());
      }
      // check state again since data_buffer may be full and sendLocalReply with 413.
      // TODO: better not trigger 413 here.
      if (state.state() == FilterState::WaitingAllData) {
        done = doDataGo(state, data, end_stream);
      }
      break;
    }
    // NP: not break, continue
    FALLTHRU;
  case FilterState::ProcessingHeader:
  case FilterState::ProcessingData:
    ENVOY_LOG(debug, "golang filter appending data to buffer");
    state.addBufferData(data);
    break;
  default:
    ENVOY_LOG(error, "unexpected state: {}", state.stateStr());
    // TODO: terminate stream?
    break;
  }

  ENVOY_LOG(debug, "golang filter doData, return: {}", done);

  return done;
}

bool Filter::doTrailerGo(ProcessorState& state, Http::HeaderMap& trailers) {
  ENVOY_LOG(debug, "golang filter passing trailers to golang, state: {}, phase: {}",
            state.stateStr(), state.phaseStr());

  state.processTrailer();

  ASSERT(req_ != nullptr);
  req_->phase = static_cast<int>(state.phase());
  auto status =
      dynamic_lib_->envoyGoFilterOnHttpHeader(req_, 1, trailers.size(), trailers.byteSize());

  return state.handleTrailerGolangStatus(static_cast<GolangStatus>(status));
}

bool Filter::doTrailer(ProcessorState& state, Http::HeaderMap& trailers) {
  ENVOY_LOG(debug, "golang filter doTrailer, state: {}, phase: {}", state.stateStr(),
            state.phaseStr());

  ASSERT(!state.getEndStream() && !state.isProcessingEndStream());

  {
    Thread::LockGuard lock(mutex_);
    trailers_ = &trailers;
  }

  bool done = false;
  Buffer::OwnedImpl body;
  switch (state.state()) {
  case FilterState::WaitingTrailer:
    done = doTrailerGo(state, trailers);
    break;
  case FilterState::WaitingData:
    done = doTrailerGo(state, trailers);
    break;
  case FilterState::WaitingAllData:
    ENVOY_LOG(debug, "golang filter data buffer is empty: {}", state.isBufferDataEmpty());
    // do data first
    if (!state.isBufferDataEmpty()) {
      done = doDataGo(state, state.getBufferData(), false);
      // NP: can not use done as condition here, since done will be false
      // maybe we can remove the done variable totally? by using state_ only?
      // continue trailers
      if (state.state() == FilterState::WaitingTrailer) {
        state.continueDoData();
        done = doTrailerGo(state, trailers);
      }
    } else {
      state.continueDoData();
      done = doTrailerGo(state, trailers);
    }
    break;
  case FilterState::ProcessingHeader:
  case FilterState::ProcessingData:
    // do nothing, wait previous task
    break;
  default:
    ENVOY_LOG(error, "unexpected state: {}", state.stateStr());
    // TODO: terminate stream?
    break;
  }

  ENVOY_LOG(debug, "golang filter doTrailer, return: {}", done);

  return done;
}

/*** APIs for go call C ***/

void Filter::continueEncodeLocalReply(ProcessorState& state) {
  ENVOY_LOG(debug,
            "golang filter continue encodeHeader(local reply from other filters) after return from "
            "go, current state: {}, phase: {}",
            state.stateStr(), state.phaseStr());

  ENVOY_LOG(debug, "golang filter drain do data buffer before continueEncodeLocalReply");
  state.doDataList.clearAll();

  local_reply_waiting_go_ = false;
  // should use encoding_state_ now
  enter_encoding_ = true;

  auto header_end_stream = encoding_state_.getEndStream();
  if (local_trailers_ != nullptr) {
    Thread::LockGuard lock(mutex_);
    trailers_ = local_trailers_;
    header_end_stream = false;
  }
  if (!encoding_state_.isBufferDataEmpty()) {
    header_end_stream = false;
  }
  // NP: we not overwrite state end_stream in doHeadersGo
  encoding_state_.processHeader(header_end_stream);
  auto status = doHeadersGo(encoding_state_, *local_headers_, header_end_stream);
  continueStatusInternal(status);
}

void Filter::continueStatusInternal(GolangStatus status) {
  ProcessorState& state = getProcessorState();
  ASSERT(state.isThreadSafe());
  auto saved_state = state.state();

  if (local_reply_waiting_go_) {
    ENVOY_LOG(debug,
              "other filter already trigger sendLocalReply, ignoring the continue status: {}, "
              "state: {}, phase: {}",
              int(status), state.stateStr(), state.phaseStr());

    continueEncodeLocalReply(state);
    return;
  }

  auto done = state.handleGolangStatus(status);
  if (done) {
    switch (saved_state) {
    case FilterState::ProcessingHeader:
      // NP: should process data first filter seen the stream is end but go doesn't,
      // otherwise, the next filter will continue with end_stream = true.

      // NP: it is safe to continueDoData after continueProcessing
      // that means injectDecodedDataToFilterChain after continueDecoding while stream is not end
      if (state.isProcessingEndStream() || !state.isStreamEnd()) {
        state.continueProcessing();
      }
      break;

    case FilterState::ProcessingData:
      state.continueDoData();
      break;

    case FilterState::ProcessingTrailer:
      state.continueDoData();
      state.continueProcessing();
      break;

    default:
      ASSERT(0, "unexpected state");
    }
  }

  // TODO: state should also grow in this case
  // state == WaitingData && bufferData is empty && seen trailers

  auto current_state = state.state();
  if ((current_state == FilterState::WaitingData &&
       (!state.isBufferDataEmpty() || state.getEndStream())) ||
      (current_state == FilterState::WaitingAllData && state.isStreamEnd())) {
    auto done = doDataGo(state, state.getBufferData(), state.getEndStream());
    if (done) {
      state.continueDoData();
    } else {
      // do not process trailers when data is not finished
      return;
    }
  }

  Thread::ReleasableLockGuard lock(mutex_);
  if (state.state() == FilterState::WaitingTrailer && trailers_ != nullptr) {
    auto trailers = trailers_;
    lock.release();
    auto done = doTrailerGo(state, *trailers);
    if (done) {
      state.continueProcessing();
    }
  }
}

void Filter::sendLocalReplyInternal(
    Http::Code response_code, absl::string_view body_text,
    std::function<void(Http::ResponseHeaderMap& headers)> modify_headers,
    Grpc::Status::GrpcStatus grpc_status, absl::string_view details) {
  ENVOY_LOG(debug, "sendLocalReply Internal, response code: {}", int(response_code));

  ProcessorState& state = getProcessorState();

  if (local_reply_waiting_go_) {
    ENVOY_LOG(debug,
              "other filter already invoked sendLocalReply or encodeHeaders, ignoring the local "
              "reply from go, code: {}, body: {}, details: {}",
              int(response_code), body_text, details);

    continueEncodeLocalReply(state);
    return;
  }

  ENVOY_LOG(debug, "golang filter drain do data buffer before sendLocalReply");
  state.doDataList.clearAll();

  // drain buffer data if it's not empty, before sendLocalReply
  state.drainBufferData();

  state.sendLocalReply(response_code, body_text, modify_headers, grpc_status, details);
}

CAPIStatus
Filter::sendLocalReply(Http::Code response_code, std::string body_text,
                       std::function<void(Http::ResponseHeaderMap& headers)> modify_headers,
                       Grpc::Status::GrpcStatus grpc_status, std::string details) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  ENVOY_LOG(debug, "sendLocalReply, response code: {}", int(response_code));

  auto weak_ptr = weak_from_this();
  state.getDispatcher().post(
      [this, &state, weak_ptr, response_code, body_text, modify_headers, grpc_status, details] {
        if (!weak_ptr.expired() && !hasDestroyed()) {
          ASSERT(state.isThreadSafe());
          sendLocalReplyInternal(response_code, body_text, modify_headers, grpc_status, details);
        } else {
          ENVOY_LOG(debug, "golang filter has gone or destroyed in sendLocalReply");
        }
      });
  return CAPIStatus::CAPIOK;
};

CAPIStatus Filter::sendPanicReply(absl::string_view details) {
  config_->stats().panic_error_.inc();
  ENVOY_LOG(error, "[go_plugin_http][{}] {}", config_->pluginName(),
            absl::StrCat("filter paniced with error details: ", details));
  // We choose not to pass along the details in the response because
  // we don't want to leak the operational details of the service for security reasons.
  // Operators should be able to view the details via the log message above
  // and use the stats for o11y
  return sendLocalReply(Http::Code::InternalServerError, "error happened in filter\r\n", nullptr,
                        Grpc::Status::WellKnownGrpcStatus::Ok, "");
}

CAPIStatus Filter::continueStatus(GolangStatus status) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  ENVOY_LOG(debug, "golang filter continue from Go, status: {}, state: {}, phase: {}", int(status),
            state.stateStr(), state.phaseStr());

  auto weak_ptr = weak_from_this();
  // TODO: skip post event to dispatcher, and return continue in the caller,
  // when it's invoked in the current envoy thread, for better performance & latency.
  state.getDispatcher().post([this, &state, weak_ptr, status] {
    if (!weak_ptr.expired() && !hasDestroyed()) {
      ASSERT(state.isThreadSafe());
      continueStatusInternal(status);
    } else {
      ENVOY_LOG(debug, "golang filter has gone or destroyed in continueStatus event");
    }
  });
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::getHeader(absl::string_view key, uint64_t* value_data, int* value_len) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  auto m = state.isProcessingHeader() ? headers_ : trailers_;
  if (m == nullptr) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  auto result = m->get(Http::LowerCaseString(key));

  if (!result.empty()) {
    auto str = result[0]->value().getStringView();
    *value_data = reinterpret_cast<uint64_t>(str.data());
    *value_len = str.length();
  }
  return CAPIStatus::CAPIOK;
}

void copyHeaderMapToGo(Http::HeaderMap& m, GoString* go_strs, char* go_buf) {
  auto i = 0;
  m.iterate([&i, &go_strs, &go_buf](const Http::HeaderEntry& header) -> Http::HeaderMap::Iterate {
    auto key = std::string(header.key().getStringView());
    auto value = std::string(header.value().getStringView());

    auto len = key.length();
    // go_strs is the heap memory of go, and the length is twice the number of headers. So range it
    // is safe.
    go_strs[i].n = len;
    go_strs[i].p = go_buf;
    // go_buf is the heap memory of go, and the length is the total length of all keys and values in
    // the header. So use memcpy is safe.
    memcpy(go_buf, key.data(), len); // NOLINT(safe-memcpy)
    go_buf += len;
    i++;

    len = value.length();
    go_strs[i].n = len;
    go_strs[i].p = go_buf;
    memcpy(go_buf, value.data(), len); // NOLINT(safe-memcpy)
    go_buf += len;
    i++;
    return Http::HeaderMap::Iterate::Continue;
  });
}

CAPIStatus Filter::copyHeaders(GoString* go_strs, char* go_buf) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (headers_ == nullptr) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  copyHeaderMapToGo(*headers_, go_strs, go_buf);
  return CAPIStatus::CAPIOK;
}

// It won't take affect immidiately while it's invoked from a Go thread, instead, it will post a
// callback to run in the envoy worker thread.
CAPIStatus Filter::setHeader(absl::string_view key, absl::string_view value, headerAction act) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (headers_ == nullptr) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }

  if (state.isThreadSafe()) {
    // it's safe to write header in the safe thread.
    switch (act) {
    case HeaderAdd:
      headers_->addCopy(Http::LowerCaseString(key), value);
      break;

    case HeaderSet:
      headers_->setCopy(Http::LowerCaseString(key), value);
      break;

    default:
      RELEASE_ASSERT(false, absl::StrCat("unknown header action: ", act));
    }

    onHeadersModified();
  } else {
    // should deep copy the string_view before post to dipatcher callback.
    auto key_str = std::string(key);
    auto value_str = std::string(value);

    auto weak_ptr = weak_from_this();
    // dispatch a callback to write header in the envoy safe thread, to make the write operation
    // safety. otherwise, there might be race between reading in the envoy worker thread and writing
    // in the Go thread.
    state.getDispatcher().post([this, weak_ptr, key_str, value_str, act] {
      if (!weak_ptr.expired() && !hasDestroyed()) {
        Thread::LockGuard lock(mutex_);
        switch (act) {
        case HeaderAdd:
          headers_->addCopy(Http::LowerCaseString(key_str), value_str);
          break;

        case HeaderSet:
          headers_->setCopy(Http::LowerCaseString(key_str), value_str);
          break;

        default:
          RELEASE_ASSERT(false, absl::StrCat("unknown header action: ", act));
        }

        onHeadersModified();
      } else {
        ENVOY_LOG(debug, "golang filter has gone or destroyed in setHeader");
      }
    });
  }

  return CAPIStatus::CAPIOK;
}

// It won't take affect immidiately while it's invoked from a Go thread, instead, it will post a
// callback to run in the envoy worker thread.
CAPIStatus Filter::removeHeader(absl::string_view key) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (headers_ == nullptr) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  if (state.isThreadSafe()) {
    // it's safe to write header in the safe thread.
    headers_->remove(Http::LowerCaseString(key));
    onHeadersModified();
  } else {
    // should deep copy the string_view before post to dipatcher callback.
    auto key_str = std::string(key);

    auto weak_ptr = weak_from_this();
    // dispatch a callback to write header in the envoy safe thread, to make the write operation
    // safety. otherwise, there might be race between reading in the envoy worker thread and writing
    // in the Go thread.
    state.getDispatcher().post([this, weak_ptr, key_str] {
      if (!weak_ptr.expired() && !hasDestroyed()) {
        Thread::LockGuard lock(mutex_);
        headers_->remove(Http::LowerCaseString(key_str));
        onHeadersModified();
      } else {
        ENVOY_LOG(debug, "golang filter has gone or destroyed in removeHeader");
      }
    });
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::copyBuffer(Buffer::Instance* buffer, char* data) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (!state.doDataList.checkExisting(buffer)) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  for (const Buffer::RawSlice& slice : buffer->getRawSlices()) {
    // data is the heap memory of go, and the length is the total length of buffer. So use memcpy is
    // safe.
    memcpy(data, static_cast<const char*>(slice.mem_), slice.len_); // NOLINT(safe-memcpy)
    data += slice.len_;
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::drainBuffer(Buffer::Instance* buffer, uint64_t length) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (!state.doDataList.checkExisting(buffer)) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }

  buffer->drain(length);
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::setBufferHelper(Buffer::Instance* buffer, absl::string_view& value,
                                   bufferAction action) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (!state.doDataList.checkExisting(buffer)) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  if (action == bufferAction::Set) {
    buffer->drain(buffer->length());
    buffer->add(value);
  } else if (action == bufferAction::Prepend) {
    buffer->prepend(value);
  } else {
    buffer->add(value);
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::copyTrailers(GoString* go_strs, char* go_buf) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (trailers_ == nullptr) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  copyHeaderMapToGo(*trailers_, go_strs, go_buf);
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::setTrailer(absl::string_view key, absl::string_view value, headerAction act) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (trailers_ == nullptr) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  if (state.isThreadSafe()) {
    switch (act) {
    case HeaderAdd:
      trailers_->addCopy(Http::LowerCaseString(key), value);
      break;

    case HeaderSet:
      trailers_->setCopy(Http::LowerCaseString(key), value);
      break;

    default:
      RELEASE_ASSERT(false, absl::StrCat("unknown header action: ", act));
    }
  } else {
    // should deep copy the string_view before post to dipatcher callback.
    auto key_str = std::string(key);
    auto value_str = std::string(value);

    auto weak_ptr = weak_from_this();
    // dispatch a callback to write trailer in the envoy safe thread, to make the write operation
    // safety. otherwise, there might be race between reading in the envoy worker thread and
    // writing in the Go thread.
    state.getDispatcher().post([this, weak_ptr, key_str, value_str, act] {
      if (!weak_ptr.expired() && !hasDestroyed()) {
        Thread::LockGuard lock(mutex_);
        switch (act) {
        case HeaderAdd:
          trailers_->addCopy(Http::LowerCaseString(key_str), value_str);
          break;

        case HeaderSet:
          trailers_->setCopy(Http::LowerCaseString(key_str), value_str);
          break;

        default:
          RELEASE_ASSERT(false, absl::StrCat("unknown header action: ", act));
        }
      } else {
        ENVOY_LOG(debug, "golang filter has gone or destroyed in setTrailer");
      }
    });
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::removeTrailer(absl::string_view key) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }
  if (trailers_ == nullptr) {
    ENVOY_LOG(debug, "invoking cgo api at invalid phase: {}", __func__);
    return CAPIStatus::CAPIInvalidPhase;
  }
  if (state.isThreadSafe()) {
    trailers_->remove(Http::LowerCaseString(key));
  } else {
    // should deep copy the string_view before post to dipatcher callback.
    auto key_str = std::string(key);

    auto weak_ptr = weak_from_this();
    // dispatch a callback to write trailer in the envoy safe thread, to make the write operation
    // safety. otherwise, there might be race between reading in the envoy worker thread and writing
    // in the Go thread.
    state.getDispatcher().post([this, weak_ptr, key_str] {
      if (!weak_ptr.expired() && !hasDestroyed()) {
        Thread::LockGuard lock(mutex_);
        trailers_->remove(Http::LowerCaseString(key_str));
      } else {
        ENVOY_LOG(debug, "golang filter has gone or destroyed in removeTrailer");
      }
    });
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::getIntegerValue(int id, uint64_t* value) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }

  switch (static_cast<EnvoyValue>(id)) {
  case EnvoyValue::Protocol:
    if (!state.streamInfo().protocol().has_value()) {
      return CAPIStatus::CAPIValueNotFound;
    }
    *value = static_cast<uint64_t>(state.streamInfo().protocol().value());
    break;
  case EnvoyValue::ResponseCode:
    if (!state.streamInfo().responseCode().has_value()) {
      return CAPIStatus::CAPIValueNotFound;
    }
    *value = state.streamInfo().responseCode().value();
    break;
  case EnvoyValue::AttemptCount:
    if (!state.streamInfo().attemptCount().has_value()) {
      return CAPIStatus::CAPIValueNotFound;
    }
    *value = state.streamInfo().attemptCount().value();
    break;
  default:
    RELEASE_ASSERT(false, absl::StrCat("invalid integer value id: ", id));
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::getStringValue(int id, uint64_t* value_data, int* value_len) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }
  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }

  // refer the string to req_->strValue, not deep clone, make sure it won't be freed while reading
  // it on the Go side.
  switch (static_cast<EnvoyValue>(id)) {
  case EnvoyValue::RouteName:
    req_->strValue = state.streamInfo().getRouteName();
    break;
  case EnvoyValue::FilterChainName: {
    const auto filter_chain_info = state.streamInfo().downstreamAddressProvider().filterChainInfo();
    req_->strValue =
        filter_chain_info.has_value() ? std::string(filter_chain_info->name()) : std::string();
    break;
  }
  case EnvoyValue::ResponseCodeDetails:
    if (!state.streamInfo().responseCodeDetails().has_value()) {
      return CAPIStatus::CAPIValueNotFound;
    }
    req_->strValue = state.streamInfo().responseCodeDetails().value();
    break;
  case EnvoyValue::DownstreamLocalAddress:
    req_->strValue = state.streamInfo().downstreamAddressProvider().localAddress()->asString();
    break;
  case EnvoyValue::DownstreamRemoteAddress:
    req_->strValue = state.streamInfo().downstreamAddressProvider().remoteAddress()->asString();
    break;
  case EnvoyValue::UpstreamLocalAddress:
    if (state.streamInfo().upstreamInfo() &&
        state.streamInfo().upstreamInfo()->upstreamLocalAddress()) {
      req_->strValue = state.streamInfo().upstreamInfo()->upstreamLocalAddress()->asString();
    } else {
      return CAPIStatus::CAPIValueNotFound;
    }
    break;
  case EnvoyValue::UpstreamRemoteAddress:
    if (state.streamInfo().upstreamInfo() &&
        state.streamInfo().upstreamInfo()->upstreamRemoteAddress()) {
      req_->strValue = state.streamInfo().upstreamInfo()->upstreamRemoteAddress()->asString();
    } else {
      return CAPIStatus::CAPIValueNotFound;
    }
    break;
  case EnvoyValue::UpstreamClusterName:
    if (state.streamInfo().upstreamClusterInfo().has_value() &&
        state.streamInfo().upstreamClusterInfo().value()) {
      req_->strValue = state.streamInfo().upstreamClusterInfo().value()->name();
    } else {
      return CAPIStatus::CAPIValueNotFound;
    }
    break;
  case EnvoyValue::VirtualClusterName:
    if (!state.streamInfo().virtualClusterName().has_value()) {
      return CAPIStatus::CAPIValueNotFound;
    }
    req_->strValue = state.streamInfo().virtualClusterName().value();
    break;
  default:
    RELEASE_ASSERT(false, absl::StrCat("invalid string value id: ", id));
  }

  *value_data = reinterpret_cast<uint64_t>(req_->strValue.data());
  *value_len = req_->strValue.length();
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::getDynamicMetadata(const std::string& filter_name, uint64_t* buf_data,
                                      int* buf_len) {
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }

  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }

  if (!state.isThreadSafe()) {
    auto weak_ptr = weak_from_this();
    ENVOY_LOG(debug, "golang filter getDynamicMetadata posting request to dispatcher");
    state.getDispatcher().post([this, &state, weak_ptr, filter_name, buf_data, buf_len] {
      ENVOY_LOG(debug, "golang filter getDynamicMetadata request in worker thread");
      if (!weak_ptr.expired() && !hasDestroyed()) {
        populateSliceWithMetadata(state, filter_name, buf_data, buf_len);
        dynamic_lib_->envoyGoRequestSemaDec(req_);
      } else {
        ENVOY_LOG(info, "golang filter has gone or destroyed in getDynamicMetadata");
      }
    });
    return CAPIStatus::CAPIYield;
  } else {
    ENVOY_LOG(debug, "golang filter getDynamicMetadata replying directly");
    populateSliceWithMetadata(state, filter_name, buf_data, buf_len);
  }

  return CAPIStatus::CAPIOK;
}

void Filter::populateSliceWithMetadata(ProcessorState& state, const std::string& filter_name,
                                       uint64_t* buf_data, int* buf_len) {
  const auto& metadata = state.streamInfo().dynamicMetadata().filter_metadata();
  const auto filter_it = metadata.find(filter_name);
  if (filter_it != metadata.end()) {
    filter_it->second.SerializeToString(&req_->strValue);
    *buf_data = reinterpret_cast<uint64_t>(req_->strValue.data());
    *buf_len = req_->strValue.length();
  }
}

CAPIStatus Filter::setDynamicMetadata(std::string filter_name, std::string key,
                                      absl::string_view buf) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }

  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }

  if (!state.isThreadSafe()) {
    auto weak_ptr = weak_from_this();
    // Since go only waits for the CAPI return code we need to create a deep copy
    // of the buffer slice and pass that to the dispatcher.
    auto buff_copy = std::string(buf);
    state.getDispatcher().post([this, &state, weak_ptr, filter_name, key, buff_copy] {
      if (!weak_ptr.expired() && !hasDestroyed()) {
        ASSERT(state.isThreadSafe());
        setDynamicMetadataInternal(state, filter_name, key, buff_copy);
      } else {
        ENVOY_LOG(info, "golang filter has gone or destroyed in setDynamicMetadata");
      }
    });
    return CAPIStatus::CAPIOK;
  }

  // it's safe to do it here since we are in the safe envoy worker thread now.
  setDynamicMetadataInternal(state, filter_name, key, buf);
  return CAPIStatus::CAPIOK;
}

void Filter::setDynamicMetadataInternal(ProcessorState& state, std::string filter_name,
                                        std::string key, const absl::string_view& buf) {
  ProtobufWkt::Struct value;
  ProtobufWkt::Value v;
  v.ParseFromArray(buf.data(), buf.length());

  (*value.mutable_fields())[key] = v;

  state.streamInfo().setDynamicMetadata(filter_name, value);
}

CAPIStatus Filter::setStringFilterState(absl::string_view key, absl::string_view value,
                                        int state_type, int life_span, int stream_sharing) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }

  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }

  if (state.isThreadSafe()) {
    state.streamInfo().filterState()->setData(
        key, std::make_shared<GoStringFilterState>(value),
        static_cast<StreamInfo::FilterState::StateType>(state_type),
        static_cast<StreamInfo::FilterState::LifeSpan>(life_span),
        static_cast<StreamInfo::StreamSharingMayImpactPooling>(stream_sharing));
  } else {
    auto key_str = std::string(key);
    auto filter_state = std::make_shared<GoStringFilterState>(value);
    auto weak_ptr = weak_from_this();
    state.getDispatcher().post(
        [this, &state, weak_ptr, key_str, filter_state, state_type, life_span, stream_sharing] {
          if (!weak_ptr.expired() && !hasDestroyed()) {
            Thread::LockGuard lock(mutex_);
            state.streamInfo().filterState()->setData(
                key_str, filter_state, static_cast<StreamInfo::FilterState::StateType>(state_type),
                static_cast<StreamInfo::FilterState::LifeSpan>(life_span),
                static_cast<StreamInfo::StreamSharingMayImpactPooling>(stream_sharing));
          } else {
            ENVOY_LOG(info, "golang filter has gone or destroyed in setStringFilterState");
          }
        });
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::getStringFilterState(absl::string_view key, uint64_t* value_data,
                                        int* value_len) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }

  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }

  if (state.isThreadSafe()) {
    auto go_filter_state =
        state.streamInfo().filterState()->getDataReadOnly<GoStringFilterState>(key);
    if (go_filter_state) {
      req_->strValue = go_filter_state->value();
      *value_data = reinterpret_cast<uint64_t>(req_->strValue.data());
      *value_len = req_->strValue.length();
    }
  } else {
    auto key_str = std::string(key);
    auto weak_ptr = weak_from_this();
    state.getDispatcher().post([this, &state, weak_ptr, key_str, value_data, value_len] {
      if (!weak_ptr.expired() && !hasDestroyed()) {
        auto go_filter_state =
            state.streamInfo().filterState()->getDataReadOnly<GoStringFilterState>(key_str);
        if (go_filter_state) {
          req_->strValue = go_filter_state->value();
          *value_data = reinterpret_cast<uint64_t>(req_->strValue.data());
          *value_len = req_->strValue.length();
        }
        dynamic_lib_->envoyGoRequestSemaDec(req_);
      } else {
        ENVOY_LOG(info, "golang filter has gone or destroyed in getStringFilterState");
      }
    });
    return CAPIStatus::CAPIYield;
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus Filter::getStringProperty(absl::string_view path, uint64_t* value_data, int* value_len,
                                     int* rc) {
  // lock until this function return since it may running in a Go thread.
  Thread::LockGuard lock(mutex_);
  if (has_destroyed_) {
    ENVOY_LOG(debug, "golang filter has been destroyed");
    return CAPIStatus::CAPIFilterIsDestroy;
  }

  auto& state = getProcessorState();
  if (!state.isProcessingInGo()) {
    ENVOY_LOG(debug, "golang filter is not processing Go");
    return CAPIStatus::CAPINotInGo;
  }

  // to access the headers_ and its friends we need to hold the lock
  activation_request_headers_ = dynamic_cast<const Http::RequestHeaderMap*>(request_headers_);
  if (enter_encoding_) {
    activation_response_headers_ = dynamic_cast<const Http::ResponseHeaderMap*>(headers_);
    activation_response_trailers_ = dynamic_cast<const Http::ResponseTrailerMap*>(trailers_);
  }

  if (state.isThreadSafe()) {
    return getStringPropertyCommon(path, value_data, value_len, state);
  }

  auto weak_ptr = weak_from_this();
  state.getDispatcher().post([this, &state, weak_ptr, path, value_data, value_len, rc] {
    if (!weak_ptr.expired() && !hasDestroyed()) {
      *rc = getStringPropertyCommon(path, value_data, value_len, state);
      dynamic_lib_->envoyGoRequestSemaDec(req_);
    } else {
      ENVOY_LOG(info, "golang filter has gone or destroyed in getStringProperty");
    }
  });
  return CAPIStatus::CAPIYield;
}

CAPIStatus Filter::getStringPropertyCommon(absl::string_view path, uint64_t* value_data,
                                           int* value_len, ProcessorState& state) {
  activation_info_ = &state.streamInfo();
  CAPIStatus status = getStringPropertyInternal(path, &req_->strValue);
  if (status == CAPIStatus::CAPIOK) {
    *value_data = reinterpret_cast<uint64_t>(req_->strValue.data());
    *value_len = req_->strValue.length();
  }
  return status;
}

absl::optional<google::api::expr::runtime::CelValue> Filter::findValue(absl::string_view name,
                                                                       Protobuf::Arena* arena) {
  // as we already support getting/setting FilterState, we don't need to implement
  // getProperty with non-attribute name & setProperty which actually work on FilterState
  return StreamActivation::FindValue(name, arena);
  // we don't need to call resetActivation as activation_xx_ is overridden when we get property
}

CAPIStatus Filter::getStringPropertyInternal(absl::string_view path, std::string* result) {
  using google::api::expr::runtime::CelValue;

  bool first = true;
  CelValue value;
  Protobuf::Arena arena;

  size_t start = 0;
  while (true) {
    if (start >= path.size()) {
      break;
    }

    size_t end = path.find('.', start);
    if (end == absl::string_view::npos) {
      end = start + path.size();
    }
    auto part = path.substr(start, end - start);
    start = end + 1;

    if (first) {
      // top-level identifier
      first = false;
      auto top_value = findValue(toAbslStringView(part), &arena);
      if (!top_value.has_value()) {
        return CAPIStatus::CAPIValueNotFound;
      }
      value = top_value.value();
    } else if (value.IsMap()) {
      auto& map = *value.MapOrDie();
      auto field = map[CelValue::CreateStringView(toAbslStringView(part))];
      if (!field.has_value()) {
        return CAPIStatus::CAPIValueNotFound;
      }
      value = field.value();
    } else if (value.IsMessage()) {
      auto msg = value.MessageOrDie();
      if (msg == nullptr) {
        return CAPIStatus::CAPIValueNotFound;
      }
      const Protobuf::Descriptor* desc = msg->GetDescriptor();
      const Protobuf::FieldDescriptor* field_desc = desc->FindFieldByName(std::string(part));
      if (field_desc == nullptr) {
        return CAPIStatus::CAPIValueNotFound;
      }
      if (field_desc->is_map()) {
        value = CelValue::CreateMap(
            Protobuf::Arena::Create<google::api::expr::runtime::FieldBackedMapImpl>(
                &arena, msg, field_desc, &arena));
      } else if (field_desc->is_repeated()) {
        value = CelValue::CreateList(
            Protobuf::Arena::Create<google::api::expr::runtime::FieldBackedListImpl>(
                &arena, msg, field_desc, &arena));
      } else {
        auto status =
            google::api::expr::runtime::CreateValueFromSingleField(msg, field_desc, &arena, &value);
        if (!status.ok()) {
          return CAPIStatus::CAPIInternalFailure;
        }
      }
    } else if (value.IsList()) {
      auto& list = *value.ListOrDie();
      int idx = 0;
      if (!absl::SimpleAtoi(toAbslStringView(part), &idx)) {
        return CAPIStatus::CAPIValueNotFound;
      }
      if (idx < 0 || idx >= list.size()) {
        return CAPIStatus::CAPIValueNotFound;
      }
      value = list[idx];
    } else {
      return CAPIStatus::CAPIValueNotFound;
    }
  }

  return serializeStringValue(value, result);
}

CAPIStatus Filter::serializeStringValue(Filters::Common::Expr::CelValue value,
                                        std::string* result) {
  using Filters::Common::Expr::CelValue;
  const Protobuf::Message* out_message;

  switch (value.type()) {
  case CelValue::Type::kString:
    result->assign(value.StringOrDie().value().data(), value.StringOrDie().value().size());
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kBytes:
    result->assign(value.BytesOrDie().value().data(), value.BytesOrDie().value().size());
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kInt64:
    result->assign(absl::StrCat(value.Int64OrDie()));
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kUint64:
    result->assign(absl::StrCat(value.Uint64OrDie()));
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kDouble:
    result->assign(absl::StrCat(value.DoubleOrDie()));
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kBool:
    result->assign(value.BoolOrDie() ? "true" : "false");
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kDuration:
    result->assign(absl::FormatDuration(value.DurationOrDie()));
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kTimestamp:
    result->assign(absl::FormatTime(value.TimestampOrDie(), absl::UTCTimeZone()));
    return CAPIStatus::CAPIOK;
  case CelValue::Type::kMessage:
    out_message = value.MessageOrDie();
    result->clear();
    if (!out_message || out_message->SerializeToString(result)) {
      return CAPIStatus::CAPIOK;
    }
    return CAPIStatus::CAPISerializationFailure;
  case CelValue::Type::kMap: {
    // so far, only headers/trailers/filter state are in Map format, and we already have API to
    // fetch them
    ENVOY_LOG(error, "map type property result is not supported yet");
    return CAPIStatus::CAPISerializationFailure;
  }
  case CelValue::Type::kList: {
    ENVOY_LOG(error, "list type property result is not supported yet");
    return CAPIStatus::CAPISerializationFailure;
  }
  default:
    return CAPIStatus::CAPISerializationFailure;
  }
}

void Filter::initRequest(ProcessorState& state) {
  // req is used by go, so need to use raw memory and then it is safe to release at the gc
  // finalize phase of the go object.
  req_ = new httpRequestInternal(weak_from_this());
  req_->configId = getMergedConfigId(state);
  req_->plugin_name.data = config_->pluginName().data();
  req_->plugin_name.len = config_->pluginName().length();
}

/* ConfigId */

uint64_t Filter::getMergedConfigId(ProcessorState& state) {
  Http::StreamFilterCallbacks* callbacks = state.getFilterCallbacks();

  // get all of the per route config
  std::list<const FilterConfigPerRoute*> route_config_list;
  callbacks->traversePerFilterConfig(
      [&route_config_list](const Router::RouteSpecificFilterConfig& cfg) {
        route_config_list.push_back(dynamic_cast<const FilterConfigPerRoute*>(&cfg));
      });

  ENVOY_LOG(debug, "golang filter route config list length: {}.", route_config_list.size());

  auto id = config_->getConfigId();
  for (auto it : route_config_list) {
    auto route_config = *it;
    id = route_config.getPluginConfigId(id, config_->pluginName());
  }

  return id;
}

/*** FilterConfig ***/

FilterConfig::FilterConfig(
    const envoy::extensions::filters::http::golang::v3alpha::Config& proto_config,
    Dso::HttpFilterDsoPtr dso_lib, const std::string& stats_prefix,
    Server::Configuration::FactoryContext& context)
    : plugin_name_(proto_config.plugin_name()), so_id_(proto_config.library_id()),
      so_path_(proto_config.library_path()), plugin_config_(proto_config.plugin_config()),
      stats_(GolangFilterStats::generateStats(stats_prefix, context.scope())), dso_lib_(dso_lib),
      metric_store_(std::make_shared<MetricStore>(context.scope().createScope(""))){};

void FilterConfig::newGoPluginConfig() {
  ENVOY_LOG(debug, "initializing golang filter config");
  std::string buf;
  auto res = plugin_config_.SerializeToString(&buf);
  ASSERT(res, "SerializeToString should always successful");
  auto buf_ptr = reinterpret_cast<unsigned long long>(buf.data());
  auto name_ptr = reinterpret_cast<unsigned long long>(plugin_name_.data());

  config_ = new httpConfigInternal(weak_from_this());
  config_->plugin_name_ptr = name_ptr;
  config_->plugin_name_len = plugin_name_.length();
  config_->config_ptr = buf_ptr;
  config_->config_len = buf.length();
  config_->is_route_config = 0;

  config_id_ = dso_lib_->envoyGoFilterNewHttpPluginConfig(config_);

  if (config_id_ == 0) {
    throw EnvoyException(
        fmt::format("golang filter failed to parse plugin config: {} {}", so_id_, so_path_));
  }

  ENVOY_LOG(debug, "golang filter new plugin config, id: {}", config_id_);
}

FilterConfig::~FilterConfig() {
  if (config_id_ > 0) {
    dso_lib_->envoyGoFilterDestroyHttpPluginConfig(config_id_, 0);
  }
}

CAPIStatus FilterConfig::defineMetric(uint32_t metric_type, absl::string_view name,
                                      uint32_t* metric_id) {
  Thread::LockGuard lock(mutex_);
  if (metric_type > static_cast<uint32_t>(MetricType::Max)) {
    return CAPIStatus::CAPIValueNotFound;
  }

  auto type = static_cast<MetricType>(metric_type);

  Stats::StatNameManagedStorage storage(name, metric_store_->scope_->symbolTable());
  Stats::StatName stat_name = storage.statName();
  if (type == MetricType::Counter) {
    auto id = metric_store_->nextCounterMetricId();
    auto c = &metric_store_->scope_->counterFromStatName(stat_name);
    metric_store_->counters_.emplace(id, c);
    *metric_id = id;
  } else if (type == MetricType::Gauge) {
    auto id = metric_store_->nextGaugeMetricId();
    auto g =
        &metric_store_->scope_->gaugeFromStatName(stat_name, Stats::Gauge::ImportMode::Accumulate);
    metric_store_->gauges_.emplace(id, g);
    *metric_id = id;
  } else { // (type == MetricType::Histogram)
    ASSERT(type == MetricType::Histogram);
    auto id = metric_store_->nextHistogramMetricId();
    auto h = &metric_store_->scope_->histogramFromStatName(stat_name,
                                                           Stats::Histogram::Unit::Unspecified);
    metric_store_->histograms_.emplace(id, h);
    *metric_id = id;
  }

  return CAPIStatus::CAPIOK;
}

CAPIStatus FilterConfig::incrementMetric(uint32_t metric_id, int64_t offset) {
  Thread::LockGuard lock(mutex_);
  auto type = static_cast<MetricType>(metric_id & MetricStore::kMetricTypeMask);
  if (type == MetricType::Counter) {
    auto it = metric_store_->counters_.find(metric_id);
    if (it != metric_store_->counters_.end()) {
      if (offset > 0) {
        it->second->add(offset);
      }
    }
  } else if (type == MetricType::Gauge) {
    auto it = metric_store_->gauges_.find(metric_id);
    if (it != metric_store_->gauges_.end()) {
      if (offset > 0) {
        it->second->add(offset);
      } else {
        it->second->sub(-offset);
      }
    }
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus FilterConfig::getMetric(uint32_t metric_id, uint64_t* value) {
  Thread::LockGuard lock(mutex_);
  auto type = static_cast<MetricType>(metric_id & MetricStore::kMetricTypeMask);
  if (type == MetricType::Counter) {
    auto it = metric_store_->counters_.find(metric_id);
    if (it != metric_store_->counters_.end()) {
      *value = it->second->value();
    }
  } else if (type == MetricType::Gauge) {
    auto it = metric_store_->gauges_.find(metric_id);
    if (it != metric_store_->gauges_.end()) {
      *value = it->second->value();
    }
  }
  return CAPIStatus::CAPIOK;
}

CAPIStatus FilterConfig::recordMetric(uint32_t metric_id, uint64_t value) {
  Thread::LockGuard lock(mutex_);
  auto type = static_cast<MetricType>(metric_id & MetricStore::kMetricTypeMask);
  if (type == MetricType::Counter) {
    auto it = metric_store_->counters_.find(metric_id);
    if (it != metric_store_->counters_.end()) {
      it->second->add(value);
    }
  } else if (type == MetricType::Gauge) {
    auto it = metric_store_->gauges_.find(metric_id);
    if (it != metric_store_->gauges_.end()) {
      it->second->set(value);
    }
  } else {
    ASSERT(type == MetricType::Histogram);
    auto it = metric_store_->histograms_.find(metric_id);
    if (it != metric_store_->histograms_.end()) {
      it->second->recordValue(value);
    }
  }
  return CAPIStatus::CAPIOK;
}

uint64_t FilterConfig::getConfigId() { return config_id_; }

FilterConfigPerRoute::FilterConfigPerRoute(
    const envoy::extensions::filters::http::golang::v3alpha::ConfigsPerRoute& config,
    Server::Configuration::ServerFactoryContext&) {
  // NP: dso may not loaded yet, can not invoke envoyGoFilterNewHttpPluginConfig yet.
  ENVOY_LOG(debug, "initializing per route golang filter config");

  for (const auto& it : config.plugins_config()) {
    auto plugin_name = it.first;
    auto route_plugin = it.second;
    RoutePluginConfigPtr conf(new RoutePluginConfig(plugin_name, route_plugin));
    ENVOY_LOG(debug, "per route golang filter config, type_url: {}",
              route_plugin.config().type_url());
    plugins_config_.insert({plugin_name, std::move(conf)});
  }
}

uint64_t FilterConfigPerRoute::getPluginConfigId(uint64_t parent_id,
                                                 std::string plugin_name) const {
  auto it = plugins_config_.find(plugin_name);
  if (it != plugins_config_.end()) {
    return it->second->getMergedConfigId(parent_id);
  }
  ENVOY_LOG(debug, "golang filter not found plugin config: {}", plugin_name);
  // not found
  return parent_id;
}

RoutePluginConfig::RoutePluginConfig(
    const std::string plugin_name,
    const envoy::extensions::filters::http::golang::v3alpha::RouterPlugin& config)
    : plugin_name_(plugin_name), plugin_config_(config.config()) {

  ENVOY_LOG(debug, "initializing golang filter route plugin config, plugin_name: {}, type_url: {}",
            plugin_name_, config.config().type_url());

  dso_lib_ = Dso::DsoManager<Dso::HttpFilterDsoImpl>::getDsoByPluginName(plugin_name_);
  if (dso_lib_ == nullptr) {
    // RoutePluginConfig may be created before FilterConfig, so dso_lib_ may be null.
    // i.e. per route config is used in LDS route_config.
    return;
  }

  config_id_ = getConfigId();
  if (config_id_ == 0) {
    throw EnvoyException(
        fmt::format("golang filter failed to parse plugin config: {}", plugin_name_));
  }
  ENVOY_LOG(debug, "golang filter new per route '{}' plugin config, id: {}", plugin_name_,
            config_id_);
};

RoutePluginConfig::~RoutePluginConfig() {
  absl::WriterMutexLock lock(&mutex_);
  if (config_id_ > 0) {
    dso_lib_->envoyGoFilterDestroyHttpPluginConfig(config_id_, 0);
  }
  if (merged_config_id_ > 0 && config_id_ != merged_config_id_) {
    dso_lib_->envoyGoFilterDestroyHttpPluginConfig(merged_config_id_, 0);
  }
}

uint64_t RoutePluginConfig::getConfigId() {
  if (dso_lib_ == nullptr) {
    dso_lib_ = Dso::DsoManager<Dso::HttpFilterDsoImpl>::getDsoByPluginName(plugin_name_);
    ASSERT(dso_lib_ != nullptr, "load at the request time, so it should not be null");
  }

  std::string buf;
  auto res = plugin_config_.SerializeToString(&buf);
  ASSERT(res, "SerializeToString is always successful");
  auto buf_ptr = reinterpret_cast<unsigned long long>(buf.data());
  auto name_ptr = reinterpret_cast<unsigned long long>(plugin_name_.data());

  config_ = new httpConfig();
  config_->plugin_name_ptr = name_ptr;
  config_->plugin_name_len = plugin_name_.length();
  config_->config_ptr = buf_ptr;
  config_->config_len = buf.length();
  config_->is_route_config = 1;
  return dso_lib_->envoyGoFilterNewHttpPluginConfig(config_);
};

uint64_t RoutePluginConfig::getMergedConfigId(uint64_t parent_id) {
  {
    // this is the fast path for most cases.
    absl::ReaderMutexLock lock(&mutex_);
    if (merged_config_id_ > 0 && cached_parent_id_ == parent_id) {
      return merged_config_id_;
    }
  }
  absl::WriterMutexLock lock(&mutex_);
  if (merged_config_id_ > 0) {
    if (cached_parent_id_ == parent_id) {
      return merged_config_id_;
    }
    // upper level config changed, merged_config_id_ is outdated.
    // there is a concurrency race:
    // 1. when A envoy worker thread is using the cached merged_config_id_ and it will call into Go
    //    after some time.
    // 2. while B envoy worker thread may update the merged_config_id_ in getMergedConfigId, that
    //    will delete the id.
    // so, we delay deleting the id in the Go side.
    dso_lib_->envoyGoFilterDestroyHttpPluginConfig(merged_config_id_, 1);
  }

  if (config_id_ == 0) {
    config_id_ = getConfigId();
    RELEASE_ASSERT(config_id_, "TODO: terminate request or passthrough");
  }

  auto name_ptr = reinterpret_cast<unsigned long long>(plugin_name_.data());
  merged_config_id_ = dso_lib_->envoyGoFilterMergeHttpPluginConfig(name_ptr, plugin_name_.length(),
                                                                   parent_id, config_id_);
  ASSERT(merged_config_id_, "config id is always grows");
  ENVOY_LOG(debug, "golang filter merge '{}' plugin config, from {} + {} to {}", plugin_name_,
            parent_id, config_id_, merged_config_id_);

  cached_parent_id_ = parent_id;
  return merged_config_id_;
};

/* ProcessorState */
ProcessorState& Filter::getProcessorState() {
  return enter_encoding_ ? dynamic_cast<ProcessorState&>(encoding_state_)
                         : dynamic_cast<ProcessorState&>(decoding_state_);
};

} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/golang/filters/http/source/config.h"

#include "envoy/registry/registry.h"

#include "source/common/common/fmt.h"

#include "contrib/golang/common/dso/dso.h"
#include "contrib/golang/filters/http/source/golang_filter.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Golang {

Http::FilterFactoryCb GolangFilterConfig::createFilterFactoryFromProtoTyped(
    const envoy::extensions::filters::http::golang::v3alpha::Config& proto_config,
    const std::string& stats_prefix, Server::Configuration::FactoryContext& context) {

  ENVOY_LOG_MISC(debug, "load golang library at parse config: {} {}", proto_config.library_id(),
                 proto_config.library_path());

  // loads DSO store a static map and a open handles leak will occur when the filter gets loaded and
  // unloaded.
  // TODO: unload DSO when filter updated.
  auto dso_lib = Dso::DsoManager<Dso::HttpFilterDsoImpl>::load(
      proto_config.library_id(), proto_config.library_path(), proto_config.plugin_name());
  if (dso_lib == nullptr) {
    throw EnvoyException(fmt::format("golang_filter: load library failed: {} {}",
                                     proto_config.library_id(), proto_config.library_path()));
  }

  FilterConfigSharedPtr config = std::make_shared<FilterConfig>(
      proto_config, dso_lib, fmt::format("{}golang.", stats_prefix), context);
  config->newGoPluginConfig();
  return [config, dso_lib](Http::FilterChainFactoryCallbacks& callbacks) {
    auto filter = std::make_shared<Filter>(config, dso_lib);
    callbacks.addStreamFilter(filter);
    callbacks.addAccessLogHandler(filter);
  };
}

Router::RouteSpecificFilterConfigConstSharedPtr
GolangFilterConfig::createRouteSpecificFilterConfigTyped(
    const envoy::extensions::filters::http::golang::v3alpha::ConfigsPerRoute& proto_config,
    Server::Configuration::ServerFactoryContext& context, ProtobufMessage::ValidationVisitor&) {
  return std::make_shared<FilterConfigPerRoute>(proto_config, context);
}

/**
 * Static registration for the Golang filter. @see RegisterFactory.
 */
REGISTER_FACTORY(GolangFilterConfig, Server::Configuration::NamedHttpFilterConfigFactory);

} // namespace Golang
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load("@io_bazel_rules_go//go:def.bzl", "go_library")

licenses(["notice"])  # Apache 2

go_library(
    name = "utils",
    srcs = ["string.go"],
    importpath = "github.com/envoyproxy/envoy/contrib/golang/common/go/utils",
    visibility = ["//visibility:public"],
)
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package utils

import (
	"reflect"
	"unsafe"
)

func BytesToString(ptr uint64, len uint64) string {
	var s string
	var sHdr = (*reflect.StringHeader)(unsafe.Pointer(&s))
	sHdr.Data = uintptr(ptr)
	sHdr.Len = int(len)
	return s
}

func BytesToSlice(ptr uint64, len uint64) []byte {
	var s []byte
	var sHdr = (*reflect.SliceHeader)(unsafe.Pointer(&s))
	sHdr.Data = uintptr(ptr)
	sHdr.Len = int(len)
	sHdr.Cap = int(len)
	return s
}

// BufferToSlice convert the memory buffer from C to a slice with reserved len.
func BufferToSlice(ptr uint64, len uint64) []byte {
	var s []byte
	var sHdr = (*reflect.SliceHeader)(unsafe.Pointer(&s))
	sHdr.Data = uintptr(ptr)
	sHdr.Len = int(len)
	sHdr.Cap = int(len)
	return s
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package api

import "unsafe"

type HttpCAPI interface {
	HttpContinue(r unsafe.Pointer, status uint64)
	HttpSendLocalReply(r unsafe.Pointer, responseCode int, bodyText string, headers map[string][]string, grpcStatus int64, details string)

	// Send a specialized reply that indicates that the filter has failed on the go side. Internally this is used for
	// when unhandled panics are detected.
	HttpSendPanicReply(r unsafe.Pointer, details string)
	// experience api, memory unsafe
	HttpGetHeader(r unsafe.Pointer, key string) string
	HttpCopyHeaders(r unsafe.Pointer, num uint64, bytes uint64) map[string][]string
	HttpSetHeader(r unsafe.Pointer, key string, value string, add bool)
	HttpRemoveHeader(r unsafe.Pointer, key string)

	HttpGetBuffer(r unsafe.Pointer, bufferPtr uint64, length uint64) []byte
	HttpDrainBuffer(r unsafe.Pointer, bufferPtr uint64, length uint64)
	HttpSetBufferHelper(r unsafe.Pointer, bufferPtr uint64, value string, action BufferAction)
	HttpSetBytesBufferHelper(r unsafe.Pointer, bufferPtr uint64, value []byte, action BufferAction)

	HttpCopyTrailers(r unsafe.Pointer, num uint64, bytes uint64) map[string][]string
	HttpSetTrailer(r unsafe.Pointer, key string, value string, add bool)
	HttpRemoveTrailer(r unsafe.Pointer, key string)

	HttpGetStringValue(r unsafe.Pointer, id int) (string, bool)
	HttpGetIntegerValue(r unsafe.Pointer, id int) (uint64, bool)

	HttpGetDynamicMetadata(r unsafe.Pointer, filterName string) map[string]interface{}
	HttpSetDynamicMetadata(r unsafe.Pointer, filterName string, key string, value interface{})

	HttpLog(level LogType, message string)
	HttpLogLevel() LogType

	HttpFinalize(r unsafe.Pointer, reason int)
	HttpConfigFinalize(c unsafe.Pointer)

	HttpSetStringFilterState(r unsafe.Pointer, key string, value string, stateType StateType, lifeSpan LifeSpan, streamSharing StreamSharing)
	HttpGetStringFilterState(r unsafe.Pointer, key string) string

	HttpGetStringProperty(r unsafe.Pointer, key string) (string, error)

	HttpDefineMetric(c unsafe.Pointer, metricType MetricType, name string) uint32
	HttpIncrementMetric(c unsafe.Pointer, metricId uint32, offset int64)
	HttpGetMetric(c unsafe.Pointer, metricId uint32) uint64
	HttpRecordMetric(c unsafe.Pointer, metricId uint32, value uint64)
}

type NetworkCAPI interface {
	// DownstreamWrite writes buffer data into downstream connection.
	DownstreamWrite(f unsafe.Pointer, bufferPtr unsafe.Pointer, bufferLen int, endStream int)
	// DownstreamClose closes the downstream connection
	DownstreamClose(f unsafe.Pointer, closeType int)
	// DownstreamFinalize cleans up the resource of downstream connection, should be called only by runtime.SetFinalizer
	DownstreamFinalize(f unsafe.Pointer, reason int)
	// DownstreamInfo gets the downstream connection info of infoType
	DownstreamInfo(f unsafe.Pointer, infoType int) string
	// GetFilterState gets the filter state of key
	GetFilterState(f unsafe.Pointer, key string) string
	// SetFilterState sets the filter state of key to value
	SetFilterState(f unsafe.Pointer, key string, value string, stateType StateType, lifeSpan LifeSpan, streamSharing StreamSharing)

	// UpstreamConnect creates an envoy upstream connection to address
	UpstreamConnect(libraryID string, addr string, connID uint64) unsafe.Pointer
	// UpstreamWrite writes buffer data into upstream connection.
	UpstreamWrite(f unsafe.Pointer, bufferPtr unsafe.Pointer, bufferLen int, endStream int)
	// UpstreamClose closes the upstream connection
	UpstreamClose(f unsafe.Pointer, closeType int)
	// UpstreamFinalize cleans up the resource of upstream connection, should be called only by runtime.SetFinalizer
	UpstreamFinalize(f unsafe.Pointer, reason int)
	// UpstreamInfo gets the upstream connection info of infoType
	UpstreamInfo(f unsafe.Pointer, infoType int) string
}

type CommonCAPI interface {
	Log(level LogType, message string)
	LogLevel() LogType
}

type commonCApiImpl struct{}

var cAPI CommonCAPI = &commonCApiImpl{}

// SetCommonCAPI for mock cAPI
func SetCommonCAPI(api CommonCAPI) {
	cAPI = api
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package api

import "errors"

// ****************** filter status start ******************//
type StatusType int

const (
	Running                StatusType = 0
	LocalReply             StatusType = 1
	Continue               StatusType = 2
	StopAndBuffer          StatusType = 3
	StopAndBufferWatermark StatusType = 4
	StopNoBuffer           StatusType = 5
)

// header status
// refer https://github.com/envoyproxy/envoy/blob/main/envoy/http/filter.h
const (
	HeaderContinue                     StatusType = 100
	HeaderStopIteration                StatusType = 101
	HeaderContinueAndDontEndStream     StatusType = 102
	HeaderStopAllIterationAndBuffer    StatusType = 103
	HeaderStopAllIterationAndWatermark StatusType = 104
)

// data status
// refer https://github.com/envoyproxy/envoy/blob/main/envoy/http/filter.h
const (
	DataContinue                  StatusType = 200
	DataStopIterationAndBuffer    StatusType = 201
	DataStopIterationAndWatermark StatusType = 202
	DataStopIterationNoBuffer     StatusType = 203
)

// Trailer status
// refer https://github.com/envoyproxy/envoy/blob/main/envoy/http/filter.h
const (
	TrailerContinue      StatusType = 300
	TrailerStopIteration StatusType = 301
)

//****************** filter status end ******************//

// ****************** log level start ******************//
type LogType int

// refer https://github.com/envoyproxy/envoy/blob/main/source/common/common/base_logger.h
const (
	Trace    LogType = 0
	Debug    LogType = 1
	Info     LogType = 2
	Warn     LogType = 3
	Error    LogType = 4
	Critical LogType = 5
)

func (self LogType) String() string {
	switch self {
	case Trace:
		return "trace"
	case Debug:
		return "debug"
	case Info:
		return "info"
	case Warn:
		return "warn"
	case Error:
		return "error"
	case Critical:
		return "critical"
	}
	return "unknown"
}

//******************* log level end *******************//

// ****************** HeaderMap start ******************//

// refer https://github.com/envoyproxy/envoy/blob/main/envoy/http/header_map.h
type HeaderMap interface {
	// GetRaw is unsafe, reuse the memory from Envoy
	GetRaw(name string) string

	// Get value of key
	// If multiple values associated with this key, first one will be returned.
	Get(key string) (string, bool)

	// Values returns all values associated with the given key.
	// The returned slice is not a copy.
	Values(key string) []string

	// Set key-value pair in header map, the previous pair will be replaced if exists.
	// It may not take affects immediately in the Envoy thread side when it's invoked in a Go thread.
	Set(key, value string)

	// Add value for given key.
	// Multiple headers with the same key may be added with this function.
	// Use Set for setting a single header for the given key.
	// It may not take affects immediately in the Envoy thread side when it's invoked in a Go thread.
	Add(key, value string)

	// Del delete pair of specified key
	// It may not take affects immediately in the Envoy thread side when it's invoked in a Go thread.
	Del(key string)

	// Range calls f sequentially for each key and value present in the map.
	// If f returns false, range stops the iteration.
	// When there are multiple values of a key, f will be invoked multiple times with the same key and each value.
	Range(f func(key, value string) bool)

	// RangeWithCopy calls f sequentially for each key and value copied from the map.
	RangeWithCopy(f func(key, value string) bool)
}

type RequestHeaderMap interface {
	HeaderMap
	Scheme() string
	Method() string
	Host() string
	Path() string
}

type RequestTrailerMap interface {
	HeaderMap
	// others
}

type ResponseHeaderMap interface {
	HeaderMap
	Status() (int, bool)
}

type ResponseTrailerMap interface {
	HeaderMap
	// others
}

type MetadataMap interface {
}

//****************** HeaderMap end ******************//

// *************** BufferInstance start **************//
type BufferAction int

const (
	SetBuffer     BufferAction = 0
	AppendBuffer  BufferAction = 1
	PrependBuffer BufferAction = 2
)

type DataBufferBase interface {
	// Write appends the contents of p to the buffer, growing the buffer as
	// needed. The return value n is the length of p; err is always nil. If the
	// buffer becomes too large, Write will panic with ErrTooLarge.
	Write(p []byte) (n int, err error)

	// WriteString appends the string to the buffer, growing the buffer as
	// needed. The return value n is the length of s; err is always nil. If the
	// buffer becomes too large, Write will panic with ErrTooLarge.
	WriteString(s string) (n int, err error)

	// WriteByte appends the byte to the buffer, growing the buffer as
	// needed. The return value n is the length of s; err is always nil. If the
	// buffer becomes too large, Write will panic with ErrTooLarge.
	WriteByte(p byte) error

	// WriteUint16 appends the uint16 to the buffer, growing the buffer as
	// needed. The return value n is the length of s; err is always nil. If the
	// buffer becomes too large, Write will panic with ErrTooLarge.
	WriteUint16(p uint16) error

	// WriteUint32 appends the uint32 to the buffer, growing the buffer as
	// needed. The return value n is the length of s; err is always nil. If the
	// buffer becomes too large, Write will panic with ErrTooLarge.
	WriteUint32(p uint32) error

	// WriteUint64 appends the uint64 to the buffer, growing the buffer as
	// needed. The return value n is the length of s; err is always nil. If the
	// buffer becomes too large, Write will panic with ErrTooLarge.
	WriteUint64(p uint64) error

	// Bytes returns all bytes from buffer, without draining any buffered data.
	// It can be used to get fixed-length content, such as headers, body.
	// Note: do not change content in return bytes, use write instead
	Bytes() []byte

	// Drain drains a offset length of bytes in buffer.
	// It can be used with Bytes(), after consuming a fixed-length of data
	Drain(offset int)

	// Len returns the number of bytes of the unread portion of the buffer;
	// b.Len() == len(b.Bytes()).
	Len() int

	// Reset resets the buffer to be empty.
	Reset()

	// String returns the contents of the buffer as a string.
	String() string

	// Append append the contents of the slice data to the buffer.
	Append(data []byte) error
}

type BufferInstance interface {
	DataBufferBase

	// Set overwrite the whole buffer content with byte slice.
	Set([]byte) error

	// SetString overwrite the whole buffer content with string.
	SetString(string) error

	// Prepend prepend the contents of the slice data to the buffer.
	Prepend(data []byte) error

	// Prepend prepend the contents of the string data to the buffer.
	PrependString(s string) error

	// Append append the contents of the string data to the buffer.
	AppendString(s string) error
}

//*************** BufferInstance end **************//

type DestroyReason int

const (
	Normal    DestroyReason = 0
	Terminate DestroyReason = 1
)

// For each AccessLogType's meaning, see
// https://www.envoyproxy.io/docs/envoy/latest/configuration/observability/access_log/usage
// Currently, only some downstream access log types are supported
type AccessLogType int

const (
	AccessLogNotSet                                  AccessLogType = 0
	AccessLogTcpUpstreamConnected                    AccessLogType = 1
	AccessLogTcpPeriodic                             AccessLogType = 2
	AccessLogTcpConnectionEnd                        AccessLogType = 3
	AccessLogDownstreamStart                         AccessLogType = 4
	AccessLogDownstreamPeriodic                      AccessLogType = 5
	AccessLogDownstreamEnd                           AccessLogType = 6
	AccessLogUpstreamPoolReady                       AccessLogType = 7
	AccessLogUpstreamPeriodic                        AccessLogType = 8
	AccessLogUpstreamEnd                             AccessLogType = 9
	AccessLogDownstreamTunnelSuccessfullyEstablished AccessLogType = 10
)

const (
	NormalFinalize int = 0 // normal, finalize on destroy
	GCFinalize     int = 1 // finalize in GC sweep
)

type EnvoyRequestPhase int

const (
	DecodeHeaderPhase EnvoyRequestPhase = iota + 1
	DecodeDataPhase
	DecodeTrailerPhase
	EncodeHeaderPhase
	EncodeDataPhase
	EncodeTrailerPhase
)

func (e EnvoyRequestPhase) String() string {
	switch e {
	case DecodeHeaderPhase:
		return "DecodeHeader"
	case DecodeDataPhase:
		return "DecodeData"
	case DecodeTrailerPhase:
		return "DecodeTrailer"
	case EncodeHeaderPhase:
		return "EncodeHeader"
	case EncodeDataPhase:
		return "EncodeData"
	case EncodeTrailerPhase:
		return "EncodeTrailer"
	}
	return "unknown phase"
}

// Status codes returned by filters that can cause future filters to not get iterated to.
type FilterStatus int

const (
	// Continue to further filters.
	NetworkFilterContinue FilterStatus = 0
	// Stop executing further filters.
	NetworkFilterStopIteration FilterStatus = 1
)

func (s FilterStatus) String() string {
	switch s {
	case NetworkFilterContinue:
		return "Continue"
	case NetworkFilterStopIteration:
		return "StopIteration"
	}
	return "unknown"
}

// Events that occur on a connection.
type ConnectionEvent int

const (
	RemoteClose      ConnectionEvent = 0
	LocalClose       ConnectionEvent = 1
	Connected        ConnectionEvent = 2
	ConnectedZeroRtt ConnectionEvent = 3
)

func (e ConnectionEvent) String() string {
	switch e {
	case RemoteClose:
		return "RemoteClose"
	case LocalClose:
		return "LocalClose"
	case Connected:
		return "Connected"
	case ConnectedZeroRtt:
		return "ConnectedZeroRtt"
	}
	return "unknown"
}

// Type of connection close to perform.
type ConnectionCloseType int

const (
	// Flush pending write data before raising ConnectionEvent::LocalClose
	FlushWrite ConnectionCloseType = 0
	// Do not flush any pending data. Write the pending data to buffer and then immediately
	// raise ConnectionEvent::LocalClose
	NoFlush ConnectionCloseType = 1
	// Flush pending write data and delay raising a ConnectionEvent::LocalClose
	// until the delayed_close_timeout expires
	FlushWriteAndDelay ConnectionCloseType = 2
	// Do not write/flush any pending data and immediately raise ConnectionEvent::LocalClose
	Abort ConnectionCloseType = 3
	// Do not write/flush any pending data and immediately raise
	// ConnectionEvent::LocalClose. Envoy will try to close the connection with RST flag.
	AbortReset ConnectionCloseType = 4
)

func (t ConnectionCloseType) String() string {
	switch t {
	case FlushWrite:
		return "FlushWrite"
	case NoFlush:
		return "NoFlush"
	case FlushWriteAndDelay:
		return "FlushWriteAndDelay"
	case Abort:
		return "Abort"
	case AbortReset:
		return "AbortReset"
	}
	return "unknown"
}

type PoolFailureReason int

const (
	// A resource overflowed and policy prevented a new connection from being created.
	Overflow PoolFailureReason = 0
	// A local connection failure took place while creating a new connection.
	LocalConnectionFailure PoolFailureReason = 1
	// A remote connection failure took place while creating a new connection.
	RemoteConnectionFailure PoolFailureReason = 2
	// A timeout occurred while creating a new connection.
	Timeout PoolFailureReason = 3
)

func (r PoolFailureReason) String() string {
	switch r {
	case Overflow:
		return "Overflow"
	case LocalConnectionFailure:
		return "LocalConnectionFailure"
	case RemoteConnectionFailure:
		return "RemoteConnectionFailure"
	case Timeout:
		return "Timeout"
	}
	return "unknown"
}

type ConnectionInfoType int

const (
	ConnectionInfoLocalAddr  ConnectionInfoType = 0
	ConnectionInfoRemoteAddr ConnectionInfoType = 1
)

func (t ConnectionInfoType) String() string {
	switch t {
	case ConnectionInfoLocalAddr:
		return "ConnectionInfoLocalAddr"
	case ConnectionInfoRemoteAddr:
		return "ConnectionInfoRemoteAddr"
	}
	return "unknown"
}

// *************** errors start **************//
var (
	ErrInternalFailure = errors.New("internal failure")
	ErrValueNotFound   = errors.New("value not found")
	// Failed to serialize the value when we fetch the value as string
	ErrSerializationFailure = errors.New("serialization failure")
)

// *************** errors end **************//
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package api

import "fmt"

func (c *commonCApiImpl) Log(level LogType, message string) {
	panic("To implement")
}

func (c *commonCApiImpl) LogLevel() LogType {
	panic("To implement")
}

func LogTrace(message string) {
	cAPI.Log(Trace, message)
}

func LogDebug(message string) {
	cAPI.Log(Debug, message)
}

func LogInfo(message string) {
	cAPI.Log(Info, message)
}

func LogWarn(message string) {
	cAPI.Log(Warn, message)
}

func LogError(message string) {
	cAPI.Log(Error, message)
}

func LogCritical(message string) {
	cAPI.Log(Critical, message)
}

func LogTracef(format string, v ...any) {
	LogTrace(fmt.Sprintf(format, v...))
}

func LogDebugf(format string, v ...any) {
	LogDebug(fmt.Sprintf(format, v...))
}

func LogInfof(format string, v ...any) {
	LogInfo(fmt.Sprintf(format, v...))
}

func LogWarnf(format string, v ...any) {
	LogWarn(fmt.Sprintf(format, v...))
}

func LogErrorf(format string, v ...any) {
	LogError(fmt.Sprintf(format, v...))
}

func LogCriticalf(format string, v ...any) {
	LogCritical(fmt.Sprintf(format, v...))
}

func GetLogLevel() LogType {
	return cAPI.LogLevel()
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package api

import "google.golang.org/protobuf/types/known/anypb"

type (
	// PassThroughStreamEncoderFilter provides the no-op implementation of the StreamEncoderFilter interface.
	PassThroughStreamEncoderFilter struct{}
	// PassThroughStreamDecoderFilter provides the no-op implementation of the StreamDecoderFilter interface.
	PassThroughStreamDecoderFilter struct{}
	// PassThroughStreamFilter provides the no-op implementation of the StreamFilter interface.
	PassThroughStreamFilter struct {
		PassThroughStreamDecoderFilter
		PassThroughStreamEncoderFilter
	}

	// EmptyDownstreamFilter provides the no-op implementation of the DownstreamFilter interface
	EmptyDownstreamFilter struct{}
	// EmptyUpstreamFilter provides the no-op implementation of the UpstreamFilter interface
	EmptyUpstreamFilter struct{}
)

// request
type StreamDecoderFilter interface {
	DecodeHeaders(RequestHeaderMap, bool) StatusType
	DecodeData(BufferInstance, bool) StatusType
	DecodeTrailers(RequestTrailerMap) StatusType
}

func (*PassThroughStreamDecoderFilter) DecodeHeaders(RequestHeaderMap, bool) StatusType {
	return Continue
}

func (*PassThroughStreamDecoderFilter) DecodeData(BufferInstance, bool) StatusType {
	return Continue
}

func (*PassThroughStreamDecoderFilter) DecodeTrailers(RequestTrailerMap) StatusType {
	return Continue
}

// response
type StreamEncoderFilter interface {
	EncodeHeaders(ResponseHeaderMap, bool) StatusType
	EncodeData(BufferInstance, bool) StatusType
	EncodeTrailers(ResponseTrailerMap) StatusType
}

func (*PassThroughStreamEncoderFilter) EncodeHeaders(ResponseHeaderMap, bool) StatusType {
	return Continue
}

func (*PassThroughStreamEncoderFilter) EncodeData(BufferInstance, bool) StatusType {
	return Continue
}

func (*PassThroughStreamEncoderFilter) EncodeTrailers(ResponseTrailerMap) StatusType {
	return Continue
}

type StreamFilter interface {
	// http request
	StreamDecoderFilter
	// response stream
	StreamEncoderFilter

	// log
	OnLog()
	OnLogDownstreamStart()
	OnLogDownstreamPeriodic()

	// destroy filter
	OnDestroy(DestroyReason)
	// TODO add more for stream complete
}

func (*PassThroughStreamFilter) OnLog() {
}

func (*PassThroughStreamFilter) OnLogDownstreamStart() {
}

func (*PassThroughStreamFilter) OnLogDownstreamPeriodic() {
}

func (*PassThroughStreamFilter) OnDestroy(DestroyReason) {
}

type StreamFilterConfigParser interface {
	Parse(any *anypb.Any, callbacks ConfigCallbackHandler) (interface{}, error)
	Merge(parentConfig interface{}, childConfig interface{}) interface{}
}

type StreamFilterConfigFactory func(config interface{}) StreamFilterFactory
type StreamFilterFactory func(callbacks FilterCallbackHandler) StreamFilter

// stream info
// refer https://github.com/envoyproxy/envoy/blob/main/envoy/stream_info/stream_info.h
type StreamInfo interface {
	GetRouteName() string
	FilterChainName() string
	// Protocol return the request's protocol.
	Protocol() (string, bool)
	// ResponseCode return the response code.
	ResponseCode() (uint32, bool)
	// ResponseCodeDetails return the response code details.
	ResponseCodeDetails() (string, bool)
	// AttemptCount return the number of times the request was attempted upstream.
	AttemptCount() uint32
	// Get the dynamic metadata of the request
	DynamicMetadata() DynamicMetadata
	// DownstreamLocalAddress return the downstream local address.
	DownstreamLocalAddress() string
	// DownstreamRemoteAddress return the downstream remote address.
	DownstreamRemoteAddress() string
	// UpstreamLocalAddress return the upstream local address.
	UpstreamLocalAddress() (string, bool)
	// UpstreamRemoteAddress return the upstream remote address.
	UpstreamRemoteAddress() (string, bool)
	// UpstreamClusterName return the upstream host cluster.
	UpstreamClusterName() (string, bool)
	// FilterState return the filter state interface.
	FilterState() FilterState
	// VirtualClusterName returns the name of the virtual cluster which got matched
	VirtualClusterName() (string, bool)

	// Some fields in stream info can be fetched via GetProperty
	// For example, startTime() is equal to GetProperty("request.time")
}

type StreamFilterCallbacks interface {
	StreamInfo() StreamInfo
}

type FilterCallbacks interface {
	StreamFilterCallbacks
	// Continue or SendLocalReply should be last API invoked, no more code after them.
	Continue(StatusType)
	SendLocalReply(responseCode int, bodyText string, headers map[string][]string, grpcStatus int64, details string)
	// RecoverPanic recover panic in defer and terminate the request by SendLocalReply with 500 status code.
	RecoverPanic()
	Log(level LogType, msg string)
	LogLevel() LogType
	// GetProperty fetch Envoy attribute and return the value as a string.
	// The list of attributes can be found in https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/advanced/attributes.
	// If the fetch succeeded, a string will be returned.
	// If the value is a timestamp, it is returned as a timestamp string like "2023-07-31T07:21:40.695646+00:00".
	// If the fetch failed (including the value is not found), an error will be returned.
	//
	// The error can be one of:
	// * ErrInternalFailure
	// * ErrSerializationFailure (Currently, fetching attributes in List/Map type are unsupported)
	// * ErrValueNotFound
	GetProperty(key string) (string, error)
	// TODO add more for filter callbacks
}

type FilterCallbackHandler interface {
	FilterCallbacks
}

type DynamicMetadata interface {
	Get(filterName string) map[string]interface{}
	Set(filterName string, key string, value interface{})
}

type DownstreamFilter interface {
	// Called when a connection is first established.
	OnNewConnection() FilterStatus
	// Called when data is read on the connection.
	OnData(buffer []byte, endOfStream bool) FilterStatus
	// Callback for connection events.
	OnEvent(event ConnectionEvent)
	// Called when data is to be written on the connection.
	OnWrite(buffer []byte, endOfStream bool) FilterStatus
}

func (*EmptyDownstreamFilter) OnNewConnection() FilterStatus {
	return NetworkFilterContinue
}

func (*EmptyDownstreamFilter) OnData(buffer []byte, endOfStream bool) FilterStatus {
	return NetworkFilterContinue
}

func (*EmptyDownstreamFilter) OnEvent(event ConnectionEvent) {
}

func (*EmptyDownstreamFilter) OnWrite(buffer []byte, endOfStream bool) FilterStatus {
	return NetworkFilterContinue
}

type UpstreamFilter interface {
	// Called when a connection is available to process a request/response.
	OnPoolReady(cb ConnectionCallback)
	// Called when a pool error occurred and no connection could be acquired for making the request.
	OnPoolFailure(poolFailureReason PoolFailureReason, transportFailureReason string)
	// Invoked when data is delivered from the upstream connection.
	OnData(buffer []byte, endOfStream bool)
	// Callback for connection events.
	OnEvent(event ConnectionEvent)
}

func (*EmptyUpstreamFilter) OnPoolReady(cb ConnectionCallback) {
}

func (*EmptyUpstreamFilter) OnPoolFailure(poolFailureReason PoolFailureReason, transportFailureReason string) {
}

func (*EmptyUpstreamFilter) OnData(buffer []byte, endOfStream bool) FilterStatus {
	return NetworkFilterContinue
}

func (*EmptyUpstreamFilter) OnEvent(event ConnectionEvent) {
}

type ConnectionCallback interface {
	// StreamInfo returns the stream info of the connection
	StreamInfo() StreamInfo
	// Write data to the connection.
	Write(buffer []byte, endStream bool)
	// Close the connection.
	Close(closeType ConnectionCloseType)
}

type StateType int

const (
	StateTypeReadOnly StateType = 0
	StateTypeMutable  StateType = 1
)

type LifeSpan int

const (
	LifeSpanFilterChain LifeSpan = 0
	LifeSpanRequest     LifeSpan = 1
	LifeSpanConnection  LifeSpan = 2
	LifeSpanTopSpan     LifeSpan = 3
)

type StreamSharing int

const (
	None                             StreamSharing = 0
	SharedWithUpstreamConnection     StreamSharing = 1
	SharedWithUpstreamConnectionOnce StreamSharing = 2
)

type FilterState interface {
	SetString(key, value string, stateType StateType, lifeSpan LifeSpan, streamSharing StreamSharing)
	GetString(key string) string
}

type MetricType uint32

const (
	Counter   MetricType = 0
	Gauge     MetricType = 1
	Histogram MetricType = 2
)

type ConfigCallbacks interface {
	// Define a metric, for different MetricType, name must be different,
	// for same MetricType, the same name will share a metric.
	DefineCounterMetric(name string) CounterMetric
	DefineGaugeMetric(name string) GaugeMetric
	// TODO Histogram
}

type ConfigCallbackHandler interface {
	ConfigCallbacks
}

type CounterMetric interface {
	Increment(offset int64)
	Get() uint64
	Record(value uint64)
}

type GaugeMetric interface {
	Increment(offset int64)
	Get() uint64
	Record(value uint64)
}

// TODO
type HistogramMetric interface {
}
#pragma once

// NOLINT(namespace-envoy)

#ifdef __cplusplus
extern "C" {
#endif

#include <stdint.h> // NOLINT(modernize-deprecated-headers)

typedef struct { // NOLINT(modernize-use-using)
  const char* data;
  uint64_t len;
} Cstring;

typedef struct { // NOLINT(modernize-use-using)
  Cstring plugin_name;
  uint64_t configId;
  int phase;
} httpRequest;

typedef struct { // NOLINT(modernize-use-using)
  uint64_t plugin_name_ptr;
  uint64_t plugin_name_len;
  uint64_t config_ptr;
  uint64_t config_len;
  int is_route_config;
} httpConfig;

typedef enum { // NOLINT(modernize-use-using)
  Set,
  Append,
  Prepend,
} bufferAction;

typedef enum { // NOLINT(modernize-use-using)
  HeaderSet,
  HeaderAdd,
} headerAction;

// The return value of C Api that invoking from Go.
typedef enum { // NOLINT(modernize-use-using)
  CAPIOK = 0,
  CAPIFilterIsGone = -1,
  CAPIFilterIsDestroy = -2,
  CAPINotInGo = -3,
  CAPIInvalidPhase = -4,
  CAPIValueNotFound = -5,
  CAPIYield = -6,
  CAPIInternalFailure = -7,
  CAPISerializationFailure = -8,
} CAPIStatus;

CAPIStatus envoyGoFilterHttpContinue(void* r, int status);
CAPIStatus envoyGoFilterHttpSendLocalReply(void* r, int response_code, void* body_text_data,
                                           int body_text_len, void* headers, int headers_num,
                                           long long int grpc_status, void* details_data,
                                           int details_len);
CAPIStatus envoyGoFilterHttpSendPanicReply(void* r, void* details_data, int details_len);

CAPIStatus envoyGoFilterHttpGetHeader(void* r, void* key_data, int key_len, uint64_t* value_data,
                                      int* value_len);
CAPIStatus envoyGoFilterHttpCopyHeaders(void* r, void* strs, void* buf);
CAPIStatus envoyGoFilterHttpSetHeaderHelper(void* r, void* key_data, int key_len, void* value_data,
                                            int value_len, headerAction action);
CAPIStatus envoyGoFilterHttpRemoveHeader(void* r, void* key_data, int key_len);

CAPIStatus envoyGoFilterHttpGetBuffer(void* r, uint64_t buffer, void* value);
CAPIStatus envoyGoFilterHttpDrainBuffer(void* r, uint64_t buffer, uint64_t length);
CAPIStatus envoyGoFilterHttpSetBufferHelper(void* r, uint64_t buffer, void* data, int length,
                                            bufferAction action);

CAPIStatus envoyGoFilterHttpCopyTrailers(void* r, void* strs, void* buf);
CAPIStatus envoyGoFilterHttpSetTrailer(void* r, void* key_data, int key_len, void* value,
                                       int value_len, headerAction action);
CAPIStatus envoyGoFilterHttpRemoveTrailer(void* r, void* key_data, int key_len);

CAPIStatus envoyGoFilterHttpGetStringValue(void* r, int id, uint64_t* value_data, int* value_len);
CAPIStatus envoyGoFilterHttpGetIntegerValue(void* r, int id, uint64_t* value);

CAPIStatus envoyGoFilterHttpGetDynamicMetadata(void* r, void* name_data, int name_len,
                                               uint64_t* value_data, int* value_len);
CAPIStatus envoyGoFilterHttpSetDynamicMetadata(void* r, void* name_data, int name_len,
                                               void* key_data, int key_len, void* buf_data,
                                               int buf_len);

void envoyGoFilterLog(uint32_t level, void* message_data, int message_len);
uint32_t envoyGoFilterLogLevel();

void envoyGoFilterHttpFinalize(void* r, int reason);
void envoyGoConfigHttpFinalize(void* c);

CAPIStatus envoyGoFilterHttpSetStringFilterState(void* r, void* key_data, int key_len,
                                                 void* value_data, int value_len, int state_type,
                                                 int life_span, int stream_sharing);
CAPIStatus envoyGoFilterHttpGetStringFilterState(void* r, void* key_data, int key_len,
                                                 uint64_t* value_data, int* value_len);
CAPIStatus envoyGoFilterHttpGetStringProperty(void* r, void* key_data, int key_len,
                                              uint64_t* value_data, int* value_len, int* rc);

CAPIStatus envoyGoFilterHttpDefineMetric(void* c, uint32_t metric_type, void* name_data,
                                         int name_len, uint32_t* metric_id);
CAPIStatus envoyGoFilterHttpIncrementMetric(void* c, uint32_t metric_id, int64_t offset);
CAPIStatus envoyGoFilterHttpGetMetric(void* c, uint32_t metric_id, uint64_t* value);
CAPIStatus envoyGoFilterHttpRecordMetric(void* c, uint32_t metric_id, uint64_t value);

// downstream
CAPIStatus envoyGoFilterDownstreamClose(void* wrapper, int close_type);
CAPIStatus envoyGoFilterDownstreamWrite(void* f, void* buffer_ptr, int buffer_len, int end_stream);
void envoyGoFilterDownstreamFinalize(void* wrapper, int reason);
CAPIStatus envoyGoFilterDownstreamInfo(void* wrapper, int t, void* ret);

void* envoyGoFilterUpstreamConnect(void* library_id, void* addr, uint64_t conn_id);
CAPIStatus envoyGoFilterUpstreamWrite(void* u, void* buffer_ptr, int buffer_len, int end_stream);
CAPIStatus envoyGoFilterUpstreamClose(void* wrapper, int close_type);
void envoyGoFilterUpstreamFinalize(void* wrapper, int reason);
CAPIStatus envoyGoFilterUpstreamInfo(void* wrapper, int t, void* ret);

// filter state
CAPIStatus envoyGoFilterSetFilterState(void* wrapper, void* key, void* value, int state_type,
                                       int life_span, int stream_sharing);
CAPIStatus envoyGoFilterGetFilterState(void* wrapper, void* key, void* value);

#ifdef __cplusplus
} // extern "C"
#endif
load(
    "@io_bazel_rules_go//go:def.bzl",
    "go_library",
)

licenses(["notice"])  # Apache 2

go_library(
    name = "api",
    srcs = [
        "api.h",
        "capi.go",
        "filter.go",
        "logger.go",
        "type.go",
    ],
    cgo = True,
    clinkopts = select({
        "@io_bazel_rules_go//go/platform:android": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "@io_bazel_rules_go//go/platform:darwin": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:ios": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:linux": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "//conditions:default": [],
    }),
    importpath = "github.com/envoyproxy/envoy/contrib/golang/common/go/api",
    visibility = ["//visibility:public"],
    deps = [
        "@org_golang_google_protobuf//types/known/anypb",
    ],
)
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package api_impl

/*
// ref https://github.com/golang/go/issues/25832

#cgo CFLAGS: -I../api
#cgo linux LDFLAGS: -Wl,-unresolved-symbols=ignore-all
#cgo darwin LDFLAGS: -Wl,-undefined,dynamic_lookup

#include <stdlib.h>
#include <string.h>

#include "api.h"

*/
import "C"
import (
	"unsafe"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/api"
)

type commonCApiImpl struct{}

// The default log format is:
// [2023-08-09 03:04:15.985][1390][critical][golang] [contrib/golang/common/log/cgo.cc:27] msg

func (c *commonCApiImpl) Log(level api.LogType, message string) {
	C.envoyGoFilterLog(C.uint32_t(level), unsafe.Pointer(unsafe.StringData(message)), C.int(len(message)))
}

func (c *commonCApiImpl) LogLevel() api.LogType {
	return api.LogType(C.envoyGoFilterLogLevel())
}

func init() {
	api.SetCommonCAPI(&commonCApiImpl{})
}
load("@io_bazel_rules_go//go:def.bzl", "go_library")

licenses(["notice"])  # Apache 2

go_library(
    name = "api_impl",
    srcs = [
        "api.h",
        "capi_impl.go",
    ],
    cgo = True,
    clinkopts = select({
        "@io_bazel_rules_go//go/platform:android": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "@io_bazel_rules_go//go/platform:darwin": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:ios": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:linux": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "//conditions:default": [],
    }),
    importpath = "github.com/envoyproxy/envoy/contrib/golang/common/go/api_impl",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/api",
    ],
)
#include "contrib/golang/common/dso/dso.h"

#include "source/common/common/assert.h"

namespace Envoy {
namespace Dso {

template <typename T>
bool dlsymInternal(T& fn, void* handler, const std::string name, const std::string symbol) {
  if (!handler) {
    return false;
  }

  fn = reinterpret_cast<T>(dlsym(handler, symbol.c_str()));
  if (!fn) {
    ENVOY_LOG_MISC(error, "lib: {}, cannot find symbol: {}, err: {}", name, symbol, dlerror());
    return false;
  }

  return true;
}

Dso::Dso(const std::string dso_name) : dso_name_(dso_name) {
  ENVOY_LOG_MISC(debug, "loading symbols from so file: {}", dso_name);

  handler_ = dlopen(dso_name.c_str(), RTLD_LAZY);
  if (!handler_) {
    ENVOY_LOG_MISC(error, "cannot load : {} error: {}", dso_name, dlerror());
    return;
  }

  // loaded is set to true by default when dlopen successfully,
  // and will set to false when load symbol failed.
  loaded_ = true;
}

Dso::~Dso() {
  // The dl library maintains reference counts for library handles, so a dynamic library is not
  // deallocated until dlclose() has been called on it as many times as dlopen() has succeeded on
  // it.
  if (handler_ != nullptr) {
    dlclose(handler_);
  }
}

HttpFilterDsoImpl::HttpFilterDsoImpl(const std::string dso_name) : HttpFilterDso(dso_name) {
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_new_http_plugin_config_)>(
      envoy_go_filter_new_http_plugin_config_, handler_, dso_name,
      "envoyGoFilterNewHttpPluginConfig");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_merge_http_plugin_config_)>(
      envoy_go_filter_merge_http_plugin_config_, handler_, dso_name,
      "envoyGoFilterMergeHttpPluginConfig");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_destroy_http_plugin_config_)>(
      envoy_go_filter_destroy_http_plugin_config_, handler_, dso_name,
      "envoyGoFilterDestroyHttpPluginConfig");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_http_header_)>(
      envoy_go_filter_on_http_header_, handler_, dso_name, "envoyGoFilterOnHttpHeader");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_http_data_)>(
      envoy_go_filter_on_http_data_, handler_, dso_name, "envoyGoFilterOnHttpData");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_http_log_)>(
      envoy_go_filter_on_http_log_, handler_, dso_name, "envoyGoFilterOnHttpLog");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_http_destroy_)>(
      envoy_go_filter_on_http_destroy_, handler_, dso_name, "envoyGoFilterOnHttpDestroy");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_go_request_sema_dec_)>(
      envoy_go_filter_go_request_sema_dec_, handler_, dso_name, "envoyGoRequestSemaDec");
}

GoUint64 HttpFilterDsoImpl::envoyGoFilterNewHttpPluginConfig(httpConfig* p0) {
  ASSERT(envoy_go_filter_new_http_plugin_config_ != nullptr);
  return envoy_go_filter_new_http_plugin_config_(p0);
}

GoUint64 HttpFilterDsoImpl::envoyGoFilterMergeHttpPluginConfig(GoUint64 p0, GoUint64 p1,
                                                               GoUint64 p2, GoUint64 p3) {
  ASSERT(envoy_go_filter_merge_http_plugin_config_ != nullptr);
  return envoy_go_filter_merge_http_plugin_config_(p0, p1, p2, p3);
}

void HttpFilterDsoImpl::envoyGoFilterDestroyHttpPluginConfig(GoUint64 p0, GoInt p1) {
  ASSERT(envoy_go_filter_destroy_http_plugin_config_ != nullptr);
  return envoy_go_filter_destroy_http_plugin_config_(p0, p1);
}

GoUint64 HttpFilterDsoImpl::envoyGoFilterOnHttpHeader(httpRequest* p0, GoUint64 p1, GoUint64 p2,
                                                      GoUint64 p3) {
  ASSERT(envoy_go_filter_on_http_header_ != nullptr);
  return envoy_go_filter_on_http_header_(p0, p1, p2, p3);
}

GoUint64 HttpFilterDsoImpl::envoyGoFilterOnHttpData(httpRequest* p0, GoUint64 p1, GoUint64 p2,
                                                    GoUint64 p3) {
  ASSERT(envoy_go_filter_on_http_data_ != nullptr);
  return envoy_go_filter_on_http_data_(p0, p1, p2, p3);
}

void HttpFilterDsoImpl::envoyGoFilterOnHttpLog(httpRequest* p0, int p1) {
  ASSERT(envoy_go_filter_on_http_log_ != nullptr);
  envoy_go_filter_on_http_log_(p0, GoUint64(p1));
}

void HttpFilterDsoImpl::envoyGoFilterOnHttpDestroy(httpRequest* p0, int p1) {
  ASSERT(envoy_go_filter_on_http_destroy_ != nullptr);
  envoy_go_filter_on_http_destroy_(p0, GoUint64(p1));
}

void HttpFilterDsoImpl::envoyGoRequestSemaDec(httpRequest* p0) {
  ASSERT(envoy_go_filter_go_request_sema_dec_ != nullptr);
  envoy_go_filter_go_request_sema_dec_(p0);
}

ClusterSpecifierDsoImpl::ClusterSpecifierDsoImpl(const std::string dso_name)
    : ClusterSpecifierDso(dso_name) {
  loaded_ &= dlsymInternal<decltype(envoy_go_cluster_specifier_new_plugin_)>(
      envoy_go_cluster_specifier_new_plugin_, handler_, dso_name,
      "envoyGoClusterSpecifierNewPlugin");
  loaded_ &= dlsymInternal<decltype(envoy_go_on_cluster_specify_)>(
      envoy_go_on_cluster_specify_, handler_, dso_name, "envoyGoOnClusterSpecify");
}

GoUint64 ClusterSpecifierDsoImpl::envoyGoClusterSpecifierNewPlugin(GoUint64 config_ptr,
                                                                   GoUint64 config_len) {
  ASSERT(envoy_go_cluster_specifier_new_plugin_ != nullptr);
  return envoy_go_cluster_specifier_new_plugin_(config_ptr, config_len);
}

GoInt64 ClusterSpecifierDsoImpl::envoyGoOnClusterSpecify(GoUint64 plugin_ptr, GoUint64 header_ptr,
                                                         GoUint64 plugin_id, GoUint64 buffer_ptr,
                                                         GoUint64 buffer_len) {
  ASSERT(envoy_go_on_cluster_specify_ != nullptr);
  return envoy_go_on_cluster_specify_(plugin_ptr, header_ptr, plugin_id, buffer_ptr, buffer_len);
}

NetworkFilterDsoImpl::NetworkFilterDsoImpl(const std::string dso_name)
    : NetworkFilterDso(dso_name) {
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_network_filter_config_)>(
      envoy_go_filter_on_network_filter_config_, handler_, dso_name,
      "envoyGoFilterOnNetworkFilterConfig");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_downstream_connection_)>(
      envoy_go_filter_on_downstream_connection_, handler_, dso_name,
      "envoyGoFilterOnDownstreamConnection");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_downstream_data_)>(
      envoy_go_filter_on_downstream_data_, handler_, dso_name, "envoyGoFilterOnDownstreamData");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_downstream_event_)>(
      envoy_go_filter_on_downstream_event_, handler_, dso_name, "envoyGoFilterOnDownstreamEvent");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_downstream_write_)>(
      envoy_go_filter_on_downstream_write_, handler_, dso_name, "envoyGoFilterOnDownstreamWrite");

  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_upstream_connection_ready_)>(
      envoy_go_filter_on_upstream_connection_ready_, handler_, dso_name,
      "envoyGoFilterOnUpstreamConnectionReady");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_upstream_connection_failure_)>(
      envoy_go_filter_on_upstream_connection_failure_, handler_, dso_name,
      "envoyGoFilterOnUpstreamConnectionFailure");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_upstream_data_)>(
      envoy_go_filter_on_upstream_data_, handler_, dso_name, "envoyGoFilterOnUpstreamData");
  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_upstream_event_)>(
      envoy_go_filter_on_upstream_event_, handler_, dso_name, "envoyGoFilterOnUpstreamEvent");

  loaded_ &= dlsymInternal<decltype(envoy_go_filter_on_sema_dec_)>(
      envoy_go_filter_on_sema_dec_, handler_, dso_name, "envoyGoFilterOnSemaDec");
}

GoUint64 NetworkFilterDsoImpl::envoyGoFilterOnNetworkFilterConfig(GoUint64 library_id_ptr,
                                                                  GoUint64 library_id_len,
                                                                  GoUint64 config_ptr,
                                                                  GoUint64 config_len) {
  ASSERT(envoy_go_filter_on_network_filter_config_ != nullptr);
  return envoy_go_filter_on_network_filter_config_(library_id_ptr, library_id_len, config_ptr,
                                                   config_len);
}

GoUint64 NetworkFilterDsoImpl::envoyGoFilterOnDownstreamConnection(void* w,
                                                                   GoUint64 plugin_name_ptr,
                                                                   GoUint64 plugin_name_len,
                                                                   GoUint64 config_id) {
  ASSERT(envoy_go_filter_on_downstream_connection_ != nullptr);
  return envoy_go_filter_on_downstream_connection_(w, plugin_name_ptr, plugin_name_len, config_id);
}

GoUint64 NetworkFilterDsoImpl::envoyGoFilterOnDownstreamData(void* w, GoUint64 data_size,
                                                             GoUint64 data_ptr, GoInt slice_num,
                                                             GoInt end_of_stream) {
  ASSERT(envoy_go_filter_on_downstream_data_ != nullptr);
  return envoy_go_filter_on_downstream_data_(w, data_size, data_ptr, slice_num, end_of_stream);
}

void NetworkFilterDsoImpl::envoyGoFilterOnDownstreamEvent(void* w, GoInt event) {
  ASSERT(envoy_go_filter_on_downstream_event_ != nullptr);
  envoy_go_filter_on_downstream_event_(w, event);
}

GoUint64 NetworkFilterDsoImpl::envoyGoFilterOnDownstreamWrite(void* w, GoUint64 data_size,
                                                              GoUint64 data_ptr, GoInt slice_num,
                                                              GoInt end_of_stream) {
  ASSERT(envoy_go_filter_on_downstream_write_ != nullptr);
  return envoy_go_filter_on_downstream_write_(w, data_size, data_ptr, slice_num, end_of_stream);
}

void NetworkFilterDsoImpl::envoyGoFilterOnUpstreamConnectionReady(void* w, GoUint64 conn_id) {
  ASSERT(envoy_go_filter_on_upstream_connection_ready_ != nullptr);
  envoy_go_filter_on_upstream_connection_ready_(w, conn_id);
}

void NetworkFilterDsoImpl::envoyGoFilterOnUpstreamConnectionFailure(void* w, GoInt reason,
                                                                    GoUint64 conn_id) {
  ASSERT(envoy_go_filter_on_upstream_connection_failure_ != nullptr);
  envoy_go_filter_on_upstream_connection_failure_(w, reason, conn_id);
}

void NetworkFilterDsoImpl::envoyGoFilterOnUpstreamData(void* w, GoUint64 data_size,
                                                       GoUint64 data_ptr, GoInt slice_num,
                                                       GoInt end_of_stream) {
  ASSERT(envoy_go_filter_on_upstream_data_ != nullptr);
  envoy_go_filter_on_upstream_data_(w, data_size, data_ptr, slice_num, end_of_stream);
}

void NetworkFilterDsoImpl::envoyGoFilterOnUpstreamEvent(void* w, GoInt event) {
  ASSERT(envoy_go_filter_on_upstream_event_ != nullptr);
  envoy_go_filter_on_upstream_event_(w, event);
}

void NetworkFilterDsoImpl::envoyGoFilterOnSemaDec(void* w) {
  ASSERT(envoy_go_filter_on_sema_dec_ != nullptr);
  envoy_go_filter_on_sema_dec_(w);
}

} // namespace Dso
} // namespace Envoy
#include "mocks.h"

namespace Envoy {
namespace Dso {

MockHttpFilterDsoImpl::MockHttpFilterDsoImpl() : HttpFilterDso("mock") {}
MockHttpFilterDsoImpl::~MockHttpFilterDsoImpl() = default;

} // namespace Dso
} // namespace Envoy
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "simple.so",
    srcs = [
        "simple.go",
    ],
    out = "simple.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/common/dso/test/test_data",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
)

go_binary(
    name = "bad.so",
    srcs = [
        "bad.go",
    ],
    out = "bad.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/common/dso/test/test_data",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
)
package main

import "C"

// missing the envoyGoClusterSpecifierNewPlugin symbol.

//export envoyGoOnClusterSpecify
func envoyGoOnClusterSpecify(pluginPtr uint64, headerPtr uint64, pluginId uint64, bufferPtr uint64, bufferLen uint64) int64 {
	return 0
}

func main() {
}
package main

/*
typedef struct {
  int foo;
} httpRequest;

typedef struct {
  unsigned long long int plugin_name_ptr;
  unsigned long long int plugin_name_len;
  unsigned long long int config_ptr;
  unsigned long long int config_len;
  int is_route_config;
} httpConfig;
*/
import "C"
import (
	"sync"
	"unsafe"
)

var configCache = &sync.Map{}

//export envoyGoFilterNewHttpPluginConfig
func envoyGoFilterNewHttpPluginConfig(c *C.httpConfig) uint64 {
	// already existing return 0, just for testing the destroy api.
	if _, ok := configCache.Load(uint64(c.config_len)); ok {
		return 0
	}
	// mark this configLen already existing
	configCache.Store(uint64(c.config_len), uint64(c.config_len))
	return uint64(c.config_len)
}

//export envoyGoFilterDestroyHttpPluginConfig
func envoyGoFilterDestroyHttpPluginConfig(id uint64, needDelay int) {
	configCache.Delete(id)
}

//export envoyGoFilterMergeHttpPluginConfig
func envoyGoFilterMergeHttpPluginConfig(namePtr, nameLen, parentId, childId uint64) uint64 {
	return 0
}

//export envoyGoFilterOnHttpHeader
func envoyGoFilterOnHttpHeader(r *C.httpRequest, endStream, headerNum, headerBytes uint64) uint64 {
	return 0
}

//export envoyGoFilterOnHttpData
func envoyGoFilterOnHttpData(r *C.httpRequest, endStream, buffer, length uint64) uint64 {
	return 0
}

//export envoyGoFilterOnHttpLog
func envoyGoFilterOnHttpLog(r *C.httpRequest, logType uint64) {
}

//export envoyGoFilterOnHttpDestroy
func envoyGoFilterOnHttpDestroy(r *C.httpRequest, reason uint64) {
}

//export envoyGoClusterSpecifierNewPlugin
func envoyGoClusterSpecifierNewPlugin(configPtr uint64, configLen uint64) uint64 {
	return 200
}

//export envoyGoOnClusterSpecify
func envoyGoOnClusterSpecify(pluginPtr uint64, headerPtr uint64, pluginId uint64, bufferPtr uint64, bufferLen uint64) int64 {
	return 0
}

//export envoyGoFilterOnNetworkFilterConfig
func envoyGoFilterOnNetworkFilterConfig(libraryIDPtr uint64, libraryIDLen uint64, configPtr uint64, configLen uint64) uint64 {
	return 100
}

//export envoyGoFilterOnDownstreamConnection
func envoyGoFilterOnDownstreamConnection(wrapper unsafe.Pointer, pluginNamePtr uint64, pluginNameLen uint64,
	configID uint64) uint64 {
	return 0
}

//export envoyGoFilterOnDownstreamData
func envoyGoFilterOnDownstreamData(wrapper unsafe.Pointer, dataSize uint64, dataPtr uint64, sliceNum int, endOfStream int) uint64 {
	return 0
}

//export envoyGoFilterOnDownstreamEvent
func envoyGoFilterOnDownstreamEvent(wrapper unsafe.Pointer, event int) {}

//export envoyGoFilterOnDownstreamWrite
func envoyGoFilterOnDownstreamWrite(wrapper unsafe.Pointer, dataSize uint64, dataPtr uint64, sliceNum int, endOfStream int) uint64 {
	return 0
}

//export envoyGoFilterOnUpstreamConnectionReady
func envoyGoFilterOnUpstreamConnectionReady(wrapper unsafe.Pointer, connID uint64) {}

//export envoyGoFilterOnUpstreamConnectionFailure
func envoyGoFilterOnUpstreamConnectionFailure(wrapper unsafe.Pointer, reason int, connID uint64) {}

//export envoyGoFilterOnUpstreamData
func envoyGoFilterOnUpstreamData(wrapper unsafe.Pointer, dataSize uint64, dataPtr uint64, sliceNum int, endOfStream int) {
}

//export envoyGoFilterOnUpstreamEvent
func envoyGoFilterOnUpstreamEvent(wrapper unsafe.Pointer, event int) {}

//export envoyGoFilterOnSemaDec
func envoyGoFilterOnSemaDec(wrapper unsafe.Pointer) {
}

//export envoyGoRequestSemaDec
func envoyGoRequestSemaDec(r *C.httpRequest) {
}

func main() {
}
#pragma once

#include <string>

#include "contrib/golang/common/dso/dso.h"
#include "gmock/gmock.h"

namespace Envoy {
namespace Dso {

class MockHttpFilterDsoImpl : public HttpFilterDso {
public:
  MockHttpFilterDsoImpl();
  ~MockHttpFilterDsoImpl() override;

  MOCK_METHOD(GoUint64, envoyGoFilterNewHttpPluginConfig, (httpConfig * p0));
  MOCK_METHOD(GoUint64, envoyGoFilterMergeHttpPluginConfig,
              (GoUint64 p0, GoUint64 p1, GoUint64 p2, GoUint64 p3));
  MOCK_METHOD(void, envoyGoFilterDestroyHttpPluginConfig, (GoUint64 p0, GoInt p1));
  MOCK_METHOD(GoUint64, envoyGoFilterOnHttpHeader,
              (httpRequest * p0, GoUint64 p1, GoUint64 p2, GoUint64 p3));
  MOCK_METHOD(GoUint64, envoyGoFilterOnHttpData,
              (httpRequest * p0, GoUint64 p1, GoUint64 p2, GoUint64 p3));
  MOCK_METHOD(void, envoyGoFilterOnHttpLog, (httpRequest * p0, int p1));
  MOCK_METHOD(void, envoyGoFilterOnHttpDestroy, (httpRequest * p0, int p1));
  MOCK_METHOD(void, envoyGoRequestSemaDec, (httpRequest * p0));
};

class MockNetworkFilterDsoImpl : public NetworkFilterDso {
public:
  MockNetworkFilterDsoImpl() = default;
  ~MockNetworkFilterDsoImpl() override = default;

  MOCK_METHOD(GoUint64, envoyGoFilterOnNetworkFilterConfig,
              (GoUint64 libraryIDPtr, GoUint64 libraryIDLen, GoUint64 configPtr,
               GoUint64 configLen));
  MOCK_METHOD(GoUint64, envoyGoFilterOnDownstreamConnection,
              (void* w, GoUint64 pluginNamePtr, GoUint64 pluginNameLen, GoUint64 configID));
  MOCK_METHOD(GoUint64, envoyGoFilterOnDownstreamData,
              (void* w, GoUint64 dataSize, GoUint64 dataPtr, GoInt sliceNum, GoInt endOfStream));
  MOCK_METHOD(void, envoyGoFilterOnDownstreamEvent, (void* w, GoInt event));
  MOCK_METHOD(GoUint64, envoyGoFilterOnDownstreamWrite,
              (void* w, GoUint64 dataSize, GoUint64 dataPtr, GoInt sliceNum, GoInt endOfStream));

  MOCK_METHOD(void, envoyGoFilterOnUpstreamConnectionReady, (void* w, GoUint64 conn_id));
  MOCK_METHOD(void, envoyGoFilterOnUpstreamConnectionFailure,
              (void* w, GoInt reason, GoUint64 conn_id));
  MOCK_METHOD(void, envoyGoFilterOnUpstreamData,
              (void* w, GoUint64 dataSize, GoUint64 dataPtr, GoInt sliceNum, GoInt endOfStream));
  MOCK_METHOD(void, envoyGoFilterOnUpstreamEvent, (void* w, GoInt event));

  MOCK_METHOD(void, envoyGoFilterOnSemaDec, (void* w));
};

} // namespace Dso
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_mock",
    "envoy_cc_test",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "dso_test",
    srcs = ["dso_test.cc"],
    data = [
        "//contrib/golang/common/dso/test/test_data:simple.so",
    ],
    deps = [
        "//contrib/golang/common/dso:dso_lib",
        "//test/test_common:utility_lib",
    ],
)

envoy_cc_mock(
    name = "dso_mocks",
    srcs = ["mocks.cc"],
    hdrs = ["mocks.h"],
    deps = [
        "//contrib/golang/common/dso:dso_lib",
    ],
)
#include <string>

#include "envoy/registry/registry.h"

#include "test/test_common/environment.h"
#include "test/test_common/utility.h"

#include "contrib/golang/common/dso/dso.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Dso {
namespace {

std::string genSoPath(std::string name) {
  return TestEnvironment::substitute("{{ test_rundir }}/contrib/golang/common/dso/test/test_data/" +
                                     name);
}

TEST(DsoInstanceTest, SimpleAPI) {
  auto path = genSoPath("simple.so");
  HttpFilterDsoPtr dso(new HttpFilterDsoImpl(path));
  httpConfig* config = new httpConfig();
  config->config_len = 100;
  EXPECT_EQ(dso->envoyGoFilterNewHttpPluginConfig(config), 100);
}

TEST(DsoManagerTest, Pub) {
  auto id = "simple.so";
  auto plugin_name = "example";
  auto path = genSoPath(id);

  {
    // get before load http filter dso
    auto dso = DsoManager<HttpFilterDsoImpl>::getDsoByPluginName(plugin_name);
    EXPECT_EQ(dso, nullptr);

    // first time load http filter dso
    dso = DsoManager<HttpFilterDsoImpl>::load(id, path, plugin_name);
    EXPECT_NE(dso, nullptr);

    // get after load http filter dso
    dso = DsoManager<HttpFilterDsoImpl>::getDsoByPluginName(plugin_name);
    EXPECT_NE(dso, nullptr);
    httpConfig* config = new httpConfig();
    config->config_len = 200;
    EXPECT_EQ(dso->envoyGoFilterNewHttpPluginConfig(config), 200);

    // second time load http filter dso
    dso = DsoManager<HttpFilterDsoImpl>::load(id, path, plugin_name);
    EXPECT_NE(dso, nullptr);
  }

  {
    // first time load cluster specifier dso
    auto cluster_dso = DsoManager<ClusterSpecifierDsoImpl>::load(id, path);
    EXPECT_NE(cluster_dso, nullptr);

    EXPECT_EQ(cluster_dso->envoyGoClusterSpecifierNewPlugin(0, 0), 200);
  }

  {
    // get before load network filter dso
    auto dso = DsoManager<NetworkFilterDsoImpl>::getDsoByID(id);
    EXPECT_EQ(dso, nullptr);

    // first time load network filter dso
    auto res = DsoManager<NetworkFilterDsoImpl>::load(id, path);
    EXPECT_NE(res, nullptr);

    // get after load network filter dso
    dso = DsoManager<NetworkFilterDsoImpl>::getDsoByID(id);
    EXPECT_NE(dso, nullptr);
    EXPECT_EQ(dso->envoyGoFilterOnNetworkFilterConfig(0, 0, 0, 0), 100);

    // second time load network filter dso
    res = DsoManager<NetworkFilterDsoImpl>::load(id, path);
    EXPECT_NE(dso, nullptr);
  }
}

// missing a symbol
TEST(DsoInstanceTest, BadSo) {
  auto path = genSoPath("bad.so");
  ClusterSpecifierDsoPtr dso(new ClusterSpecifierDsoImpl(path));
  EXPECT_EQ(dso->loaded(), false);
}

// remove plugin config
TEST(DsoInstanceTest, RemovePluginConfig) {
  auto path = genSoPath("simple.so");
  HttpFilterDsoPtr dso(new HttpFilterDsoImpl(path));
  httpConfig* config = new httpConfig();
  config->config_len = 300;
  EXPECT_EQ(dso->envoyGoFilterNewHttpPluginConfig(config), 300);
  // new again, return 0, since it's already existing
  EXPECT_EQ(dso->envoyGoFilterNewHttpPluginConfig(config), 0);

  // remove it
  dso->envoyGoFilterDestroyHttpPluginConfig(300, 0);
  // new again, after removed.
  EXPECT_EQ(dso->envoyGoFilterNewHttpPluginConfig(config), 300);

  // remove twice should be ok
  dso->envoyGoFilterDestroyHttpPluginConfig(300, 0);
  dso->envoyGoFilterDestroyHttpPluginConfig(300, 0);
}

} // namespace
} // namespace Dso
} // namespace Envoy
#pragma once

/* Code generated by cmd/cgo; DO NOT EDIT. */

/* package http */

// NOLINT(namespace-envoy)

#line 1 "cgo-builtin-export-prolog"

#include <stddef.h> // NOLINT(modernize-deprecated-headers)

#ifndef GO_CGO_EXPORT_PROLOGUE_H
#define GO_CGO_EXPORT_PROLOGUE_H

#ifndef GO_CGO_GOSTRING_TYPEDEF
typedef struct { // NOLINT(modernize-use-using)
  const char* p;
  ptrdiff_t n;
} _GoString_;
#endif

#endif

/* Start of preamble from import "C" comments. */

#line 20 "export.go"

// ref https://github.com/golang/go/issues/25832

#include <stdlib.h> // NOLINT(modernize-deprecated-headers)
#include <string.h> // NOLINT(modernize-deprecated-headers)

#include "api.h"

#line 1 "cgo-generated-wrapper"

/* End of preamble from import "C" comments. */

/* Start of boilerplate cgo prologue. */
#line 1 "cgo-gcc-export-header-prolog"

#ifndef GO_CGO_PROLOGUE_H
#define GO_CGO_PROLOGUE_H

typedef signed char GoInt8;          // NOLINT(modernize-use-using)
typedef unsigned char GoUint8;       // NOLINT(modernize-use-using)
typedef short GoInt16;               // NOLINT(modernize-use-using)
typedef unsigned short GoUint16;     // NOLINT(modernize-use-using)
typedef int GoInt32;                 // NOLINT(modernize-use-using)
typedef unsigned int GoUint32;       // NOLINT(modernize-use-using)
typedef long long GoInt64;           // NOLINT(modernize-use-using)
typedef unsigned long long GoUint64; // NOLINT(modernize-use-using)
typedef GoInt64 GoInt;               // NOLINT(modernize-use-using)
typedef GoUint64 GoUint;             // NOLINT(modernize-use-using)
typedef size_t GoUintptr;            // NOLINT(modernize-use-using)
typedef float GoFloat32;             // NOLINT(modernize-use-using)
typedef double GoFloat64;            // NOLINT(modernize-use-using)
#ifdef _MSC_VER
#include <complex.h>
typedef _Fcomplex GoComplex64;  // NOLINT(modernize-use-using)
typedef _Dcomplex GoComplex128; // NOLINT(modernize-use-using)
#else
typedef float _Complex GoComplex64;   // NOLINT(modernize-use-using)
typedef double _Complex GoComplex128; // NOLINT(modernize-use-using)
#endif

/*
  static assertion to make sure the file is being used on architecture
  at least with matching size of GoInt.
*/
typedef char // NOLINT(modernize-use-using)
    _check_for_64_bit_pointer_matching_GoInt[sizeof(void*) == 64 / 8 ? 1 : -1];

#ifndef GO_CGO_GOSTRING_TYPEDEF
typedef _GoString_ GoString; // NOLINT(modernize-use-using)
#endif
typedef void* GoMap;  // NOLINT(modernize-use-using)
typedef void* GoChan; // NOLINT(modernize-use-using)
typedef struct {      // NOLINT(modernize-use-using)
  void* t;
  void* v;
} GoInterface;
typedef struct { // NOLINT(modernize-use-using)
  void* data;
  GoInt len;
  GoInt cap;
} GoSlice;

#endif

/* End of boilerplate cgo prologue. */

#ifdef __cplusplus
extern "C" {
#endif

// go:linkname envoyGoFilterNewHttpPluginConfig
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoFilterNewHttpPluginConfig
extern GoUint64 envoyGoFilterNewHttpPluginConfig(httpConfig* p0);

// go:linkname envoyGoFilterDestroyHttpPluginConfig
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoFilterDestroyHttpPluginConfig
extern void envoyGoFilterDestroyHttpPluginConfig(GoUint64 id, GoInt need_delay);

// go:linkname envoyGoFilterMergeHttpPluginConfig
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoFilterMergeHttpPluginConfig
extern GoUint64 envoyGoFilterMergeHttpPluginConfig(GoUint64 name_ptr, GoUint64 name_len,
                                                   GoUint64 parent_id, GoUint64 child_id);

// go:linkname envoyGoFilterOnHttpHeader
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoFilterOnHttpHeader
extern GoUint64 envoyGoFilterOnHttpHeader(httpRequest* r, GoUint64 end_stream, GoUint64 header_num,
                                          GoUint64 header_bytes);

// go:linkname envoyGoFilterOnHttpData
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoFilterOnHttpData
extern GoUint64 envoyGoFilterOnHttpData(httpRequest* r, GoUint64 end_stream, GoUint64 buffer,
                                        GoUint64 length);

// go:linkname envoyGoFilterOnHttpLog
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoFilterOnHttpLog
extern void envoyGoFilterOnHttpLog(httpRequest* r, GoUint64 type);

// go:linkname envoyGoFilterOnHttpDestroy
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoFilterOnHttpDestroy
extern void envoyGoFilterOnHttpDestroy(httpRequest* r, GoUint64 reason);

// go:linkname envoyGoRequestSemaDec
// github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http.envoyGoRequestSemaDec
extern void envoyGoRequestSemaDec(httpRequest* r);

// go:linkname envoyGoOnClusterSpecify
// github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/cluster_specifier.envoyGoOnClusterSpecify
extern GoInt64 envoyGoOnClusterSpecify(GoUint64 plugin_ptr, GoUint64 header_ptr, GoUint64 plugin_id,
                                       GoUint64 buffer_ptr, GoUint64 buffer_len);

// go:linkname envoyGoClusterSpecifierNewPlugin
// github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/cluster_specifier.envoyGoClusterSpecifierNewPlugin
extern GoUint64 envoyGoClusterSpecifierNewPlugin(GoUint64 config_ptr, GoUint64 config_len);

// go:linkname envoyGoFilterOnNetworkFilterConfig
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnNetworkFilterConfig
extern GoUint64 envoyGoFilterOnNetworkFilterConfig(GoUint64 library_id_ptr, GoUint64 library_id_len,
                                                   GoUint64 config_ptr, GoUint64 config_len);

// go:linkname envoyGoFilterOnDownstreamConnection
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnDownstreamConnection
extern GoUint64 envoyGoFilterOnDownstreamConnection(void* f, GoUint64 plugin_name_ptr,
                                                    GoUint64 plugin_name_len, GoUint64 config_id);

// go:linkname envoyGoFilterOnDownstreamData
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnDownstreamData
extern GoUint64 envoyGoFilterOnDownstreamData(void* f, GoUint64 data_size, GoUint64 data_ptr,
                                              GoInt slice_num, GoInt end_of_stream);

// go:linkname envoyGoFilterOnDownstreamWrite
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnDownstreamWrite
extern GoUint64 envoyGoFilterOnDownstreamWrite(void* f, GoUint64 data_size, GoUint64 data_ptr,
                                               GoInt slice_num, GoInt end_of_stream);

// go:linkname envoyGoFilterOnDownstreamEvent
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnDownstreamEvent
extern void envoyGoFilterOnDownstreamEvent(void* f, GoInt event);

// go:linkname envoyGoFilterOnUpstreamConnectionReady
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnUpstreamConnectionReady
extern void envoyGoFilterOnUpstreamConnectionReady(void* f, GoUint64 conn_id);

// go:linkname envoyGoFilterOnUpstreamConnectionFailure
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnUpstreamConnectionFailure
extern void envoyGoFilterOnUpstreamConnectionFailure(void* f, GoInt reason, GoUint64 conn_id);

// go:linkname envoyGoFilterOnUpstreamData
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnUpstreamData
extern GoUint64 envoyGoFilterOnUpstreamData(void* f, GoUint64 data_size, GoUint64 data_ptr,
                                            GoInt slice_num, GoInt end_of_stream);

// go:linkname envoyGoFilterOnUpstreamEvent
// github.com/envoyproxy/envoy/contrib/golang/filters/network/source/go/pkg/network.envoyGoFilterOnUpstreamEvent
extern void envoyGoFilterOnUpstreamEvent(void* f, GoInt event);

#ifdef __cplusplus
}
#endif
#pragma once

#include <dlfcn.h>

#include <memory>
#include <string>

#include "source/common/common/logger.h"

#include "absl/synchronization/mutex.h"
#include "contrib/golang/common/dso/libgolang.h"

namespace Envoy {
namespace Dso {

class Dso {
public:
  Dso() = default;
  Dso(const std::string dso_name);
  ~Dso();
  bool loaded() { return loaded_; }

protected:
  const std::string dso_name_;
  void* handler_{nullptr};
  bool loaded_{false};
};

class HttpFilterDso : public Dso {
public:
  HttpFilterDso(const std::string dso_name) : Dso(dso_name){};
  virtual ~HttpFilterDso() = default;

  virtual GoUint64 envoyGoFilterNewHttpPluginConfig(httpConfig* p0) PURE;
  virtual GoUint64 envoyGoFilterMergeHttpPluginConfig(GoUint64 p0, GoUint64 p1, GoUint64 p2,
                                                      GoUint64 p3) PURE;
  virtual void envoyGoFilterDestroyHttpPluginConfig(GoUint64 p0, GoInt p1) PURE;
  virtual GoUint64 envoyGoFilterOnHttpHeader(httpRequest* p0, GoUint64 p1, GoUint64 p2,
                                             GoUint64 p3) PURE;
  virtual GoUint64 envoyGoFilterOnHttpData(httpRequest* p0, GoUint64 p1, GoUint64 p2,
                                           GoUint64 p3) PURE;
  virtual void envoyGoFilterOnHttpLog(httpRequest* p0, int p1) PURE;
  virtual void envoyGoFilterOnHttpDestroy(httpRequest* p0, int p1) PURE;
  virtual void envoyGoRequestSemaDec(httpRequest* p0) PURE;
};

class HttpFilterDsoImpl : public HttpFilterDso {
public:
  HttpFilterDsoImpl(const std::string dso_name);
  ~HttpFilterDsoImpl() override = default;

  GoUint64 envoyGoFilterNewHttpPluginConfig(httpConfig* p0) override;
  GoUint64 envoyGoFilterMergeHttpPluginConfig(GoUint64 p0, GoUint64 p1, GoUint64 p2,
                                              GoUint64 p3) override;
  void envoyGoFilterDestroyHttpPluginConfig(GoUint64 p0, GoInt p1) override;
  GoUint64 envoyGoFilterOnHttpHeader(httpRequest* p0, GoUint64 p1, GoUint64 p2,
                                     GoUint64 p3) override;
  GoUint64 envoyGoFilterOnHttpData(httpRequest* p0, GoUint64 p1, GoUint64 p2, GoUint64 p3) override;
  void envoyGoFilterOnHttpLog(httpRequest* p0, int p1) override;
  void envoyGoFilterOnHttpDestroy(httpRequest* p0, int p1) override;
  void envoyGoRequestSemaDec(httpRequest* p0) override;

private:
  GoUint64 (*envoy_go_filter_new_http_plugin_config_)(httpConfig* p0) = {nullptr};
  GoUint64 (*envoy_go_filter_merge_http_plugin_config_)(GoUint64 p0, GoUint64 p1, GoUint64 p2,
                                                        GoUint64 p3) = {nullptr};
  void (*envoy_go_filter_destroy_http_plugin_config_)(GoUint64 p0, GoInt p1) = {nullptr};
  GoUint64 (*envoy_go_filter_on_http_header_)(httpRequest* p0, GoUint64 p1, GoUint64 p2,
                                              GoUint64 p3) = {nullptr};
  GoUint64 (*envoy_go_filter_on_http_data_)(httpRequest* p0, GoUint64 p1, GoUint64 p2,
                                            GoUint64 p3) = {nullptr};
  void (*envoy_go_filter_on_http_log_)(httpRequest* p0, GoUint64 p1) = {nullptr};
  void (*envoy_go_filter_on_http_destroy_)(httpRequest* p0, GoUint64 p1) = {nullptr};
  void (*envoy_go_filter_go_request_sema_dec_)(httpRequest* p0) = {nullptr};
};

class ClusterSpecifierDso : public Dso {
public:
  ClusterSpecifierDso(const std::string dso_name) : Dso(dso_name){};
  virtual ~ClusterSpecifierDso() = default;

  virtual GoInt64 envoyGoOnClusterSpecify(GoUint64 plugin_ptr, GoUint64 header_ptr,
                                          GoUint64 plugin_id, GoUint64 buffer_ptr,
                                          GoUint64 buffer_len) PURE;
  virtual GoUint64 envoyGoClusterSpecifierNewPlugin(GoUint64 config_ptr, GoUint64 config_len) PURE;
};

class ClusterSpecifierDsoImpl : public ClusterSpecifierDso {
public:
  ClusterSpecifierDsoImpl(const std::string dso_name);
  ~ClusterSpecifierDsoImpl() override = default;

  GoInt64 envoyGoOnClusterSpecify(GoUint64 plugin_ptr, GoUint64 header_ptr, GoUint64 plugin_id,
                                  GoUint64 buffer_ptr, GoUint64 buffer_len) override;
  GoUint64 envoyGoClusterSpecifierNewPlugin(GoUint64 config_ptr, GoUint64 config_len) override;

private:
  GoUint64 (*envoy_go_cluster_specifier_new_plugin_)(GoUint64 config_ptr,
                                                     GoUint64 config_len) = {nullptr};
  GoUint64 (*envoy_go_on_cluster_specify_)(GoUint64 plugin_ptr, GoUint64 header_ptr,
                                           GoUint64 plugin_id, GoUint64 buffer_ptr,
                                           GoUint64 buffer_len) = {nullptr};
};

using HttpFilterDsoPtr = std::shared_ptr<HttpFilterDso>;
using ClusterSpecifierDsoPtr = std::shared_ptr<ClusterSpecifierDso>;

class NetworkFilterDso : public Dso {
public:
  NetworkFilterDso() = default;
  NetworkFilterDso(const std::string dso_name) : Dso(dso_name){};
  virtual ~NetworkFilterDso() = default;

  virtual GoUint64 envoyGoFilterOnNetworkFilterConfig(GoUint64 library_id_ptr,
                                                      GoUint64 library_id_len, GoUint64 config_ptr,
                                                      GoUint64 config_len) PURE;
  virtual GoUint64 envoyGoFilterOnDownstreamConnection(void* w, GoUint64 plugin_name_ptr,
                                                       GoUint64 plugin_name_len,
                                                       GoUint64 config_id) PURE;
  virtual GoUint64 envoyGoFilterOnDownstreamData(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                                 GoInt slice_num, GoInt end_of_stream) PURE;
  virtual void envoyGoFilterOnDownstreamEvent(void* w, GoInt event) PURE;
  virtual GoUint64 envoyGoFilterOnDownstreamWrite(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                                  GoInt slice_num, GoInt end_of_stream) PURE;

  virtual void envoyGoFilterOnUpstreamConnectionReady(void* w, GoUint64 conn_id) PURE;
  virtual void envoyGoFilterOnUpstreamConnectionFailure(void* w, GoInt reason,
                                                        GoUint64 conn_id) PURE;
  virtual void envoyGoFilterOnUpstreamData(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                           GoInt slice_num, GoInt end_of_stream) PURE;
  virtual void envoyGoFilterOnUpstreamEvent(void* w, GoInt event) PURE;

  virtual void envoyGoFilterOnSemaDec(void* w) PURE;
};

class NetworkFilterDsoImpl : public NetworkFilterDso {
public:
  NetworkFilterDsoImpl(const std::string dso_name);
  ~NetworkFilterDsoImpl() override = default;

  GoUint64 envoyGoFilterOnNetworkFilterConfig(GoUint64 library_id_ptr, GoUint64 library_id_len,
                                              GoUint64 config_ptr, GoUint64 config_len) override;
  GoUint64 envoyGoFilterOnDownstreamConnection(void* w, GoUint64 plugin_name_ptr,
                                               GoUint64 plugin_name_len,
                                               GoUint64 config_id) override;
  GoUint64 envoyGoFilterOnDownstreamData(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                         GoInt slice_num, GoInt end_of_stream) override;
  void envoyGoFilterOnDownstreamEvent(void* w, GoInt event) override;
  GoUint64 envoyGoFilterOnDownstreamWrite(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                          GoInt slice_num, GoInt end_of_stream) override;

  void envoyGoFilterOnUpstreamConnectionReady(void* w, GoUint64 conn_id) override;
  void envoyGoFilterOnUpstreamConnectionFailure(void* w, GoInt reason, GoUint64 conn_id) override;
  void envoyGoFilterOnUpstreamData(void* w, GoUint64 data_size, GoUint64 data_ptr, GoInt slice_num,
                                   GoInt end_of_stream) override;
  void envoyGoFilterOnUpstreamEvent(void* w, GoInt event) override;

  void envoyGoFilterOnSemaDec(void* w) override;

private:
  GoUint64 (*envoy_go_filter_on_network_filter_config_)(GoUint64 library_id_ptr,
                                                        GoUint64 library_id_len,
                                                        GoUint64 config_ptr,
                                                        GoUint64 config_len) = {nullptr};
  GoUint64 (*envoy_go_filter_on_downstream_connection_)(void* w, GoUint64 plugin_name_ptr,
                                                        GoUint64 plugin_name_len,
                                                        GoUint64 config_id) = {nullptr};
  GoUint64 (*envoy_go_filter_on_downstream_data_)(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                                  GoInt slice_num, GoInt end_of_stream) = {nullptr};
  void (*envoy_go_filter_on_downstream_event_)(void* w, GoInt event) = {nullptr};
  GoUint64 (*envoy_go_filter_on_downstream_write_)(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                                   GoInt slice_num,
                                                   GoInt end_of_stream) = {nullptr};

  void (*envoy_go_filter_on_upstream_connection_ready_)(void* w, GoUint64 conn_id) = {nullptr};
  void (*envoy_go_filter_on_upstream_connection_failure_)(void* w, GoInt reason,
                                                          GoUint64 conn_id) = {nullptr};
  void (*envoy_go_filter_on_upstream_data_)(void* w, GoUint64 data_size, GoUint64 data_ptr,
                                            GoInt slice_num, GoInt end_of_stream) = {nullptr};
  void (*envoy_go_filter_on_upstream_event_)(void* w, GoInt event) = {nullptr};

  void (*envoy_go_filter_on_sema_dec_)(void* w) = {nullptr};
};

using NetworkFilterDsoPtr = std::shared_ptr<NetworkFilterDso>;

/*
 * We do not unload a dynamic library once it is loaded. This is because
 * Go shared library could not be unload by dlclose yet, see:
 * https://github.com/golang/go/issues/11100
 */
template <class T> class DsoManager {

public:
  /**
   * Load the go plugin dynamic library.
   * @param dso_id is unique ID for dynamic library.
   * @param dso_name used to specify the absolute path of the dynamic library.
   * @param plugin_name used to specify the unique plugin name.
   * @return nullptr if load are invalid.
   */
  static std::shared_ptr<T> load(std::string dso_id, std::string dso_name,
                                 std::string plugin_name) {
    auto dso = load(dso_id, dso_name);
    if (dso != nullptr) {
      DsoStoreType& dsoStore = getDsoStore();
      absl::WriterMutexLock lock(&dsoStore.mutex_);
      dsoStore.plugin_name_to_dso_[plugin_name] = dso;
    }
    return dso;
  };

  /**
   * Load the go plugin dynamic library.
   * @param dso_id is unique ID for dynamic library.
   * @param dso_name used to specify the absolute path of the dynamic library.
   * @return nullptr if load are invalid.
   */
  static std::shared_ptr<T> load(std::string dso_id, std::string dso_name) {
    ENVOY_LOG_MISC(debug, "load {} {} dso instance.", dso_id, dso_name);

    DsoStoreType& dsoStore = getDsoStore();
    absl::WriterMutexLock lock(&dsoStore.mutex_);
    auto it = dsoStore.id_to_dso_.find(dso_id);
    if (it != dsoStore.id_to_dso_.end()) {
      return it->second;
    }

    auto dso = std::make_shared<T>(dso_name);
    if (!dso->loaded()) {
      return nullptr;
    }
    dsoStore.id_to_dso_[dso_id] = dso;
    return dso;
  };

  /**
   * Get the go plugin dynamic library.
   * @param dso_id is unique ID for dynamic library.
   * @return nullptr if get failed. Otherwise, return the DSO instance.
   */
  static std::shared_ptr<T> getDsoByID(std::string dso_id) {
    DsoStoreType& dsoStore = getDsoStore();
    absl::ReaderMutexLock lock(&dsoStore.mutex_);
    auto it = dsoStore.id_to_dso_.find(dso_id);
    if (it != dsoStore.id_to_dso_.end()) {
      return it->second;
    }
    return nullptr;
  };

  /**
   * Get the go plugin dynamic library by plugin name.
   * @param plugin_name is unique ID for a plugin, one DSO may contains multiple plugins.
   * @return nullptr if get failed. Otherwise, return the DSO instance.
   */
  static std::shared_ptr<T> getDsoByPluginName(std::string plugin_name) {
    DsoStoreType& dsoStore = getDsoStore();
    absl::ReaderMutexLock lock(&dsoStore.mutex_);
    auto it = dsoStore.plugin_name_to_dso_.find(plugin_name);
    if (it != dsoStore.plugin_name_to_dso_.end()) {
      return it->second;
    }
    return nullptr;
  };

private:
  using DsoMapType = absl::flat_hash_map<std::string, std::shared_ptr<T>>;
  struct DsoStoreType {
    DsoMapType id_to_dso_ ABSL_GUARDED_BY(mutex_){{
        {"", nullptr},
    }};
    DsoMapType plugin_name_to_dso_ ABSL_GUARDED_BY(mutex_){{
        {"", nullptr},
    }};
    absl::Mutex mutex_;
  };

  static DsoStoreType& getDsoStore() { MUTABLE_CONSTRUCT_ON_FIRST_USE(DsoStoreType); }
};

} // namespace Dso
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_library(
    name = "dso_lib",
    srcs = ["dso.cc"],
    hdrs = [
        "api.h",
        "dso.h",
        "libgolang.h",
    ],
    deps = [
        "//source/common/common:minimal_logger_lib",
        "//source/common/common:utility_lib",
    ],
)
#include "contrib/golang/common/log/cgo.h"

namespace Envoy {
namespace Extensions {
namespace Common {
namespace Golang {

/* FilterLogger */
void FilterLogger::log(uint32_t level, absl::string_view message) const {
  switch (static_cast<spdlog::level::level_enum>(level)) {
  case spdlog::level::trace:
    ENVOY_LOG(trace, "{}", message);
    return;
  case spdlog::level::debug:
    ENVOY_LOG(debug, "{}", message);
    return;
  case spdlog::level::info:
    ENVOY_LOG(info, "{}", message);
    return;
  case spdlog::level::warn:
    ENVOY_LOG(warn, "{}", message);
    return;
  case spdlog::level::err:
    ENVOY_LOG(error, "{}", message);
    return;
  case spdlog::level::critical:
    ENVOY_LOG(critical, "{}", message);
    return;
  case spdlog::level::off:
    // means not logging
    return;
  case spdlog::level::n_levels:
    PANIC("not implemented");
  }

  ENVOY_LOG(error, "undefined log level {} with message '{}'", level, message);

  PANIC_DUE_TO_CORRUPT_ENUM;
}

uint32_t FilterLogger::level() const { return static_cast<uint32_t>(ENVOY_LOGGER().level()); }

const FilterLogger& getFilterLogger() { CONSTRUCT_ON_FIRST_USE(FilterLogger); }

// The returned absl::string_view only refer to Go memory,
// should not use it after the current cgo call returns.
absl::string_view stringViewFromGoPointer(void* p, int len) {
  return {static_cast<const char*>(p), static_cast<size_t>(len)};
}

#ifdef __cplusplus
extern "C" {
#endif

void envoyGoFilterLog(uint32_t level, void* message_data, int message_len) {
  auto mesg = stringViewFromGoPointer(message_data, message_len);
  getFilterLogger().log(level, mesg);
}

uint32_t envoyGoFilterLogLevel() { return getFilterLogger().level(); }

#ifdef __cplusplus
}
#endif
} // namespace Golang
} // namespace Common
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "source/common/common/utility.h"

#include "contrib/golang/common/dso/dso.h"

namespace Envoy {
namespace Extensions {
namespace Common {
namespace Golang {

class FilterLogger : Logger::Loggable<Logger::Id::golang> {
public:
  FilterLogger() = default;

  void log(uint32_t level, absl::string_view message) const;
  uint32_t level() const;
};

} // namespace Golang
} // namespace Common
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_library(
    name = "log_lib",
    srcs = ["cgo.cc"],
    hdrs = [
        "cgo.h",
    ],
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/dso:dso_lib",
        "//source/common/common:utility_lib",
    ],
)
#include "envoy/extensions/filters/network/http_connection_manager/v3/http_connection_manager.pb.h"

#include "test/config/v2_link_hacks.h"
#include "test/integration/http_integration.h"
#include "test/test_common/utility.h"

#include "contrib/golang/router/cluster_specifier/source/config.h"
#include "contrib/golang/router/cluster_specifier/source/golang_cluster_specifier.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Router {
namespace Golang {
namespace {

class GolangClusterSpecifierIntegrationTest : public Envoy::HttpIntegrationTest,
                                              public testing::Test {
public:
  GolangClusterSpecifierIntegrationTest()
      : HttpIntegrationTest(Http::CodecType::HTTP1, Network::Address::IpVersion::v4) {}

  void initializeRoute(const std::string& vhost_config_yaml) {
    envoy::config::route::v3::VirtualHost vhost;
    TestUtility::loadFromYaml(vhost_config_yaml, vhost);
    config_helper_.addVirtualHost(vhost);
    initialize();
  }
};

static const auto yaml_fmt =
    R"EOF(
name: test_golang_cluster_specifier_plugin
domains:
- test.com
routes:
- name: test_route_1
  match:
    prefix: /
  route:
    inline_cluster_specifier_plugin:
      extension:
        name: golang
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.router.cluster_specifier.golang.v3alpha.Config
          library_id: %s
          library_path: %s
          default_cluster: %s
          config:
            "@type": type.googleapis.com/xds.type.v3.TypedStruct
            value:
              invalid_prefix: "/admin/"
              default_prefix: "/default/"
              panic_prefix: "/panic/"
)EOF";

std::string genSoPath(std::string name) {
  return TestEnvironment::substitute(
      "{{ test_rundir }}/contrib/golang/router/cluster_specifier/test/test_data/" + name +
      "/plugin.so");
}

// Go plugin choose cluster: "cluster_0"
TEST_F(GolangClusterSpecifierIntegrationTest, OK) {
  auto so_id = "simple";
  auto yaml_string = absl::StrFormat(yaml_fmt, so_id, genSoPath(so_id), "");
  initializeRoute(yaml_string);

  codec_client_ = makeHttpConnection(lookupPort("http"));

  Http::TestResponseHeaderMapImpl response_headers{
      {"server", "envoy"},
      {":status", "200"},
  };

  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "GET"}, {":path", "/test"}, {":scheme", "http"}, {":authority", "test.com"}};

  auto response = sendRequestAndWaitForResponse(request_headers, 0, response_headers, 0);

  ASSERT_TRUE(response->waitForEndStream());
  EXPECT_TRUE(response->complete());
  EXPECT_EQ(response->headers().getStatusValue(), "200");

  cleanupUpstreamAndDownstream();
}

// Go plugin return cluster: "cluster_unknown"
TEST_F(GolangClusterSpecifierIntegrationTest, UnknownCluster) {
  auto so_id = "simple";
  auto yaml_string = absl::StrFormat(yaml_fmt, so_id, genSoPath(so_id), "");
  initializeRoute(yaml_string);

  codec_client_ = makeHttpConnection(lookupPort("http"));

  Http::TestRequestHeaderMapImpl request_headers{{":method", "GET"},
                                                 {":path", "/admin/user"},
                                                 {":scheme", "http"},
                                                 {":authority", "test.com"}};

  // Request with the "/admin/" prefix URI, the "cluster_unknown" name will be return by the cluster
  // specifier plugin.
  auto response = codec_client_->makeHeaderOnlyRequest(request_headers);
  ASSERT_TRUE(response->waitForEndStream());
  EXPECT_TRUE(response->complete());
  EXPECT_THAT(response->headers(), Http::HttpStatusIs("503"));

  cleanupUpstreamAndDownstream();
}

// Go plugin choose cluster: "", using the default_cluster: "cluster_0"
TEST_F(GolangClusterSpecifierIntegrationTest, DefaultCluster_OK) {
  auto so_id = "simple";
  auto yaml_string = absl::StrFormat(yaml_fmt, so_id, genSoPath(so_id), "cluster_0");
  initializeRoute(yaml_string);

  codec_client_ = makeHttpConnection(lookupPort("http"));

  Http::TestResponseHeaderMapImpl response_headers{
      {"server", "envoy"},
      {":status", "200"},
  };

  // Request with the "/default/" prefix URI, the "" empty name will be return by the cluster
  // specifier plugin.
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "GET"}, {":path", "/default/1"}, {":scheme", "http"}, {":authority", "test.com"}};

  auto response = sendRequestAndWaitForResponse(request_headers, 0, response_headers, 0);

  ASSERT_TRUE(response->waitForEndStream());
  EXPECT_TRUE(response->complete());
  EXPECT_EQ(response->headers().getStatusValue(), "200");

  cleanupUpstreamAndDownstream();
}

// Go plugin choose cluster: "", using the default_cluster: "cluster_unknown"
TEST_F(GolangClusterSpecifierIntegrationTest, DefaultCluster_Unknown) {
  auto so_id = "simple";
  auto yaml_string = absl::StrFormat(yaml_fmt, so_id, genSoPath(so_id), "cluster_unknown");
  initializeRoute(yaml_string);

  codec_client_ = makeHttpConnection(lookupPort("http"));

  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "GET"}, {":path", "/default/1"}, {":scheme", "http"}, {":authority", "test.com"}};

  // Request with the "/default/" prefix URI, the "" empty name will be return by the cluster
  // specifier plugin.
  auto response = codec_client_->makeHeaderOnlyRequest(request_headers);
  ASSERT_TRUE(response->waitForEndStream());
  EXPECT_TRUE(response->complete());
  EXPECT_THAT(response->headers(), Http::HttpStatusIs("503"));

  cleanupUpstreamAndDownstream();
}

// Go plugin panic: "", using the default_cluster: "cluster_0"
TEST_F(GolangClusterSpecifierIntegrationTest, Panic_OK) {
  auto so_id = "simple";
  auto yaml_string = absl::StrFormat(yaml_fmt, so_id, genSoPath(so_id), "cluster_0");
  initializeRoute(yaml_string);

  codec_client_ = makeHttpConnection(lookupPort("http"));

  Http::TestResponseHeaderMapImpl response_headers{
      {"server", "envoy"},
      {":status", "200"},
  };

  // The go cluster specifier plugin will panic with the "/panic/" prefix URI.
  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "GET"}, {":path", "/panic/1"}, {":scheme", "http"}, {":authority", "test.com"}};

  auto response = sendRequestAndWaitForResponse(request_headers, 0, response_headers, 0);

  ASSERT_TRUE(response->waitForEndStream());
  EXPECT_TRUE(response->complete());
  EXPECT_EQ(response->headers().getStatusValue(), "200");

  cleanupUpstreamAndDownstream();
}

// Go plugin panic, using the default_cluster: "cluster_unknown"
TEST_F(GolangClusterSpecifierIntegrationTest, Panic_Unknown) {
  auto so_id = "simple";
  auto yaml_string = absl::StrFormat(yaml_fmt, so_id, genSoPath(so_id), "cluster_unknown");
  initializeRoute(yaml_string);

  codec_client_ = makeHttpConnection(lookupPort("http"));

  Http::TestRequestHeaderMapImpl request_headers{
      {":method", "GET"}, {":path", "/panic/1"}, {":scheme", "http"}, {":authority", "test.com"}};

  // The go cluster specifier plugin will panic with the "/panic/" prefix URI.
  auto response = codec_client_->makeHeaderOnlyRequest(request_headers);
  ASSERT_TRUE(response->waitForEndStream());
  EXPECT_TRUE(response->complete());
  EXPECT_THAT(response->headers(), Http::HttpStatusIs("503"));

  cleanupUpstreamAndDownstream();
}

} // namespace
} // namespace Golang
} // namespace Router
} // namespace Envoy
package main

import (
	xds "github.com/cncf/xds/go/xds/type/v3"
	"github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/api"
	"github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/cluster_specifier"
	"google.golang.org/protobuf/types/known/anypb"
)

func init() {
	cluster_specifier.RegisterClusterSpecifierConfigFactory(configFactory)
}

func configFactory(config *anypb.Any) api.ClusterSpecifier {
	configStruct := &xds.TypedStruct{}
	if err := config.UnmarshalTo(configStruct); err != nil {
		panic(err)
	}
	plugin := &clusterSpecifier{}
	m := configStruct.Value.AsMap()
	if value, ok := m["invalid_prefix"]; ok {
		if valueStr, ok := value.(string); ok {
			plugin.invalidPrefix = valueStr
		}
	}
	if value, ok := m["default_prefix"]; ok {
		if valueStr, ok := value.(string); ok {
			plugin.defaultPrefix = valueStr
		}
	}
	if value, ok := m["panic_prefix"]; ok {
		if valueStr, ok := value.(string); ok {
			plugin.panicPrefix = valueStr
		}
	}
	return plugin
}

func main() {}
module example.com/routeconfig

go 1.18

require (
	github.com/cncf/xds/go v0.0.0-20231128003011-0fa0005c9caa
	github.com/envoyproxy/envoy v1.28.0
)

require (
	google.golang.org/genproto/googleapis/api v0.0.0-20240102182953-50ed04b92917 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20240102182953-50ed04b92917 // indirect
)

require (
	github.com/envoyproxy/protoc-gen-validate v1.0.2 // indirect
	github.com/golang/protobuf v1.5.3 // indirect
	google.golang.org/protobuf v1.32.0
)

replace github.com/envoyproxy/envoy => ../../../../../../../
package main

import (
	"strings"

	"github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/api"
)

type clusterSpecifier struct {
	invalidPrefix string
	defaultPrefix string
	panicPrefix   string
}

func (s *clusterSpecifier) Cluster(header api.RequestHeaderMap) string {
	path, _ := header.Get(":path")

	// block the request with an unknown cluster.
	if strings.HasPrefix(path, s.invalidPrefix) {
		return "cluster_unknown"
	}

	// return "" will using the default_cluster in the C++ side.
	if strings.HasPrefix(path, s.defaultPrefix) {
		return ""
	}

	// panic, will using the default_cluster in the C++ side.
	if strings.HasPrefix(path, s.panicPrefix) {
		panic("test")
	}

	return "cluster_0"
}
load("@io_bazel_rules_go//go:def.bzl", "go_binary")

licenses(["notice"])  # Apache 2

go_binary(
    name = "plugin.so",
    srcs = [
        "config.go",
        "plugin.go",
    ],
    out = "plugin.so",
    cgo = True,
    importpath = "github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/test/test_data/simple",
    linkmode = "c-shared",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/router/cluster_specifier/source/go/pkg/api",
        "//contrib/golang/router/cluster_specifier/source/go/pkg/cluster_specifier",
        "@com_github_cncf_xds_go//xds/type/v3:type",
        "@org_golang_google_protobuf//types/known/anypb",
        "@org_golang_google_protobuf//types/known/structpb",
    ],
)
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "golang_integration_test",
    srcs = ["golang_integration_test.cc"],
    data = [
        "//contrib/golang/router/cluster_specifier/test/test_data/simple:plugin.so",
    ],
    env = {"GODEBUG": "cgocheck=0"},
    deps = [
        "//contrib/golang/router/cluster_specifier/source:config",
        "//source/exe:main_common_lib",
        "//test/config:v2_link_hacks",
        "//test/integration:http_integration_lib",
        "//test/test_common:utility_lib",
        "@envoy_api//envoy/extensions/filters/network/http_connection_manager/v3:pkg_cc_proto",
    ],
)
#include "contrib/golang/router/cluster_specifier/source/golang_cluster_specifier.h"

#include <chrono>

#include "source/common/router/config_impl.h"

namespace Envoy {
namespace Router {
namespace Golang {

// limit the max length of cluster name that could return from the Golang cluster specifier plugin,
// to avoid memory security vulnerability since there might be a bug in Golang side.
#define MAX_CLUSTER_LENGTH 8192

ClusterConfig::ClusterConfig(const GolangClusterProto& config)
    : so_id_(config.library_id()), so_path_(config.library_path()),
      default_cluster_(config.default_cluster()), config_(config.config()) {
  ENVOY_LOG_MISC(debug, "load golang library at parse cluster specifier plugin config: {} {}",
                 so_id_, so_path_);

  // loads DSO store a static map and a open handles leak will occur when the filter gets loaded and
  // unloaded.
  // TODO: unload DSO when filter updated.
  dynamic_lib_ = Envoy::Dso::DsoManager<Dso::ClusterSpecifierDsoImpl>::load(so_id_, so_path_);
  if (dynamic_lib_ == nullptr) {
    throw EnvoyException(fmt::format("golang_cluster_specifier_plugin: load library failed: {} {}",
                                     so_id_, so_path_));
  }

  std::string str;
  if (!config_.SerializeToString(&str)) {
    throw EnvoyException(
        fmt::format("golang_cluster_specifier_plugin: serialize config to string failed: {} {}",
                    so_id_, so_path_));
  }

  auto ptr = reinterpret_cast<unsigned long long>(str.data());
  auto len = str.length();
  plugin_id_ = dynamic_lib_->envoyGoClusterSpecifierNewPlugin(ptr, len);
  if (plugin_id_ == 0) {
    throw EnvoyException(
        fmt::format("golang_cluster_specifier_plugin: generate plugin failed in golang side: {} {}",
                    so_id_, so_path_));
  }
}

RouteConstSharedPtr
GolangClusterSpecifierPlugin::route(RouteConstSharedPtr parent,
                                    const Http::RequestHeaderMap& header) const {
  ASSERT(dynamic_cast<const RouteEntryImplBase*>(parent.get()) != nullptr);
  int buffer_len = 256;
  std::string buffer;
  std::string cluster;
  auto dlib = config_->getDsoLib();
  ASSERT(dlib != nullptr);

  while (true) {
    buffer.reserve(buffer_len);
    auto plugin_id = config_->getPluginId();
    auto header_ptr = reinterpret_cast<uint64_t>(&header);
    auto plugin_ptr = reinterpret_cast<uint64_t>(this);
    auto buffer_ptr = reinterpret_cast<uint64_t>(buffer.data());
    auto new_len =
        dlib->envoyGoOnClusterSpecify(plugin_ptr, header_ptr, plugin_id, buffer_ptr, buffer_len);

    if (new_len <= 0) {
      ENVOY_LOG(debug, "golang cluster specifier choose the default cluster");
      cluster = config_->defaultCluster();
      break;
    } else if (new_len <= buffer_len) {
      ENVOY_LOG(debug, "buffer size fit the cluster name from golang");
      cluster = std::string{buffer.data(), size_t(new_len)};
      break;
    } else {
      RELEASE_ASSERT(new_len <= MAX_CLUSTER_LENGTH, "cluster name too long");
      ENVOY_LOG(debug, "need larger size of buffer to save the cluster name in golang, try again");
      buffer_len = new_len;
    }
  }

  return std::make_shared<RouteEntryImplBase::DynamicRouteEntry>(
      dynamic_cast<const RouteEntryImplBase*>(parent.get()), parent, cluster);
}

void GolangClusterSpecifierPlugin::log(absl::string_view& msg) const {
  ENVOY_LOG(error, "{}", msg);
}

} // namespace Golang
} // namespace Router
} // namespace Envoy
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package cluster_specifier

/*
// ref https://github.com/golang/go/issues/25832

#cgo linux LDFLAGS: -Wl,-unresolved-symbols=ignore-all
#cgo darwin LDFLAGS: -Wl,-undefined,dynamic_lookup

#include <stdlib.h>
#include <string.h>

#include "api.h"

*/
import "C"

import (
	"sync"
	"sync/atomic"

	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/anypb"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/utils"
)

const errCodeInvalidConfig = 0

var (
	pluginNumGenerator uint64
	pluginCache        = sync.Map{}
)

//export envoyGoClusterSpecifierNewPlugin
func envoyGoClusterSpecifierNewPlugin(configPtr uint64, configLen uint64) uint64 {
	if clusterSpecifierConfigFactory == nil {
		panic("no cluster specifier config factory registered")
	}

	buf := utils.BytesToSlice(configPtr, configLen)
	var any anypb.Any
	if err := proto.Unmarshal(buf, &any); err != nil {
		return errCodeInvalidConfig
	}

	plugin := clusterSpecifierConfigFactory(&any)
	pluginNum := atomic.AddUint64(&pluginNumGenerator, 1)

	pluginCache.Store(pluginNum, plugin)

	return pluginNum
}

//export envoyGoClusterSpecifierDestroyPlugin
func envoyGoClusterSpecifierDestroyPlugin(id uint64) {
	pluginCache.Delete(id)
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package cluster_specifier

import (
	"github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/api"
)

var clusterSpecifierConfigFactory api.ClusterSpecifierConfigFactory

func RegisterClusterSpecifierConfigFactory(f api.ClusterSpecifierConfigFactory) {
	clusterSpecifierConfigFactory = f
}

func getClusterSpecifier(configId uint64) api.ClusterSpecifier {
	if v, ok := pluginCache.Load(configId); ok {
		return v.(api.ClusterSpecifier)
	}
	return nil
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package cluster_specifier

import (
	"github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/api"
)

type httpHeaderMap struct {
	headerPtr uint64
}

var _ api.RequestHeaderMap = (*httpHeaderMap)(nil)

func (h *httpHeaderMap) Get(key string) (string, bool) {
	var value string
	found := cAPI.HttpGetHeader(h.headerPtr, &key, &value)
	return value, found
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package cluster_specifier

/*
// ref https://github.com/golang/go/issues/25832

#cgo linux LDFLAGS: -Wl,-unresolved-symbols=ignore-all
#cgo darwin LDFLAGS: -Wl,-undefined,dynamic_lookup

#include <stdlib.h>
#include <string.h>

#include "api.h"
*/
import "C"
import (
	"unsafe"

	"github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/api"
)

const foundHeaderValue = 1

type httpCApiImpl struct{}

func (c *httpCApiImpl) HttpGetHeader(headerPtr uint64, key *string, value *string) bool {
	found := C.envoyGoClusterSpecifierGetHeader(C.ulonglong(headerPtr), unsafe.Pointer(key), unsafe.Pointer(value))
	return int(found) == foundHeaderValue
}

func (c *httpCApiImpl) HttpLogError(pluginPtr uint64, msg *string) {
	C.envoyGoClusterSpecifierLogError(C.ulonglong(pluginPtr), unsafe.Pointer(msg))
}

var cAPI api.HttpCAPI = &httpCApiImpl{}

// SetHttpCAPI for mock cAPI
func SetHttpCAPI(api api.HttpCAPI) {
	cAPI = api
}
#pragma once

// NOLINT(namespace-envoy)

#ifdef __cplusplus
extern "C" {
#endif

int envoyGoClusterSpecifierGetHeader(unsigned long long header_ptr, void* key, void* value);
void envoyGoClusterSpecifierLogError(unsigned long long plugin_ptr, void* msg);

#ifdef __cplusplus
} // extern "C"
#endif
load("@io_bazel_rules_go//go:def.bzl", "go_library")

licenses(["notice"])  # Apache 2

go_library(
    name = "cluster_specifier",
    srcs = [
        "api.h",
        "capi_impl.go",
        "config.go",
        "factory.go",
        "shim.go",
        "type.go",
    ],
    cgo = True,
    clinkopts = select({
        "@io_bazel_rules_go//go/platform:android": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "@io_bazel_rules_go//go/platform:darwin": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:ios": [
            "-Wl,-undefined,dynamic_lookup",
        ],
        "@io_bazel_rules_go//go/platform:linux": [
            "-Wl,-unresolved-symbols=ignore-all",
        ],
        "//conditions:default": [],
    }),
    importpath = "github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/cluster_specifier",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/golang/common/go/utils",
        "//contrib/golang/router/cluster_specifier/source/go/pkg/api",
        "@org_golang_google_protobuf//proto",
        "@org_golang_google_protobuf//types/known/anypb",
    ],
)
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package cluster_specifier

import "C"
import (
	"fmt"

	"github.com/envoyproxy/envoy/contrib/golang/common/go/utils"
)

const errPanic = -1

//export envoyGoOnClusterSpecify
func envoyGoOnClusterSpecify(pluginPtr uint64, headerPtr uint64, pluginId uint64, bufferPtr uint64, bufferLen uint64) (l int64) {
	defer func() {
		if err := recover(); err != nil {
			msg := fmt.Sprintf("golang cluster specifier plugin panic: %v", err)
			cAPI.HttpLogError(pluginPtr, &msg)
			l = errPanic
		}
	}()
	header := &httpHeaderMap{
		headerPtr: headerPtr,
	}
	specifier := getClusterSpecifier(pluginId)
	if specifier == nil {
		panic(fmt.Sprintf("no registered cluster specifier plugin for id: %d", pluginId))
	}
	cluster := specifier.Cluster(header)
	clusterLen := uint64(len(cluster))
	if clusterLen == 0 {
		// means use the default cluster
		return 0
	}
	if clusterLen > bufferLen {
		// buffer length is not large enough.
		return int64(clusterLen)
	}
	buffer := utils.BufferToSlice(bufferPtr, clusterLen)
	copy(buffer, cluster)
	return int64(clusterLen)
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package api

type HttpCAPI interface {
	HttpGetHeader(headerPtr uint64, key *string, value *string) bool
	HttpLogError(pluginPtr uint64, msg *string)
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package api

import "google.golang.org/protobuf/types/known/anypb"

type ClusterSpecifierConfigParser interface {
	Parse(any *anypb.Any) interface{}
}

type ClusterSpecifier interface {
	// Cluster return the cluster name that will be used in the Envoy side,
	// Envoy will use the default_cluster in the plugin config when return an empty string, or panic happens.
	Cluster(RequestHeaderMap) string
}

type ClusterSpecifierFactory func(config interface{}) ClusterSpecifier

type ClusterSpecifierConfigFactory func(any *anypb.Any) ClusterSpecifier

type RequestHeaderMap interface {
	// Get value of key
	// If multiple values associated with this key, first one will be returned.
	Get(key string) (string, bool)
}
load("@io_bazel_rules_go//go:def.bzl", "go_library")

licenses(["notice"])  # Apache 2

go_library(
    name = "api",
    srcs = [
        "capi.go",
        "cluster.go",
    ],
    importpath = "github.com/envoyproxy/envoy/contrib/golang/router/cluster_specifier/source/go/pkg/api",
    visibility = ["//visibility:public"],
    deps = [
        "@org_golang_google_protobuf//types/known/anypb",
    ],
)
#pragma once

#include "contrib/golang/router/cluster_specifier/source/golang_cluster_specifier.h"

namespace Envoy {
namespace Router {
namespace Golang {

class GolangClusterSpecifierPluginFactoryConfig : public ClusterSpecifierPluginFactoryConfig {
public:
  GolangClusterSpecifierPluginFactoryConfig() = default;
  ClusterSpecifierPluginSharedPtr
  createClusterSpecifierPlugin(const Protobuf::Message& config,
                               Server::Configuration::CommonFactoryContext&) override;

  ProtobufTypes::MessagePtr createEmptyConfigProto() override {
    return std::make_unique<GolangClusterProto>();
  }

  std::string name() const override { return "envoy.router.cluster_specifier_plugin.golang"; }
};

} // namespace Golang
} // namespace Router
} // namespace Envoy
#pragma once

#include "envoy/router/cluster_specifier_plugin.h"

#include "source/common/http/utility.h"

#include "contrib/envoy/extensions/router/cluster_specifier/golang/v3alpha/golang.pb.h"
#include "contrib/golang/common/dso/dso.h"

namespace Envoy {
namespace Router {
namespace Golang {

using GolangClusterProto = envoy::extensions::router::cluster_specifier::golang::v3alpha::Config;

class ClusterConfig : Logger::Loggable<Logger::Id::http> {
public:
  ClusterConfig(const GolangClusterProto& config);
  uint64_t getPluginId() { return plugin_id_; };
  const std::string& defaultCluster() { return default_cluster_; }
  Dso::ClusterSpecifierDsoPtr getDsoLib() { return dynamic_lib_; }

private:
  const std::string so_id_;
  const std::string so_path_;
  const std::string default_cluster_;
  const ProtobufWkt::Any config_;
  uint64_t plugin_id_{0};
  Dso::ClusterSpecifierDsoPtr dynamic_lib_;
};

using ClusterConfigSharedPtr = std::shared_ptr<ClusterConfig>;

class GolangClusterSpecifierPlugin : public ClusterSpecifierPlugin,
                                     Logger::Loggable<Logger::Id::http> {
public:
  GolangClusterSpecifierPlugin(ClusterConfigSharedPtr config) : config_(config){};

  RouteConstSharedPtr route(RouteConstSharedPtr parent,
                            const Http::RequestHeaderMap& header) const override;
  void log(absl::string_view& msg) const;

private:
  ClusterConfigSharedPtr config_;
};

} // namespace Golang
} // namespace Router
} // namespace Envoy
#include "contrib/golang/router/cluster_specifier/source/golang_cluster_specifier.h"

namespace Envoy {
namespace Router {
namespace Golang {

//
// These functions should only be invoked in the current Envoy worker thread.
//

enum GetHeaderResult {
  Mising = 0,
  Found = 1,
};

absl::string_view referGoString(void* str) {
  if (str == nullptr) {
    return "";
  }
  auto go_str = reinterpret_cast<GoString*>(str);
  return absl::string_view(go_str->p, go_str->n); // NOLINT(modernize-return-braced-init-list)
}

#ifdef __cplusplus
extern "C" {
#endif

// Get the value of the specified header key from the request header map.
// Only use the first value when there are multiple values associated with the key.
int envoyGoClusterSpecifierGetHeader(unsigned long long header_ptr, void* key, void* value) {
  auto header = reinterpret_cast<Http::RequestHeaderMap*>(header_ptr);
  auto key_str = referGoString(key);
  auto go_value = reinterpret_cast<GoString*>(value);
  auto result = header->get(Http::LowerCaseString(key_str));

  if (!result.empty()) {
    auto str = result[0]->value().getStringView();
    go_value->p = str.data();
    go_value->n = str.length();
    return static_cast<int>(GetHeaderResult::Found);
  }
  return static_cast<int>(GetHeaderResult::Mising);
}

// Log the message with the error level.
void envoyGoClusterSpecifierLogError(unsigned long long plugin_ptr, void* msg) {
  auto msgStr = referGoString(msg);
  auto plugin = reinterpret_cast<GolangClusterSpecifierPlugin*>(plugin_ptr);
  plugin->log(msgStr);
}

#ifdef __cplusplus
}
#endif

} // namespace Golang
} // namespace Router
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_contrib_extension",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

# Golang cluster specifier plugin.

envoy_contrib_package()

envoy_cc_library(
    name = "golang_cluster_specifier_lib",
    srcs = [
        "golang_cluster_specifier.cc",
    ],
    hdrs = [
        "golang_cluster_specifier.h",
    ],
    deps = [
        ":cgo",
        "//contrib/golang/common/dso:dso_lib",
        "//envoy/router:cluster_specifier_plugin_interface",
        "//source/common/common:utility_lib",
        "//source/common/http:utility_lib",
        "//source/common/router:config_lib",
        "@envoy_api//contrib/envoy/extensions/router/cluster_specifier/golang/v3alpha:pkg_cc_proto",
    ],
)

envoy_cc_contrib_extension(
    name = "config",
    srcs = ["config.cc"],
    hdrs = ["config.h"],
    deps = [
        ":golang_cluster_specifier_lib",
        "//envoy/registry",
    ],
)

envoy_cc_library(
    name = "cgo",
    srcs = ["cgo.cc"],
    hdrs = [
        "golang_cluster_specifier.h",
    ],
    deps = [
        "//contrib/golang/common/dso:dso_lib",
        "//envoy/router:cluster_specifier_plugin_interface",
        "//source/common/common:utility_lib",
        "//source/common/http:utility_lib",
        "@envoy_api//contrib/envoy/extensions/router/cluster_specifier/golang/v3alpha:pkg_cc_proto",
    ],
)
#include "contrib/golang/router/cluster_specifier/source/config.h"

#include <chrono>

namespace Envoy {
namespace Router {
namespace Golang {

ClusterSpecifierPluginSharedPtr
GolangClusterSpecifierPluginFactoryConfig::createClusterSpecifierPlugin(
    const Protobuf::Message& config, Server::Configuration::CommonFactoryContext&) {
  const auto& typed_config = dynamic_cast<const GolangClusterProto&>(config);
  auto cluster_config = std::make_shared<ClusterConfig>(typed_config);
  return std::make_shared<GolangClusterSpecifierPlugin>(cluster_config);
}

REGISTER_FACTORY(GolangClusterSpecifierPluginFactoryConfig, ClusterSpecifierPluginFactoryConfig);

} // namespace Golang
} // namespace Router
} // namespace Envoy
#include "test/mocks/server/factory_context.h"

#include "contrib/dynamo/filters/http/source/config.h"
#include "contrib/envoy/extensions/filters/http/dynamo/v3/dynamo.pb.h"
#include "contrib/envoy/extensions/filters/http/dynamo/v3/dynamo.pb.validate.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {
namespace {

TEST(DynamoFilterConfigTest, DynamoFilter) {
  NiceMock<Server::Configuration::MockFactoryContext> context;
  DynamoFilterConfig factory;
  envoy::extensions::filters::http::dynamo::v3::Dynamo proto_config;
  Http::FilterFactoryCb cb =
      factory.createFilterFactoryFromProto(proto_config, "stats", context).value();
  Http::MockFilterChainFactoryCallbacks filter_callback;
  EXPECT_CALL(filter_callback, addStreamFilter(_));
  cb(filter_callback);
}

} // namespace
} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include <string>

#include "test/mocks/stats/mocks.h"

#include "contrib/dynamo/filters/http/source/dynamo_stats.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {
namespace {

TEST(DynamoStats, PartitionIdStatString) {
  Stats::IsolatedStoreImpl store;
  auto build_partition_string =
      [&store](const std::string& stat_prefix, const std::string& table_name,
               const std::string& operation, const std::string& partition_id) -> std::string {
    DynamoStats stats(*store.rootScope(), stat_prefix);
    Stats::Counter& counter = stats.buildPartitionStatCounter(table_name, operation, partition_id);
    return counter.name();
  };

  {
    std::string stats_prefix = "prefix.";
    std::string table_name = "locations";
    std::string operation = "GetItem";
    std::string partition_id = "6235c781-1d0d-47a3-a4ea-eec04c5883ca";
    std::string partition_stat_string =
        build_partition_string(stats_prefix, table_name, operation, partition_id);
    std::string expected_stat_string =
        "prefix.dynamodb.table.locations.capacity.GetItem.__partition_id=c5883ca";
    EXPECT_EQ(expected_stat_string, partition_stat_string);
  }

  {
    std::string stats_prefix = "http.egress_dynamodb_iad.";
    std::string table_name = "locations-sandbox-partition-test-iad-mytest-really-long-name";
    std::string operation = "GetItem";
    std::string partition_id = "6235c781-1d0d-47a3-a4ea-eec04c5883ca";

    std::string partition_stat_string =
        build_partition_string(stats_prefix, table_name, operation, partition_id);
    std::string expected_stat_string =
        "http.egress_dynamodb_iad.dynamodb.table.locations-sandbox-partition-test-iad-mytest-"
        "really-long-name.capacity.GetItem.__partition_id=c5883ca";
    EXPECT_EQ(expected_stat_string, partition_stat_string);
  }
  {
    std::string stats_prefix = "http.egress_dynamodb_iad.";
    std::string table_name = "locations-sandbox-partition-test-iad-mytest-rea";
    std::string operation = "GetItem";
    std::string partition_id = "6235c781-1d0d-47a3-a4ea-eec04c5883ca";

    std::string partition_stat_string =
        build_partition_string(stats_prefix, table_name, operation, partition_id);
    std::string expected_stat_string = "http.egress_dynamodb_iad.dynamodb.table.locations-sandbox-"
                                       "partition-test-iad-mytest-rea.capacity.GetItem.__partition_"
                                       "id=c5883ca";

    EXPECT_EQ(expected_stat_string, partition_stat_string);
  }
}

} // namespace
} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include <memory>
#include <string>

#include "source/common/buffer/buffer_impl.h"
#include "source/common/http/header_map_impl.h"

#include "test/mocks/http/mocks.h"
#include "test/mocks/runtime/mocks.h"
#include "test/mocks/stats/mocks.h"
#include "test/test_common/printers.h"
#include "test/test_common/utility.h"

#include "contrib/dynamo/filters/http/source/dynamo_filter.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::NiceMock;
using testing::Property;
using testing::Return;

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {
namespace {

class DynamoFilterTest : public testing::Test {
public:
  void setup(bool enabled) {
    ON_CALL(loader_.snapshot_, featureEnabled("dynamodb.filter_enabled", 100))
        .WillByDefault(Return(enabled));
    EXPECT_CALL(loader_.snapshot_, featureEnabled("dynamodb.filter_enabled", 100));

    auto stats = std::make_shared<DynamoStats>(*stats_.rootScope(), "prefix.");
    filter_ = std::make_unique<DynamoFilter>(loader_, stats,
                                             decoder_callbacks_.dispatcher().timeSource());

    filter_->setDecoderFilterCallbacks(decoder_callbacks_);
    filter_->setEncoderFilterCallbacks(encoder_callbacks_);
  }

  ~DynamoFilterTest() override { filter_->onDestroy(); }

  NiceMock<Stats::MockStore> stats_;
  std::unique_ptr<DynamoFilter> filter_;
  NiceMock<Runtime::MockLoader> loader_;
  std::string stat_prefix_{"prefix."};
  NiceMock<Http::MockStreamDecoderFilterCallbacks> decoder_callbacks_;
  NiceMock<Http::MockStreamEncoderFilterCallbacks> encoder_callbacks_;
  Http::TestRequestTrailerMapImpl request_trailers_;
};

TEST_F(DynamoFilterTest, OperatorPresent) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.Get"},
                                                 {"random", "random"}};

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, true));
  Http::MetadataMap metadata_map{{"metadata", "metadata"}};
  EXPECT_EQ(Http::FilterMetadataStatus::Continue, filter_->decodeMetadata(metadata_map));
  EXPECT_EQ(Http::FilterMetadataStatus::Continue, filter_->encodeMetadata(metadata_map));

  Http::TestResponseHeaderMapImpl continue_headers{{":status", "100"}};
  EXPECT_EQ(Http::Filter1xxHeadersStatus::Continue, filter_->encode1xxHeaders(continue_headers));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation_missing")).Times(0);
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table_missing"));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.Get.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.Get.upstream_rq_total_200"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.Get.upstream_rq_total"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.Get.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.Get.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.Get.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.operation.Get.upstream_rq_time_2xx"), _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.operation.Get.upstream_rq_time_200"), _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.operation.Get.upstream_rq_time"), _));

  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, true));
}

TEST_F(DynamoFilterTest, JsonBodyNotWellFormed) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.GetItem"},
                                                 {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));

  Buffer::OwnedImpl buffer;
  buffer.add("test", 4);
  buffer.add("test2", 5);

  EXPECT_CALL(stats_, counter("prefix.dynamodb.invalid_req_body"));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->decodeData(buffer, true));
}

TEST_F(DynamoFilterTest, BothOperationAndTableIncorrect) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version"}, {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, true));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation_missing"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table_missing"));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, true));
}

TEST_F(DynamoFilterTest, HandleErrorTypeTableMissing) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version"}, {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, true));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation_missing"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table_missing"));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "400"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::InstancePtr error_data(new Buffer::OwnedImpl());
  std::string internal_error =
      "{\"__type\":\"com.amazonaws.dynamodb.v20120810#ValidationException\"}";
  error_data->add(internal_error);
  EXPECT_CALL(stats_, counter("prefix.dynamodb.error.no_table.ValidationException"));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(*error_data, true));

  error_data->add("}", 1);
  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer,
            filter_->encodeData(*error_data, false));
  EXPECT_CALL(encoder_callbacks_, encodingBuffer()).WillRepeatedly(Return(error_data.get()));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.invalid_resp_body"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation_missing"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table_missing"));
  Http::TestResponseTrailerMapImpl response_trailers;
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->encodeTrailers(response_trailers));
}

TEST_F(DynamoFilterTest, HandleErrorTypeTablePresent) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.GetItem"},
                                                 {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));

  Buffer::OwnedImpl buffer;
  std::string buffer_content = "{\"TableName\":\"locations\"}";
  buffer.add(buffer_content);
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->decodeData(buffer, true));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "400"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::OwnedImpl error_data;
  std::string internal_error =
      "{\"__type\":\"com.amazonaws.dynamodb.v20120810#ValidationException\"}";
  error_data.add(internal_error);
  EXPECT_CALL(stats_, counter("prefix.dynamodb.error.locations.ValidationException"));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total_4xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total_400"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time_4xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time_400",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.GetItem.upstream_rq_time_4xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.GetItem.upstream_rq_time_400"),
                          _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.operation.GetItem.upstream_rq_time"), _));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_4xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_400"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_4xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_400",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_4xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_400"),
                          _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.table.locations.upstream_rq_time"), _));

  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(error_data, true));
}

TEST_F(DynamoFilterTest, BatchMultipleTables) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.BatchGetItem"},
                                                 {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));

  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = R"EOF(
{
  "RequestItems": {
    "table_1": { "test1" : "something" },
    "table_2": { "test2" : "something" }
  }
}
)EOF";
  buffer->add(buffer_content);

  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(*buffer, false));
  EXPECT_CALL(decoder_callbacks_, decodingBuffer()).WillRepeatedly(Return(buffer.get()));
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->decodeTrailers(request_trailers_));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_CALL(stats_, counter("prefix.dynamodb.multiple_tables"));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_200"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time"),
                          _));

  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, true));
}

TEST_F(DynamoFilterTest, BatchMultipleTablesUnprocessedKeys) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.BatchGetItem"},
                                                 {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));

  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = R"EOF(
{
  "RequestItems": {
    "table_1": { "test1" : "something" },
    "table_2": { "test2" : "something" }
  }
}
)EOF";
  buffer->add(buffer_content);

  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(*buffer, false));
  EXPECT_CALL(decoder_callbacks_, decodingBuffer()).WillRepeatedly(Return(buffer.get()));
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->decodeTrailers(request_trailers_));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_CALL(stats_, counter("prefix.dynamodb.multiple_tables"));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_200"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time"),
                          _));

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::OwnedImpl empty_data;
  Buffer::InstancePtr response_data(new Buffer::OwnedImpl());
  std::string response_content = R"EOF(
{
  "UnprocessedKeys": {
    "table_1": { "test1" : "something" },
    "table_2": { "test2" : "something" }
  }
}
)EOF";
  response_data->add(response_content);

  EXPECT_CALL(stats_, counter("prefix.dynamodb.error.table_1.BatchFailureUnprocessedKeys"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.error.table_2.BatchFailureUnprocessedKeys"));
  EXPECT_CALL(encoder_callbacks_, encodingBuffer()).WillRepeatedly(Return(response_data.get()));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(empty_data, true));
}

TEST_F(DynamoFilterTest, BatchMultipleTablesNoUnprocessedKeys) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.BatchGetItem"},
                                                 {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));

  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = R"EOF(
{
  "RequestItems": {
    "table_1": { "test1" : "something" },
    "table_2": { "test2" : "something" }
  }
}
)EOF";
  buffer->add(buffer_content);

  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(*buffer, false));
  EXPECT_CALL(decoder_callbacks_, decodingBuffer()).WillRepeatedly(Return(buffer.get()));
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->decodeTrailers(request_trailers_));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_CALL(stats_, counter("prefix.dynamodb.multiple_tables"));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_200"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time"),
                          _));

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::OwnedImpl empty_data;
  Buffer::InstancePtr response_data(new Buffer::OwnedImpl());
  std::string response_content = R"EOF(
{
  "UnprocessedKeys": {
  }
}
)EOF";
  response_data->add(response_content);

  EXPECT_CALL(encoder_callbacks_, encodingBuffer()).WillOnce(Return(response_data.get()));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(empty_data, true));
}

TEST_F(DynamoFilterTest, BatchMultipleTablesInvalidResponseBody) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.BatchGetItem"},
                                                 {"random", "random"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));

  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = R"EOF(
{
  "RequestItems": {
    "table_1": { "test1" : "something" },
    "table_2": { "test2" : "something" }
  }
}
)EOF";
  buffer->add(buffer_content);

  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(*buffer, false));
  EXPECT_CALL(decoder_callbacks_, decodingBuffer()).WillRepeatedly(Return(buffer.get()));
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->decodeTrailers(request_trailers_));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_CALL(stats_, counter("prefix.dynamodb.multiple_tables"));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_200"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time"),
                          _));

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::OwnedImpl empty_data;
  Buffer::InstancePtr response_data(new Buffer::OwnedImpl());
  std::string response_content = R"EOF(
{
  "UnprocessedKeys": {
    "table_1": { "test1" : "something" },
    "table_2": { "test2" : "something" }
  }
}
)EOF";
  response_data->add(response_content);
  response_data->add("}", 1);

  EXPECT_CALL(stats_, counter("prefix.dynamodb.invalid_resp_body"));
  EXPECT_CALL(encoder_callbacks_, encodingBuffer()).WillOnce(Return(response_data.get()));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(empty_data, true));
}

TEST_F(DynamoFilterTest, BothOperationAndTableCorrect) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.GetItem"}};
  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = "{\"TableName\":\"locations\"";
  buffer->add(buffer_content);
  EXPECT_CALL(decoder_callbacks_, decodingBuffer()).WillRepeatedly(Return(buffer.get()));
  Buffer::OwnedImpl data;
  data.add("}", 1);

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));
  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(data, false));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->decodeData(data, true));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total_200"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.GetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.GetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.operation.GetItem.upstream_rq_time"), _));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_200"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.table.locations.upstream_rq_time"), _));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, true));
}

TEST_F(DynamoFilterTest, OperatorPresentRuntimeDisabled) {
  setup(false);

  EXPECT_CALL(stats_, counter(_)).Times(0);
  EXPECT_CALL(stats_, deliverHistogramToSinks(_, _)).Times(0);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.operator"},
                                                 {"random", "random"}};
  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};

  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->decodeHeaders(request_headers, true));
  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, true));
  Http::TestResponseTrailerMapImpl response_trailers;
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->encodeTrailers(response_trailers));
}

TEST_F(DynamoFilterTest, PartitionIdStats) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.GetItem"}};
  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = "{\"TableName\":\"locations\"";
  buffer->add(buffer_content);
  ON_CALL(decoder_callbacks_, decodingBuffer()).WillByDefault(Return(buffer.get()));
  Buffer::OwnedImpl data;
  data.add("}", 1);

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));
  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(data, false));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->decodeData(data, true));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total_200"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.GetItem.upstream_rq_total"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.GetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.GetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.GetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.operation.GetItem.upstream_rq_time"), _));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_200"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.table.locations.upstream_rq_time"), _));

  EXPECT_CALL(stats_,
              counter("prefix.dynamodb.table.locations.capacity.GetItem.__partition_id=ition_1"));
  EXPECT_CALL(stats_,
              counter("prefix.dynamodb.table.locations.capacity.GetItem.__partition_id=ition_2"));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::OwnedImpl empty_data;
  Buffer::InstancePtr response_data(new Buffer::OwnedImpl());
  std::string response_content = R"EOF(
    {
      "ConsumedCapacity": {
        "Partitions": {
          "partition_1" : 0.5,
          "partition_2" : 3.0
        }
      }
    }
    )EOF";

  response_data->add(response_content);

  EXPECT_CALL(encoder_callbacks_, encodingBuffer()).WillOnce(Return(response_data.get()));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(empty_data, true));
}

TEST_F(DynamoFilterTest, NoPartitionIdStatsForMultipleTables) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.BatchGetItem"}};
  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = R"EOF(
{
  "RequestItems": {
    "table_1": { "test1" : "something" },
    "table_2": { "test2" : "something" }
  }
}
)EOF";
  buffer->add(buffer_content);
  ON_CALL(decoder_callbacks_, decodingBuffer()).WillByDefault(Return(buffer.get()));

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));
  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(*buffer, false));
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->decodeTrailers(request_trailers_));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.multiple_tables"));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_200"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time"),
                          _));

  EXPECT_CALL(
      stats_,
      counter("prefix.dynamodb.table.locations.capacity.BatchGetItem.__partition_id=ition_1"))
      .Times(0);
  EXPECT_CALL(
      stats_,
      counter("prefix.dynamodb.table.locations.capacity.BatchGetItem.__partition_id=ition_2"))
      .Times(0);

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::OwnedImpl empty_data;
  Buffer::InstancePtr response_data(new Buffer::OwnedImpl());
  std::string response_content = R"EOF(
    {
      "ConsumedCapacity": {
        "Partitions": {
          "partition_1" : 0.5,
          "partition_2" : 3.0
        }
      }
    }
    )EOF";

  response_data->add(response_content);

  EXPECT_CALL(encoder_callbacks_, encodingBuffer()).WillOnce(Return(response_data.get()));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(empty_data, true));
}

TEST_F(DynamoFilterTest, PartitionIdStatsForSingleTableBatchOperation) {
  setup(true);

  Http::TestRequestHeaderMapImpl request_headers{{"x-amz-target", "version.BatchGetItem"}};
  Buffer::InstancePtr buffer(new Buffer::OwnedImpl());
  std::string buffer_content = R"EOF(
{
  "RequestItems": {
    "locations": { "test1" : "something" }
  }
}
)EOF";
  buffer->add(buffer_content);
  ON_CALL(decoder_callbacks_, decodingBuffer()).WillByDefault(Return(buffer.get()));

  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->decodeHeaders(request_headers, false));
  EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->decodeData(*buffer, false));
  EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->decodeTrailers(request_trailers_));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.multiple_tables")).Times(0);

  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.operation.BatchGetItem.upstream_rq_total_200"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.operation.BatchGetItem.upstream_rq_time"),
                          _));

  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_2xx"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total_200"));
  EXPECT_CALL(stats_, counter("prefix.dynamodb.table.locations.upstream_rq_total"));

  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_2xx",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time_200",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, histogram("prefix.dynamodb.table.locations.upstream_rq_time",
                                Stats::Histogram::Unit::Milliseconds));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_2xx"),
                          _));
  EXPECT_CALL(stats_, deliverHistogramToSinks(
                          Property(&Stats::Metric::name,
                                   "prefix.dynamodb.table.locations.upstream_rq_time_200"),
                          _));
  EXPECT_CALL(
      stats_,
      deliverHistogramToSinks(
          Property(&Stats::Metric::name, "prefix.dynamodb.table.locations.upstream_rq_time"), _));

  EXPECT_CALL(
      stats_,
      counter("prefix.dynamodb.table.locations.capacity.BatchGetItem.__partition_id=ition_1"));
  EXPECT_CALL(
      stats_,
      counter("prefix.dynamodb.table.locations.capacity.BatchGetItem.__partition_id=ition_2"));

  Http::TestResponseHeaderMapImpl response_headers{{":status", "200"}};
  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
            filter_->encodeHeaders(response_headers, false));

  Buffer::OwnedImpl empty_data;
  Buffer::InstancePtr response_data(new Buffer::OwnedImpl());
  std::string response_content = R"EOF(
    {
      "ConsumedCapacity": {
        "Partitions": {
          "partition_1" : 0.5,
          "partition_2" : 3.0
        }
      }
    }
    )EOF";

  response_data->add(response_content);

  EXPECT_CALL(encoder_callbacks_, encodingBuffer()).WillOnce(Return(response_data.get()));
  EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(empty_data, true));
}

} // namespace
} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "dynamo_filter_test",
    srcs = ["dynamo_filter_test.cc"],
    deps = [
        "//contrib/dynamo/filters/http/source:dynamo_filter_lib",
        "//source/common/buffer:buffer_lib",
        "//source/common/http:header_map_lib",
        "//test/mocks/http:http_mocks",
        "//test/mocks/runtime:runtime_mocks",
        "//test/mocks/stats:stats_mocks",
        "//test/mocks/upstream:upstream_mocks",
        "//test/test_common:utility_lib",
    ],
)

envoy_cc_test(
    name = "dynamo_request_parser_test",
    srcs = ["dynamo_request_parser_test.cc"],
    deps = [
        "//contrib/dynamo/filters/http/source:dynamo_request_parser_lib",
        "//source/common/http:header_map_lib",
        "//source/common/json:json_loader_lib",
        "//test/test_common:utility_lib",
    ],
)

envoy_cc_test(
    name = "dynamo_stats_test",
    srcs = ["dynamo_stats_test.cc"],
    deps = [
        "//contrib/dynamo/filters/http/source:dynamo_stats_lib",
        "//source/common/stats:stats_lib",
        "//test/mocks/stats:stats_mocks",
    ],
)

envoy_cc_test(
    name = "config_test",
    srcs = ["config_test.cc"],
    deps = [
        "//contrib/dynamo/filters/http/source:config",
        "//test/mocks/server:factory_context_mocks",
        "@envoy_api//contrib/envoy/extensions/filters/http/dynamo/v3:pkg_cc_proto",
    ],
)
#include <string>
#include <vector>

#include "source/common/http/header_map_impl.h"
#include "source/common/json/json_loader.h"

#include "test/test_common/printers.h"
#include "test/test_common/utility.h"

#include "contrib/dynamo/filters/http/source/dynamo_request_parser.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {
namespace {

TEST(DynamoRequestParser, parseOperation) {
  // Well formed x-amz-target header, in a format, Version.Operation
  {
    Http::TestRequestHeaderMapImpl headers{{"X", "X"}, {"x-amz-target", "X.Operation"}};
    EXPECT_EQ("Operation", RequestParser::parseOperation(headers));
  }

  // Not well formed x-amz-target header.
  {
    Http::TestRequestHeaderMapImpl headers{{"X", "X"}, {"x-amz-target", "X,Operation"}};
    EXPECT_EQ("", RequestParser::parseOperation(headers));
  }

  // Too many entries in the Version.Operation.
  {
    Http::TestRequestHeaderMapImpl headers{{"X", "X"},
                                           {"x-amz-target", "NOT_VALID.NOT_VALID.NOT_VALID"}};
    EXPECT_EQ("", RequestParser::parseOperation(headers));
  }

  // Required header is not present in the headers
  {
    Http::TestRequestHeaderMapImpl headers{{"Z", "Z"}};
    EXPECT_EQ("", RequestParser::parseOperation(headers));
  }
}

TEST(DynamoRequestParser, parseTableNameSingleOperation) {
  std::vector<std::string> supported_single_operations{"GetItem", "Query",      "Scan",
                                                       "PutItem", "UpdateItem", "DeleteItem"};

  {
    std::string json_string = R"EOF(
    {
      "TableName": "Pets",
      "Key": {
        "AnimalType": {"S": "Dog"},
        "Name": {"S": "Fido"}
      }
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    // Supported operation
    for (const std::string& operation : supported_single_operations) {
      EXPECT_EQ("Pets", RequestParser::parseTable(operation, *json_data).table_name);
    }

    // Not supported operation
    EXPECT_EQ("", RequestParser::parseTable("NotSupportedOperation", *json_data).table_name);
  }

  {
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(R"({"TableName":"Pets"})");
    EXPECT_EQ("Pets", RequestParser::parseTable("GetItem", *json_data).table_name);
  }
}

TEST(DynamoRequestParser, parseTableNameTransactOperation) {
  std::vector<std::string> supported_transact_operations{"TransactGetItems", "TransactWriteItems"};
  // testing single table operation
  {
    std::string json_string = R"EOF(
    {
      "TransactItems": [
        { "Update": { "TableName": "Pets", "Key": { "Name": {"S": "Maxine"} }, "AnimalType": {"S": "Dog"} } },
        { "Put": { "TableName": "Pets", "Key": { "Name": {"S": "Max"} }, "AnimalType": {"S": "Puppy"} } },
        { "Put": { "TableName": "Pets", "Key": { "Name": {"S": "Oscar"} }, "AnimalType": {"S": "Puppy"} } },
        { "Put": { "TableName": "Pets", "Key": { "Name": {"S": "Chloe"} }, "AnimalType": {"S": "Puppy"} } }
      ]
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    for (const std::string& operation : supported_transact_operations) {
      RequestParser::TableDescriptor table = RequestParser::parseTable(operation, *json_data);
      EXPECT_EQ("Pets", table.table_name);
      EXPECT_TRUE(table.is_single_table);
    }
  }

  // testing multi-table operation
  {
    std::string json_string = R"EOF(
    {
      "TransactItems": [
        { "Put": { "TableName": "Pets", "Key": { "AnimalType": {"S": "Dog"}, "Name": {"S": "Fido"} } } },
        { "Delete": { "TableName": "Strays", "Key": { "AnimalType": {"S": "Dog"}, "Name": {"S": "Fido"} } } },
        { "Put": { "TableName": "Pets", "Key": { "AnimalType": {"S": "Cat"}, "Name": {"S": "Max"} } } },
        { "Delete": { "TableName": "Strays", "Key": { "AnimalType": {"S": "Cat"}, "Name": {"S": "Max"} } } }
      ]
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    for (const std::string& operation : supported_transact_operations) {
      RequestParser::TableDescriptor table = RequestParser::parseTable(operation, *json_data);
      EXPECT_EQ("", table.table_name);
      EXPECT_FALSE(table.is_single_table);
    }
  }

  // testing missing table
  {
    std::string json_string = R"EOF(
    {
      "TransactItems": [
        { "Put": { "TableName": "" } },
        { "Delete": { "TableName": "Strays", "Key": { "AnimalType": {"S": "Dog"}, "Name": {"S": "Fido"} } } },
        { "Put": { "TableName": "Pets", "Key": { "AnimalType": {"S": "Cat"}, "Name": {"S": "Max"} } } },
        { "Delete": { "TableName": "Strays", "Key": { "AnimalType": {"S": "Cat"}, "Name": {"S": "Max"} } } }
      ]
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    for (const std::string& operation : supported_transact_operations) {
      RequestParser::TableDescriptor table = RequestParser::parseTable(operation, *json_data);
      EXPECT_EQ("", table.table_name);
      EXPECT_TRUE(table.is_single_table);
    }
  }
}

TEST(DynamoRequestParser, parseErrorType) {
  {
    EXPECT_EQ("ResourceNotFoundException",
              RequestParser::parseErrorType(*Json::Factory::loadFromString(
                  "{\"__type\":\"com.amazonaws.dynamodb.v20120810#ResourceNotFoundException\"}")));
  }

  {
    EXPECT_EQ("ResourceNotFoundException",
              RequestParser::parseErrorType(*Json::Factory::loadFromString(
                  "{\"__type\":\"com.amazonaws.dynamodb.v20120810#ResourceNotFoundException\","
                  "\"message\":\"Requested resource not found: Table: tablename not found\"}")));
  }

  {
    EXPECT_EQ("", RequestParser::parseErrorType(
                      *Json::Factory::loadFromString("{\"__type\":\"UnKnownError\"}")));
  }
}

TEST(DynamoRequestParser, parseTableNameBatchOperation) {
  {
    std::string json_string = R"EOF(
    {
      "RequestItems": {
        "table_1": { "test1" : "something" },
        "table_2": { "test2" : "something" }
      }
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    RequestParser::TableDescriptor table = RequestParser::parseTable("BatchGetItem", *json_data);
    EXPECT_EQ("", table.table_name);
    EXPECT_FALSE(table.is_single_table);
  }

  {
    std::string json_string = R"EOF(
    {
      "RequestItems": {
        "table_2": { "test1" : "something" },
        "table_2": { "test2" : "something" }
      }
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    RequestParser::TableDescriptor table = RequestParser::parseTable("BatchGetItem", *json_data);
    EXPECT_EQ("table_2", table.table_name);
    EXPECT_TRUE(table.is_single_table);
  }

  {
    std::string json_string = R"EOF(
    {
      "RequestItems": {
        "table_2": { "test1" : "something" },
        "table_2": { "test2" : "something" },
        "table_3": { "test3" : "something" }
      }
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    RequestParser::TableDescriptor table = RequestParser::parseTable("BatchGetItem", *json_data);
    EXPECT_EQ("", table.table_name);
    EXPECT_FALSE(table.is_single_table);
  }

  {
    std::string json_string = R"EOF(
    {
      "RequestItems": {
        "table_2": { "test1" : "something" },
        "table_2": { "test2" : "something" }
      }
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    RequestParser::TableDescriptor table = RequestParser::parseTable("BatchWriteItem", *json_data);
    EXPECT_EQ("table_2", table.table_name);
    EXPECT_TRUE(table.is_single_table);
  }

  {
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString("{}");
    RequestParser::TableDescriptor table =
        RequestParser::parseTable("BatchWriteItem", *Json::Factory::loadFromString("{}"));
    EXPECT_EQ("", table.table_name);
    EXPECT_TRUE(table.is_single_table);
  }

  {
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString("{\"RequestItems\":{}}");
    RequestParser::TableDescriptor table = RequestParser::parseTable("BatchWriteItem", *json_data);
    EXPECT_EQ("", table.table_name);
    EXPECT_TRUE(table.is_single_table);
  }

  {
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString("{}");
    RequestParser::TableDescriptor table = RequestParser::parseTable("BatchGetItem", *json_data);
    EXPECT_EQ("", table.table_name);
    EXPECT_TRUE(table.is_single_table);
  }
}
TEST(DynamoRequestParser, parseBatchUnProcessedKeys) {
  {
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString("{}");
    std::vector<std::string> unprocessed_tables =
        RequestParser::parseBatchUnProcessedKeys(*json_data);
    EXPECT_EQ(0u, unprocessed_tables.size());
  }
  {
    std::vector<std::string> unprocessed_tables = RequestParser::parseBatchUnProcessedKeys(
        *Json::Factory::loadFromString("{\"UnprocessedKeys\":{}}"));
    EXPECT_EQ(0u, unprocessed_tables.size());
  }

  {
    std::vector<std::string> unprocessed_tables = RequestParser::parseBatchUnProcessedKeys(
        *Json::Factory::loadFromString(R"({"UnprocessedKeys":{"table_1" :{}}})"));
    EXPECT_EQ("table_1", unprocessed_tables[0]);
    EXPECT_EQ(1u, unprocessed_tables.size());
  }

  {
    std::string json_string = R"EOF(
    {
      "UnprocessedKeys": {
        "table_1": { "test1" : "something" },
        "table_2": { "test2" : "something" }
      }
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    std::vector<std::string> unprocessed_tables =
        RequestParser::parseBatchUnProcessedKeys(*json_data);
    EXPECT_TRUE(find(unprocessed_tables.begin(), unprocessed_tables.end(), "table_1") !=
                unprocessed_tables.end());
    EXPECT_TRUE(find(unprocessed_tables.begin(), unprocessed_tables.end(), "table_2") !=
                unprocessed_tables.end());
    EXPECT_EQ(2u, unprocessed_tables.size());
  }
}

TEST(DynamoRequestParser, parsePartitionIds) {
  {
    std::vector<RequestParser::PartitionDescriptor> partitions =
        RequestParser::parsePartitions(*Json::Factory::loadFromString("{}"));
    EXPECT_EQ(0u, partitions.size());
  }
  {
    std::vector<RequestParser::PartitionDescriptor> partitions =
        RequestParser::parsePartitions(*Json::Factory::loadFromString("{\"ConsumedCapacity\":{}}"));
    EXPECT_EQ(0u, partitions.size());
  }
  {
    std::vector<RequestParser::PartitionDescriptor> partitions = RequestParser::parsePartitions(
        *Json::Factory::loadFromString(R"({"ConsumedCapacity":{ "Partitions":{}}})"));
    EXPECT_EQ(0u, partitions.size());
  }
  {
    std::string json_string = R"EOF(
    {
      "ConsumedCapacity": {
        "Partitions": {
          "partition_1" : 0.5,
          "partition_2" : 3.0
        }
      }
    }
    )EOF";
    Json::ObjectSharedPtr json_data = Json::Factory::loadFromString(json_string);

    std::vector<RequestParser::PartitionDescriptor> partitions =
        RequestParser::parsePartitions(*json_data);
    for (const RequestParser::PartitionDescriptor& partition : partitions) {
      if (partition.partition_id_ == "partition_1") {
        EXPECT_EQ(1u, partition.capacity_);
      } else {
        EXPECT_EQ(3u, partition.capacity_);
      }
    }
    EXPECT_EQ(2u, partitions.size());
  }
}

} // namespace
} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <string>

#include "envoy/server/filter_config.h"

#include "source/extensions/filters/http/common/factory_base.h"

#include "contrib/envoy/extensions/filters/http/dynamo/v3/dynamo.pb.h"
#include "contrib/envoy/extensions/filters/http/dynamo/v3/dynamo.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

/**
 * Config registration for http dynamodb filter.
 */
class DynamoFilterConfig
    : public Common::FactoryBase<envoy::extensions::filters::http::dynamo::v3::Dynamo> {
public:
  DynamoFilterConfig() : FactoryBase("envoy.filters.http.dynamo") {}

private:
  Http::FilterFactoryCb createFilterFactoryFromProtoTyped(
      const envoy::extensions::filters::http::dynamo::v3::Dynamo& proto_config,
      const std::string& stats_prefix, Server::Configuration::FactoryContext& context) override;
};

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/dynamo/filters/http/source/dynamo_filter.h"

#include <chrono>
#include <cstdint>
#include <string>
#include <vector>

#include "source/common/buffer/buffer_impl.h"
#include "source/common/common/assert.h"
#include "source/common/common/fmt.h"
#include "source/common/http/codes.h"
#include "source/common/http/exception.h"
#include "source/common/http/utility.h"
#include "source/common/json/json_loader.h"

#include "absl/container/fixed_array.h"
#include "contrib/dynamo/filters/http/source/dynamo_request_parser.h"
#include "contrib/dynamo/filters/http/source/dynamo_stats.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

Http::FilterHeadersStatus DynamoFilter::decodeHeaders(Http::RequestHeaderMap& headers, bool) {
  if (enabled_) {
    start_decode_ = time_source_.monotonicTime();
    operation_ = RequestParser::parseOperation(headers);
    return Http::FilterHeadersStatus::StopIteration;
  } else {
    return Http::FilterHeadersStatus::Continue;
  }
}

Http::FilterDataStatus DynamoFilter::decodeData(Buffer::Instance& data, bool end_stream) {
  if (enabled_ && end_stream) {
    onDecodeComplete(data);
  }

  if (!enabled_ || end_stream) {
    return Http::FilterDataStatus::Continue;
  } else {
    // Buffer until the complete request has been processed.
    return Http::FilterDataStatus::StopIterationAndBuffer;
  }
}

Http::FilterTrailersStatus DynamoFilter::decodeTrailers(Http::RequestTrailerMap&) {
  if (enabled_) {
    Buffer::OwnedImpl empty;
    onDecodeComplete(empty);
  }

  return Http::FilterTrailersStatus::Continue;
}

void DynamoFilter::onDecodeComplete(const Buffer::Instance& data) {
  std::string body = buildBody(decoder_callbacks_->decodingBuffer(), data);
  if (!body.empty()) {
    try {
      Json::ObjectSharedPtr json_body = Json::Factory::loadFromString(body);
      table_descriptor_ = RequestParser::parseTable(operation_, *json_body);
    } catch (const Json::Exception& jsonEx) {
      // Body parsing failed. This should not happen, just put a stat for that.
      stats_->incCounter({stats_->invalid_req_body_});
    }
  }
}

void DynamoFilter::onEncodeComplete(const Buffer::Instance& data) {
  ASSERT(enabled_);
  uint64_t status = Http::Utility::getResponseStatus(*response_headers_);
  chargeBasicStats(status);

  std::string body = buildBody(encoder_callbacks_->encodingBuffer(), data);
  if (!body.empty()) {
    try {
      Json::ObjectSharedPtr json_body = Json::Factory::loadFromString(body);
      chargeTablePartitionIdStats(*json_body);

      if (Http::CodeUtility::is4xx(status)) {
        chargeFailureSpecificStats(*json_body);
      }
      // Batch Operations will always return status 200 for a partial or full success. Check
      // unprocessed keys to determine partial success.
      // http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.BatchOperations
      if (RequestParser::isBatchOperation(operation_)) {
        chargeUnProcessedKeysStats(*json_body);
      }
    } catch (const Json::Exception&) {
      // Body parsing failed. This should not happen, just put a stat for that.
      stats_->incCounter({stats_->invalid_resp_body_});
    }
  }
}

Http::FilterHeadersStatus DynamoFilter::encodeHeaders(Http::ResponseHeaderMap& headers,
                                                      bool end_stream) {
  Http::FilterHeadersStatus status = Http::FilterHeadersStatus::Continue;
  if (enabled_) {
    response_headers_ = &headers;

    if (end_stream) {
      Buffer::OwnedImpl empty;
      onEncodeComplete(empty);
    } else {
      status = Http::FilterHeadersStatus::StopIteration;
    }
  }

  return status;
}

Http::FilterDataStatus DynamoFilter::encodeData(Buffer::Instance& data, bool end_stream) {
  if (enabled_ && end_stream) {
    onEncodeComplete(data);
  }

  if (!enabled_ || end_stream) {
    return Http::FilterDataStatus::Continue;
  } else {
    // Buffer until the complete response has been processed.
    return Http::FilterDataStatus::StopIterationAndBuffer;
  }
}

Http::FilterTrailersStatus DynamoFilter::encodeTrailers(Http::ResponseTrailerMap&) {
  if (enabled_) {
    Buffer::OwnedImpl empty;
    onEncodeComplete(empty);
  }

  return Http::FilterTrailersStatus::Continue;
}

std::string DynamoFilter::buildBody(const Buffer::Instance* buffered,
                                    const Buffer::Instance& last) {
  std::string body;
  body.reserve((buffered ? buffered->length() : 0) + last.length());
  if (buffered) {
    for (const Buffer::RawSlice& slice : buffered->getRawSlices()) {
      body.append(static_cast<const char*>(slice.mem_), slice.len_);
    }
  }

  for (const Buffer::RawSlice& slice : last.getRawSlices()) {
    body.append(static_cast<const char*>(slice.mem_), slice.len_);
  }

  return body;
}

void DynamoFilter::chargeBasicStats(uint64_t status) {
  if (!operation_.empty()) {
    chargeStatsPerEntity(operation_, "operation", status);
  } else {
    stats_->incCounter({stats_->operation_missing_});
  }

  if (!table_descriptor_.table_name.empty()) {
    chargeStatsPerEntity(table_descriptor_.table_name, "table", status);
  } else if (table_descriptor_.is_single_table) {
    stats_->incCounter({stats_->table_missing_});
  } else {
    stats_->incCounter({stats_->multiple_tables_});
  }
}

void DynamoFilter::chargeStatsPerEntity(const std::string& entity, const std::string& entity_type,
                                        uint64_t status) {
  std::chrono::milliseconds latency = std::chrono::duration_cast<std::chrono::milliseconds>(
      time_source_.monotonicTime() - start_decode_);

  size_t group_index = DynamoStats::groupIndex(status);
  Stats::StatNameDynamicPool dynamic(stats_->symbolTable());

  const Stats::StatName entity_type_name =
      stats_->getBuiltin(entity_type, stats_->unknown_entity_type_);
  const Stats::StatName entity_name = dynamic.add(entity);

  // TODO(jmarantz): Consider using a similar mechanism to common/http/codes.cc
  // to avoid creating dynamic stat-names for common statuses.
  const Stats::StatName total_name = dynamic.add(absl::StrCat("upstream_rq_total_", status));
  const Stats::StatName time_name = dynamic.add(absl::StrCat("upstream_rq_time_", status));

  stats_->incCounter({entity_type_name, entity_name, stats_->upstream_rq_total_});
  const Stats::StatName total_group = stats_->upstream_rq_total_groups_[group_index];
  stats_->incCounter({entity_type_name, entity_name, total_group});
  stats_->incCounter({entity_type_name, entity_name, total_name});

  stats_->recordHistogram({entity_type_name, entity_name, stats_->upstream_rq_time_},
                          Stats::Histogram::Unit::Milliseconds, latency.count());
  const Stats::StatName time_group = stats_->upstream_rq_time_groups_[group_index];
  stats_->recordHistogram({entity_type_name, entity_name, time_group},
                          Stats::Histogram::Unit::Milliseconds, latency.count());
  stats_->recordHistogram({entity_type_name, entity_name, time_name},
                          Stats::Histogram::Unit::Milliseconds, latency.count());
}

void DynamoFilter::chargeUnProcessedKeysStats(const Json::Object& json_body) {
  // The unprocessed keys block contains a list of tables and keys for that table that did not
  // complete apart of the batch operation. Only the table names will be logged for errors.
  std::vector<std::string> unprocessed_tables = RequestParser::parseBatchUnProcessedKeys(json_body);
  for (const std::string& unprocessed_table : unprocessed_tables) {
    Stats::StatNameDynamicStorage storage(unprocessed_table, stats_->symbolTable());
    stats_->incCounter(
        {stats_->error_, storage.statName(), stats_->batch_failure_unprocessed_keys_});
  }
}

void DynamoFilter::chargeFailureSpecificStats(const Json::Object& json_body) {
  std::string error_type = RequestParser::parseErrorType(json_body);

  if (!error_type.empty()) {
    Stats::StatNameDynamicPool dynamic(stats_->symbolTable());
    if (table_descriptor_.table_name.empty()) {
      stats_->incCounter({stats_->error_, stats_->no_table_, dynamic.add(error_type)});
    } else {
      stats_->incCounter(
          {stats_->error_, dynamic.add(table_descriptor_.table_name), dynamic.add(error_type)});
    }
  } else {
    stats_->incCounter({stats_->empty_response_body_});
  }
}

void DynamoFilter::chargeTablePartitionIdStats(const Json::Object& json_body) {
  if (table_descriptor_.table_name.empty() || operation_.empty()) {
    return;
  }

  std::vector<RequestParser::PartitionDescriptor> partitions =
      RequestParser::parsePartitions(json_body);
  for (const RequestParser::PartitionDescriptor& partition : partitions) {
    stats_
        ->buildPartitionStatCounter(table_descriptor_.table_name, operation_,
                                    partition.partition_id_)
        .add(partition.capacity_);
  }
}

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <cstdint>
#include <string>

#include "envoy/http/filter.h"
#include "envoy/runtime/runtime.h"
#include "envoy/stats/scope.h"

#include "source/common/json/json_loader.h"

#include "contrib/dynamo/filters/http/source/dynamo_request_parser.h"
#include "contrib/dynamo/filters/http/source/dynamo_stats.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

/**
 * DynamoDb filter to process egress request to dynamo and capture comprehensive stats
 * It captures RPS/latencies:
 *  1) Per table per response code (and group of response codes, e.g., 2xx/3xx/etc)
 *  2) Per operation per response code (and group of response codes, e.g., 2xx/3xx/etc)
 */
class DynamoFilter : public Http::StreamFilter {
public:
  DynamoFilter(Runtime::Loader& runtime, const DynamoStatsSharedPtr& stats, TimeSource& time_source)
      : runtime_(runtime), stats_(stats), time_source_(time_source) {
    enabled_ = runtime_.snapshot().featureEnabled("dynamodb.filter_enabled", 100);
  }

  // Http::StreamFilterBase
  void onDestroy() override {}

  // Http::StreamDecoderFilter
  Http::FilterHeadersStatus decodeHeaders(Http::RequestHeaderMap& headers,
                                          bool end_stream) override;
  Http::FilterDataStatus decodeData(Buffer::Instance& data, bool end_stream) override;
  Http::FilterTrailersStatus decodeTrailers(Http::RequestTrailerMap&) override;
  void setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks& callbacks) override {
    decoder_callbacks_ = &callbacks;
  }

  // Http::StreamEncoderFilter
  Http::Filter1xxHeadersStatus encode1xxHeaders(Http::ResponseHeaderMap&) override {
    return Http::Filter1xxHeadersStatus::Continue;
  }
  Http::FilterHeadersStatus encodeHeaders(Http::ResponseHeaderMap&, bool) override;
  Http::FilterDataStatus encodeData(Buffer::Instance&, bool) override;
  Http::FilterTrailersStatus encodeTrailers(Http::ResponseTrailerMap&) override;
  Http::FilterMetadataStatus encodeMetadata(Http::MetadataMap&) override {
    return Http::FilterMetadataStatus::Continue;
  }
  void setEncoderFilterCallbacks(Http::StreamEncoderFilterCallbacks& callbacks) override {
    encoder_callbacks_ = &callbacks;
  }

private:
  void onDecodeComplete(const Buffer::Instance& data);
  void onEncodeComplete(const Buffer::Instance& data);
  std::string buildBody(const Buffer::Instance* buffered, const Buffer::Instance& last);
  void chargeBasicStats(uint64_t status);
  void chargeStatsPerEntity(const std::string& entity, const std::string& entity_type,
                            uint64_t status);
  void chargeFailureSpecificStats(const Json::Object& json_body);
  void chargeUnProcessedKeysStats(const Json::Object& json_body);
  void chargeTablePartitionIdStats(const Json::Object& json_body);

  Runtime::Loader& runtime_;
  const DynamoStatsSharedPtr stats_;

  bool enabled_{};
  std::string operation_{};
  RequestParser::TableDescriptor table_descriptor_{"", true};
  std::string error_type_{};
  MonotonicTime start_decode_;
  Http::ResponseHeaderMap* response_headers_;
  Http::StreamDecoderFilterCallbacks* decoder_callbacks_{};
  Http::StreamEncoderFilterCallbacks* encoder_callbacks_{};
  TimeSource& time_source_;
};

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_extension",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

# AWS DynamoDB L7 HTTP filter (observability): https://aws.amazon.com/dynamodb/
# Public docs: https://envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/dynamodb_filter

envoy_contrib_package()

envoy_cc_library(
    name = "dynamo_filter_lib",
    srcs = ["dynamo_filter.cc"],
    hdrs = ["dynamo_filter.h"],
    deps = [
        ":dynamo_request_parser_lib",
        ":dynamo_stats_lib",
        "//envoy/http:filter_interface",
        "//envoy/runtime:runtime_interface",
        "//source/common/buffer:buffer_lib",
        "//source/common/http:codes_lib",
        "//source/common/http:exception_lib",
    ],
)

envoy_cc_library(
    name = "dynamo_request_parser_lib",
    srcs = ["dynamo_request_parser.cc"],
    hdrs = ["dynamo_request_parser.h"],
    deps = [
        "//envoy/http:header_map_interface",
        "//source/common/common:utility_lib",
        "//source/common/json:json_loader_lib",
    ],
)

envoy_cc_extension(
    name = "config",
    srcs = ["config.cc"],
    hdrs = ["config.h"],
    deps = [
        ":dynamo_filter_lib",
        "//envoy/registry",
        "//envoy/server:filter_config_interface",
        "//source/extensions/filters/http/common:factory_base_lib",
        "@envoy_api//contrib/envoy/extensions/filters/http/dynamo/v3:pkg_cc_proto",
    ],
)

envoy_cc_library(
    name = "dynamo_stats_lib",
    srcs = ["dynamo_stats.cc"],
    hdrs = ["dynamo_stats.h"],
    deps = [
        ":dynamo_request_parser_lib",
        "//envoy/stats:stats_interface",
        "//source/common/stats:symbol_table_lib",
        "//source/common/stats:utility_lib",
    ],
)
#include "contrib/dynamo/filters/http/source/dynamo_request_parser.h"

#include <cmath>
#include <cstdint>
#include <string>
#include <vector>

#include "source/common/common/utility.h"

#include "absl/strings/match.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

/**
 * Basic json request/response format:
 * https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations_Amazon_DynamoDB.html
 */
const Http::LowerCaseString RequestParser::X_AMZ_TARGET("X-AMZ-TARGET");

// clang-format off
const std::vector<std::string> RequestParser::SINGLE_TABLE_OPERATIONS{
    "CreateTable",
    "DeleteItem",
    "DeleteTable",
    "DescribeTable",
    "GetItem",
    "PutItem",
    "Query",
    "Scan",
    "UpdateItem",
    "UpdateTable"};

const std::vector<std::string> RequestParser::SUPPORTED_ERROR_TYPES{
    // 4xx
    "AccessDeniedException",
    "ConditionalCheckFailedException",
    "IdempotentParameterMismatchException",
    "IncompleteSignatureException",
    "ItemCollectionSizeLimitExceededException",
    "LimitExceededException",
    "MissingAuthenticationTokenException",
    "ProvisionedThroughputExceededException",
    "ResourceInUseException",
    "ResourceNotFoundException",
    "ThrottlingException",
    "TransactionCanceledException",
    "TransactionInProgressException",
    "UnrecognizedClientException",
    "ValidationException",
    // Errors not listed in the error handling section of DynamoDB developer guide, but observed in runtime
    "InvalidSignatureException", // https://github.com/aws/aws-sdk-go/issues/2598#issuecomment-526398896
};
// clang-format on

const std::vector<std::string> RequestParser::BATCH_OPERATIONS{"BatchGetItem", "BatchWriteItem"};

const std::vector<std::string> RequestParser::TRANSACT_OPERATIONS{"TransactGetItems",
                                                                  "TransactWriteItems"};
const std::vector<std::string> RequestParser::TRANSACT_ITEM_OPERATIONS{"ConditionCheck", "Delete",
                                                                       "Get", "Put", "Update"};

std::string RequestParser::parseOperation(const Http::HeaderMap& header_map) {
  std::string operation;

  const auto x_amz_target = header_map.get(X_AMZ_TARGET);
  if (!x_amz_target.empty()) {
    // Normally x-amz-target contains Version.Operation, e.g., DynamoDB_20160101.GetItem
    // AWS is trusted. Using the first value is fine.
    auto version_and_operation =
        StringUtil::splitToken(x_amz_target[0]->value().getStringView(), ".");
    if (version_and_operation.size() == 2) {
      operation = std::string{version_and_operation[1]};
    }
  }

  return operation;
}

RequestParser::TableDescriptor RequestParser::parseTable(const std::string& operation,
                                                         const Json::Object& json_data) {
  TableDescriptor table{"", true};

  // Simple operations on a single table, have "TableName" explicitly specified.
  if (find(SINGLE_TABLE_OPERATIONS.begin(), SINGLE_TABLE_OPERATIONS.end(), operation) !=
      SINGLE_TABLE_OPERATIONS.end()) {
    table.table_name = json_data.getString("TableName", "");
  } else if (find(BATCH_OPERATIONS.begin(), BATCH_OPERATIONS.end(), operation) !=
             BATCH_OPERATIONS.end()) {
    Json::ObjectSharedPtr tables = json_data.getObject("RequestItems", true);
    tables->iterate([&table](const std::string& key, const Json::Object&) {
      if (table.table_name.empty()) {
        table.table_name = key;
      } else {
        if (table.table_name != key) {
          table.table_name = "";
          table.is_single_table = false;
          return false;
        }
      }
      return true;
    });
  } else if (find(TRANSACT_OPERATIONS.begin(), TRANSACT_OPERATIONS.end(), operation) !=
             TRANSACT_OPERATIONS.end()) {
    std::vector<Json::ObjectSharedPtr> transact_items =
        json_data.getObjectArray("TransactItems", true);
    for (const Json::ObjectSharedPtr& transact_item : transact_items) {
      const auto next_table_name = getTableNameFromTransactItem(*transact_item);
      if (!next_table_name.has_value()) {
        // if an operation is missing a table name, we want to throw the normal set of errors
        table.table_name = "";
        table.is_single_table = true;
        break;
      }
      if (table.table_name.empty()) {
        table.table_name = next_table_name.value();
      } else if (table.table_name != next_table_name.value()) {
        table.table_name = "";
        table.is_single_table = false;
        break;
      }
    }
  }
  return table;
}

absl::optional<std::string>
RequestParser::getTableNameFromTransactItem(const Json::Object& transact_item) {
  for (const std::string& operation : TRANSACT_ITEM_OPERATIONS) {
    Json::ObjectSharedPtr item = transact_item.getObject(operation, true);
    std::string table_name = item->getString("TableName", "");
    if (!table_name.empty()) {
      return absl::make_optional(table_name);
    }
  }
  return absl::nullopt;
}

std::vector<std::string> RequestParser::parseBatchUnProcessedKeys(const Json::Object& json_data) {
  std::vector<std::string> unprocessed_tables;
  Json::ObjectSharedPtr tables = json_data.getObject("UnprocessedKeys", true);
  tables->iterate([&unprocessed_tables](const std::string& key, const Json::Object&) {
    unprocessed_tables.emplace_back(key);
    return true;
  });

  return unprocessed_tables;
}

std::string RequestParser::parseErrorType(const Json::Object& json_data) {
  std::string error_type = json_data.getString("__type", "");
  if (error_type.empty()) {
    return "";
  }

  for (const std::string& supported_error_type : SUPPORTED_ERROR_TYPES) {
    if (absl::EndsWith(error_type, supported_error_type)) {
      return supported_error_type;
    }
  }

  return "";
}

bool RequestParser::isBatchOperation(const std::string& operation) {
  return find(BATCH_OPERATIONS.begin(), BATCH_OPERATIONS.end(), operation) !=
         BATCH_OPERATIONS.end();
}

std::vector<RequestParser::PartitionDescriptor>
RequestParser::parsePartitions(const Json::Object& json_data) {
  std::vector<RequestParser::PartitionDescriptor> partition_descriptors;

  Json::ObjectSharedPtr partitions =
      json_data.getObject("ConsumedCapacity", true)->getObject("Partitions", true);
  partitions->iterate([&partition_descriptors, &partitions](const std::string& key,
                                                            const Json::Object&) {
    // For a given partition id, the amount of capacity used is returned in the body as a double.
    // A stat will be created to track the capacity consumed for the operation, table and partition.
    // Stats counter only increments by whole numbers, capacity is round up to the nearest integer
    // to account for this.
    uint64_t capacity_integer = static_cast<uint64_t>(std::ceil(partitions->getDouble(key, 0.0)));
    partition_descriptors.emplace_back(key, capacity_integer);
    return true;
  });

  return partition_descriptors;
}

void RequestParser::forEachStatString(const StringFn& fn) {
  for (const std::string& str : SINGLE_TABLE_OPERATIONS) {
    fn(str);
  }
  for (const std::string& str : SUPPORTED_ERROR_TYPES) {
    fn(str);
  }
  for (const std::string& str : BATCH_OPERATIONS) {
    fn(str);
  }
  for (const std::string& str : TRANSACT_OPERATIONS) {
    fn(str);
  }
  for (const std::string& str : TRANSACT_ITEM_OPERATIONS) {
    fn(str);
  }
}

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <cstdint>
#include <string>
#include <vector>

#include "envoy/http/header_map.h"

#include "source/common/json/json_loader.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

/**
 * Request parser for dynamodb request/response.
 *
 * Basic dynamodb json request/response format:
 * http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Appendix.CurrentAPI.html
 */
class RequestParser {
public:
  struct TableDescriptor {
  public:
    std::string table_name;
    bool is_single_table;
  };

  struct PartitionDescriptor {
    PartitionDescriptor(const std::string& partition, uint64_t capacity)
        : partition_id_(partition), capacity_(capacity) {}
    std::string partition_id_;
    uint64_t capacity_;
  };

  /**
   * Parse operation out of x-amz-target header.
   * @return empty string if operation cannot be parsed.
   */
  static std::string parseOperation(const Http::HeaderMap& header_map);

  /**
   * Parse table name out of data, based on the operation.
   * @return empty string as TableDescriptor.table_name if table name cannot be parsed out of valid
   *json data
   * or if operation is not in the list of operations that we support.
   *
   * For simple operations on single table, e.g., GetItem, PutItem, Query etc @return table
   * name in TableDescriptor.table_name.
   *
   * For batch operations, e.g. BatchGetItem/BatchWriteItem, @return table name in
   *TableDescriptor.table_name if it's only one
   * table used in all operations, @return empty string in TableDescriptor.table_name and
   *TableDescriptor.is_single_table=false in case of multiple.
   *
   * @throw Json::Exception if data is not in valid Json format.
   */
  static TableDescriptor parseTable(const std::string& operation, const Json::Object& json_data);

  /**
   * @return string name of table in transaction object, or empty string if none
   */
  static absl::optional<std::string>
  getTableNameFromTransactItem(const Json::Object& transact_item);

  /**
   * Parse error details which might be provided for a given response code.
   * @return empty string if cannot get error details.
   * For the full list of errors, see
   * http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/CommonErrors.html
   * Operation specific errors, for example, error section of
   * http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html
   *
   * @throw Json::Exception if data is not in valid Json format.
   */
  static std::string parseErrorType(const Json::Object& json_data);

  /**
   * Parse unprocessed keys for batch operation results.
   * @return empty set if there are no unprocessed keys or a set of table names that did not get
   * processed in the batch operation.
   */
  static std::vector<std::string> parseBatchUnProcessedKeys(const Json::Object& json_data);

  /**
   * @return true if the operation is in the set of supported BATCH_OPERATIONS
   */
  static bool isBatchOperation(const std::string& operation);

  /**
   * Parse the Partition ids and the consumed capacity from the body.
   * @return empty set if there is no partition data or a set of partition data containing
   * the partition id as a string and the capacity consumed as an integer.
   *
   * @throw Json::Exception if data is not in valid Json format.
   */
  static std::vector<PartitionDescriptor> parsePartitions(const Json::Object& json_data);

  using StringFn = std::function<void(const std::string&)>;

  /**
   * Calls a function for every string that is likely to be included as a token
   * in a stat. This is not functionally necessary, but can reduce potentially
   * contented access to create entries in the symbol table in the hot path.
   *
   * @param fn the function to call for every potential stat name.
   */
  static void forEachStatString(const StringFn& fn);

private:
  static const Http::LowerCaseString X_AMZ_TARGET;
  static const std::vector<std::string> SINGLE_TABLE_OPERATIONS;
  static const std::vector<std::string> BATCH_OPERATIONS;
  static const std::vector<std::string> TRANSACT_OPERATIONS;
  static const std::vector<std::string> TRANSACT_ITEM_OPERATIONS;

  // http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html
  static const std::vector<std::string> SUPPORTED_ERROR_TYPES;

  RequestParser() = default;
};

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/dynamo/filters/http/source/dynamo_stats.h"

#include <memory>
#include <string>

#include "envoy/stats/scope.h"

#include "source/common/stats/symbol_table.h"

#include "contrib/dynamo/filters/http/source/dynamo_request_parser.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

DynamoStats::DynamoStats(Stats::Scope& scope, const std::string& prefix)
    : scope_(scope), stat_name_set_(scope.symbolTable().makeSet("Dynamo")),
      prefix_(stat_name_set_->add(prefix + "dynamodb")),
      batch_failure_unprocessed_keys_(stat_name_set_->add("BatchFailureUnprocessedKeys")),
      capacity_(stat_name_set_->add("capacity")),
      empty_response_body_(stat_name_set_->add("empty_response_body")),
      error_(stat_name_set_->add("error")),
      invalid_req_body_(stat_name_set_->add("invalid_req_body")),
      invalid_resp_body_(stat_name_set_->add("invalid_resp_body")),
      multiple_tables_(stat_name_set_->add("multiple_tables")),
      no_table_(stat_name_set_->add("no_table")),
      operation_missing_(stat_name_set_->add("operation_missing")),
      table_(stat_name_set_->add("table")), table_missing_(stat_name_set_->add("table_missing")),
      upstream_rq_time_(stat_name_set_->add("upstream_rq_time")),
      upstream_rq_total_(stat_name_set_->add("upstream_rq_total")),
      unknown_entity_type_(stat_name_set_->add("unknown_entity_type")),
      unknown_operation_(stat_name_set_->add("unknown_operation")) {
  upstream_rq_total_groups_[0] = stat_name_set_->add("upstream_rq_total_unknown");
  upstream_rq_time_groups_[0] = stat_name_set_->add("upstream_rq_time_unknown");
  for (size_t i = 1; i < DynamoStats::NumGroupEntries; ++i) {
    upstream_rq_total_groups_[i] = stat_name_set_->add(fmt::format("upstream_rq_total_{}xx", i));
    upstream_rq_time_groups_[i] = stat_name_set_->add(fmt::format("upstream_rq_time_{}xx", i));
  }
  RequestParser::forEachStatString(
      [this](const std::string& str) { stat_name_set_->rememberBuiltin(str); });
  for (uint32_t status_code : {200, 400, 403, 502}) {
    stat_name_set_->rememberBuiltin(absl::StrCat("upstream_rq_time_", status_code));
    stat_name_set_->rememberBuiltin(absl::StrCat("upstream_rq_total_", status_code));
  }
  stat_name_set_->rememberBuiltins({"operation", "table"});
}

Stats::ElementVec DynamoStats::addPrefix(const Stats::ElementVec& names) {
  Stats::ElementVec names_with_prefix;
  names_with_prefix.reserve(1 + names.size());
  names_with_prefix.push_back(prefix_);
  names_with_prefix.insert(names_with_prefix.end(), names.begin(), names.end());
  return names_with_prefix;
}

void DynamoStats::incCounter(const Stats::ElementVec& names) {
  Stats::Utility::counterFromElements(scope_, addPrefix(names)).inc();
}

void DynamoStats::recordHistogram(const Stats::ElementVec& names, Stats::Histogram::Unit unit,
                                  uint64_t value) {
  Stats::Utility::histogramFromElements(scope_, addPrefix(names), unit).recordValue(value);
}

Stats::Counter& DynamoStats::buildPartitionStatCounter(const std::string& table_name,
                                                       const std::string& operation,
                                                       const std::string& partition_id) {
  // Use the last 7 characters of the partition id.
  absl::string_view id_last_7 = absl::string_view(partition_id).substr(partition_id.size() - 7);
  std::string partition = absl::StrCat("__partition_id=", id_last_7);
  return Stats::Utility::counterFromElements(
      scope_,
      addPrefix({table_, Stats::DynamicName(table_name), capacity_,
                 getBuiltin(operation, unknown_operation_), Stats::DynamicName(partition)}));
}

size_t DynamoStats::groupIndex(uint64_t status) {
  size_t index = status / 100;
  if (index >= NumGroupEntries) {
    index = 0; // status-code 600 or higher is unknown.
  }
  return index;
}

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <memory>
#include <string>

#include "envoy/stats/scope.h"

#include "source/common/stats/symbol_table.h"
#include "source/common/stats/utility.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

class DynamoStats {
public:
  DynamoStats(Stats::Scope& scope, const std::string& prefix);

  void incCounter(const Stats::ElementVec& names);
  void recordHistogram(const Stats::ElementVec& names, Stats::Histogram::Unit unit, uint64_t value);

  /**
   * Creates the partition id stats string. The stats format is
   * "<stat_prefix>table.<table_name>.capacity.<operation>.__partition_id=<partition_id>".
   * Partition ids and dynamodb table names can be long. To satisfy the string
   * length, we truncate, taking only the last 7 characters of the partition id.
   */
  Stats::Counter& buildPartitionStatCounter(const std::string& table_name,
                                            const std::string& operation,
                                            const std::string& partition_id);

  static size_t groupIndex(uint64_t status);

  /**
   * Finds a StatName by string.
   */
  Stats::StatName getBuiltin(const std::string& str, Stats::StatName fallback) {
    return stat_name_set_->getBuiltin(str, fallback);
  }

  Stats::SymbolTable& symbolTable() { return scope_.symbolTable(); }

private:
  Stats::ElementVec addPrefix(const Stats::ElementVec& names);

  Stats::Scope& scope_;
  Stats::StatNameSetPtr stat_name_set_;
  const Stats::StatName prefix_;

public:
  const Stats::StatName batch_failure_unprocessed_keys_;
  const Stats::StatName capacity_;
  const Stats::StatName empty_response_body_;
  const Stats::StatName error_;
  const Stats::StatName invalid_req_body_;
  const Stats::StatName invalid_resp_body_;
  const Stats::StatName multiple_tables_;
  const Stats::StatName no_table_;
  const Stats::StatName operation_missing_;
  const Stats::StatName table_;
  const Stats::StatName table_missing_;
  const Stats::StatName upstream_rq_time_;
  const Stats::StatName upstream_rq_total_;
  const Stats::StatName upstream_rq_unknown_;
  const Stats::StatName unknown_entity_type_;
  const Stats::StatName unknown_operation_;

  // Keep group codes for HTTP status codes through the 500s.
  static constexpr size_t NumGroupEntries = 6;
  Stats::StatName upstream_rq_total_groups_[NumGroupEntries];
  Stats::StatName upstream_rq_time_groups_[NumGroupEntries];
};
using DynamoStatsSharedPtr = std::shared_ptr<DynamoStats>;

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/dynamo/filters/http/source/config.h"

#include <string>

#include "envoy/registry/registry.h"

#include "contrib/dynamo/filters/http/source/dynamo_filter.h"
#include "contrib/dynamo/filters/http/source/dynamo_stats.h"
#include "contrib/envoy/extensions/filters/http/dynamo/v3/dynamo.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Dynamo {

Http::FilterFactoryCb DynamoFilterConfig::createFilterFactoryFromProtoTyped(
    const envoy::extensions::filters::http::dynamo::v3::Dynamo&, const std::string& stats_prefix,
    Server::Configuration::FactoryContext& context) {
  auto stats = std::make_shared<DynamoStats>(context.scope(), stats_prefix);
  return [&context, stats](Http::FilterChainFactoryCallbacks& callbacks) -> void {
    callbacks.addStreamFilter(
        std::make_shared<Dynamo::DynamoFilter>(context.serverFactoryContext().runtime(), stats,
                                               context.serverFactoryContext().timeSource()));
  };
}

/**
 * Static registration for the http dynamodb filter. @see RegisterFactory.
 */
LEGACY_REGISTER_FACTORY(DynamoFilterConfig, Server::Configuration::NamedHttpFilterConfigFactory,
                        "envoy.http_dynamo_filter");

} // namespace Dynamo
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "test/mocks/server/factory_context.h"

#include "contrib/envoy/extensions/filters/http/language/v3alpha/language.pb.h"
#include "contrib/language/filters/http/source/config.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::NiceMock;

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Language {

using LanguageFilterConfig = envoy::extensions::filters::http::language::v3alpha::Language;

using ConfigSharedPtr = std::shared_ptr<LanguageFilterConfig>;

class ConfigTest : public testing::Test {
public:
  void initializeFilter(const std::string& yaml) {
    envoy::extensions::filters::http::language::v3alpha::Language proto_config;
    TestUtility::loadFromYaml(yaml, proto_config);

    factory_.createFilterFactoryFromProtoTyped(proto_config, "stats", factory_context_);
  }

private:
  LanguageFilterFactory factory_;
  NiceMock<Server::Configuration::MockFactoryContext> factory_context_;
};

TEST_F(ConfigTest, ValidConfig) {
  const std::string yaml = R"EOF(
default_language: en
supported_languages: [fr, en-uk]
)EOF";

  EXPECT_NO_THROW(initializeFilter(yaml));
}

} // namespace Language
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "test/integration/http_integration.h"

namespace Envoy {
class HttpFilterLanguageIntegrationTest
    : public testing::TestWithParam<Network::Address::IpVersion>,
      public HttpIntegrationTest {
public:
  HttpFilterLanguageIntegrationTest()
      : HttpIntegrationTest(Http::CodecClient::Type::HTTP1, GetParam()) {}

  void TearDown() override {
    fake_upstream_connection_.reset();
    fake_upstreams_.clear();
  }

  void initializeConfig(std::string default_language, std::string supported_languages) {
    const std::string yaml = R"EOF(
name: envoy.filters.http.language
typed_config:
  "@type": type.googleapis.com/envoy.extensions.filters.http.language.v3alpha.Language
  default_language: {}
  supported_languages: {}
)EOF";
    config_helper_.prependFilter(fmt::format(yaml, default_language, supported_languages));
  }
};

INSTANTIATE_TEST_SUITE_P(IpVersions, HttpFilterLanguageIntegrationTest,
                         testing::ValuesIn(TestEnvironment::getIpVersionsForTest()),
                         TestUtility::ipTestParamsToString);

TEST_P(HttpFilterLanguageIntegrationTest, DefaultLanguageFallback) {
  initializeConfig("en", "[fr]");
  initialize();

  codec_client_ = makeHttpConnection(lookupPort("http"));

  auto response = codec_client_->makeHeaderOnlyRequest(Http::TestRequestHeaderMapImpl{
      {":method", "GET"}, {":path", "/"}, {":scheme", "http"}, {":authority", "host"}});

  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));
  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));
  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));

  EXPECT_FALSE(
      upstream_request_->headers().get(Envoy::Http::LowerCaseString{"x-language"}).empty());
  EXPECT_EQ("en", upstream_request_->headers()
                      .get(Http::LowerCaseString{"x-language"})[0]
                      ->value()
                      .getStringView());

  codec_client_->close();

  ASSERT_TRUE(fake_upstream_connection_->close());
  ASSERT_TRUE(fake_upstream_connection_->waitForDisconnect());
}

TEST_P(HttpFilterLanguageIntegrationTest, AcceptLanguageHeader) {
  initializeConfig("en", "[en, en-uk, de, dk, es, fr, zh, zh-tw]");
  initialize();

  codec_client_ = makeHttpConnection(lookupPort("http"));

  auto response = codec_client_->makeHeaderOnlyRequest(Http::TestRequestHeaderMapImpl{
      {":method", "GET"},
      {":path", "/"},
      {":scheme", "http"},
      {":authority", "host"},
      {"accept-language", "fr-CH,fr;q=0.9,en;q=0.8,de;q=0.7,*;q=0.5"}});

  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));
  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));
  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));

  EXPECT_FALSE(
      upstream_request_->headers().get(Envoy::Http::LowerCaseString{"x-language"}).empty());
  EXPECT_EQ("fr", upstream_request_->headers()
                      .get(Http::LowerCaseString{"x-language"})[0]
                      ->value()
                      .getStringView());

  codec_client_->close();

  ASSERT_TRUE(fake_upstream_connection_->close());
  ASSERT_TRUE(fake_upstream_connection_->waitForDisconnect());
}

TEST_P(HttpFilterLanguageIntegrationTest, InvalidAcceptLanguageHeader) {
  initializeConfig("en", "[en, en-uk, de, dk, es, fr, zh, zh-tw]");
  initialize();

  codec_client_ = makeHttpConnection(lookupPort("http"));

  auto response = codec_client_->makeHeaderOnlyRequest(
      Http::TestRequestHeaderMapImpl{{":method", "GET"},
                                     {":path", "/"},
                                     {":scheme", "http"},
                                     {":authority", "host"},
                                     {"accept-language", "foobar;;;;0000.20;0-"}});

  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));
  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));
  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));

  EXPECT_FALSE(
      upstream_request_->headers().get(Envoy::Http::LowerCaseString{"x-language"}).empty());
  EXPECT_EQ("en", upstream_request_->headers()
                      .get(Http::LowerCaseString{"x-language"})[0]
                      ->value()
                      .getStringView());

  codec_client_->close();

  ASSERT_TRUE(fake_upstream_connection_->close());
  ASSERT_TRUE(fake_upstream_connection_->waitForDisconnect());
}
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "language_config_test",
    srcs = ["language_config_test.cc"],
    deps = [
        "//contrib/language/filters/http/source:config_lib",
        "//test/mocks/server:factory_context_mocks",
        "//test/test_common:utility_lib",
        "@envoy_api//contrib/envoy/extensions/filters/http/language/v3alpha:pkg_cc_proto",
    ],
)

envoy_cc_test(
    name = "language_integration_test",
    srcs = ["language_integration_test.cc"],
    deps = [
        "//contrib/language/filters/http/source:config_lib",
        "//test/integration:http_protocol_integration_lib",
    ],
)
#pragma once

#include "source/extensions/filters/http/common/factory_base.h"

#include "contrib/envoy/extensions/filters/http/language/v3alpha/language.pb.h"
#include "contrib/envoy/extensions/filters/http/language/v3alpha/language.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Language {

/**
 * Config registration for the language detection filter (i18n). @see NamedHttpFilterConfigFactory.
 */
class LanguageFilterFactory
    : public Common::FactoryBase<envoy::extensions::filters::http::language::v3alpha::Language> {
public:
  LanguageFilterFactory() : FactoryBase("envoy.filters.http.language") {}

  Http::FilterFactoryCb createFilterFactoryFromProtoTyped(
      const envoy::extensions::filters::http::language::v3alpha::Language& proto_config,
      const std::string& stat_prefix, Server::Configuration::FactoryContext& context) override;
};

DECLARE_FACTORY(LanguageFilterFactory);

} // namespace Language
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/language/filters/http/source/language_filter.h"

#include <string>

#include "source/common/common/empty_string.h"
#include "source/common/common/logger.h"
#include "source/common/common/utility.h"
#include "source/common/http/utility.h"

#include "absl/strings/ascii.h"
#include "unicode/locid.h"
#include "unicode/utypes.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Language {

// Validates a string for Accept-Language header value compliance.
// See https://httpwg.org/specs/rfc7231.html#header.accept-language
bool validateHeaderValue(absl::string_view value_str) {
  for (auto ch : value_str) {
    if (!(absl::ascii_isalnum(ch) || ch == '_' || ch == '-' || ch == '=' || ch == '.' ||
          ch == ',' || ch == ';' || ch == '*' || ch == ' ')) {
      return false;
    }
  }

  return true;
}

LanguageFilterConfigImpl::LanguageFilterConfigImpl(
    const std::shared_ptr<icu::Locale> default_locale,
    const std::shared_ptr<icu::LocaleMatcher> locale_matcher, const bool clear_route_cache,
    const std::string& stats_prefix, Stats::Scope& scope)
    : default_locale_(std::move(default_locale)), locale_matcher_(std::move(locale_matcher)),
      clear_route_cache_(clear_route_cache),
      stats_(LanguageFilter::generateStats(stats_prefix, scope)) {}

const std::shared_ptr<icu::Locale>& LanguageFilterConfigImpl::defaultLocale() const {
  return default_locale_;
}

const std::shared_ptr<icu::LocaleMatcher>& LanguageFilterConfigImpl::localeMatcher() const {
  return locale_matcher_;
}

bool LanguageFilterConfigImpl::clearRouteCache() const { return clear_route_cache_; }

LanguageStats& LanguageFilterConfigImpl::stats() { return stats_; }

LanguageFilter::LanguageFilter(const LanguageFilterConfigSharedPtr config)
    : config_(std::move(config)) {}

void LanguageFilter::onDestroy() {}

LanguageStats LanguageFilter::generateStats(const std::string& prefix, Stats::Scope& scope) {
  const std::string final_prefix = prefix + "language.";
  return {LANGUAGE_FILTER_STATS(POOL_COUNTER_PREFIX(scope, final_prefix))};
}

Http::FilterHeadersStatus LanguageFilter::decodeHeaders(Http::RequestHeaderMap& request_headers,
                                                        bool) {
  const auto header = request_headers.get(AcceptLanguage);
  if (!header.empty()) {
    const absl::string_view value_str = header[0]->value().getStringView();

    if (!value_str.empty() && value_str.length() <= MAX_ACCEPT_LANGUAGE_SIZE &&
        validateHeaderValue(value_str)) {
      const std::string value = std::string(value_str);

      UErrorCode errorCode = U_ZERO_ERROR;
      const icu::Locale* result =
          config_->localeMatcher()->getBestMatchForListString(value, errorCode);

      if (U_SUCCESS(errorCode)) {
        const std::string language_tag = result->toLanguageTag<std::string>(errorCode);

        if (U_SUCCESS(errorCode)) {
          if (!language_tag.empty()) {
            request_headers.addCopy(Language, language_tag);

            if (config_->clearRouteCache()) {
              ENVOY_LOG(debug, "clearing route cache");
              decoder_callbacks_->downstreamCallbacks()->clearRouteCache();
            }

            config_->stats().header_.inc();

            return Http::FilterHeadersStatus::Continue;
          }
        }
      }
    }
  }

  // Default language fallback
  request_headers.addCopy(Language, config_->defaultLocale()->getLanguage());
  config_->stats().default_language_.inc();

  return Http::FilterHeadersStatus::Continue;
}

Http::FilterDataStatus LanguageFilter::decodeData(Buffer::Instance&, bool) {
  return Http::FilterDataStatus::Continue;
}

Http::FilterTrailersStatus LanguageFilter::decodeTrailers(Http::RequestTrailerMap&) {
  return Http::FilterTrailersStatus::Continue;
}

void LanguageFilter::setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks& callbacks) {
  decoder_callbacks_ = &callbacks;
}

std::string LanguageFilter::matchValue(icu::Locale& locale) {
  UErrorCode errorCode = U_ZERO_ERROR;
  const icu::Locale* result = config_->localeMatcher()->getBestMatch(locale, errorCode);

  if (U_SUCCESS(errorCode)) {
    const auto language_tag = result->toLanguageTag<std::string>(errorCode);

    if (U_SUCCESS(errorCode)) {
      return language_tag;
    } else {
      ENVOY_LOG(error, "ICU error code icu::LocaleMatcher toLanguageTag: {}", errorCode);
    }
  } else {
    ENVOY_LOG(error, "ICU error code icu::LocaleMatcher getBestMatch: {}", errorCode);
  }

  return EMPTY_STRING;
}

} // namespace Language
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <string>

#include "envoy/http/filter.h"
#include "envoy/stats/stats_macros.h"

#include "source/common/common/logger.h"

#include "unicode/localematcher.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Language {

/**
 * All stats for the language detection filter (i18n). @see stats_macros.h
 */
// clang-format off
#define LANGUAGE_FILTER_STATS(COUNTER) \
  COUNTER(header)                      \
  COUNTER(default_language)
// clang-format on

/**
 * Wrapper struct filter stats. @see stats_macros.h
 */
struct LanguageStats {
  LANGUAGE_FILTER_STATS(GENERATE_COUNTER_STRUCT)
};

/**
 * Abstract filter configuration.
 */
class LanguageFilterConfig {
public:
  virtual ~LanguageFilterConfig() = default;

  virtual const std::shared_ptr<icu::Locale>& defaultLocale() const PURE;

  virtual const std::shared_ptr<icu::LocaleMatcher>& localeMatcher() const PURE;

  virtual bool clearRouteCache() const PURE;

  virtual LanguageStats& stats() PURE;
};

/**
 * Configuration for the language detection filter (i18n).
 */
class LanguageFilterConfigImpl : public LanguageFilterConfig {
public:
  LanguageFilterConfigImpl(const std::shared_ptr<icu::Locale> default_locale,
                           const std::shared_ptr<icu::LocaleMatcher> locale_matcher,
                           const bool clear_route_cache, const std::string& stats_prefix,
                           Stats::Scope& scope);

  const std::shared_ptr<icu::Locale>& defaultLocale() const override;

  const std::shared_ptr<icu::LocaleMatcher>& localeMatcher() const override;

  bool clearRouteCache() const override;

  LanguageStats& stats() override;

private:
  const std::shared_ptr<icu::Locale> default_locale_;

  const std::shared_ptr<icu::LocaleMatcher> locale_matcher_;

  const bool clear_route_cache_;

  LanguageStats stats_;
};
using LanguageFilterConfigSharedPtr = std::shared_ptr<LanguageFilterConfig>;

class LanguageFilter : public Http::StreamDecoderFilter, Logger::Loggable<Logger::Id::filter> {
public:
  LanguageFilter(LanguageFilterConfigSharedPtr config);

  static constexpr uint32_t MAX_ACCEPT_LANGUAGE_SIZE = 128;

  static LanguageStats generateStats(const std::string& prefix, Stats::Scope& scope);

  const Http::LowerCaseString Language{"x-language"};
  const Http::LowerCaseString AcceptLanguage{"accept-language"};

  // Http::StreamDecoderFilter
  Http::FilterHeadersStatus decodeHeaders(Http::RequestHeaderMap&, bool) override;
  Http::FilterDataStatus decodeData(Buffer::Instance&, bool) override;
  Http::FilterTrailersStatus decodeTrailers(Http::RequestTrailerMap&) override;

  void setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks&) override;

  // Http::StreamFilterBase
  void onDestroy() override;

private:
  LanguageFilterConfigSharedPtr config_;

  Http::StreamDecoderFilterCallbacks* decoder_callbacks_{};

  std::string matchValue(icu::Locale&);
};

} // namespace Language
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_contrib_extension",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

# Language detection filter
# Public docs: https://envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/language_filter

envoy_contrib_package()

envoy_cc_contrib_extension(
    name = "config_lib",
    srcs = ["config.cc"],
    hdrs = ["config.h"],
    tags = ["skip_on_windows"],
    deps = [
        ":filter_lib",
        "//envoy/registry",
        "//envoy/server:filter_config_interface",
        "//source/extensions/filters/http/common:factory_base_lib",
        "@envoy_api//contrib/envoy/extensions/filters/http/language/v3alpha:pkg_cc_proto",
    ],
)

envoy_cc_library(
    name = "filter_lib",
    srcs = ["language_filter.cc"],
    hdrs = ["language_filter.h"],
    tags = ["skip_on_windows"],
    deps = [
        "//bazel/foreign_cc:unicode_icu",
        "//source/common/common:empty_string",
        "//source/common/common:minimal_logger_lib",
        "//source/common/common:utility_lib",
        "//source/common/http:utility_lib",
    ],
)
#include "contrib/language/filters/http/source/config.h"

#include "envoy/common/exception.h"
#include "envoy/registry/registry.h"

#include "contrib/language/filters/http/source/language_filter.h"
#include "unicode/localematcher.h"
#include "unicode/utypes.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace Language {

struct LocaleHash {
  size_t operator()(icu::Locale const value) const {
    return absl::Hash<int32_t>{}(value.hashCode());
  }
};

Http::FilterFactoryCb LanguageFilterFactory::createFilterFactoryFromProtoTyped(
    const envoy::extensions::filters::http::language::v3alpha::Language& proto_config,
    const std::string& stats_prefix, Server::Configuration::FactoryContext& context) {
  const auto default_locale = icu::Locale(proto_config.default_language().data());

  if (default_locale.isBogus()) {
    throw EnvoyException(fmt::format("Failed to create icu::Locale from default_language: {}",
                                     proto_config.default_language().data()));
  }

  absl::flat_hash_set<icu::Locale, LocaleHash> supported_languages({default_locale});

  for (const auto& supported_language : proto_config.supported_languages()) {
    const auto locale = icu::Locale(supported_language.data());

    if (locale.isBogus()) {
      throw EnvoyException(fmt::format("Failed to create icu::Locale from supported_languages: {}",
                                       supported_language.data()));
    }

    supported_languages.insert(locale);
  }

  UErrorCode errorCode = U_ZERO_ERROR;
  const auto locale_matcher = std::make_shared<icu::LocaleMatcher>(
      icu::LocaleMatcher::Builder()
          .setSupportedLocales(supported_languages.begin(), supported_languages.end())
          .setDefaultLocale(&default_locale)
          .build(errorCode));

  if (U_FAILURE(errorCode)) {
    throw EnvoyException(fmt::format("Failed to initialize icu::LocaleMatcher::Builder: ICU error "
                                     "code icu::LocaleMatcher::Builder build: {}",
                                     errorCode));
  }

  auto config = std::make_shared<LanguageFilterConfigImpl>(
      std::make_shared<icu::Locale>(default_locale), locale_matcher,
      proto_config.clear_route_cache(), stats_prefix, context.scope());

  return [config](Http::FilterChainFactoryCallbacks& callbacks) -> void {
    auto filter = std::make_shared<LanguageFilter>(config);
    callbacks.addStreamDecoderFilter(Http::StreamDecoderFilterSharedPtr{filter});
  };
}

/**
 * Static registration for the language detection filter (i18n). @see RegisterFactory.
 */
REGISTER_FACTORY(LanguageFilterFactory, Server::Configuration::NamedHttpFilterConfigFactory);

} // namespace Language
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include <memory>
#include <string>

#include "source/common/protobuf/message_validator_impl.h"
#include "source/common/protobuf/utility.h"
#include "source/common/secret/secret_provider_impl.h"

#include "test/mocks/server/factory_context.h"

#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.h"
#include "contrib/sxg/filters/http/source/config.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

using testing::NiceMock;
using testing::Return;

namespace {

void expectCreateFilter(std::string yaml, bool is_sds_config) {
  FilterFactory factory;
  ProtobufTypes::MessagePtr proto_config = factory.createEmptyConfigProto();
  TestUtility::loadFromYaml(yaml, *proto_config);
  testing::NiceMock<Server::Configuration::MockFactoryContext> context;
  context.server_factory_context_.cluster_manager_.initializeClusters({"foo"}, {});

  // This returns non-nullptr for certificate and private_key.
  auto& secret_manager =
      context.server_factory_context_.cluster_manager_.cluster_manager_factory_.secretManager();
  if (is_sds_config) {
    ON_CALL(secret_manager, findOrCreateGenericSecretProvider(_, _, _, _))
        .WillByDefault(Return(std::make_shared<Secret::GenericSecretConfigProviderImpl>(
            envoy::extensions::transport_sockets::tls::v3::GenericSecret())));
  } else {
    ON_CALL(secret_manager, findStaticGenericSecretProvider(_))
        .WillByDefault(Return(std::make_shared<Secret::GenericSecretConfigProviderImpl>(
            envoy::extensions::transport_sockets::tls::v3::GenericSecret())));
  }
  EXPECT_CALL(context, messageValidationVisitor());
  EXPECT_CALL(context.server_factory_context_, clusterManager());
  EXPECT_CALL(context, scope());
  EXPECT_CALL(context.server_factory_context_, timeSource());
  EXPECT_CALL(context.server_factory_context_, api());
  EXPECT_CALL(context, initManager()).Times(2);
  EXPECT_CALL(context, getTransportSocketFactoryContext());
  Http::FilterFactoryCb cb =
      factory.createFilterFactoryFromProto(*proto_config, "stats", context).value();
  Http::MockFilterChainFactoryCallbacks filter_callback;
  EXPECT_CALL(filter_callback, addStreamFilter(_));
  cb(filter_callback);
}

// This loads one of the secrets in credentials, and fails the other one.
void expectInvalidSecretConfig(const std::string& failed_secret_name,
                               const std::string& exception_message) {
  const std::string yaml = R"YAML(
certificate:
  name: certificate
private_key:
  name: private_key
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
)YAML";

  FilterFactory factory;
  ProtobufTypes::MessagePtr proto_config = factory.createEmptyConfigProto();
  TestUtility::loadFromYaml(yaml, *proto_config);
  NiceMock<Server::Configuration::MockFactoryContext> context;

  auto& secret_manager =
      context.server_factory_context_.cluster_manager_.cluster_manager_factory_.secretManager();
  ON_CALL(secret_manager, findStaticGenericSecretProvider(
                              failed_secret_name == "private_key" ? "certificate" : "private_key"))
      .WillByDefault(Return(std::make_shared<Secret::GenericSecretConfigProviderImpl>(
          envoy::extensions::transport_sockets::tls::v3::GenericSecret())));

  EXPECT_THROW_WITH_MESSAGE(
      factory.createFilterFactoryFromProto(*proto_config, "stats", context).status().IgnoreError(),
      EnvoyException, exception_message);
}

} // namespace

TEST(ConfigTest, CreateFilterStaticSecretProvider) {
  const std::string yaml = R"YAML(
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
)YAML";
  expectCreateFilter(yaml, false);
}

TEST(ConfigTest, CreateFilterHasSdsSecret) {
  const std::string yaml = R"YAML(
certificate:
  name: certificate
  sds_config:
    path: "xxxx"
    resource_api_version: V3
private_key:
  name: private_key
  sds_config:
    path: "xxxx"
    resource_api_version: V3
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
)YAML";

  expectCreateFilter(yaml, true);
}

TEST(ConfigTest, InvalidCertificateSecret) {
  expectInvalidSecretConfig("certificate", "invalid certificate secret configuration");
}

TEST(ConfigTest, InvalidPrivateKeySecret) {
  expectInvalidSecretConfig("private_key", "invalid private_key secret configuration");
}

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "filter_test",
    srcs = ["filter_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/sxg/filters/http/source:config",
        "//contrib/sxg/filters/http/source:sxg_lib",
        "//source/common/secret:secret_manager_impl_lib",
        "//source/extensions/filters/http/common:pass_through_filter_lib",
        "//test/integration:http_integration_lib",
        "//test/mocks/server:server_mocks",
        "//test/mocks/upstream:upstream_mocks",
        "@envoy_api//contrib/envoy/extensions/filters/http/sxg/v3alpha:pkg_cc_proto",
    ],
)

envoy_cc_test(
    name = "config_test",
    srcs = ["config_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/sxg/filters/http/source:config",
        "//contrib/sxg/filters/http/source:sxg_lib",
        "//source/common/secret:secret_manager_impl_lib",
        "//source/extensions/filters/http/common:pass_through_filter_lib",
        "//test/integration:http_integration_lib",
        "//test/mocks/server:server_mocks",
        "//test/mocks/upstream:upstream_mocks",
        "@envoy_api//contrib/envoy/extensions/filters/http/sxg/v3alpha:pkg_cc_proto",
    ],
)
#include <memory>

#include "envoy/stats/stats.h"

#include "source/common/secret/secret_manager_impl.h"

#include "test/mocks/http/mocks.h"
#include "test/mocks/server/mocks.h"
#include "test/test_common/simulated_time_system.h"
#include "test/test_common/utility.h"

#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.h"
#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.validate.h"
#include "contrib/sxg/filters/http/source/filter.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

using testing::_;
using testing::NiceMock;
using testing::Return;
using testing::ReturnRef;

class MockSecretReader : public SecretReader {
public:
  MockSecretReader(const std::string& certificate, const std::string& private_key)
      : certificate_(certificate), private_key_(private_key){};

  const std::string& certificate() const override { return certificate_; }
  const std::string& privateKey() const override { return private_key_; }

private:
  const std::string certificate_;
  const std::string private_key_;
};

class MockEncoder : public Encoder {
public:
  MOCK_METHOD(void, setOrigin, (const std::string), (override));
  MOCK_METHOD(void, setUrl, (const std::string), (override));
  MOCK_METHOD(bool, loadSigner, (), (override));
  MOCK_METHOD(bool, loadHeaders, (Http::ResponseHeaderMap*), (override));
  MOCK_METHOD(bool, loadContent, (Buffer::Instance&), (override));
  MOCK_METHOD(bool, getEncodedResponse, (), (override));
  MOCK_METHOD(Buffer::BufferFragment*, writeSxg, (), (override));
};

int extractIntFromBytes(std::string bytes, size_t offset, size_t size) {
  if (size <= 0 || size > 8 || bytes.size() < offset + size) {
    return 0;
  }
  int value = 0;
  for (size_t i = 0; i < size; i++) {
    value <<= 8;
    value |= (0xff & bytes[offset + i]);
  }
  return value;
}

bool writeIntToBytes(std::string& bytes, uint64_t int_to_write, size_t offset, size_t size) {
  if (size <= 0 || size > 8 || bytes.size() < offset + size) {
    return false;
  }
  for (int i = size - 1; i >= 0; i--) {
    char byte = 0xff & int_to_write;
    bytes[offset + i] = byte;
    int_to_write >>= 8;
  }
  return true;
}

// The sig value of the SXG document is unique, so we strip it in tests
bool clearSignature(std::string& buffer) {
  if (buffer.find("sxg1-b3", 0, 7) == std::string::npos) {
    return false;
  }
  if (buffer[7] != '\0') {
    return false;
  }

  // The fallback URL length is contained in the 2 bytes following the sxg-b3
  // prefix string and the nullptr byte that follows. We need to know this length
  // because the signature length is located after the fallback URL.
  size_t fallback_url_size_offset = 8;
  size_t fallback_url_size = extractIntFromBytes(buffer, fallback_url_size_offset, 2);

  // the signature length is contained in the 3 bytes following the fallback URL
  size_t sig_size_offset = fallback_url_size_offset + 2 + fallback_url_size;
  size_t sig_size = extractIntFromBytes(buffer, sig_size_offset, 3);

  const size_t sig_pos = buffer.find("sig=*");
  if (sig_pos == std::string::npos) {
    return false;
  }

  const size_t start = sig_pos + 5;
  const size_t len = buffer.find('*', start) - start;

  // decrement the sig_size in the SXG document by the calculated length
  const size_t modified_sig_size = sig_size - len;
  if (!writeIntToBytes(buffer, modified_sig_size, sig_size_offset, 3)) {
    return false;
  }

  // replace the signature piece with empty string
  buffer.erase(start, len);

  return true;
}

class FilterTest : public testing::Test {
public:
  FilterTest() = default;

  void setConfiguration() {
    std::string config_str(R"YAML(
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
)YAML");
    setConfiguration(config_str);
  }

  void setConfiguration(const std::string& config_str) {
    std::string certificate(R"PEM(
-----BEGIN CERTIFICATE-----
MIIBhjCCASygAwIBAgIJAIH9REPqIFXTMAkGByqGSM49BAEwMjEUMBIGA1UEAwwL
ZXhhbXBsZS5vcmcxDTALBgNVBAoMBFRlc3QxCzAJBgNVBAYTAlVTMB4XDTIxMDEx
MzAxMDcwMVoXDTIxMDQxMzAxMDcwMVowMjEUMBIGA1UEAwwLZXhhbXBsZS5vcmcx
DTALBgNVBAoMBFRlc3QxCzAJBgNVBAYTAlVTMFkwEwYHKoZIzj0CAQYIKoZIzj0D
AQcDQgAE4ZrHsGLEiP+pV70a8zIERNcu9MBJHHfbeqLUqwGWWU2/YHObf58nE9to
c6lgrko2JdbV6TyWLVUc/M0Pn+OVSaMsMCowEAYKKwYBBAHWeQIBFgQCBQAwFgYD
VR0RBA8wDYILZXhhbXBsZS5vcmcwCQYHKoZIzj0EAQNJADBGAiEAuQJjX+z7j4hR
xtxfs4VPY5RsF5Sawd+mtluRxpoURcsCIQCIGU/11jcuS0UbIpt4B5Gb1UJlSKGi
Dgu+2OKt7qVPrA==
-----END CERTIFICATE-----
)PEM");
    std::string private_key(R"PEM(
-----BEGIN EC PARAMETERS-----
BggqhkjOPQMBBw==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHcCAQEEIJyGXecxIQtBwBJWU4Sc5A8UHNt5HnOBR9Oh11AGYa/2oAoGCCqGSM49
AwEHoUQDQgAE4ZrHsGLEiP+pV70a8zIERNcu9MBJHHfbeqLUqwGWWU2/YHObf58n
E9toc6lgrko2JdbV6TyWLVUc/M0Pn+OVSQ==
-----END EC PRIVATE KEY-----
)PEM");

    setConfiguration(config_str, certificate, private_key);
  }

  void setConfiguration(const std::string& config_str, const std::string& certificate,
                        const std::string& private_key) {
    envoy::extensions::filters::http::sxg::v3alpha::SXG proto;
    TestUtility::loadFromYaml(config_str, proto);

    time_system_.setSystemTime(std::chrono::seconds(1610503040));

    auto secret_reader = std::make_shared<MockSecretReader>(certificate, private_key);
    config_ = std::make_shared<FilterConfig>(proto, time_system_, secret_reader, "", scope_);
  }

  void setFilter() {
    if (encoder_ == nullptr) {
      encoder_ = std::make_unique<EncoderImpl>(config_);
    }
    setFilter(std::make_shared<Filter>(config_, encoder_));
  }

  void setFilter(std::shared_ptr<Filter> filter) {
    filter_ = filter;
    filter_->setDecoderFilterCallbacks(decoder_callbacks_);
    filter_->setEncoderFilterCallbacks(encoder_callbacks_);
  }

  void testPassthroughHtml(Http::TestRequestHeaderMapImpl& request_headers,
                           Http::TestResponseHeaderMapImpl& response_headers,
                           bool client_can_accept_sxg) {
    testPassthroughHtml(request_headers, response_headers, nullptr, client_can_accept_sxg);
  }

  void testPassthroughHtml(Http::TestRequestHeaderMapImpl& request_headers,
                           Http::TestResponseHeaderMapImpl& response_headers,
                           Http::TestResponseTrailerMapImpl* response_trailers,
                           bool client_can_accept_sxg) {
    EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->decodeHeaders(request_headers, false));
    EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, false));
    Buffer::OwnedImpl data("<html><body>hi!</body></html>\n");

    auto on_modify_encoding_buffer = [&data](std::function<void(Buffer::Instance&)> cb) {
      cb(data);
    };
    EXPECT_CALL(encoder_callbacks_, modifyEncodingBuffer)
        .WillRepeatedly(Invoke(on_modify_encoding_buffer));

    EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(data, true));
    if (response_trailers) {
      EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->encodeTrailers(*response_trailers));
    }

    EXPECT_EQ(response_headers.get(Http::LowerCaseString("content-type")).size(), 1);
    EXPECT_EQ(
        response_headers.get(Http::LowerCaseString("content-type"))[0]->value().getStringView(),
        "text/html");
    EXPECT_EQ("<html><body>hi!</body></html>\n", data.toString());

    const Envoy::Http::LowerCaseString x_client_can_accept_sxg_key("x-client-can-accept-sxg");
    if (client_can_accept_sxg) {
      EXPECT_FALSE(request_headers.get(x_client_can_accept_sxg_key).empty());
      EXPECT_EQ("true",
                request_headers.get(x_client_can_accept_sxg_key)[0]->value().getStringView());
      EXPECT_EQ(1UL, store_.counter("sxg.total_client_can_accept_sxg").value());
    } else {
      const Envoy::Http::LowerCaseString x_client_can_accept_sxg_key("x-client-can-accept-sxg");
      EXPECT_TRUE(request_headers.get(x_client_can_accept_sxg_key).empty());
      EXPECT_EQ(0UL, store_.counter("sxg.total_client_can_accept_sxg").value());
    }
    EXPECT_EQ(0UL, store_.counter("sxg.total_should_sign").value());
    EXPECT_EQ(0UL, store_.counter("sxg.total_exceeded_max_payload_size").value());
    EXPECT_EQ(0UL, store_.counter("sxg.total_signed_attempts").value());
    EXPECT_EQ(0UL, store_.counter("sxg.total_signed_succeeded").value());
    EXPECT_EQ(0UL, store_.counter("sxg.total_signed_failed").value());
  }

  void testFallbackToHtml(Http::TestRequestHeaderMapImpl& request_headers,
                          Http::TestResponseHeaderMapImpl& response_headers,
                          bool exceeded_max_payload_size, bool attempted_encode) {
    EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->decodeHeaders(request_headers, false));
    EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
              filter_->encodeHeaders(response_headers, false));
    Buffer::OwnedImpl data("<html><body>hi!</body></html>\n");

    auto on_modify_encoding_buffer = [&data](std::function<void(Buffer::Instance&)> cb) {
      cb(data);
    };
    EXPECT_CALL(encoder_callbacks_, modifyEncodingBuffer)
        .WillRepeatedly(Invoke(on_modify_encoding_buffer));

    EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(data, true));
    EXPECT_EQ(response_headers.get(Http::LowerCaseString("content-type")).size(), 1);
    EXPECT_EQ(
        response_headers.get(Http::LowerCaseString("content-type"))[0]->value().getStringView(),
        "text/html");
    EXPECT_EQ("<html><body>hi!</body></html>\n", data.toString());

    const Envoy::Http::LowerCaseString x_client_can_accept_sxg_key("x-client-can-accept-sxg");
    EXPECT_FALSE(request_headers.get(x_client_can_accept_sxg_key).empty());
    EXPECT_EQ("true", request_headers.get(x_client_can_accept_sxg_key)[0]->value().getStringView());
    EXPECT_EQ(1UL, store_.counter("sxg.total_client_can_accept_sxg").value());
    EXPECT_EQ(1UL, store_.counter("sxg.total_should_sign").value());
    EXPECT_EQ(exceeded_max_payload_size ? 1UL : 0UL,
              store_.counter("sxg.total_exceeded_max_payload_size").value());
    EXPECT_EQ(attempted_encode ? 1UL : 0L, store_.counter("sxg.total_signed_attempts").value());
    EXPECT_EQ(0UL, store_.counter("sxg.total_signed_succeeded").value());
    EXPECT_EQ(attempted_encode ? 1UL : 0UL, store_.counter("sxg.total_signed_failed").value());
  }

  void testEncodeSignedExchange(Http::TestRequestHeaderMapImpl& request_headers,
                                Http::TestResponseHeaderMapImpl& response_headers) {
    testEncodeSignedExchange(request_headers, response_headers, nullptr);
  }

  void testEncodeSignedExchange(Http::TestRequestHeaderMapImpl& request_headers,
                                Http::TestResponseHeaderMapImpl& response_headers,
                                Http::TestResponseTrailerMapImpl* response_trailers) {
    const Buffer::OwnedImpl sxg(
        "sxg1-b3\0\0\x1Ehttps://example.org/hello.html\0\x1\0\0\0\x84"
        "label;cert-sha256=*unJ3rwJT2DwWlJAw1lfVLvPjeYoJh0+QUQ97zJQPZtc=*;cert-url=\"https://"
        "example.org/.sxg/"
        "cert.cbor?d=ba7277af0253d83c\";date=1610416640;expires=1611021440;integrity=\"digest/"
        "mi-sha256-03\";sig=**;validity-url=\"https://example.org/.sxg/"
        "validity.msg\"\xA4"
        "FdigestX9mi-sha256-03=0x0E2wkWVYOJ7Gq8+Kfaiyjo3gYCyaijhGGgkzjPoTo=G:statusC200Lcontent-"
        "typeItext/htmlPcontent-encodingLmi-sha256-03\0\0\0\0\0\0\x10\0<html><body>hi!</body></"
        "html>\n",
        472);
    testEncodeSignedExchange(request_headers, response_headers, response_trailers, sxg);
  }

  void testEncodeSignedExchange(Http::TestRequestHeaderMapImpl& request_headers,
                                Http::TestResponseHeaderMapImpl& response_headers,
                                const Buffer::OwnedImpl& sxg) {
    testEncodeSignedExchange(request_headers, response_headers, nullptr, sxg);
  }

  void testEncodeSignedExchange(Http::TestRequestHeaderMapImpl& request_headers,
                                Http::TestResponseHeaderMapImpl& response_headers,
                                Http::TestResponseTrailerMapImpl* response_trailers,
                                const Buffer::OwnedImpl& sxg) {
    EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->decodeHeaders(request_headers, false));
    EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
              filter_->encodeHeaders(response_headers, false));

    Buffer::OwnedImpl accumulated_data;

    EXPECT_CALL(encoder_callbacks_, addEncodedData(_, false))
        .Times(2)
        .WillRepeatedly(Invoke(
            [&accumulated_data](Buffer::Instance& data, bool) { accumulated_data.add(data); }));

    auto on_modify_encoding_buffer =
        [&accumulated_data](std::function<void(Buffer::Instance&)> cb) { cb(accumulated_data); };
    EXPECT_CALL(encoder_callbacks_, modifyEncodingBuffer)
        .WillRepeatedly(Invoke(on_modify_encoding_buffer));

    Buffer::OwnedImpl chunk1("<html><body>hi!", 15);
    Buffer::OwnedImpl chunk2("</body></html>\n", 15);
    EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->encodeData(chunk1, false));
    if (response_trailers) {
      EXPECT_EQ(Http::FilterDataStatus::StopIterationAndBuffer, filter_->encodeData(chunk2, false));
      EXPECT_EQ(Http::FilterTrailersStatus::Continue, filter_->encodeTrailers(*response_trailers));
    } else {
      EXPECT_EQ(Http::FilterDataStatus::Continue, filter_->encodeData(chunk2, true));
    }

    std::string result = accumulated_data.toString();
    EXPECT_TRUE(clearSignature(result));
    EXPECT_EQ(response_headers.get(Http::LowerCaseString("content-type")).size(), 1);
    EXPECT_EQ(
        response_headers.get(Http::LowerCaseString("content-type"))[0]->value().getStringView(),
        "application/signed-exchange;v=b3");
    EXPECT_EQ(response_headers.get(Http::LowerCaseString("content-length")).size(), 1);
    EXPECT_EQ(
        response_headers.get(Http::LowerCaseString("content-length"))[0]->value().getStringView(),
        std::to_string(accumulated_data.length()));
    EXPECT_EQ(sxg.toString(), result);

    const Envoy::Http::LowerCaseString x_client_can_accept_sxg_key("x-client-can-accept-sxg");
    EXPECT_FALSE(request_headers.get(x_client_can_accept_sxg_key).empty());
    EXPECT_EQ("true", request_headers.get(x_client_can_accept_sxg_key)[0]->value().getStringView());
    EXPECT_EQ(1UL, store_.counter("sxg.total_client_can_accept_sxg").value());
    EXPECT_EQ(1UL, store_.counter("sxg.total_should_sign").value());
    EXPECT_EQ(0UL, store_.counter("sxg.total_exceeded_max_payload_size").value());
    EXPECT_EQ(1UL, store_.counter("sxg.total_signed_attempts").value());
    EXPECT_EQ(1UL, store_.counter("sxg.total_signed_succeeded").value());
    EXPECT_EQ(0UL, store_.counter("sxg.total_signed_failed").value());
  }

  void callDoSxgAgain() { filter_->doSxg(); }

  Stats::TestUtil::TestStore store_;
  Stats::Scope& scope_{*store_.rootScope()};
  Event::SimulatedTimeSystem time_system_;
  std::shared_ptr<FilterConfig> config_;
  std::unique_ptr<Encoder> encoder_;
  NiceMock<Http::MockStreamDecoderFilterCallbacks> decoder_callbacks_;
  NiceMock<Http::MockStreamEncoderFilterCallbacks> encoder_callbacks_;
  std::shared_ptr<Filter> filter_;
};

// Verifies that the OAuth SDSSecretReader correctly updates dynamic generic secret.
TEST_F(FilterTest, SdsDynamicGenericSecret) {
  NiceMock<Server::MockConfigTracker> config_tracker;
  Secret::SecretManagerImpl secret_manager{config_tracker};
  envoy::config::core::v3::ConfigSource config_source;

  NiceMock<Server::Configuration::MockTransportSocketFactoryContext> secret_context;
  NiceMock<LocalInfo::MockLocalInfo> local_info;
  Api::ApiPtr api = Api::createApiForTest();
  NiceMock<Init::MockManager> init_manager;
  Init::TargetHandlePtr init_handle;
  NiceMock<Event::MockDispatcher> dispatcher;
  EXPECT_CALL(secret_context.server_context_, localInfo()).WillRepeatedly(ReturnRef(local_info));
  EXPECT_CALL(secret_context.server_context_, api()).WillRepeatedly(ReturnRef(*api));
  EXPECT_CALL(secret_context.server_context_, mainThreadDispatcher())
      .WillRepeatedly(ReturnRef(dispatcher));
  EXPECT_CALL(secret_context, initManager()).Times(0);
  EXPECT_CALL(init_manager, add(_))
      .WillRepeatedly(Invoke([&init_handle](const Init::Target& target) {
        init_handle = target.createHandle("test");
      }));

  auto certificate_secret_provider = secret_manager.findOrCreateGenericSecretProvider(
      config_source, "certificate", secret_context, init_manager);
  auto certificate_callback = secret_context.cluster_manager_.subscription_factory_.callbacks_;
  auto private_key_secret_provider = secret_manager.findOrCreateGenericSecretProvider(
      config_source, "private_key", secret_context, init_manager);
  auto private_key_callback = secret_context.cluster_manager_.subscription_factory_.callbacks_;

  SDSSecretReader secret_reader(certificate_secret_provider, private_key_secret_provider, *api);
  EXPECT_TRUE(secret_reader.certificate().empty());
  EXPECT_TRUE(secret_reader.privateKey().empty());

  const std::string yaml_client = R"YAML(
name: certificate
generic_secret:
  secret:
    inline_string: "certificate_test"
)YAML";

  envoy::extensions::transport_sockets::tls::v3::Secret typed_secret;
  TestUtility::loadFromYaml(yaml_client, typed_secret);
  const auto decoded_resources_client = TestUtility::decodeResources({typed_secret});

  EXPECT_TRUE(certificate_callback->onConfigUpdate(decoded_resources_client.refvec_, "").ok());
  EXPECT_EQ(secret_reader.certificate(), "certificate_test");
  EXPECT_EQ(secret_reader.privateKey(), "");

  const std::string yaml_token = R"YAML(
name: private_key
generic_secret:
  secret:
    inline_string: "private_key_test"
)YAML";
  TestUtility::loadFromYaml(yaml_token, typed_secret);
  const auto decoded_resources_token = TestUtility::decodeResources({typed_secret});

  EXPECT_TRUE(private_key_callback->onConfigUpdate(decoded_resources_token.refvec_, "").ok());
  EXPECT_EQ(secret_reader.certificate(), "certificate_test");
  EXPECT_EQ(secret_reader.privateKey(), "private_key_test");

  const std::string yaml_client_recheck = R"EOF(
name: certificate
generic_secret:
  secret:
    inline_string: "certificate_test_recheck"
)EOF";
  TestUtility::loadFromYaml(yaml_client_recheck, typed_secret);
  const auto decoded_resources_client_recheck = TestUtility::decodeResources({typed_secret});

  EXPECT_TRUE(
      certificate_callback->onConfigUpdate(decoded_resources_client_recheck.refvec_, "").ok());
  EXPECT_EQ(secret_reader.certificate(), "certificate_test_recheck");
  EXPECT_EQ(secret_reader.privateKey(), "private_key_test");
}

TEST_F(FilterTest, NoHostHeader) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, AcceptTextHtml) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"accept", "text/html"}, {"host", "example.org"}, {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, HtmlWithTrailers) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"accept", "text/html"}, {"host", "example.org"}, {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{{"content-type", "text/html"},
                                                   {":status", "200"}};
  Http::TestResponseTrailerMapImpl response_trailers{{"x-test-sample-trailer", "wait for me!"}};
  testPassthroughHtml(request_headers, response_headers, &response_trailers, false);
}

TEST_F(FilterTest, NoPathHeader) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, NoAcceptHeader) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"host", "example.org"}, {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, NoStatusHeader) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{{"content-type", "text/html"},
                                                   {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, true);
}

TEST_F(FilterTest, Status404) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "404"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, true);
}

TEST_F(FilterTest, XShouldEncodeNotSet) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{{"content-type", "text/html"},
                                                   {":status", "200"}};
  testPassthroughHtml(request_headers, response_headers, true);
}

TEST_F(FilterTest, AcceptTextHtmlWithQ) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "text/html;q=0.8"},
                                                 {":protocol", "https"},
                                                 {":host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, AcceptApplicationSignedExchangeNoVersion) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"accept", "application/signed-exchange"}, {"host", "example.org"}, {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, AcceptApplicationSignedExchangeWithVersionB2) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b2"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, AcceptApplicationSignedExchangeWithVersionB3) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testEncodeSignedExchange(request_headers, response_headers);
}

TEST_F(FilterTest, AcceptApplicationSignedExchangeWithVersionB3WithQ) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"accept", "application/signed-exchange;v=b3;q=0.9"},
      {"host", "example.org"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testEncodeSignedExchange(request_headers, response_headers);
}

TEST_F(FilterTest, AcceptMultipleTextHtml) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.8,text/html;q=0.9"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testPassthroughHtml(request_headers, response_headers, false);
}

TEST_F(FilterTest, AcceptMultipleSignedExchange) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testEncodeSignedExchange(request_headers, response_headers);
}

TEST_F(FilterTest, ResponseExceedsMaxPayloadSize) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  EXPECT_CALL(encoder_callbacks_, encoderBufferLimit).WillRepeatedly(Return(10));
  testFallbackToHtml(request_headers, response_headers, true, false);
}

TEST_F(FilterTest, ResponseExceedsMaxPayloadSizeEncodeFail) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  EXPECT_CALL(encoder_callbacks_, encoderBufferLimit)
      .WillOnce(Return(100000))
      .WillRepeatedly(Return(10));
  testFallbackToHtml(request_headers, response_headers, true, true);
}

TEST_F(FilterTest, UrlWithQueryParam) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"accept", "application/signed-exchange;v=b3;q=0.9"},
      {"host", "example.org"},
      {":path", "/hello.html?good=bye"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  const Buffer::OwnedImpl expected_sxg(
      "sxg1-b3\0\0\x27https://example.org/hello.html?good=bye\0\x1\0\0\0\x84"
      "label;cert-sha256=*unJ3rwJT2DwWlJAw1lfVLvPjeYoJh0+QUQ97zJQPZtc=*;cert-url=\"https://"
      "example.org/.sxg/"
      "cert.cbor?d=ba7277af0253d83c\";date=1610416640;expires=1611021440;integrity=\"digest/"
      "mi-sha256-03\";sig=**;validity-url=\"https://example.org/.sxg/"
      "validity.msg\"\xA4"
      "FdigestX9mi-sha256-03=0x0E2wkWVYOJ7Gq8+Kfaiyjo3gYCyaijhGGgkzjPoTo=G:statusC200Lcontent-"
      "typeItext/htmlPcontent-encodingLmi-sha256-03\0\0\0\0\0\0\x10\0<html><body>hi!</body></"
      "html>\n",
      481);
  testEncodeSignedExchange(request_headers, response_headers, expected_sxg);
}

TEST_F(FilterTest, CborValdityFullUrls) {
  setConfiguration({R"YAML(
cbor_url: "https://amp.example.org/cert.cbor"
validity_url: "https://amp.example.org/validity.msg"
)YAML"});
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  const Buffer::OwnedImpl expected_sxg(
      "sxg1-b3\0\0\x1Ehttps://example.org/hello.html\0\0\xFE\0\0\x84"
      "label;cert-sha256=*unJ3rwJT2DwWlJAw1lfVLvPjeYoJh0+QUQ97zJQPZtc=*;cert-url=\"https://"
      "amp.example.org/"
      "cert.cbor?d=ba7277af0253d83c\";date=1610416640;expires=1611021440;integrity=\"digest/"
      "mi-sha256-03\";sig=**;validity-url=\"https://amp.example.org/"
      "validity.msg\"\xA4"
      "FdigestX9mi-sha256-03=0x0E2wkWVYOJ7Gq8+Kfaiyjo3gYCyaijhGGgkzjPoTo=G:statusC200Lcontent-"
      "typeItext/htmlPcontent-encodingLmi-sha256-03\0\0\0\0\0\0\x10\0<html><body>hi!</body></"
      "html>\n",
      470);
  testEncodeSignedExchange(request_headers, response_headers, expected_sxg);
}

TEST_F(FilterTest, WithHttpTrailers) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"accept", "application/signed-exchange;v=b3;q=0.9"},
      {"host", "example.org"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  Http::TestResponseTrailerMapImpl response_trailers{{"x-test-sample-trailer", "wait for me!"}};
  testEncodeSignedExchange(request_headers, response_headers, &response_trailers);
}

TEST_F(FilterTest, WithCustomShouldEncodeHeader) {
  setConfiguration({R"YAML(
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
should_encode_sxg_header: "x-custom-should-encode-sxg"
)YAML"});
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{
      {"accept", "application/signed-exchange;v=b3;q=0.9"},
      {"host", "example.org"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-custom-should-encode-sxg", "true"}};
  testEncodeSignedExchange(request_headers, response_headers);
}

TEST_F(FilterTest, FilterXEnvoyHeaders) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{{"content-type", "text/html"},
                                                   {":status", "200"},
                                                   {"x-should-encode-sxg", "true"},
                                                   {"x-envoy-something", "something"}};
  testEncodeSignedExchange(request_headers, response_headers);
}

TEST_F(FilterTest, FilterCustomHeaders) {
  setConfiguration({R"YAML(
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
header_prefix_filters:
 - "x-foo-"
 - "x-bar-"
)YAML"});
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{{"content-type", "text/html"},
                                                   {":status", "200"},
                                                   {"x-should-encode-sxg", "true"},
                                                   {"x-foo-bar", "foo"},
                                                   {"x-bar-baz", "bar"}};
  testEncodeSignedExchange(request_headers, response_headers);
  const Envoy::Http::LowerCaseString x_client_can_accept_sxg_key("x-client-can-accept-sxg");
}

TEST_F(FilterTest, CustomHeader) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{{"content-type", "text/html"},
                                                   {":status", "200"},
                                                   {"x-should-encode-sxg", "true"},
                                                   {"x-special-header", "very special"}};
  const Buffer::OwnedImpl expected_sxg(
      "sxg1-b3\0\0\x1Ehttps://example.org/hello.html\0\x1\0\0\0\xA2"
      "label;cert-sha256=*unJ3rwJT2DwWlJAw1lfVLvPjeYoJh0+QUQ97zJQPZtc=*;cert-url=\"https://"
      "example.org/.sxg/"
      "cert.cbor?d=ba7277af0253d83c\";date=1610416640;expires=1611021440;integrity=\"digest/"
      "mi-sha256-03\";sig=**;validity-url=\"https://example.org/.sxg/"
      "validity.msg\"\xA5"
      "FdigestX9mi-sha256-03=0x0E2wkWVYOJ7Gq8+Kfaiyjo3gYCyaijhGGgkzjPoTo=G:statusC200Lcontent-"
      "typeItext/htmlPcontent-encodingLmi-sha256-03Px-special-headerLvery special"
      "\0\0\0\0\0\0\x10\0<html><body>hi!</body></html>\n",
      502);
  testEncodeSignedExchange(request_headers, response_headers, expected_sxg);
}

TEST_F(FilterTest, ExtraHeaders) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{{"content-type", "text/html"},
                                                   {":status", "200"},
                                                   {"x-should-encode-sxg", "true"},
                                                   {"x-special-header", "twice"},
                                                   {"x-special-header", "as special"}};
  const Buffer::OwnedImpl expected_sxg(
      "sxg1-b3\0\0\x1Ehttps://example.org/hello.html\0\x1\x0\0\0\xA6"
      "label;cert-sha256=*unJ3rwJT2DwWlJAw1lfVLvPjeYoJh0+QUQ97zJQPZtc=*;cert-url=\"https://"
      "example.org/.sxg/"
      "cert.cbor?d=ba7277af0253d83c\";date=1610416640;expires=1611021440;integrity=\"digest/"
      "mi-sha256-03\";sig=**;validity-url=\"https://example.org/.sxg/"
      "validity.msg\"\xA5"
      "FdigestX9mi-sha256-03=0x0E2wkWVYOJ7Gq8+Kfaiyjo3gYCyaijhGGgkzjPoTo=G:statusC200Lcontent-"
      "typeItext/htmlPcontent-encodingLmi-sha256-03Px-special-headerP"
      "twice,as special"
      "\0\0\0\0\0\0\x10\0<html><body>hi!</body></html>\n",
      506);

  testEncodeSignedExchange(request_headers, response_headers, expected_sxg);
}

TEST_F(FilterTest, TestDoubleDoSxg) {
  setConfiguration();
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testEncodeSignedExchange(request_headers, response_headers);
  callDoSxgAgain();
}

TEST_F(FilterTest, LoadHeadersFailure) {
  setConfiguration();
  encoder_ = std::make_unique<MockEncoder>();
  setFilter();
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setOrigin);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setUrl);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadHeaders).WillOnce(Return(false));

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testFallbackToHtml(request_headers, response_headers, false, true);
}

TEST_F(FilterTest, LoadContentFailure) {
  setConfiguration();
  encoder_ = std::make_unique<MockEncoder>();
  setFilter();
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setOrigin);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setUrl);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadHeaders).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadContent).WillOnce(Return(false));

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testFallbackToHtml(request_headers, response_headers, false, true);
}

TEST_F(FilterTest, GetEncodedResponseFailure) {
  setConfiguration();
  encoder_ = std::make_unique<MockEncoder>();
  setFilter();
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setOrigin);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setUrl);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadHeaders).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadContent).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), getEncodedResponse)
      .WillOnce(Return(false));

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testFallbackToHtml(request_headers, response_headers, false, true);
}

TEST_F(FilterTest, LoadSignerFailure) {
  setConfiguration();
  encoder_ = std::make_unique<MockEncoder>();
  setFilter();
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setOrigin);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setUrl);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadHeaders).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadContent).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), getEncodedResponse)
      .WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadSigner).WillOnce(Return(false));

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testFallbackToHtml(request_headers, response_headers, false, true);
}

TEST_F(FilterTest, WriteSxgFailure) {
  setConfiguration();
  encoder_ = std::make_unique<MockEncoder>();
  setFilter();
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setOrigin);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), setUrl);
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadHeaders).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadContent).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), getEncodedResponse)
      .WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), loadSigner).WillOnce(Return(true));
  EXPECT_CALL(*static_cast<MockEncoder*>(encoder_.get()), writeSxg).WillOnce(Return(nullptr));

  Http::TestRequestHeaderMapImpl request_headers{
      {"host", "example.org"},
      {"accept", "application/signed-exchange;v=b3;q=0.9,text/html;q=0.8"},
      {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testFallbackToHtml(request_headers, response_headers, false, true);
}

// MyCombinedCertKeyId
TEST_F(FilterTest, CombiedCertificateId) {
  const std::string certificate(R"PEM(
-----BEGIN CERTIFICATE-----
MIIBhjCCASygAwIBAgIJAIH9REPqIFXTMAkGByqGSM49BAEwMjEUMBIGA1UEAwwL
ZXhhbXBsZS5vcmcxDTALBgNVBAoMBFRlc3QxCzAJBgNVBAYTAlVTMB4XDTIxMDEx
MzAxMDcwMVoXDTIxMDQxMzAxMDcwMVowMjEUMBIGA1UEAwwLZXhhbXBsZS5vcmcx
DTALBgNVBAoMBFRlc3QxCzAJBgNVBAYTAlVTMFkwEwYHKoZIzj0CAQYIKoZIzj0D
AQcDQgAE4ZrHsGLEiP+pV70a8zIERNcu9MBJHHfbeqLUqwGWWU2/YHObf58nE9to
c6lgrko2JdbV6TyWLVUc/M0Pn+OVSaMsMCowEAYKKwYBBAHWeQIBFgQCBQAwFgYD
VR0RBA8wDYILZXhhbXBsZS5vcmcwCQYHKoZIzj0EAQNJADBGAiEAuQJjX+z7j4hR
xtxfs4VPY5RsF5Sawd+mtluRxpoURcsCIQCIGU/11jcuS0UbIpt4B5Gb1UJlSKGi
Dgu+2OKt7qVPrA==
-----END CERTIFICATE-----
-----BEGIN EC PARAMETERS-----
BggqhkjOPQMBBw==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHcCAQEEIJyGXecxIQtBwBJWU4Sc5A8UHNt5HnOBR9Oh11AGYa/2oAoGCCqGSM49
AwEHoUQDQgAE4ZrHsGLEiP+pV70a8zIERNcu9MBJHHfbeqLUqwGWWU2/YHObf58n
E9toc6lgrko2JdbV6TyWLVUc/M0Pn+OVSQ==
-----END EC PRIVATE KEY-----
)PEM");

  setConfiguration({R"YAML(
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
)YAML"},
                   certificate, certificate);
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"accept", "application/signed-exchange;v=b3"},
                                                 {"host", "example.org"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};
  testEncodeSignedExchange(request_headers, response_headers);
}

TEST_F(FilterTest, BadCertificateId) {
  const std::string certificate("");
  const std::string private_key(R"PEM(
-----BEGIN EC PARAMETERS-----
BggqhkjOPQMBBw==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHcCAQEEIJyGXecxIQtBwBJWU4Sc5A8UHNt5HnOBR9Oh11AGYa/2oAoGCCqGSM49
AwEHoUQDQgAE4ZrHsGLEiP+pV70a8zIERNcu9MBJHHfbeqLUqwGWWU2/YHObf58n
E9toc6lgrko2JdbV6TyWLVUc/M0Pn+OVSQ==
-----END EC PRIVATE KEY-----
)PEM");

  setConfiguration({R"YAML(
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
)YAML"},
                   certificate, private_key);
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"host", "example.org"},
                                                 {"accept", "application/signed-exchange;v=b3"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};

  testFallbackToHtml(request_headers, response_headers, false, true);
}
std::string private_key(R"PEM(
-----BEGIN EC PARAMETERS-----
BggqhkjOPQMBBw==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHcCAQEEIJyGXecxIQtBwBJWU4Sc5A8UHNt5HnOBR9Oh11AGYa/2oAoGCCqGSM49
AwEHoUQDQgAE4ZrHsGLEiP+pV70a8zIERNcu9MBJHHfbeqLUqwGWWU2/YHObf58n
E9toc6lgrko2JdbV6TyWLVUc/M0Pn+OVSQ==
-----END EC PRIVATE KEY-----
)PEM");

TEST_F(FilterTest, BadPriKeyId) {
  const std::string certificate(R"PEM(
-----BEGIN CERTIFICATE-----
MIIBhjCCASygAwIBAgIJAIH9REPqIFXTMAkGByqGSM49BAEwMjEUMBIGA1UEAwwL
ZXhhbXBsZS5vcmcxDTALBgNVBAoMBFRlc3QxCzAJBgNVBAYTAlVTMB4XDTIxMDEx
MzAxMDcwMVoXDTIxMDQxMzAxMDcwMVowMjEUMBIGA1UEAwwLZXhhbXBsZS5vcmcx
DTALBgNVBAoMBFRlc3QxCzAJBgNVBAYTAlVTMFkwEwYHKoZIzj0CAQYIKoZIzj0D
AQcDQgAE4ZrHsGLEiP+pV70a8zIERNcu9MBJHHfbeqLUqwGWWU2/YHObf58nE9to
c6lgrko2JdbV6TyWLVUc/M0Pn+OVSaMsMCowEAYKKwYBBAHWeQIBFgQCBQAwFgYD
VR0RBA8wDYILZXhhbXBsZS5vcmcwCQYHKoZIzj0EAQNJADBGAiEAuQJjX+z7j4hR
xtxfs4VPY5RsF5Sawd+mtluRxpoURcsCIQCIGU/11jcuS0UbIpt4B5Gb1UJlSKGi
Dgu+2OKt7qVPrA==
-----END CERTIFICATE-----
)PEM");
  const std::string private_key("");

  setConfiguration({R"YAML(
cbor_url: "/.sxg/cert.cbor"
validity_url: "/.sxg/validity.msg"
)YAML"},
                   certificate, private_key);
  setFilter();

  Http::TestRequestHeaderMapImpl request_headers{{"host", "example.org"},
                                                 {"accept", "application/signed-exchange;v=b3"},
                                                 {":path", "/hello.html"}};
  Http::TestResponseHeaderMapImpl response_headers{
      {"content-type", "text/html"}, {":status", "200"}, {"x-should-encode-sxg", "true"}};

  testFallbackToHtml(request_headers, response_headers, false, true);
}

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/sxg/filters/http/source/filter_config.h"

#include <string>

#include "envoy/http/codes.h"
#include "envoy/server/filter_config.h"
#include "envoy/stats/scope.h"

#include "source/common/common/utility.h"
#include "source/common/http/headers.h"
#include "source/common/stats/utility.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

template <class T>
const std::vector<std::string> initializeHeaderPrefixFilters(const T& filters_proto) {
  std::vector<std::string> filters;
  filters.reserve(filters_proto.size());

  for (const auto& filter : filters_proto) {
    filters.emplace_back(filter);
  }

  return filters;
}

FilterConfig::FilterConfig(const envoy::extensions::filters::http::sxg::v3alpha::SXG& proto_config,
                           TimeSource& time_source, std::shared_ptr<SecretReader> secret_reader,
                           const std::string& stat_prefix, Stats::Scope& scope)
    : stats_(generateStats(stat_prefix + "sxg.", scope)),
      duration_(proto_config.has_duration() ? proto_config.duration().seconds() : 604800UL),
      cbor_url_(proto_config.cbor_url()), validity_url_(proto_config.validity_url()),
      mi_record_size_(proto_config.mi_record_size() ? proto_config.mi_record_size() : 4096L),
      client_can_accept_sxg_header_(proto_config.client_can_accept_sxg_header().length() > 0
                                        ? proto_config.client_can_accept_sxg_header()
                                        : "x-client-can-accept-sxg"),
      should_encode_sxg_header_(proto_config.should_encode_sxg_header().length() > 0
                                    ? proto_config.should_encode_sxg_header()
                                    : "x-should-encode-sxg"),
      header_prefix_filters_(initializeHeaderPrefixFilters(proto_config.header_prefix_filters())),
      time_source_(time_source), secret_reader_(secret_reader) {}

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <string>

#include "source/extensions/filters/http/common/factory_base.h"

#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.h"
#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

class FilterFactory : public Extensions::HttpFilters::Common::FactoryBase<
                          envoy::extensions::filters::http::sxg::v3alpha::SXG> {
public:
  FilterFactory() : FactoryBase("envoy.filters.http.sxg") {}

private:
  Http::FilterFactoryCb createFilterFactoryFromProtoTyped(
      const envoy::extensions::filters::http::sxg::v3alpha::SXG& config,
      const std::string& stats_prefix, Server::Configuration::FactoryContext& context) override;
};

DECLARE_FACTORY(FilterFactory);

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/sxg/filters/http/source/filter.h"

#include <string>

#include "envoy/http/codes.h"
#include "envoy/stats/scope.h"

#include "source/common/common/utility.h"
#include "source/common/http/headers.h"
#include "source/common/stats/utility.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

Http::RegisterCustomInlineHeader<Http::CustomInlineHeaderRegistry::Type::RequestHeaders>
    accept_handle(Http::CustomHeaders::get().Accept);

Http::FilterHeadersStatus Filter::decodeHeaders(Http::RequestHeaderMap& headers, bool) {
  ENVOY_LOG(debug, "sxg filter from decodeHeaders: {}", headers);
  if (headers.Host() && headers.Path() && clientAcceptSXG(headers)) {
    client_accept_sxg_ = true;
    headers.setReference(xCanAcceptSxgKey(), xCanAcceptSxgValue());
    auto origin = fmt::format("https://{}", headers.getHostValue());
    auto url = fmt::format("{}{}", origin, headers.getPathValue());
    encoder_->setOrigin(origin);
    encoder_->setUrl(url);
    config_->stats().total_client_can_accept_sxg_.inc();
  }
  return Http::FilterHeadersStatus::Continue;
}

void Filter::setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks& callbacks) {
  decoder_callbacks_ = &callbacks;
}

Http::FilterHeadersStatus Filter::encodeHeaders(Http::ResponseHeaderMap& headers, bool) {
  ENVOY_LOG(debug, "sxg filter from Filter::encodeHeaders");

  if (client_accept_sxg_ && shouldEncodeSXG(headers)) {
    response_headers_ = &headers;
    should_encode_sxg_ = true;
    config_->stats().total_should_sign_.inc();
    return Http::FilterHeadersStatus::StopIteration;
  }

  return Http::FilterHeadersStatus::Continue;
}

Http::FilterDataStatus Filter::encodeData(Buffer::Instance& data, bool end_stream) {
  ENVOY_LOG(debug, "sxg filter from encodeData end_stream: {}", end_stream);

  if (!should_encode_sxg_) {
    return Http::FilterDataStatus::Continue;
  }

  data_total_ += data.length();
  if (encoderBufferLimitReached(data_total_)) {
    should_encode_sxg_ = false;
    return Http::FilterDataStatus::Continue;
  }

  encoder_callbacks_->addEncodedData(data, false);

  if (!end_stream) {
    // We need to know the size of the response in order to generate the SXG, so we wait.
    return Http::FilterDataStatus::StopIterationAndBuffer;
  }

  doSxg();
  return Http::FilterDataStatus::Continue;
}

Http::FilterTrailersStatus Filter::encodeTrailers(Http::ResponseTrailerMap&) {
  if (should_encode_sxg_) {
    doSxg();
  }
  return Http::FilterTrailersStatus::Continue;
}

void Filter::doSxg() {
  if (finished_) {
    return;
  }

  finished_ = true;

  encoder_callbacks_->modifyEncodingBuffer([this](Buffer::Instance& enc_buf) {
    config_->stats().total_signed_attempts_.inc();

    if (!encoder_->loadHeaders(response_headers_)) {
      config_->stats().total_signed_failed_.inc();
      return;
    }

    if (!encoder_->loadContent(enc_buf)) {
      config_->stats().total_signed_failed_.inc();
      return;
    }

    if (!encoder_->getEncodedResponse()) {
      config_->stats().total_signed_failed_.inc();
      return;
    }

    if (!encoder_->loadSigner()) {
      config_->stats().total_signed_failed_.inc();
      return;
    }

    auto output = encoder_->writeSxg();
    if (!output) {
      config_->stats().total_signed_failed_.inc();
      return;
    }

    // Make sure that the resulting SXG isn't too big before adding it to the encoding
    // buffer. Note that since the buffer fragment hasn't been added to the enc_buf
    // yet, we need to call done() directly.
    if (encoderBufferLimitReached(output->size() + 100)) {
      output->done();
      config_->stats().total_signed_failed_.inc();
      return;
    }

    enc_buf.drain(enc_buf.length());
    enc_buf.addBufferFragment(*output);

    response_headers_->setContentLength(enc_buf.length());
    response_headers_->setContentType(sxgContentType());

    config_->stats().total_signed_succeeded_.inc();
  });
}

void Filter::setEncoderFilterCallbacks(Http::StreamEncoderFilterCallbacks& callbacks) {
  encoder_callbacks_ = &callbacks;
}

bool Filter::clientAcceptSXG(const Http::RequestHeaderMap& headers) {
  const absl::string_view accept = headers.getInlineValue(accept_handle.handle());

  absl::string_view html_q_value = "0";
  absl::string_view sxg_q_value = "";
  // Client can accept signed exchange if accept header has:
  // a) application/signed-exchange
  // b) with appropriate version (v=b3)
  // c) q-value of signed exchange is >= that of text/html
  // from: https://web.dev/signed-exchanges/#best-practices
  for (const auto& token : StringUtil::splitToken(accept, ",")) {
    const auto& type = StringUtil::trim(StringUtil::cropRight(token, ";"));
    absl::string_view q_value = "1";
    absl::string_view version = "";

    const auto params = StringUtil::cropLeft(token, ";");
    for (const auto& param : StringUtil::splitToken(params, ";")) {
      if (absl::EqualsIgnoreCase("q", StringUtil::trim(StringUtil::cropRight(param, "=")))) {
        q_value = StringUtil::trim(StringUtil::cropLeft(param, "="));
      }
      if (absl::EqualsIgnoreCase("v", StringUtil::trim(StringUtil::cropRight(param, "=")))) {
        version = StringUtil::trim(StringUtil::cropLeft(param, "="));
      }
    }

    if (type == sxgContentTypeUnversioned() && version == acceptedSxgVersion()) {
      sxg_q_value = q_value;
    } else if (type == htmlContentType()) {
      html_q_value = q_value;
    }
  }

  return sxg_q_value.compare(html_q_value) >= 0;
}

bool Filter::shouldEncodeSXG(const Http::ResponseHeaderMap& headers) {
  if (!(headers.Status() && headers.getStatusValue() == "200")) {
    return false;
  }

  const auto x_should_encode_sxg_header = headers.get(xShouldEncodeSxgKey());
  return !x_should_encode_sxg_header.empty();
}

bool Filter::encoderBufferLimitReached(uint64_t buffer_length) {
  const auto limit = encoder_callbacks_->encoderBufferLimit();
  const auto header_size = response_headers_->byteSize();

  ENVOY_LOG(debug,
            "Envoy::Extensions::HttpFilters::SXG::Filter::encoderBufferLimitReached limit: {}, "
            "header_size: {} buffer_length: {}",
            limit, header_size, buffer_length);

  // note that a value of 0 indicates that no limits are enforced
  if (limit && header_size + buffer_length > limit) {
    config_->stats().total_exceeded_max_payload_size_.inc();
    return true;
  }
  return false;
}

const Http::LowerCaseString& Filter::xCanAcceptSxgKey() const {
  return config_->clientCanAcceptSXGHeader();
}

const std::string& Filter::xCanAcceptSxgValue() const {
  CONSTRUCT_ON_FIRST_USE(std::string, "true");
}

const Http::LowerCaseString& Filter::xShouldEncodeSxgKey() const {
  return config_->shouldEncodeSXGHeader();
}

const std::string& Filter::htmlContentType() const {
  CONSTRUCT_ON_FIRST_USE(std::string, "text/html");
}

const std::string& Filter::sxgContentTypeUnversioned() const {
  CONSTRUCT_ON_FIRST_USE(std::string, "application/signed-exchange");
}

const std::string& Filter::acceptedSxgVersion() const { CONSTRUCT_ON_FIRST_USE(std::string, "b3"); }

const std::string& Filter::sxgContentType() const {
  CONSTRUCT_ON_FIRST_USE(std::string, "application/signed-exchange;v=b3");
}

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "envoy/server/filter_config.h"
#include "envoy/stats/scope.h"
#include "envoy/stats/stats_macros.h"

#include "source/common/config/datasource.h"
#include "source/extensions/filters/http/common/pass_through_filter.h"

#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.h"
#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

#define ALL_SXG_STATS(COUNTER)                                                                     \
  COUNTER(total_client_can_accept_sxg)                                                             \
  COUNTER(total_should_sign)                                                                       \
  COUNTER(total_exceeded_max_payload_size)                                                         \
  COUNTER(total_signed_attempts)                                                                   \
  COUNTER(total_signed_succeeded)                                                                  \
  COUNTER(total_signed_failed)

struct SignedExchangeStats {
  ALL_SXG_STATS(GENERATE_COUNTER_STRUCT)
};

// Helper class used to fetch secrets (usually from SDS).
class SecretReader {
public:
  virtual ~SecretReader() = default;
  virtual const std::string& certificate() const PURE;
  virtual const std::string& privateKey() const PURE;
};

class SDSSecretReader : public SecretReader {
public:
  SDSSecretReader(Secret::GenericSecretConfigProviderSharedPtr certificate_provider,
                  Secret::GenericSecretConfigProviderSharedPtr private_key_provider, Api::Api& api)
      : update_callback_client_(readAndWatchSecret(certificate_, certificate_provider, api)),
        update_callback_token_(readAndWatchSecret(private_key_, private_key_provider, api)) {}

  // SecretReader
  const std::string& certificate() const override { return certificate_; }
  const std::string& privateKey() const override { return private_key_; }

private:
  Envoy::Common::CallbackHandlePtr
  readAndWatchSecret(std::string& value,
                     Secret::GenericSecretConfigProviderSharedPtr& secret_provider, Api::Api& api) {
    const auto* secret = secret_provider->secret();
    if (secret != nullptr) {
      value = Config::DataSource::read(secret->secret(), true, api);
    }

    return secret_provider->addUpdateCallback([secret_provider, &api, &value]() {
      const auto* secret = secret_provider->secret();
      if (secret != nullptr) {
        value = Config::DataSource::read(secret->secret(), true, api);
      }
    });
  }

  std::string certificate_;
  std::string private_key_;

  Envoy::Common::CallbackHandlePtr update_callback_client_;
  Envoy::Common::CallbackHandlePtr update_callback_token_;
};

class FilterConfig : public Logger::Loggable<Logger::Id::filter> {
public:
  FilterConfig(const envoy::extensions::filters::http::sxg::v3alpha::SXG& proto_config,
               TimeSource& time_source, std::shared_ptr<SecretReader> secret_reader,
               const std::string& stat_prefix, Stats::Scope&);
  ~FilterConfig() = default;

  const SignedExchangeStats stats() { return stats_; };

  long duration() const { return duration_; };
  long miRecordSize() const { return mi_record_size_; };
  const std::string& cborUrl() const { return cbor_url_; };
  const std::string& validityUrl() const { return validity_url_; };
  TimeSource& timeSource() { return time_source_; };
  const Http::LowerCaseString& clientCanAcceptSXGHeader() { return client_can_accept_sxg_header_; }
  const Http::LowerCaseString& shouldEncodeSXGHeader() { return should_encode_sxg_header_; }
  const std::vector<std::string>& headerPrefixFilters() { return header_prefix_filters_; }

  const std::string& certificate() const { return secret_reader_->certificate(); }
  const std::string& privateKey() const { return secret_reader_->privateKey(); }

private:
  static SignedExchangeStats generateStats(const std::string& prefix, Stats::Scope& scope) {
    return SignedExchangeStats{ALL_SXG_STATS(POOL_COUNTER_PREFIX(scope, prefix))};
  }

  SignedExchangeStats stats_;

  const long duration_;
  const std::string cbor_url_;
  const std::string validity_url_;
  const long mi_record_size_;
  const Http::LowerCaseString client_can_accept_sxg_header_;
  const Http::LowerCaseString should_encode_sxg_header_;
  const std::vector<std::string> header_prefix_filters_;

  TimeSource& time_source_;
  const std::shared_ptr<SecretReader> secret_reader_;
  const std::string certificate_identifier_;
  const std::string private_key_identifier_;
};

using FilterConfigSharedPtr = std::shared_ptr<FilterConfig>;

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "source/common/buffer/buffer_impl.h"
#include "source/common/config/datasource.h"

#include "contrib/sxg/filters/http/source/filter_config.h"
#include "libsxg.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

/**
 * Helper type to facilitate comparing an absl::string_view key to a std::string.
 */
struct StringCmp {
  using IsTransparent = void;
  bool operator()(absl::string_view a, absl::string_view b) const { return a < b; }
};

class Encoder {
public:
  virtual ~Encoder() = default;

  virtual void setOrigin(const std::string origin) PURE;
  virtual void setUrl(const std::string url) PURE;
  virtual bool loadSigner() PURE;
  virtual bool loadHeaders(Http::ResponseHeaderMap* headers) PURE;
  virtual bool loadContent(Buffer::Instance& data) PURE;
  virtual bool getEncodedResponse() PURE;
  virtual Buffer::BufferFragment* writeSxg() PURE;
};

using EncoderPtr = std::unique_ptr<Encoder>;

class EncoderImpl : public Encoder, Logger::Loggable<Logger::Id::filter> {
public:
  explicit EncoderImpl(const FilterConfigSharedPtr& config)
      : headers_(sxg_empty_header()), raw_response_(sxg_empty_raw_response()),
        signer_list_(sxg_empty_signer_list()), encoded_response_(sxg_empty_encoded_response()),
        config_(config) {}

  ~EncoderImpl() override;

  // Filter::Encoder
  void setOrigin(const std::string origin) override;
  void setUrl(const std::string url) override;
  bool loadHeaders(Http::ResponseHeaderMap* headers) override;
  bool loadSigner() override;
  bool loadContent(Buffer::Instance& data) override;
  bool getEncodedResponse() override;
  Buffer::BufferFragment* writeSxg() override;

private:
  friend class EncoderTest;

  sxg_header_t headers_;
  sxg_raw_response_t raw_response_;
  sxg_signer_list_t signer_list_;
  sxg_encoded_response_t encoded_response_;
  FilterConfigSharedPtr config_;
  std::string origin_;
  std::string url_;

  uint64_t getTimestamp();
  const std::string toAbsolute(const std::string& url_or_relative_path) const;
  const std::string getCborUrl(const std::string& cert_digest) const;
  const std::string getValidityUrl() const;

  X509* loadX09Cert();
  EVP_PKEY* loadPrivateKey();
  const std::string& sxgSigLabel() const;
  const std::string generateCertDigest(X509* cert) const;

  using HeaderFilterSet = std::set<absl::string_view, StringCmp>;
  const HeaderFilterSet& filteredResponseHeaders() const;
};

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/sxg/filters/http/source/encoder.h"

#include <openssl/bio.h>
#include <openssl/evp.h>
#include <openssl/pem.h>
#include <openssl/x509.h>

#include <chrono>

#include "source/common/buffer/buffer_impl.h"
#include "source/common/http/headers.h"

#include "absl/strings/escaping.h"
#include "contrib/sxg/filters/http/source/filter_config.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

EncoderImpl::~EncoderImpl() {
  sxg_header_release(&headers_);
  sxg_raw_response_release(&raw_response_);
  sxg_signer_list_release(&signer_list_);
  sxg_encoded_response_release(&encoded_response_);
}

void EncoderImpl::setOrigin(const std::string origin) { origin_ = origin; };

void EncoderImpl::setUrl(const std::string url) { url_ = url; };

bool EncoderImpl::loadHeaders(Http::ResponseHeaderMap* headers) {
  const auto& filtered_headers = filteredResponseHeaders();
  bool retval = true;
  headers->iterate([this, filtered_headers,
                    &retval](const Http::HeaderEntry& header) -> Http::HeaderMap::Iterate {
    const auto& header_key = header.key().getStringView();

    // filter x-envoy-* headers
    if (absl::StartsWith(header_key, ThreadSafeSingleton<Http::PrefixValue>::get().prefix())) {
      return Http::HeaderMap::Iterate::Continue;
    }
    // filter out the header that we use as a flag to trigger encoding
    if (config_->shouldEncodeSXGHeader().get() == header_key) {
      return Http::HeaderMap::Iterate::Continue;
    }
    // filter out other headers by prefix
    for (const auto& prefix_filter : config_->headerPrefixFilters()) {
      if (absl::StartsWith(header_key, prefix_filter)) {
        return Http::HeaderMap::Iterate::Continue;
      }
    }
    // filter out headers that are not allowed to be encoded in the SXG document
    if (filtered_headers.find(header_key) != filtered_headers.end()) {
      return Http::HeaderMap::Iterate::Continue;
    }

    const auto header_value = header.value().getStringView();
    if (!sxg_header_append_string(std::string(header_key).c_str(),
                                  std::string(header_value).c_str(), &headers_)) {
      retval = false;
      return Http::HeaderMap::Iterate::Break;
    }
    return Http::HeaderMap::Iterate::Continue;
  });

  return retval;
}

bool EncoderImpl::loadContent(Buffer::Instance& data) {
  const size_t size = data.length();
  if (!sxg_buffer_resize(size, &raw_response_.payload)) {
    return false;
  }
  data.copyOut(0, size, raw_response_.payload.data);

  return true;
}

constexpr uint64_t ONE_DAY_IN_SECONDS = 86400L;

bool EncoderImpl::loadSigner() {
  // backdate timestamp by 1 day, to account for clock skew
  const uint64_t date = getTimestamp() - ONE_DAY_IN_SECONDS;

  const uint64_t expires = date + static_cast<uint64_t>(config_->duration());
  const auto validity_url = getValidityUrl();

  X509* cert = loadX09Cert();
  const auto cert_digest = generateCertDigest(cert);
  const auto cbor_url = getCborUrl(cert_digest);

  EVP_PKEY* pri_key = loadPrivateKey();

  const auto retval =
      cert && pri_key &&
      sxg_add_ecdsa_signer(sxgSigLabel().c_str(), date, expires, validity_url.c_str(), pri_key,
                           cert, cbor_url.c_str(), &signer_list_);

  if (cert) {
    X509_free(cert);
  }
  if (pri_key) {
    EVP_PKEY_free(pri_key);
  }
  return retval;
}

bool EncoderImpl::getEncodedResponse() {
  // Pass response headers to the response before encoding
  if (!sxg_header_copy(&headers_, &raw_response_.header)) {
    return false;
  }
  if (!sxg_encode_response(config_->miRecordSize(), &raw_response_, &encoded_response_)) {
    return false;
  }
  return true;
}

Buffer::BufferFragment* EncoderImpl::writeSxg() {
  sxg_buffer_t result = sxg_empty_buffer();
  if (!sxg_generate(url_.c_str(), &signer_list_, &encoded_response_, &result)) {
    sxg_buffer_release(&result);
    return nullptr;
  }

  return new Buffer::BufferFragmentImpl(
      result.data, result.size,
      [result](const void*, size_t, const Buffer::BufferFragmentImpl* this_fragment) {
        // Capture of result by value passes a const, but sxg_buffer_release does not accept
        // a const buffer_t*, so we have to cast it back. This is OK since the important
        // operation performed by sxg_buffer_release is to release the data buffer.
        sxg_buffer_release(const_cast<sxg_buffer_t*>(&result));
        delete this_fragment;
      });
}

uint64_t EncoderImpl::getTimestamp() {
  const auto now = config_->timeSource().systemTime();
  const auto ts = std::abs(static_cast<int>(
      std::chrono::duration_cast<std::chrono::seconds>(now.time_since_epoch()).count()));

  return ts;
}

const std::string EncoderImpl::toAbsolute(const std::string& url_or_relative_path) const {
  if (!url_or_relative_path.empty() && url_or_relative_path[0] == '/') {
    return origin_ + url_or_relative_path;
  } else {
    return url_or_relative_path;
  }
}

const std::string EncoderImpl::getValidityUrl() const { return toAbsolute(config_->validityUrl()); }

const std::string EncoderImpl::getCborUrl(const std::string& cert_digest) const {
  return fmt::format("{}?d={}", toAbsolute(config_->cborUrl()), cert_digest);
}

X509* EncoderImpl::loadX09Cert() {
  X509* cert = nullptr;
  BIO* bio = BIO_new(BIO_s_mem());
  RELEASE_ASSERT(bio != nullptr, "");

  if (BIO_puts(bio, config_->certificate().c_str()) >= 0) {
    cert = PEM_read_bio_X509(bio, nullptr, nullptr, nullptr);
  }

  BIO_vfree(bio);
  return cert;
}

EVP_PKEY* EncoderImpl::loadPrivateKey() {
  EVP_PKEY* pri_key = nullptr;
  BIO* bio = BIO_new(BIO_s_mem());
  RELEASE_ASSERT(bio != nullptr, "");

  if (BIO_puts(bio, config_->privateKey().c_str()) >= 0) {
    pri_key = PEM_read_bio_PrivateKey(bio, nullptr, nullptr, nullptr);
  }

  BIO_vfree(bio);
  return pri_key;
}

const uint8_t CERT_DIGEST_BYTES = 8;

const std::string EncoderImpl::generateCertDigest(X509* cert) const {
  uint8_t out[EVP_MAX_MD_SIZE];
  unsigned out_len;
  if (!(X509_digest(cert, EVP_sha256(), out, &out_len) && out_len >= CERT_DIGEST_BYTES)) {
    return "";
  }

  return absl::BytesToHexString(
      absl::string_view(reinterpret_cast<const char*>(out), CERT_DIGEST_BYTES));
}

const std::string& EncoderImpl::sxgSigLabel() const {
  // this is currently ignored, so an arbitrary string is safe to use
  CONSTRUCT_ON_FIRST_USE(std::string, "label");
}

const EncoderImpl::HeaderFilterSet& EncoderImpl::filteredResponseHeaders() const {
  CONSTRUCT_ON_FIRST_USE(
      HeaderFilterSet,
      {
          // handled by libsxg, or explicitly by this filter
          ":status",
          // hop-by-hop headers, see:
          // https://tools.ietf.org/id/draft-yasskin-http-origin-signed-responses-05.html#uncached-headers
          "connection",
          "keep-alive",
          "proxy-connection",
          "trailer",
          "transfer-encoding",
          "upgrade",
          // Stateful headers, see:
          // https://tools.ietf.org/id/draft-yasskin-http-origin-signed-responses-05.html#stateful-headers
          // and blocked in http://crrev.com/c/958945.
          "authentication-control",
          "authentication-info",
          "clear-site-data",
          "optional-www-authenticate",
          "proxy-authenticate",
          "proxy-authentication-info",
          "public-key-pins",
          "sec-websocket-accept",
          "set-cookie",
          "set-cookie2",
          "setprofile",
          "strict-transport-security",
          "www-authenticate",
          // other stateful headers
          "vary",
          "cache-control",
      });
}

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_contrib_extension",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_library(
    name = "sxg_lib",
    srcs = [
        "encoder.cc",
        "filter.cc",
        "filter_config.cc",
    ],
    hdrs = [
        "encoder.h",
        "filter.h",
        "filter_config.h",
    ],
    external_deps = ["libsxg"],
    deps = [
        "//envoy/server:filter_config_interface",
        "//source/common/config:datasource_lib",
        "//source/common/http:codes_lib",
        "//source/common/stats:symbol_table_lib",
        "//source/common/stats:utility_lib",
        "//source/extensions/filters/http/common:pass_through_filter_lib",
        "@envoy_api//contrib/envoy/extensions/filters/http/sxg/v3alpha:pkg_cc_proto",
        # use boringssl alias to select fips vs non-fips version.
        "//bazel:boringssl",
    ],
)

envoy_cc_contrib_extension(
    name = "config",
    srcs = ["config.cc"],
    hdrs = ["config.h"],
    deps = [
        ":sxg_lib",
        "//envoy/registry",
        "//source/extensions/filters/http/common:factory_base_lib",
        "@envoy_api//contrib/envoy/extensions/filters/http/sxg/v3alpha:pkg_cc_proto",
    ],
)
#pragma once

#include "envoy/stats/scope.h"

#include "source/extensions/filters/http/common/pass_through_filter.h"

#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.h"
#include "contrib/sxg/filters/http/source/encoder.h"
#include "contrib/sxg/filters/http/source/filter_config.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

/**
 * Transaction flow:
 * 1. check accept request header for whether client can accept sxg
 * 2. check response headers for flag to indicate whether downstream wants SXG encoding
 * 3. if both true, buffer response body until stream end and then run through the libsxg encoder
 * thingy
 *
 */
class Filter : public Http::PassThroughFilter, Logger::Loggable<Logger::Id::filter> {
public:
  Filter(const FilterConfigSharedPtr& config, const EncoderPtr& encoder)
      : config_(config), encoder_(encoder) {}

  // Http::StreamDecoderFilter
  Http::FilterHeadersStatus decodeHeaders(Http::RequestHeaderMap&, bool end_stream) override;
  void setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks&) override;

  // Http::StreamEncodeFilter
  Http::FilterHeadersStatus encodeHeaders(Http::ResponseHeaderMap&, bool end_stream) override;
  Http::FilterDataStatus encodeData(Buffer::Instance& data, bool end_stream) override;
  Http::FilterTrailersStatus encodeTrailers(Http::ResponseTrailerMap&) override;
  void setEncoderFilterCallbacks(Http::StreamEncoderFilterCallbacks& callbacks) override;

private:
  friend class FilterTest;

  bool client_accept_sxg_{false};
  bool should_encode_sxg_{false};
  std::shared_ptr<FilterConfig> config_;
  Http::ResponseHeaderMap* response_headers_;
  uint64_t data_total_{0};
  bool finished_{false};
  const EncoderPtr& encoder_;

  Http::StreamDecoderFilterCallbacks* decoder_callbacks_;
  Http::StreamEncoderFilterCallbacks* encoder_callbacks_;

  void doSxg();

  const absl::string_view urlStripQueryFragment(absl::string_view path) const;

  bool clientAcceptSXG(const Http::RequestHeaderMap& headers);
  bool shouldEncodeSXG(const Http::ResponseHeaderMap& headers);
  bool encoderBufferLimitReached(uint64_t buffer_length);
  const Http::LowerCaseString& xCanAcceptSxgKey() const;
  const std::string& xCanAcceptSxgValue() const;
  const Http::LowerCaseString& xShouldEncodeSxgKey() const;
  const std::string& htmlContentType() const;
  const std::string& sxgContentTypeUnversioned() const;
  const std::string& acceptedSxgVersion() const;
  const std::string& sxgContentType() const;
};

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/sxg/filters/http/source/config.h"

#include <memory>
#include <string>

#include "envoy/registry/registry.h"
#include "envoy/secret/secret_manager.h"
#include "envoy/secret/secret_provider.h"

#include "source/common/protobuf/utility.h"

#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.h"
#include "contrib/envoy/extensions/filters/http/sxg/v3alpha/sxg.pb.validate.h"
#include "contrib/sxg/filters/http/source/encoder.h"
#include "contrib/sxg/filters/http/source/filter.h"

namespace Envoy {
namespace Extensions {
namespace HttpFilters {
namespace SXG {

namespace {
Secret::GenericSecretConfigProviderSharedPtr
secretsProvider(const envoy::extensions::transport_sockets::tls::v3::SdsSecretConfig& config,
                Secret::SecretManager& secret_manager,
                Server::Configuration::TransportSocketFactoryContext& transport_socket_factory,
                Init::Manager& init_manager) {
  if (config.has_sds_config()) {
    return secret_manager.findOrCreateGenericSecretProvider(config.sds_config(), config.name(),
                                                            transport_socket_factory, init_manager);
  } else {
    return secret_manager.findStaticGenericSecretProvider(config.name());
  }
}
} // namespace

Http::FilterFactoryCb FilterFactory::createFilterFactoryFromProtoTyped(
    const envoy::extensions::filters::http::sxg::v3alpha::SXG& proto_config,
    const std::string& stat_prefix, Server::Configuration::FactoryContext& context) {
  const auto& certificate = proto_config.certificate();
  const auto& private_key = proto_config.private_key();

  auto& server_context = context.serverFactoryContext();

  auto& cluster_manager = server_context.clusterManager();
  auto& secret_manager = cluster_manager.clusterManagerFactory().secretManager();
  auto& transport_socket_factory = context.getTransportSocketFactoryContext();
  auto secret_provider_certificate =
      secretsProvider(certificate, secret_manager, transport_socket_factory, context.initManager());
  if (secret_provider_certificate == nullptr) {
    throw EnvoyException("invalid certificate secret configuration");
  }
  auto secret_provider_private_key =
      secretsProvider(private_key, secret_manager, transport_socket_factory, context.initManager());
  if (secret_provider_private_key == nullptr) {
    throw EnvoyException("invalid private_key secret configuration");
  }

  auto secret_reader = std::make_shared<SDSSecretReader>(
      secret_provider_certificate, secret_provider_private_key, server_context.api());
  auto config = std::make_shared<FilterConfig>(proto_config, server_context.timeSource(),
                                               secret_reader, stat_prefix, context.scope());
  return [config](Http::FilterChainFactoryCallbacks& callbacks) -> void {
    const EncoderPtr encoder = std::make_unique<EncoderImpl>(config);
    callbacks.addStreamFilter(std::make_shared<Filter>(config, encoder));
  };
}

REGISTER_FACTORY(FilterFactory, Server::Configuration::NamedHttpFilterConfigFactory);

} // namespace SXG
} // namespace HttpFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/common/sqlutils/source/sqlutils.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace Common {
namespace SQLUtils {

// MetadataFromSQLTest class is used for parameterized tests.
// The values in the tests are:
// std::string - SQL query
// bool - whether to expect SQL parsing to be successful
// std::map<std::string, std::list<std::string>> map of expected tables accessed based on the query.
// The map is checked only when parsing was successful. Map is indexed by table name and points to
// list of operations performed on the table. For example table1: "select", "insert" says that there
// was SELECT and INSERT operations on table1.
// DecoderAttributes is a map containing additional attributes which augment creating metadata.
class MetadataFromSQLTest
    : public ::testing::TestWithParam<
          std::tuple<std::string, bool, std::map<std::string, std::list<std::string>>,
                     SQLUtils::DecoderAttributes>> {};

// Test takes SQL query as a parameter and checks if the parsing
// produces the correct metadata.
// Metadata is 2-level structure. First layer is list of resources
// over which the SQL query operates: in our case is list of tables.
// Under each table there is secondary list which contains operations performed
// on the table, like "select", "insert", etc.
TEST_P(MetadataFromSQLTest, ParsingAndMetadataTest) {
  // Get the SQL query
  const std::string& query = std::get<0>(GetParam());
  // vector of queries to check.
  std::vector<std::string> test_queries;
  test_queries.push_back(query);

  // Create uppercase and lowercase versions of the queries and put
  // them into vector of queries to check
  test_queries.push_back(absl::AsciiStrToLower(query));
  test_queries.push_back(absl::AsciiStrToUpper(query));

  while (!test_queries.empty()) {
    std::string test_query = test_queries.back();
    ProtobufWkt::Struct metadata;

    // Check if the parsing result is what expected.
    ASSERT_EQ(std::get<1>(GetParam()),
              SQLUtils::setMetadata(test_query, std::get<3>(GetParam()), metadata));

    // If parsing was expected to fail do not check parsing values.
    if (!std::get<1>(GetParam())) {
      return;
    }

    // Access metadata fields, where parsing results are stored.
    auto& fields = *metadata.mutable_fields();

    // Get the names of resources which SQL query operates on.
    std::map<std::string, std::list<std::string>> expected_tables = std::get<2>(GetParam());
    // Check if query results return the same number of resources as expected.
    ASSERT_EQ(expected_tables.size(), fields.size());
    for (const auto& i : fields) {
      // Get from created metadata the list of operations on the resource
      const auto& operations = i;
      std::string table_name = operations.first;

      std::transform(table_name.begin(), table_name.end(), table_name.begin(),
                     [](unsigned char c) { return std::tolower(c); });
      // Get the list of expected operations on the same resource from test param.
      const auto& table_name_it = expected_tables.find(table_name);
      // Make sure that a resource (table) found in metadata is expected.
      ASSERT_NE(expected_tables.end(), table_name_it);
      auto& operations_list = table_name_it->second;
      // The number of expected operations and created in metadata must be the same.
      ASSERT_EQ(operations_list.size(), operations.second.list_value().values().size());
      // Now iterate over the operations list found in metadata and check if the same operation
      // is listed as expected in test param.
      for (const auto& j : operations.second.list_value().values()) {
        // Find that operation in test params.
        const auto operation_it =
            std::find(operations_list.begin(), operations_list.end(), j.string_value());
        ASSERT_NE(operations_list.end(), operation_it);
        // Erase the operation. At the end of the test this list should be empty what means
        // that we found all expected operations.
        operations_list.erase(operation_it);
      }
      // Make sure that we went through all expected operations.
      ASSERT_TRUE(operations_list.empty());
      // Remove the table from the list. At the end of the test this list must be empty.
      expected_tables.erase(table_name_it);
    }

    ASSERT_TRUE(expected_tables.empty());
    test_queries.pop_back();
  }
}

// Note: This parameterized test's queries are converted to all lowercase and all uppercase
// to validate that parser is case-insensitive. The test routine converts to uppercase and
// lowercase entire query string, not only SQL keywords. This introduces a problem when comparing
// tables' names when verifying parsing result. Therefore the test converts table names to lowercase
// before comparing. It however requires that all table names in the queries below use lowercase
// only.
#define TEST_VALUE(...)                                                                            \
  std::tuple<std::string, bool, std::map<std::string, std::list<std::string>>,                     \
             SQLUtils::DecoderAttributes> {                                                        \
    __VA_ARGS__                                                                                    \
  }
INSTANTIATE_TEST_SUITE_P(
    SQLUtilsTestSuite, MetadataFromSQLTest,
    ::testing::Values(
        TEST_VALUE("blahblah;", false, {}, {}),

        TEST_VALUE("CREATE TABLE IF NOT EXISTS table1(Usr VARCHAR(40),Count INT);", true,
                   {{"table1", {"create"}}}, {}),
        TEST_VALUE("CREATE TABLE IF NOT EXISTS `table number 1`(Usr VARCHAR(40),Count INT);", true,
                   {{"table number 1.testdb", {"create"}}}, {{"database", "testdb"}}),
        TEST_VALUE(
            "CREATE TABLE IF NOT EXISTS table1(Usr VARCHAR(40),Count INT); SELECT * from table1;",
            true, {{"table1", {"select", "create"}}}, {}),
        TEST_VALUE(
            "CREATE TABLE IF NOT EXISTS table1(Usr VARCHAR(40),Count INT); SELECT * from table2;",
            true, {{"table1", {"create"}}, {"table2", {"select"}}}, {{"user", "testusr"}}),

        TEST_VALUE("CREATE TABLE table1(Usr VARCHAR(40),Count INT);", true,
                   {{"table1", {"create"}}}, {}),
        TEST_VALUE("CREATE TABLE;", false, {}, {}),
        TEST_VALUE("CREATE TEMPORARY table table1(Usr VARCHAR(40),Count INT);", true,
                   {{"table1", {"create"}}}, {}),
        TEST_VALUE("DROP TABLE IF EXISTS table1", true, {{"table1", {"drop"}}}, {}),
        TEST_VALUE("ALTER TABLE table1 add column Id varchar (20);", true, {{"table1", {"alter"}}},
                   {}),
        TEST_VALUE("INSERT INTO table1 (Usr, Count) VALUES ('allsp2', 3);", true,
                   {{"table1", {"insert"}}}, {}),
        TEST_VALUE("INSERT LOW_PRIORITY INTO table1 (Usr, Count) VALUES ('allsp2', 3);", true,
                   {{"table1", {"insert"}}}, {}),
        TEST_VALUE("INSERT IGNORE INTO table1 (Usr, Count) VALUES ('allsp2', 3);", true,
                   {{"table1", {"insert"}}}, {}),
        TEST_VALUE("INSERT INTO table1 (Usr, Count) VALUES ('allsp2', 3);SELECT * from table1",
                   true, {{"table1", {"insert", "select"}}}, {}),
        TEST_VALUE("DELETE FROM table1 WHERE Count > 3;", true, {{"table1", {"delete"}}}, {}),
        TEST_VALUE("DELETE LOW_PRIORITY FROM table1 WHERE Count > 3;", true,
                   {{"table1", {"delete"}}}, {}),
        TEST_VALUE("DELETE QUICK FROM table1 WHERE Count > 3;", true, {{"table1", {"delete"}}}, {}),
        TEST_VALUE("DELETE IGNORE FROM table1 WHERE Count > 3;", true, {{"table1", {"delete"}}},
                   {}),

        TEST_VALUE("SELECT * FROM table1 WHERE Count = 1;", true, {{"table1", {"select"}}}, {}),
        TEST_VALUE("SELECT * FROM table1 WHERE Count = 1;", true, {{"table1", {"select"}}}, {}),
        TEST_VALUE("SELECT product.category FROM table1 WHERE Count = 1;", true,
                   {{"table1", {"select"}}, {"product", {"unknown"}}}, {}),
        TEST_VALUE("SELECT DISTINCT Usr FROM table1;", true, {{"table1", {"select"}}}, {}),
        TEST_VALUE("SELECT Usr, Count FROM table1 ORDER BY Count DESC;", true,
                   {{"table1.testdb", {"select"}}}, {{"user", "testuser"}, {"database", "testdb"}}),
        TEST_VALUE("SELECT 12 AS a, a FROM table1 GROUP BY a;", true, {{"table1", {"select"}}}, {}),
        TEST_VALUE("SELECT;", false, {}, {}), TEST_VALUE("SELECT Usr, Count FROM;", false, {}, {}),
        TEST_VALUE("INSERT INTO table1 SELECT * FROM table2;", true,
                   {{"table1", {"insert"}}, {"table2", {"select"}}}, {}),
        TEST_VALUE("INSERT INTO table1 SELECT tbl_temp1.fld_order_id FROM table2;", true,
                   {{"tbl_temp1", {"unknown"}}, {"table2", {"select"}}, {"table1", {"insert"}}},
                   {}),
        TEST_VALUE("UPDATE table1 SET col1 = col1 + 1", true, {{"table1", {"update"}}}, {}),
        TEST_VALUE("UPDATE LOW_PRIORITY table1 SET col1 = col1 + 1", true, {{"table1", {"update"}}},
                   {}),
        TEST_VALUE("UPDATE IGNORE table1 SET col1 = col1 + 1", true, {{"table1", {"update"}}}, {}),
        TEST_VALUE("UPDATE table1 SET  column1=(SELECT * columnX from table2);", true,
                   {{"table1", {"update"}}, {"table2", {"select"}}}, {}),

        // operations on database should not create any metadata
        TEST_VALUE("CREATE DATABASE testdb;", true, {}, {}),
        TEST_VALUE("CREATE DATABASE IF NOT EXISTS testdb;", true, {}, {}),
        TEST_VALUE("ALTER DATABASE testdb CHARACTER SET charset_name;", true, {}, {}),
        TEST_VALUE("ALTER DATABASE testdb default CHARACTER SET charset_name;", true, {}, {}),
        TEST_VALUE("ALTER DATABASE testdb default CHARACTER SET = charset_name;", true, {}, {}),
        TEST_VALUE("ALTER SCHEMA testdb default CHARACTER SET = charset_name;", true, {}, {}),

        // The following DROP DATABASE tests should not produce metadata.
        TEST_VALUE("DROP DATABASE testdb;", true, {}, {}),
        TEST_VALUE("DROP DATABASE IF EXISTS testdb;", true, {}, {}),

        // Schema. Should be parsed fine, but should not produce any metadata
        TEST_VALUE("SHOW databases;", true, {}, {}), TEST_VALUE("SHOW tables;", true, {}, {}),
        TEST_VALUE("SELECT * FROM;", false, {}, {}),
        TEST_VALUE("SELECT 1 FROM tabletest1;", true, {{"tabletest1", {"select"}}}, {}),

        // SQL parser now can not recognize use keywords as identifier.
        TEST_VALUE("CREATE TABLE test ( text VARCHAR(255) );", false, {}, {}),
        // Keyword as function.
        TEST_VALUE("SHOW DATABASE();", false, {}, {})));

} // namespace SQLUtils
} // namespace Common
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "sqlutils_tests",
    srcs = [
        "sqlutils_test.cc",
    ],
    external_deps = ["sqlparser"],
    deps = [
        "//contrib/common/sqlutils/source:sqlutils_lib",
    ],
)
#pragma once

#include "source/common/protobuf/utility.h"

#include "include/sqlparser/SQLParser.h"

namespace Envoy {
namespace Extensions {
namespace Common {
namespace SQLUtils {

class SQLUtils {
public:
  using DecoderAttributes = std::map<std::string, std::string>;
  /**
   * Method parses SQL query string and writes output to metadata.
   * @param query supplies SQL statement.
   * @param attr supplies attributes which cannot be extracted from SQL query but are
   *    required to create proper metadata. For example database name may be sent
   *    by a client when it initially connects to the server, not along each SQL query.
   * @param metadata supplies placeholder where metadata should be written.
   * @return True if parsing was successful and False if parsing failed.
   *         If True was returned the metadata contains result of parsing. The results are
   *         stored in metadata.mutable_fields.
   **/
  static bool setMetadata(const std::string& query, const DecoderAttributes& attr,
                          ProtobufWkt::Struct& metadata);
};

} // namespace SQLUtils
} // namespace Common
} // namespace Extensions
} // namespace Envoy
#include "contrib/common/sqlutils/source/sqlutils.h"

namespace Envoy {
namespace Extensions {
namespace Common {
namespace SQLUtils {

bool SQLUtils::setMetadata(const std::string& query, const DecoderAttributes& attr,
                           ProtobufWkt::Struct& metadata) {
  hsql::SQLParserResult result;

  hsql::SQLParser::parse(query, &result);

  if (!result.isValid()) {
    return false;
  }

  std::string database;
  // Check if the attributes map contains database name.
  const auto it = attr.find("database");
  if (it != attr.end()) {
    database = absl::StrCat(".", it->second);
  }

  auto& fields = *metadata.mutable_fields();

  for (auto i = 0u; i < result.size(); ++i) {
    if (result.getStatement(i)->type() == hsql::StatementType::kStmtShow) {
      continue;
    }
    hsql::TableAccessMap table_access_map;
    // Get names of accessed tables.
    result.getStatement(i)->tablesAccessed(table_access_map);
    for (auto& it : table_access_map) {
      auto& operations = *fields[it.first + database].mutable_list_value();
      // For each table get names of operations performed on that table.
      for (const auto& ot : it.second) {
        operations.add_values()->set_string_value(ot);
      }
    }
  }

  return true;
}

} // namespace SQLUtils
} // namespace Common
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_library(
    name = "sqlutils_lib",
    srcs = ["sqlutils.cc"],
    hdrs = ["sqlutils.h"],
    external_deps = ["sqlparser"],
    deps = [
        "//source/common/protobuf:utility_lib",
    ],
)
load(":contrib_build_config.bzl", "CONTRIB_EXTENSIONS")

# linter requires indirection for @bazel_tools definitions
def envoy_contrib_linux_x86_64_constraints():
    return [
        "@platforms//os:linux",
        "@platforms//cpu:x86_64",
    ]

def envoy_contrib_linux_aarch64_constraints():
    return [
        "@platforms//os:linux",
        "@platforms//cpu:aarch64",
    ]

ARM64_SKIP_CONTRIB_TARGETS = [
    "envoy.tls.key_providers.cryptomb",
    "envoy.tls.key_providers.qat",
    "envoy.network.connection_balance.dlb",
    "envoy.compression.qatzip.compressor",
]
PPC_SKIP_CONTRIB_TARGETS = [
    "envoy.tls.key_providers.cryptomb",
    "envoy.tls.key_providers.qat",
    "envoy.matching.input_matchers.hyperscan",
    "envoy.network.connection_balance.dlb",
    "envoy.regex_engines.hyperscan",
    "envoy.compression.qatzip.compressor",
]

FIPS_SKIP_CONTRIB_TARGETS = [
    "envoy.compression.qatzip.compressor",
]

def envoy_all_contrib_extensions(denylist = []):
    return [v + "_envoy_extension" for k, v in CONTRIB_EXTENSIONS.items() if not k in denylist]
# See bazel/README.md for details on how this system works.
CONTRIB_EXTENSIONS = {
    #
    # Compression
    #

    "envoy.compression.qatzip.compressor":                      "//contrib/qat/compression/qatzip/compressor/source:config",

    #
    # HTTP filters
    #
    "envoy.filters.http.checksum":                              "//contrib/checksum/filters/http/source:config",
    "envoy.filters.http.dynamo":                                "//contrib/dynamo/filters/http/source:config",
    "envoy.filters.http.golang":                                "//contrib/golang/filters/http/source:config",
    "envoy.filters.http.language":                              "//contrib/language/filters/http/source:config_lib",
    "envoy.filters.http.squash":                                "//contrib/squash/filters/http/source:config",
    "envoy.filters.http.sxg":                                   "//contrib/sxg/filters/http/source:config",

    #
    # Network filters
    #

    "envoy.filters.network.client_ssl_auth":                    "//contrib/client_ssl_auth/filters/network/source:config",
    "envoy.filters.network.kafka_broker":                       "//contrib/kafka/filters/network/source/broker:config_lib",
    "envoy.filters.network.kafka_mesh":                         "//contrib/kafka/filters/network/source/mesh:config_lib",
    "envoy.filters.network.mysql_proxy":                        "//contrib/mysql_proxy/filters/network/source:config",
    "envoy.filters.network.postgres_proxy":                     "//contrib/postgres_proxy/filters/network/source:config",
    "envoy.filters.network.rocketmq_proxy":                     "//contrib/rocketmq_proxy/filters/network/source:config",
    "envoy.filters.network.generic_proxy":                      "//contrib/generic_proxy/filters/network/source:config",
    "envoy.filters.network.golang":                             "//contrib/golang/filters/network/source:config",

    #
    # Sip proxy
    #

    "envoy.filters.network.sip_proxy":                          "//contrib/sip_proxy/filters/network/source:config",
    "envoy.filters.sip.router":                                 "//contrib/sip_proxy/filters/network/source/router:config",

    #
    # Private key providers
    #

    "envoy.tls.key_providers.cryptomb":                         "//contrib/cryptomb/private_key_providers/source:config",
    "envoy.tls.key_providers.qat":                              "//contrib/qat/private_key_providers/source:config",

    #
    # Socket interface extensions
    #

    "envoy.bootstrap.vcl":                                      "//contrib/vcl/source:config",

    #
    # Input matchers
    #

    "envoy.matching.input_matchers.hyperscan":                  "//contrib/hyperscan/matching/input_matchers/source:config",

    #
    # Connection Balance extensions
    #

    "envoy.network.connection_balance.dlb":                     "//contrib/dlb/source:connection_balancer",

    #
    # Regex engines
    #

    "envoy.regex_engines.hyperscan":                            "//contrib/hyperscan/regex_engines/source:config",

    #
    # Extensions for generic proxy
    #
    "envoy.filters.generic.router":                             "//contrib/generic_proxy/filters/network/source/router:config",
    "envoy.generic_proxy.codecs.dubbo":                         "//contrib/generic_proxy/filters/network/source/codecs/dubbo:config",
    "envoy.generic_proxy.codecs.kafka":                         "//contrib/generic_proxy/filters/network/source/codecs/kafka:config",

    #
    # xDS delegates
    #

    "envoy.xds_delegates.kv_store":                            "//contrib/config/source:kv_store_xds_delegate",

    #
    # cluster specifier plugin
    #

    "envoy.router.cluster_specifier_plugin.golang":             "//contrib/golang/router/cluster_specifier/source:config",
}
#pragma once

#include <vector>

#include "contrib/kafka/filters/network/source/kafka_request.h"
#include "contrib/kafka/filters/network/source/kafka_response.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {

/**
 * Payload-related test utilities.
 * This class is intended to be an entry point for all generated methods.
 *
 * The methods declared here are implemented in generated files:
 * - request_utilities.cc (from request_utilities_cc.j2) - for requests,
 * - response_utilities.cc (from response_utilities_cc.j2) - for responses,
 * as they are derived from Kafka protocol specification.
 */
class MessageUtilities {
private:
  MessageUtilities() = default;

public:
  /**
   * What are the supported request / response types.
   */
  static std::vector<int16_t> apiKeys();

  /**
   * How many request types are supported for given api key.
   */
  static int16_t requestApiVersions(const int16_t api_key);

  /**
   * Make example requests with given api_key.
   * One message per api version in given api key.
   * The message correlation id-s start at value provided.
   */
  static std::vector<AbstractRequestSharedPtr> makeRequests(const int16_t api_key,
                                                            int32_t& correlation_id);

  /**
   * Make example requests, one message per given api key + api version pair.
   * The message correlation id-s start at 0.
   */
  static std::vector<AbstractRequestSharedPtr> makeAllRequests();

  /**
   * Get the name of request counter metric for given request type.
   */
  static std::string requestMetric(const int16_t api_key);

  /**
   * How many response types are supported for given api key.
   */
  static int16_t responseApiVersions(const int16_t api_key);

  /**
   * Make example requests with given api_key.
   * One message per api version in given api key.
   * The message correlation id-s start at value provided.
   */
  static std::vector<AbstractResponseSharedPtr> makeResponses(const int16_t api_key,
                                                              int32_t& correlation_id);

  /**
   * Make example responses, one message per given api key + api version pair.
   * The message correlation id-s start at 0.
   */
  static std::vector<AbstractResponseSharedPtr> makeAllResponses();

  /**
   * Get the name of response counter metric for given request type.
   */
  static std::string responseMetric(const int16_t api_key);
};

} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
clientPort={{ data['zk_port'] }}
dataDir={{ data['data_dir'] }}
maxClientCnxns=0
# ZK 3.5 tries to bind 8080 for introspection capacility - we do not need that.
admin.enableServer=false
broker.id=0
listeners=PLAINTEXT://127.0.0.1:{{ data['kafka_real_port'] }}
advertised.listeners=PLAINTEXT://127.0.0.1:{{ data['kafka_real_port'] }}

num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

log.dirs={{ data['data_dir'] }}
num.partitions=1
num.recovery.threads.per.data.dir=1

offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000

# As we are going to have multiple Kafka clusters (not even brokers!),
# we need to register them at different paths in ZK.
zookeeper.connect=127.0.0.1:{{ data['zk_port'] }}/{{ data['kafka_zk_instance'] }}
zookeeper.connection.timeout.ms=6000

group.initial.rebalance.delay.ms=0

# The number of __consumer_offsets partitions is reduced to make logs a bit more readable.
offsets.topic.num.partitions=5
#!/usr/bin/python

import random
import os
import shutil
import socket
import subprocess
import tempfile
from threading import Thread, Semaphore
import time
import unittest
import random

from kafka import KafkaConsumer, KafkaProducer, TopicPartition
import urllib.request


class Message:
    """
    Stores data sent to Envoy / Kafka.
    """

    def __init__(self):
        self.key = os.urandom(256)
        self.value = os.urandom(2048)
        self.headers = [('header_' + str(h), os.urandom(128)) for h in range(3)]


class IntegrationTest(unittest.TestCase):
    """
    All tests in this class depend on Envoy/Zookeeper/Kafka running.
    For each of these tests we are going to create Kafka producers and consumers, with producers
    pointing to Envoy (so the records get forwarded to target Kafka clusters) and verifying consumers
    pointing to Kafka clusters directly (as mesh filter does not yet support Fetch requests).
    We expect every operation to succeed (as they should reach Kafka) and the corresponding metrics
    to increase on Envoy side (to show that messages were received and forwarded successfully).
    """

    services = None

    @classmethod
    def setUpClass(cls):
        IntegrationTest.services = ServicesHolder()
        IntegrationTest.services.start()

    @classmethod
    def tearDownClass(cls):
        IntegrationTest.services.shut_down()

    def setUp(self):
        # We want to check if our services are okay before running any kind of test.
        IntegrationTest.services.check_state()
        self.metrics = MetricsHolder(self)

    def tearDown(self):
        # We want to check if our services are okay after running any test.
        IntegrationTest.services.check_state()

    @classmethod
    def kafka_envoy_address(cls):
        return '127.0.0.1:%s' % IntegrationTest.services.kafka_envoy_port

    @classmethod
    def kafka_cluster1_address(cls):
        return '127.0.0.1:%s' % IntegrationTest.services.kafka_real_port1

    @classmethod
    def kafka_cluster2_address(cls):
        return '127.0.0.1:%s' % IntegrationTest.services.kafka_real_port2

    @classmethod
    def envoy_stats_address(cls):
        return 'http://127.0.0.1:%s/stats' % IntegrationTest.services.envoy_monitoring_port

    def test_producing(self):
        """
        This test verifies that producer can send messages through mesh filter.
        We are going to send messages to two topics: 'apples' and 'bananas'.
        The mesh filter is configured to forward records for topics starting with 'a' (like 'apples')
        to the first cluster, and the ones starting with 'b' (so 'bananas') to the second one.

        We are going to send messages one by one, so they will not be batched in Kafka producer,
        so the filter is going to receive them one by one too.

        After sending, the consumers are going to read from Kafka clusters directly to make sure that
        nothing was lost.
        """

        messages_to_send = 100
        partition1 = TopicPartition('apples', 0)
        partition2 = TopicPartition('bananas', 0)

        producer = KafkaProducer(
            bootstrap_servers=IntegrationTest.kafka_envoy_address(), api_version=(1, 0, 0))
        offset_to_message1 = {}
        offset_to_message2 = {}
        for _ in range(messages_to_send):
            message = Message()
            future1 = producer.send(
                key=message.key,
                value=message.value,
                headers=message.headers,
                topic=partition1.topic,
                partition=partition1.partition)
            self.assertTrue(future1.get().offset >= 0)
            offset_to_message1[future1.get().offset] = message

            future2 = producer.send(
                key=message.key,
                value=message.value,
                headers=message.headers,
                topic=partition2.topic,
                partition=partition2.partition)
            self.assertTrue(future2.get().offset >= 0)
            offset_to_message2[future2.get().offset] = message
        self.assertTrue(len(offset_to_message1) == messages_to_send)
        self.assertTrue(len(offset_to_message2) == messages_to_send)
        producer.close()

        # Check the target clusters.
        self.__verify_target_kafka_cluster(
            IntegrationTest.kafka_cluster1_address(), partition1, offset_to_message1, partition2)
        self.__verify_target_kafka_cluster(
            IntegrationTest.kafka_cluster2_address(), partition2, offset_to_message2, partition1)

        # Check if requests have been received.
        self.metrics.collect_final_metrics()
        self.metrics.assert_metric_increase('produce', 200)

    def test_producing_with_batched_records(self):
        """
        Compared to previous test, we are going to have batching in Kafka producers (this is caused by high 'linger.ms' value).
        So a single request that reaches a Kafka broker might be carrying more than one record, for different partitions.
        """
        messages_to_send = 100
        partition1 = TopicPartition('apricots', 0)
        partition2 = TopicPartition('berries', 0)

        # This ensures that records to 'apricots' and 'berries' partitions.
        producer = KafkaProducer(
            bootstrap_servers=IntegrationTest.kafka_envoy_address(),
            api_version=(1, 0, 0),
            linger_ms=1000,
            batch_size=100)
        future_to_message1 = {}
        future_to_message2 = {}
        for _ in range(messages_to_send):
            message = Message()
            future1 = producer.send(
                key=message.key,
                value=message.value,
                headers=message.headers,
                topic=partition1.topic,
                partition=partition1.partition)
            future_to_message1[future1] = message

            message = Message()
            future2 = producer.send(
                key=message.key,
                value=message.value,
                headers=message.headers,
                topic=partition2.topic,
                partition=partition2.partition)
            future_to_message2[future2] = message

        offset_to_message1 = {}
        offset_to_message2 = {}
        for future in future_to_message1.keys():
            offset_to_message1[future.get().offset] = future_to_message1[future]
            self.assertTrue(future.get().offset >= 0)
        for future in future_to_message2.keys():
            offset_to_message2[future.get().offset] = future_to_message2[future]
            self.assertTrue(future.get().offset >= 0)
        self.assertTrue(len(offset_to_message1) == messages_to_send)
        self.assertTrue(len(offset_to_message2) == messages_to_send)
        producer.close()

        # Check the target clusters.
        self.__verify_target_kafka_cluster(
            IntegrationTest.kafka_cluster1_address(), partition1, offset_to_message1, partition2)
        self.__verify_target_kafka_cluster(
            IntegrationTest.kafka_cluster2_address(), partition2, offset_to_message2, partition1)

        # Check if requests have been received.
        self.metrics.collect_final_metrics()
        self.metrics.assert_metric_increase('produce', 1)

    def __verify_target_kafka_cluster(
            self, bootstrap_servers, partition, offset_to_message_map, other_partition):
        # Check if records were properly forwarded to the cluster.
        consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest')
        consumer.assign([partition])
        received_messages = []
        while (len(received_messages) < len(offset_to_message_map)):
            poll_result = consumer.poll(timeout_ms=1000)
            received_messages += poll_result[partition]
        self.assertTrue(len(received_messages) == len(offset_to_message_map))
        for record in received_messages:
            sent_message = offset_to_message_map[record.offset]
            self.assertTrue(record.key == sent_message.key)
            self.assertTrue(record.value == sent_message.value)
            self.assertTrue(record.headers == sent_message.headers)

        # Check that no records were incorrectly routed from the "other" partition (they would have created the topics).
        self.assertTrue(other_partition.topic not in consumer.topics())
        consumer.close(False)

    def test_consumer_stateful_proxy(self):
        """
        This test verifies that consumer can receive messages through the mesh filter.
        We are going to have messages in two topics: 'aaaconsumer' and 'bbbconsumer'.
        The mesh filter is configured to process fetch requests for topics starting with 'a' (like 'aaaconsumer')
        by consuming from the first cluster, and the ones starting with 'b' (so 'bbbconsumer') from the second one.
        So in the end our consumers that point at Envoy should receive records from matching upstream Kafka clusters.
        """

        # Put the messages into upstream Kafka clusters.
        partition1 = TopicPartition('aaaconsumer', 0)
        count1 = 20
        partition2 = TopicPartition('bbbconsumer', 0)
        count2 = 30
        self.__put_messages_into_upstream_kafka(
            IntegrationTest.kafka_cluster1_address(), partition1, count1)
        self.__put_messages_into_upstream_kafka(
            IntegrationTest.kafka_cluster2_address(), partition2, count2)

        # Create Kafka consumers that point at Envoy.
        consumer1 = KafkaConsumer(bootstrap_servers=IntegrationTest.kafka_envoy_address())
        consumer1.assign([partition1])
        consumer2 = KafkaConsumer(bootstrap_servers=IntegrationTest.kafka_envoy_address())
        consumer2.assign([partition2])

        # Have the consumers receive the messages from Kafka clusters through Envoy.
        received1 = []
        received2 = []
        while (len(received1) < count1):
            poll_result = consumer1.poll(timeout_ms=5000)
            for records in poll_result.values():
                received1 += records
        while (len(received2) < count2):
            poll_result = consumer2.poll(timeout_ms=5000)
            for records in poll_result.values():
                received2 += records

        # Verify that the messages sent have been received.
        self.assertTrue(len(received1) == count1)
        self.assertTrue(len(received2) == count2)

        # Cleanup
        consumer1.close(False)
        consumer2.close(False)

    def __put_messages_into_upstream_kafka(self, bootstrap_servers, partition, count):
        """
        Helper method for putting messages into Kafka directly.
        """
        producer = KafkaProducer(bootstrap_servers=bootstrap_servers)

        futures = []
        for _ in range(count):
            message = Message()
            future = producer.send(
                key=message.key,
                value=message.value,
                headers=message.headers,
                topic=partition.topic,
                partition=partition.partition)
            futures.append(future)
        for future in futures:
            offset = future.get().offset
            print('Saved message at offset %s' % (offset))
        producer.close(True)


class MetricsHolder:
    """
    Utility for storing Envoy metrics.
    Expected to be created before the test (to get initial metrics), and then to collect them at the
    end of test, so the expected increases can be verified.
    """

    def __init__(self, owner):
        self.owner = owner
        self.initial_requests, self.inital_responses = MetricsHolder.get_envoy_stats()
        self.final_requests = None
        self.final_responses = None

    def collect_final_metrics(self):
        self.final_requests, self.final_responses = MetricsHolder.get_envoy_stats()

    def assert_metric_increase(self, message_type, count):
        request_type = message_type + '_request'
        response_type = message_type + '_response'

        initial_request_value = self.initial_requests.get(request_type, 0)
        final_request_value = self.final_requests.get(request_type, 0)
        self.owner.assertGreaterEqual(final_request_value, initial_request_value + count)

        initial_response_value = self.inital_responses.get(response_type, 0)
        final_response_value = self.final_responses.get(response_type, 0)
        self.owner.assertGreaterEqual(final_response_value, initial_response_value + count)

    @staticmethod
    def get_envoy_stats():
        """
        Grab request/response metrics from envoy's stats interface.
        """

        stats_url = IntegrationTest.envoy_stats_address()
        requests = {}
        responses = {}
        with urllib.request.urlopen(stats_url) as remote_metrics_url:
            payload = remote_metrics_url.read().decode()
            lines = payload.splitlines()
            for line in lines:
                request_prefix = 'kafka.testfilter.request.'
                response_prefix = 'kafka.testfilter.response.'
                if line.startswith(request_prefix):
                    data = line[len(request_prefix):].split(': ')
                    requests[data[0]] = int(data[1])
                    pass
                if line.startswith(response_prefix) and '_response:' in line:
                    data = line[len(response_prefix):].split(': ')
                    responses[data[0]] = int(data[1])
        return [requests, responses]


class ServicesHolder:
    """
    Utility class for setting up our external dependencies: Envoy, Zookeeper
    and two Kafka clusters (single-broker each).
    """

    def __init__(self):
        self.kafka_tmp_dir = None

        self.envoy_worker = None
        self.zk_worker = None
        self.kafka_workers = None

    @staticmethod
    def get_random_listener_port():
        """
    Here we count on OS to give us some random socket.
    Obviously this method will need to be invoked in a try loop anyways, as in degenerate scenario
    someone else might have bound to it after we had closed the socket and before the service
    that's supposed to use it binds to it.
    """

        import socket
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:
            server_socket.bind(('0.0.0.0', 0))
            socket_port = server_socket.getsockname()[1]
            print('returning %s' % socket_port)
            return socket_port

    def start(self):
        """
        Starts all the services we need for integration tests.
        """

        # Find java installation that we are going to use to start Zookeeper & Kafka.
        java_directory = ServicesHolder.find_java()

        launcher_environment = os.environ.copy()
        # Make `java` visible to build script:
        # https://github.com/apache/kafka/blob/2.2.0/bin/kafka-run-class.sh#L226
        new_path = os.path.abspath(java_directory) + os.pathsep + launcher_environment['PATH']
        launcher_environment['PATH'] = new_path
        # Both ZK & Kafka use Kafka launcher script.
        # By default it sets up JMX options:
        # https://github.com/apache/kafka/blob/2.2.0/bin/kafka-run-class.sh#L167
        # But that forces the JVM to load file that is not present due to:
        # https://docs.oracle.com/javase/9/management/monitoring-and-management-using-jmx-technology.htm
        # Let's make it simple and just disable JMX.
        launcher_environment['KAFKA_JMX_OPTS'] = ' '

        # Setup a temporary directory, which will be used by Kafka & Zookeeper servers.
        self.kafka_tmp_dir = tempfile.mkdtemp()
        print('Temporary directory used for tests: ' + self.kafka_tmp_dir)

        # This directory will store the configuration files fed to services.
        config_dir = self.kafka_tmp_dir + '/config'
        os.mkdir(config_dir)
        # This directory will store Zookeeper's data (== Kafka server metadata).
        zookeeper_store_dir = self.kafka_tmp_dir + '/zookeeper_data'
        os.mkdir(zookeeper_store_dir)
        # These directories will store Kafka's data (== partitions).
        kafka_store_dir1 = self.kafka_tmp_dir + '/kafka_data1'
        os.mkdir(kafka_store_dir1)
        kafka_store_dir2 = self.kafka_tmp_dir + '/kafka_data2'
        os.mkdir(kafka_store_dir2)

        # Find the Kafka server 'bin' directory.
        kafka_bin_dir = os.path.join('.', 'external', 'kafka_server_binary', 'bin')

        # Main initialization block:
        # - generate random ports,
        # - render configuration with these ports,
        # - start services and check if they are running okay,
        # - if anything is having problems, kill everything and start again.
        while True:

            # Generate random ports.
            zk_port = ServicesHolder.get_random_listener_port()
            kafka_envoy_port = ServicesHolder.get_random_listener_port()
            kafka_real_port1 = ServicesHolder.get_random_listener_port()
            kafka_real_port2 = ServicesHolder.get_random_listener_port()
            envoy_monitoring_port = ServicesHolder.get_random_listener_port()

            # These ports need to be exposed to tests.
            self.kafka_envoy_port = kafka_envoy_port
            self.kafka_real_port1 = kafka_real_port1
            self.kafka_real_port2 = kafka_real_port2
            self.envoy_monitoring_port = envoy_monitoring_port

            # Render config file for Envoy.
            template = RenderingHelper.get_template('envoy_config_yaml.j2')
            contents = template.render(
                data={
                    'kafka_envoy_port': kafka_envoy_port,
                    'kafka_real_port1': kafka_real_port1,
                    'kafka_real_port2': kafka_real_port2,
                    'envoy_monitoring_port': envoy_monitoring_port
                })
            envoy_config_file = os.path.join(config_dir, 'envoy_config.yaml')
            with open(envoy_config_file, 'w') as fd:
                fd.write(contents)
                print('Envoy config file rendered at: ' + envoy_config_file)

            # Render config file for Zookeeper.
            template = RenderingHelper.get_template('zookeeper_properties.j2')
            contents = template.render(data={'data_dir': zookeeper_store_dir, 'zk_port': zk_port})
            zookeeper_config_file = os.path.join(config_dir, 'zookeeper.properties')
            with open(zookeeper_config_file, 'w') as fd:
                fd.write(contents)
                print('Zookeeper config file rendered at: ' + zookeeper_config_file)

            # Render config file for Kafka cluster 1.
            template = RenderingHelper.get_template('kafka_server_properties.j2')
            contents = template.render(
                data={
                    'kafka_real_port': kafka_real_port1,
                    'data_dir': kafka_store_dir1,
                    'zk_port': zk_port,
                    'kafka_zk_instance': 'instance1'
                })
            kafka_config_file1 = os.path.join(config_dir, 'kafka_server1.properties')
            with open(kafka_config_file1, 'w') as fd:
                fd.write(contents)
                print('Kafka config file rendered at: ' + kafka_config_file1)

            # Render config file for Kafka cluster 2.
            template = RenderingHelper.get_template('kafka_server_properties.j2')
            contents = template.render(
                data={
                    'kafka_real_port': kafka_real_port2,
                    'data_dir': kafka_store_dir2,
                    'zk_port': zk_port,
                    'kafka_zk_instance': 'instance2'
                })
            kafka_config_file2 = os.path.join(config_dir, 'kafka_server2.properties')
            with open(kafka_config_file2, 'w') as fd:
                fd.write(contents)
                print('Kafka config file rendered at: ' + kafka_config_file2)

            # Start the services now.
            try:

                # Start Envoy in the background, pointing to rendered config file.
                envoy_binary = ServicesHolder.find_envoy()
                # --base-id is added to allow multiple Envoy instances to run at the same time.
                envoy_args = [
                    os.path.abspath(envoy_binary), '-c', envoy_config_file, '--base-id',
                    str(random.randint(1, 999999))
                ]
                envoy_handle = subprocess.Popen(
                    envoy_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.envoy_worker = ProcessWorker(
                    envoy_handle, 'Envoy', 'starting main dispatch loop')
                self.envoy_worker.await_startup()

                # Start Zookeeper in background, pointing to rendered config file.
                zk_binary = os.path.join(kafka_bin_dir, 'zookeeper-server-start.sh')
                zk_args = [os.path.abspath(zk_binary), zookeeper_config_file]
                zk_handle = subprocess.Popen(
                    zk_args,
                    env=launcher_environment,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE)
                self.zk_worker = ProcessWorker(zk_handle, 'Zookeeper', 'binding to port')
                self.zk_worker.await_startup()

                self.kafka_workers = []

                # Start Kafka 1 in background, pointing to rendered config file.
                kafka_binary = os.path.join(kafka_bin_dir, 'kafka-server-start.sh')
                kafka_args = [os.path.abspath(kafka_binary), os.path.abspath(kafka_config_file1)]
                kafka_handle = subprocess.Popen(
                    kafka_args,
                    env=launcher_environment,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE)
                kafka_worker = ProcessWorker(kafka_handle, 'Kafka', '[KafkaServer id=0] started')
                kafka_worker.await_startup()
                self.kafka_workers.append(kafka_worker)

                # Start Kafka 2 in background, pointing to rendered config file.
                kafka_binary = os.path.join(kafka_bin_dir, 'kafka-server-start.sh')
                kafka_args = [os.path.abspath(kafka_binary), os.path.abspath(kafka_config_file2)]
                kafka_handle = subprocess.Popen(
                    kafka_args,
                    env=launcher_environment,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE)
                kafka_worker = ProcessWorker(kafka_handle, 'Kafka', '[KafkaServer id=0] started')
                kafka_worker.await_startup()
                self.kafka_workers.append(kafka_worker)

                # All services have started without problems - now we can finally finish.
                break

            except Exception as e:
                print('Could not start services, will try again', e)

                if self.kafka_workers:
                    self.kafka_worker.kill()
                    self.kafka_worker = None
                if self.zk_worker:
                    self.zk_worker.kill()
                    self.zk_worker = None
                if self.envoy_worker:
                    self.envoy_worker.kill()
                    self.envoy_worker = None

    @staticmethod
    def find_java():
        """
        This method just locates the Java installation in current directory.
        We cannot hardcode the name, as the dirname changes as per:
        https://github.com/bazelbuild/bazel/blob/master/tools/jdk/BUILD#L491
        """

        external_dir = os.path.join('.', 'external')
        for directory in os.listdir(external_dir):
            if 'remotejdk11' in directory:
                result = os.path.join(external_dir, directory, 'bin')
                print('Using Java: ' + result)
                return result
        raise Exception('Could not find Java in: ' + external_dir)

    @staticmethod
    def find_envoy():
        """
        This method locates envoy binary.
        It's present at ./contrib/exe/envoy-static (at least for mac/bazel-asan/bazel-tsan),
        or at ./external/envoy/contrib/exe/envoy-static (for bazel-compile_time_options).
        """

        candidate = os.path.join('.', 'contrib', 'exe', 'envoy-static')
        if os.path.isfile(candidate):
            return candidate
        candidate = os.path.join('.', 'external', 'envoy', 'contrib', 'exe', 'envoy-static')
        if os.path.isfile(candidate):
            return candidate
        raise Exception("Could not find Envoy")

    def shut_down(self):
        # Teardown - kill Kafka, Zookeeper, and Envoy. Then delete their data directory.
        print('Cleaning up')

        if self.kafka_workers:
            for worker in self.kafka_workers:
                worker.kill()

        if self.zk_worker:
            self.zk_worker.kill()

        if self.envoy_worker:
            self.envoy_worker.kill()

        if self.kafka_tmp_dir:
            print('Removing temporary directory: ' + self.kafka_tmp_dir)
            shutil.rmtree(self.kafka_tmp_dir)

    def check_state(self):
        self.envoy_worker.check_state()
        self.zk_worker.check_state()
        for worker in self.kafka_workers:
            worker.check_state()


class ProcessWorker:
    """
    Helper class that wraps the external service process.
    Provides ability to wait until service is ready to use (this is done by tracing logs) and
    printing service's output to stdout.
    """

    # Service is considered to be properly initialized after it has logged its startup message
    # and has been alive for INITIALIZATION_WAIT_SECONDS after that message has been seen.
    # This (clunky) design is needed because Zookeeper happens to log "binding to port" and then
    # might fail to bind.
    INITIALIZATION_WAIT_SECONDS = 3

    def __init__(self, process_handle, name, startup_message):
        # Handle to process and pretty name.
        self.process_handle = process_handle
        self.name = name

        self.startup_message = startup_message
        self.startup_message_ts = None

        # Semaphore raised when startup has finished and information regarding startup's success.
        self.initialization_semaphore = Semaphore(value=0)
        self.initialization_ok = False

        self.state_worker = Thread(target=ProcessWorker.initialization_worker, args=(self,))
        self.state_worker.start()
        self.out_worker = Thread(
            target=ProcessWorker.pipe_handler, args=(self, self.process_handle.stdout, 'out'))
        self.out_worker.start()
        self.err_worker = Thread(
            target=ProcessWorker.pipe_handler, args=(self, self.process_handle.stderr, 'err'))
        self.err_worker.start()

    @staticmethod
    def initialization_worker(owner):
        """
        Worker thread.
        Responsible for detecting if service died during initialization steps and ensuring if enough
        time has passed since the startup message has been seen.
        When either of these happens, we just raise the initialization semaphore.
        """

        while True:
            status = owner.process_handle.poll()
            if status:
                # Service died.
                print('%s did not initialize properly - finished with: %s' % (owner.name, status))
                owner.initialization_ok = False
                owner.initialization_semaphore.release()
                break
            else:
                # Service is still running.
                startup_message_ts = owner.startup_message_ts
                if startup_message_ts:
                    # The log message has been registered (by pipe_handler thread), let's just ensure that
                    # some time has passed and mark the service as running.
                    current_time = int(round(time.time()))
                    if current_time - startup_message_ts >= ProcessWorker.INITIALIZATION_WAIT_SECONDS:
                        print(
                            'Startup message seen %s seconds ago, and service is still running' %
                            (ProcessWorker.INITIALIZATION_WAIT_SECONDS),
                            flush=True)
                        owner.initialization_ok = True
                        owner.initialization_semaphore.release()
                        break
            time.sleep(1)
        print('Initialization worker for %s has finished' % (owner.name))

    @staticmethod
    def pipe_handler(owner, pipe, pipe_name):
        """
        Worker thread.
        If a service startup message is seen, then it just registers the timestamp of its appearance.
        Also prints every received message.
        """

        try:
            for raw_line in pipe:
                line = raw_line.decode().rstrip()
                print('%s(%s):' % (owner.name, pipe_name), line, flush=True)
                if owner.startup_message in line:
                    print(
                        '%s initialization message [%s] has been logged' %
                        (owner.name, owner.startup_message))
                    owner.startup_message_ts = int(round(time.time()))
        finally:
            pipe.close()
        print('Pipe handler for %s(%s) has finished' % (owner.name, pipe_name))

    def await_startup(self):
        """
        Awaits on initialization semaphore, and then verifies the initialization state.
        If everything is okay, we just continue (we can use the service), otherwise throw.
        """

        print('Waiting for %s to start...' % (self.name))
        self.initialization_semaphore.acquire()
        try:
            if self.initialization_ok:
                print('Service %s started successfully' % (self.name))
            else:
                raise Exception('%s could not start' % (self.name))
        finally:
            self.initialization_semaphore.release()

    def check_state(self):
        """
        Verifies if the service is still running. Throws if it is not.
        """

        status = self.process_handle.poll()
        if status:
            raise Exception('%s died with: %s' % (self.name, str(status)))

    def kill(self):
        """
        Utility method to kill the main service thread and all related workers.
        """

        print('Stopping service %s' % self.name)

        # Kill the real process.
        self.process_handle.kill()
        self.process_handle.wait()

        # The sub-workers are going to finish on their own, as they will detect main thread dying
        # (through pipes closing, or .poll() returning a non-null value).
        self.state_worker.join()
        self.out_worker.join()
        self.err_worker.join()

        print('Service %s has been stopped' % self.name)


class RenderingHelper:
    """
    Helper for jinja templates.
    """

    @staticmethod
    def get_template(template):
        import jinja2
        import os
        import sys
        # Templates are resolved relatively to main start script, due to main & test templates being
        # stored in different directories.
        env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(searchpath=os.path.dirname(os.path.abspath(__file__))))
        return env.get_template(template)


if __name__ == '__main__':
    unittest.main()
static_resources:
  listeners:
  - address:
      socket_address:
        address: 127.0.0.1
        port_value: {{ data['kafka_envoy_port'] }}
    filter_chains:
    - filters:
      - name: requesttypes
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.network.kafka_broker.v3.KafkaBroker
          stat_prefix: testfilter
          force_response_rewrite: true
      - name: mesh
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaMesh
          advertised_host: "127.0.0.1"
          advertised_port: {{ data['kafka_envoy_port'] }}
          upstream_clusters:
          - cluster_name: kafka_c1
            bootstrap_servers: 127.0.0.1:{{ data['kafka_real_port1'] }}
            partition_count: 1
          - cluster_name: kafka_c2
            bootstrap_servers: 127.0.0.1:{{ data['kafka_real_port2'] }}
            partition_count: 1
          forwarding_rules:
          - target_cluster: kafka_c1
            topic_prefix: a
          - target_cluster: kafka_c2
            topic_prefix: b
admin:
  access_log_path: /dev/null
  profile_path: /dev/null
  address:
    socket_address: { address: 127.0.0.1, port_value: {{ data['envoy_monitoring_port'] }} }
load("@base_pip3//:requirements.bzl", "requirement")
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_contrib_package",
    "envoy_py_test",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

# This test sets up multiple services, and this can take variable amount of time (30-60 seconds).
envoy_py_test(
    name = "kafka_mesh_integration_test",
    srcs = [
        "kafka_mesh_integration_test.py",
        "@kafka_python_client//:all",
    ],
    data = [
        "//bazel:remote_jdk11",
        "//contrib/exe:envoy-static",
        "@kafka_server_binary//:all",
    ] + glob(["*.j2"]),
    flaky = True,
    deps = [
        requirement("Jinja2"),
        requirement("MarkupSafe"),
    ],
)
#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/metadata.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::Return;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockAbstractRequestListener : public AbstractRequestListener {
public:
  MOCK_METHOD(void, onRequest, (InFlightRequestSharedPtr));
  MOCK_METHOD(void, onRequestReadyForAnswer, ());
  MOCK_METHOD(Event::Dispatcher&, dispatcher, ());
};

class MockUpstreamKafkaConfiguration : public UpstreamKafkaConfiguration {
public:
  MOCK_METHOD(absl::optional<ClusterConfig>, computeClusterConfigForTopic, (const std::string&),
              (const));
  MOCK_METHOD((std::pair<std::string, int32_t>), getAdvertisedAddress, (), (const));
};

TEST(MetadataTest, shouldBeAlwaysReadyForAnswer) {
  // given
  MockAbstractRequestListener filter;
  EXPECT_CALL(filter, onRequestReadyForAnswer());
  MockUpstreamKafkaConfiguration configuration;
  const std::pair<std::string, int32_t> advertised_address = {"host", 1234};
  EXPECT_CALL(configuration, getAdvertisedAddress()).WillOnce(Return(advertised_address));
  // First topic is going to have configuration present (42 partitions for each topic).
  const ClusterConfig topic1config = {"", 42, {}, {}};
  EXPECT_CALL(configuration, computeClusterConfigForTopic("topic1"))
      .WillOnce(Return(absl::make_optional(topic1config)));
  // Second topic is not going to have configuration present.
  EXPECT_CALL(configuration, computeClusterConfigForTopic("topic2"))
      .WillOnce(Return(absl::nullopt));
  const RequestHeader header = {METADATA_REQUEST_API_KEY, METADATA_REQUEST_MAX_VERSION, 0,
                                absl::nullopt};
  const MetadataRequestTopic t1 = MetadataRequestTopic{"topic1"};
  const MetadataRequestTopic t2 = MetadataRequestTopic{"topic2"};
  // Third topic is not going to have an explicit name.
  const MetadataRequestTopic t3 = MetadataRequestTopic{Uuid{13, 42}, absl::nullopt, TaggedFields{}};
  const MetadataRequest data = {{t1, t2, t3}};
  const auto message = std::make_shared<Request<MetadataRequest>>(header, data);
  MetadataRequestHolder testee = {filter, configuration, message};

  // when, then - invoking should immediately notify the filter.
  testee.startProcessing();

  // when, then - should always be considered finished.
  const bool finished = testee.finished();
  EXPECT_TRUE(finished);

  // when, then - the computed result is always contains correct data (confirmed by integration
  // tests).
  const auto answer = testee.computeAnswer();
  EXPECT_EQ(answer->metadata_.api_key_, header.api_key_);
  EXPECT_EQ(answer->metadata_.correlation_id_, header.correlation_id_);

  const auto response = std::dynamic_pointer_cast<Response<MetadataResponse>>(answer);
  ASSERT_TRUE(response);
  const auto topics = response->data_.topics_;
  EXPECT_EQ(topics.size(), 1);
  EXPECT_EQ(topics[0].name_, *(t1.name_));
  EXPECT_EQ(topics[0].partitions_.size(), 42);
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include <algorithm>
#include <set>

#include "test/test_common/utility.h"

#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch_record_converter.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

TEST(FetchRecordConverterImpl, shouldProcessEmptyInput) {
  // given
  const FetchRecordConverter& testee = FetchRecordConverterImpl{};
  const InboundRecordsMap input = {};

  // when
  const auto result = testee.convert(input);

  // then
  ASSERT_EQ(result.size(), 0);
}

TEST(FetchRecordConverterImpl, shouldProcessInputWithNoRecords) {
  // given
  const FetchRecordConverter& testee = FetchRecordConverterImpl{};
  InboundRecordsMap input = {};
  input[{"aaa", 0}] = {};
  input[{"aaa", 1}] = {};
  input[{"aaa", 2}] = {};
  input[{"bbb", 0}] = {};

  // when
  const std::vector<FetchableTopicResponse> result = testee.convert(input);

  // then
  ASSERT_EQ(result.size(), 2); // Number of unique topic names (not partitions).
  const auto topic1 =
      std::find_if(result.begin(), result.end(), [](auto x) { return "aaa" == x.topic_; });
  ASSERT_EQ(topic1->partitions_.size(), 3);
  const auto topic2 =
      std::find_if(result.begin(), result.end(), [](auto x) { return "bbb" == x.topic_; });
  ASSERT_EQ(topic2->partitions_.size(), 1);
}

// Helper method to generate records.
InboundRecordSharedPtr makeRecord() {
  const NullableBytes key = {Bytes(128)};
  const NullableBytes value = {Bytes(1024)};
  return std::make_shared<InboundRecord>("aaa", 0, 0, key, value);
}

TEST(FetchRecordConverterImpl, shouldProcessRecords) {
  // given
  const FetchRecordConverter& testee = FetchRecordConverterImpl{};
  InboundRecordsMap input = {};
  input[{"aaa", 0}] = {makeRecord(), makeRecord(), makeRecord()};

  // when
  const std::vector<FetchableTopicResponse> result = testee.convert(input);

  // then
  ASSERT_EQ(result.size(), 1);
  const auto& partitions = result[0].partitions_;
  ASSERT_EQ(partitions.size(), 1);
  const NullableBytes& data = partitions[0].records_;
  ASSERT_EQ(data.has_value(), true);
  ASSERT_GT(data->size(),
            3 * (128 + 1024)); // Records carry some metadata so it should always pass.

  // then - check whether metadata really says we carry 3 records.
  constexpr auto record_count_offset = 57;
  const auto ptr = reinterpret_cast<const uint32_t*>(data->data() + record_count_offset);
  const uint32_t record_count = be32toh(*ptr);
  ASSERT_EQ(record_count, 3);
} // NOLINT(clang-analyzer-cplusplus.NewDeleteLeaks)

// Here we check whether our manual implementation really works.
// https://github.com/apache/kafka/blob/3.3.2/clients/src/main/java/org/apache/kafka/common/utils/Crc32C.java
TEST(FetchRecordConverterImpl, shouldComputeCrc32c) {
  constexpr auto testee = &FetchRecordConverterImpl::computeCrc32cForTest;

  std::vector<unsigned char> arg1 = {};
  ASSERT_EQ(testee(arg1.data(), arg1.size()), 0x00000000);

  std::vector<unsigned char> arg2 = {0, 0, 0, 0};
  ASSERT_EQ(testee(arg2.data(), arg2.size()), 0x48674BC7);

  std::vector<unsigned char> arg3 = {13, 42, 13, 42, 13, 42, 13, 42};
  ASSERT_EQ(testee(arg3.data(), arg3.size()), 0xDB56B80F);
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/mocks/event/mocks.h"

#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch_record_converter.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

using testing::NiceMock;
using testing::Return;
using testing::ReturnRef;

class MockAbstractRequestListener : public AbstractRequestListener {
public:
  MOCK_METHOD(void, onRequest, (InFlightRequestSharedPtr));
  MOCK_METHOD(void, onRequestReadyForAnswer, ());
  MOCK_METHOD(Event::Dispatcher&, dispatcher, ());
};

class MockRecordCallbackProcessor : public RecordCallbackProcessor {
public:
  MOCK_METHOD(void, processCallback, (const RecordCbSharedPtr&));
  MOCK_METHOD(void, removeCallback, (const RecordCbSharedPtr&));
};

class MockFetchRecordConverter : public FetchRecordConverter {
public:
  MOCK_METHOD(std::vector<FetchableTopicResponse>, convert, (const InboundRecordsMap&), (const));
};

class FetchUnitTest : public testing::Test {
protected:
  constexpr static int64_t TEST_CORRELATION_ID = 123456;

  NiceMock<MockAbstractRequestListener> filter_;
  NiceMock<Event::MockDispatcher> dispatcher_;
  NiceMock<MockRecordCallbackProcessor> callback_processor_;
  MockFetchRecordConverter converter_;

  FetchUnitTest() { ON_CALL(filter_, dispatcher).WillByDefault(ReturnRef(dispatcher_)); }

  std::shared_ptr<FetchRequestHolder> makeTestee() {
    const RequestHeader header = {FETCH_REQUEST_API_KEY, 0, TEST_CORRELATION_ID, absl::nullopt};
    // Our request refers to aaa-0, aaa-1, bbb-10, bbb-20.
    const FetchTopic t1 = {"aaa", {{0, 0, 0}, {1, 0, 0}}};
    const FetchTopic t2 = {"bbb", {{10, 0, 0}, {20, 0, 0}}};
    const FetchRequest data = {0, 0, 0, {t1, t2}};
    const auto message = std::make_shared<Request<FetchRequest>>(header, data);
    return std::make_shared<FetchRequestHolder>(filter_, callback_processor_, message, converter_);
  }
};

TEST_F(FetchUnitTest, ShouldRegisterCallbackAndTimer) {
  // given
  const auto testee = makeTestee();
  EXPECT_CALL(callback_processor_, processCallback(_));
  EXPECT_CALL(dispatcher_, createTimer_(_));

  // when
  testee->startProcessing();

  // then
  ASSERT_FALSE(testee->finished());
}

TEST_F(FetchUnitTest, ShouldReturnProperInterest) {
  // given
  const auto testee = makeTestee();

  // when
  const TopicToPartitionsMap result = testee->interest();

  // then
  const TopicToPartitionsMap expected = {{"aaa", {0, 1}}, {"bbb", {10, 20}}};
  ASSERT_EQ(result, expected);
}

TEST_F(FetchUnitTest, ShouldCleanupAfterTimer) {
  // given
  const auto testee = makeTestee();
  testee->startProcessing();

  EXPECT_CALL(callback_processor_, removeCallback(_));
  EXPECT_CALL(dispatcher_, post(_));

  // when
  testee->markFinishedByTimer();

  // then
  ASSERT_TRUE(testee->finished());
}

// Helper method to generate records.
InboundRecordSharedPtr makeRecord() {
  return std::make_shared<InboundRecord>("aaa", 0, 0, absl::nullopt, absl::nullopt);
}

TEST_F(FetchUnitTest, ShouldReceiveRecords) {
  // given
  const auto testee = makeTestee();
  testee->startProcessing();

  // Will be invoked by the third record (delivery was finished).
  EXPECT_CALL(dispatcher_, post(_));
  // It is invoker that removes the callback - not us.
  EXPECT_CALL(callback_processor_, removeCallback(_)).Times(0);

  // when - 1
  const auto res1 = testee->receive(makeRecord());
  // then - first record got stored.
  ASSERT_EQ(res1, CallbackReply::AcceptedAndWantMore);
  ASSERT_FALSE(testee->finished());

  // when - 2
  const auto res2 = testee->receive(makeRecord());
  // then - second record got stored.
  ASSERT_EQ(res2, CallbackReply::AcceptedAndWantMore);
  ASSERT_FALSE(testee->finished());

  // when - 3
  const auto res3 = testee->receive(makeRecord());
  // then - third record got stored and no more will be accepted.
  ASSERT_EQ(res3, CallbackReply::AcceptedAndFinished);
  ASSERT_TRUE(testee->finished());

  // when - 4
  const auto res4 = testee->receive(makeRecord());
  // then - fourth record was rejected.
  ASSERT_EQ(res4, CallbackReply::Rejected);
}

TEST_F(FetchUnitTest, ShouldRejectRecordsAfterTimer) {
  // given
  const auto testee = makeTestee();
  testee->startProcessing();
  testee->markFinishedByTimer();

  // when
  const auto res = testee->receive(makeRecord());

  // then
  ASSERT_EQ(res, CallbackReply::Rejected);
}

TEST_F(FetchUnitTest, ShouldUnregisterItselfWhenAbandoned) {
  // given
  const auto testee = makeTestee();
  testee->startProcessing();

  EXPECT_CALL(callback_processor_, removeCallback(_));

  // when
  testee->abandon();

  // then - expectations are met.
}

TEST_F(FetchUnitTest, ShouldComputeAnswer) {
  // given
  const auto testee = makeTestee();
  testee->startProcessing();

  std::vector<FetchableTopicResponse> ftr = {{"aaa", {}}, {"bbb", {}}};
  EXPECT_CALL(converter_, convert(_)).WillOnce(Return(ftr));

  // when
  const AbstractResponseSharedPtr answer = testee->computeAnswer();

  // then
  ASSERT_EQ(answer->metadata_.correlation_id_, TEST_CORRELATION_ID);
  const auto response = std::dynamic_pointer_cast<Response<FetchResponse>>(answer);
  ASSERT_TRUE(response);
  const std::vector<FetchableTopicResponse> responses = response->data_.responses_;
  ASSERT_EQ(responses, ftr);
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include <set>

#include "test/test_common/utility.h"

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/produce_record_extractor.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

// Simple matcher that verifies that the input given is a collection containing correct number of
// unique (!) records for given topic-partition pairs.
MATCHER_P3(HasRecords, topic, partition, expected, "") {
  size_t expected_count = expected;
  std::set<absl::string_view> saved_key_pointers = {};
  std::set<absl::string_view> saved_value_pointers = {};
  size_t count = 0;

  for (const auto& record : arg) {
    if (record.topic_ == topic && record.partition_ == partition) {
      saved_key_pointers.insert(record.key_);
      saved_value_pointers.insert(record.value_);
      ++count;
    }
  }

  if (expected_count != count) {
    return false;
  }
  if (expected_count != saved_key_pointers.size()) {
    return false;
  }
  return saved_key_pointers.size() == saved_value_pointers.size();
}

// Helper function to create a record batch that contains a single record with 5-byte key and 5-byte
// value.
Bytes makeGoodRecordBatch() {
  // Record batch bytes get ignored (apart from magic field), so we can put 0 there.
  Bytes result = Bytes(16 + 1 + 44);
  result[16] = 2; // Record batch magic value.
  Bytes real_data = {/* Length = 36 */ 72,
                     /* Attributes */ 0,
                     /* Timestamp delta */ 0,
                     /* Offset delta */ 0,
                     /* Key length = 5 */ 10,
                     107,
                     107,
                     107,
                     107,
                     107,
                     /* Value length = 5 */ 10,
                     118,
                     118,
                     118,
                     118,
                     118,
                     /* Headers count = 2 */ 4,
                     /* Header key length = 3 */ 6,
                     49,
                     49,
                     49,
                     /* Header value length = 5 */ 10,
                     97,
                     97,
                     97,
                     97,
                     97,
                     /* Header key length = 3 */ 6,
                     50,
                     50,
                     50,
                     /* Header value length = 5 */ 10,
                     98,
                     98,
                     98,
                     98,
                     98};
  result.insert(result.end(), real_data.begin(), real_data.end());
  return result;
}

TEST(RecordExtractorImpl, shouldProcessRecordBytes) {
  // given
  const RecordExtractorImpl testee;

  const PartitionProduceData t1_ppd1 = {0, makeGoodRecordBatch()};
  const PartitionProduceData t1_ppd2 = {1, makeGoodRecordBatch()};
  const PartitionProduceData t1_ppd3 = {2, makeGoodRecordBatch()};
  const TopicProduceData tpd1 = {"topic1", {t1_ppd1, t1_ppd2, t1_ppd3}};

  // Weird input from client, protocol allows sending null value as bytes array.
  const PartitionProduceData t2_ppd = {20, absl::nullopt};
  const TopicProduceData tpd2 = {"topic2", {t2_ppd}};

  const std::vector<TopicProduceData> input = {tpd1, tpd2};

  // when
  const auto result = testee.extractRecords(input);

  // then
  EXPECT_THAT(result, HasRecords("topic1", 0, 1));
  EXPECT_THAT(result, HasRecords("topic1", 1, 1));
  EXPECT_THAT(result, HasRecords("topic1", 2, 1));
  EXPECT_THAT(result, HasRecords("topic2", 20, 0));
}

/**
 * Helper function to make record batch (batch contains 1+ records).
 * We use 'stage' parameter to make it a single function with various failure modes.
 */
const std::vector<TopicProduceData> makeTopicProduceData(const unsigned int stage) {
  Bytes bytes = makeGoodRecordBatch();
  if (1 == stage) {
    // No common fields before magic.
    bytes.erase(bytes.begin(), bytes.end());
  }
  if (2 == stage) {
    // No magic.
    bytes.erase(bytes.begin() + 16, bytes.end());
  }
  if (3 == stage) {
    // Bad magic.
    bytes[16] = 42;
  }
  if (4 == stage) {
    // No common fields after magic.
    bytes.erase(bytes.begin() + 17, bytes.end());
  }
  if (5 == stage) {
    // No record length after common fields.
    bytes[61] = 128; // This will force variable-length deserializer to wait for more bytes.
    bytes.erase(bytes.begin() + 62, bytes.end());
  }
  if (6 == stage) {
    // Record length is higher than size of real data.
    bytes.erase(bytes.begin() + 62, bytes.end());
  }
  if (7 == stage) {
    // Attributes field has negative length.
    bytes[61] = 3; /* -1 */
    bytes.erase(bytes.begin() + 62, bytes.end());
  }
  if (8 == stage) {
    // Attributes field is missing - length is valid, but there is no more data to read.
    bytes[61] = 0;
    bytes.erase(bytes.begin() + 62, bytes.end());
  }
  if (9 == stage) {
    // Header count not present - we are going to drop all 21 header bytes after value.
    bytes[61] = (36 - 21) << 1; // Length is encoded as variable length.
    bytes.erase(bytes.begin() + 77, bytes.end());
  }
  if (10 == stage) {
    // Negative variable length integer for header count.
    bytes[77] = 17;
  }
  if (11 == stage) {
    // Last header value is going to be shorter, so there will be one unconsumed byte.
    bytes[92] = 8;
  }
  const PartitionProduceData ppd = {0, bytes};
  const TopicProduceData tpd = {"topic", {ppd}};
  return {tpd};
}

TEST(RecordExtractorImpl, shouldHandleInvalidRecordBytes) {
  const RecordExtractorImpl testee;
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(1)), EnvoyException,
                          "no common fields");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(2)), EnvoyException,
                          "magic byte is not present");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(3)), EnvoyException,
                          "unknown magic value");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(4)), EnvoyException,
                          "no attribute fields");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(5)), EnvoyException,
                          "no length");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(6)), EnvoyException,
                          "not enough bytes provided");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(7)), EnvoyException,
                          "has invalid length");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(8)), EnvoyException,
                          "attributes not present");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(9)), EnvoyException,
                          "header count not present");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(10)), EnvoyException,
                          "invalid header count");
  EXPECT_THROW_WITH_REGEX(testee.extractRecords(makeTopicProduceData(11)), EnvoyException,
                          "data left after consuming record");
}

// Minor helper function.
absl::string_view bytesToStringView(const Bytes& bytes) {
  return {reinterpret_cast<const char*>(bytes.data()), bytes.size()};
}

TEST(RecordExtractorImpl, shouldExtractByteArray) {
  {
    const Bytes noBytes = Bytes(0);
    auto arg = bytesToStringView(noBytes);
    EXPECT_THROW_WITH_REGEX(RecordExtractorImpl::extractByteArray(arg), EnvoyException,
                            "byte array length not present");
  }
  {
    const Bytes nullValueBytes = {0b00000001}; // Length = -1.
    auto arg = bytesToStringView(nullValueBytes);
    EXPECT_EQ(RecordExtractorImpl::extractByteArray(arg), absl::string_view());
  }
  {
    const Bytes negativeLengthBytes = {0b01111111}; // Length = -64.
    auto arg = bytesToStringView(negativeLengthBytes);
    EXPECT_THROW_WITH_REGEX(RecordExtractorImpl::extractByteArray(arg), EnvoyException,
                            "byte array length less than -1: -64");
  }
  {
    const Bytes bigLengthBytes = {0b01111110}; // Length = 63.
    auto arg = bytesToStringView(bigLengthBytes);
    EXPECT_THROW_WITH_REGEX(RecordExtractorImpl::extractByteArray(arg), EnvoyException,
                            "byte array length larger than data provided: 63 vs 0");
  }
  {
    // Length = 4, 7 bytes follow, 4 should be consumed, 13s should stay unconsumed.
    const Bytes goodBytes = {0b00001000, 42, 42, 42, 42, 13, 13, 13};
    auto arg = bytesToStringView(goodBytes);
    EXPECT_EQ(RecordExtractorImpl::extractByteArray(arg),
              absl::string_view(reinterpret_cast<const char*>(goodBytes.data() + 1), 4));
    EXPECT_EQ(arg.data(), reinterpret_cast<const char*>(goodBytes.data() + 5));
    EXPECT_EQ(arg.size(), 3);
  }
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include <set>

#include "test/test_common/utility.h"

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/produce.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::Return;
using testing::ReturnRef;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockAbstractRequestListener : public AbstractRequestListener {
public:
  MOCK_METHOD(void, onRequest, (InFlightRequestSharedPtr));
  MOCK_METHOD(void, onRequestReadyForAnswer, ());
  MOCK_METHOD(Event::Dispatcher&, dispatcher, ());
};

class MockRecordExtractor : public RecordExtractor {
public:
  MOCK_METHOD((std::vector<OutboundRecord>), extractRecords, (const std::vector<TopicProduceData>&),
              (const));
};

class MockUpstreamKafkaFacade : public UpstreamKafkaFacade {
public:
  MOCK_METHOD(KafkaProducer&, getProducerForTopic, (const std::string&));
};

class MockKafkaProducer : public KafkaProducer {
public:
  MOCK_METHOD(void, send, (const ProduceFinishCbSharedPtr, const OutboundRecord&), ());
  MOCK_METHOD(void, markFinished, (), ());
};

class ProduceUnitTest : public testing::Test {
protected:
  MockAbstractRequestListener filter_;
  MockUpstreamKafkaFacade upstream_kafka_facade_;
  MockRecordExtractor extractor_;
};

// This is very odd corner case, that should never happen
// (as ProduceRequests with no topics/records make no sense).
TEST_F(ProduceUnitTest, ShouldHandleProduceRequestWithNoRecords) {
  // given
  const std::vector<OutboundRecord> records = {};
  EXPECT_CALL(extractor_, extractRecords(_)).WillOnce(Return(records));

  const RequestHeader header = {0, 0, 0, absl::nullopt};
  const ProduceRequest data = {0, 0, {}};
  const auto message = std::make_shared<Request<ProduceRequest>>(header, data);
  ProduceRequestHolder testee = {filter_, upstream_kafka_facade_, extractor_, message};

  // when, then - invoking should immediately notify the filter.
  EXPECT_CALL(filter_, onRequestReadyForAnswer());
  testee.startProcessing();

  // when, then - request is finished because there was nothing to do.
  const bool finished = testee.finished();
  EXPECT_TRUE(finished);

  // when, then - answer is empty.
  const auto answer = testee.computeAnswer();
  EXPECT_EQ(answer->metadata_.api_key_, header.api_key_);
  EXPECT_EQ(answer->metadata_.correlation_id_, header.correlation_id_);
}

// Typical flow without errors.
// The produce request has 2 records, that should be mapped to 2 different Kafka installations.
// The response should contain the values returned by Kafka broker.
TEST_F(ProduceUnitTest, ShouldSendRecordsInNormalFlow) {
  // given
  const OutboundRecord r1 = {"t1", 13, "aaa", "bbb", {}};
  const OutboundRecord r2 = {"t2", 42, "ccc", "ddd", {}};
  const std::vector<OutboundRecord> records = {r1, r2};
  EXPECT_CALL(extractor_, extractRecords(_)).WillOnce(Return(records));

  const RequestHeader header = {0, 0, 0, absl::nullopt};
  const ProduceRequest data = {0, 0, {}};
  const auto message = std::make_shared<Request<ProduceRequest>>(header, data);
  std::shared_ptr<ProduceRequestHolder> testee =
      std::make_shared<ProduceRequestHolder>(filter_, upstream_kafka_facade_, extractor_, message);

  // when, then - invoking should use producers to send records.
  MockKafkaProducer producer1;
  EXPECT_CALL(producer1, send(_, _));
  MockKafkaProducer producer2;
  EXPECT_CALL(producer2, send(_, _));
  EXPECT_CALL(upstream_kafka_facade_, getProducerForTopic(r1.topic_))
      .WillOnce(ReturnRef(producer1));
  EXPECT_CALL(upstream_kafka_facade_, getProducerForTopic(r2.topic_))
      .WillOnce(ReturnRef(producer2));
  testee->startProcessing();

  // when, then - request is not yet finished (2 records' delivery to be confirmed).
  EXPECT_FALSE(testee->finished());

  // when, then - first record should be delivered.
  const DeliveryMemento dm1 = {r1.value_.data(), 0, 123};
  EXPECT_TRUE(testee->accept(dm1));
  EXPECT_FALSE(testee->finished());

  const DeliveryMemento dm2 = {r2.value_.data(), 0, 234};
  // After all the deliveries have been confirmed, the filter is getting notified.
  EXPECT_CALL(filter_, onRequestReadyForAnswer());
  EXPECT_TRUE(testee->accept(dm2));
  EXPECT_TRUE(testee->finished());

  // when, then - answer gets computed and contains results.
  const auto answer = testee->computeAnswer();
  EXPECT_EQ(answer->metadata_.api_key_, header.api_key_);
  EXPECT_EQ(answer->metadata_.correlation_id_, header.correlation_id_);

  const auto response = std::dynamic_pointer_cast<Response<ProduceResponse>>(answer);
  ASSERT_TRUE(response);
  const std::vector<TopicProduceResponse> responses = response->data_.responses_;
  EXPECT_EQ(responses.size(), 2);
  EXPECT_EQ(responses[0].partition_responses_[0].error_code_, dm1.error_code_);
  EXPECT_EQ(responses[0].partition_responses_[0].base_offset_, dm1.offset_);
  EXPECT_EQ(responses[1].partition_responses_[0].error_code_, dm2.error_code_);
  EXPECT_EQ(responses[1].partition_responses_[0].base_offset_, dm2.offset_);
}

// Typical flow without errors.
// The produce request has 2 records, both pointing to same partition.
// Given that usually we cannot make any guarantees on how Kafka producer is going to append the
// records (as it depends on configuration like max number of records in flight), the first record
// is going to be saved on a bigger offset.
TEST_F(ProduceUnitTest, ShouldMergeOutboundRecordResponses) {
  // given
  const OutboundRecord r1 = {"t1", 13, "aaa", "bbb", {}};
  const OutboundRecord r2 = {r1.topic_, r1.partition_, "ccc", "ddd", {}};
  const std::vector<OutboundRecord> records = {r1, r2};
  EXPECT_CALL(extractor_, extractRecords(_)).WillOnce(Return(records));

  const RequestHeader header = {0, 0, 0, absl::nullopt};
  const ProduceRequest data = {0, 0, {}};
  const auto message = std::make_shared<Request<ProduceRequest>>(header, data);
  std::shared_ptr<ProduceRequestHolder> testee =
      std::make_shared<ProduceRequestHolder>(filter_, upstream_kafka_facade_, extractor_, message);

  // when, then - invoking should use producers to send records.
  MockKafkaProducer producer;
  EXPECT_CALL(producer, send(_, _)).Times(2);
  EXPECT_CALL(upstream_kafka_facade_, getProducerForTopic(r1.topic_))
      .WillRepeatedly(ReturnRef(producer));
  testee->startProcessing();

  // when, then - request is not yet finished (2 records' delivery to be confirmed).
  EXPECT_FALSE(testee->finished());

  // when, then - first record should be delivered.
  const DeliveryMemento dm1 = {r1.value_.data(), 0, 4242};
  EXPECT_TRUE(testee->accept(dm1));
  EXPECT_FALSE(testee->finished());

  const DeliveryMemento dm2 = {r2.value_.data(), 0, 1313};
  // After all the deliveries have been confirmed, the filter is getting notified.
  EXPECT_CALL(filter_, onRequestReadyForAnswer());
  EXPECT_TRUE(testee->accept(dm2));
  EXPECT_TRUE(testee->finished());

  // when, then - answer gets computed and contains results.
  const auto answer = testee->computeAnswer();
  EXPECT_EQ(answer->metadata_.api_key_, header.api_key_);
  EXPECT_EQ(answer->metadata_.correlation_id_, header.correlation_id_);

  const auto response = std::dynamic_pointer_cast<Response<ProduceResponse>>(answer);
  ASSERT_TRUE(response);
  const std::vector<TopicProduceResponse> responses = response->data_.responses_;
  EXPECT_EQ(responses.size(), 1);
  EXPECT_EQ(responses[0].partition_responses_.size(), 1);
  EXPECT_EQ(responses[0].partition_responses_[0].error_code_, 0);
  EXPECT_EQ(responses[0].partition_responses_[0].base_offset_, 1313);
}

// Flow with errors.
// The produce request has 2 records, both pointing to same partition.
// The first record is going to fail.
// We are going to treat whole delivery as failure.
// Bear in mind second record could get accepted, this is a difference between normal client and
// proxy (this is going to be amended when we manage to send whole record batch).
TEST_F(ProduceUnitTest, ShouldHandleDeliveryErrors) {
  // given
  const OutboundRecord r1 = {"t1", 13, "aaa", "bbb", {}};
  const OutboundRecord r2 = {r1.topic_, r1.partition_, "ccc", "ddd", {}};
  const std::vector<OutboundRecord> records = {r1, r2};
  EXPECT_CALL(extractor_, extractRecords(_)).WillOnce(Return(records));

  const RequestHeader header = {0, 0, 0, absl::nullopt};
  const ProduceRequest data = {0, 0, {}};
  const auto message = std::make_shared<Request<ProduceRequest>>(header, data);
  std::shared_ptr<ProduceRequestHolder> testee =
      std::make_shared<ProduceRequestHolder>(filter_, upstream_kafka_facade_, extractor_, message);

  // when, then - invoking should use producers to send records.
  MockKafkaProducer producer;
  EXPECT_CALL(producer, send(_, _)).Times(2);
  EXPECT_CALL(upstream_kafka_facade_, getProducerForTopic(r1.topic_))
      .WillRepeatedly(ReturnRef(producer));
  testee->startProcessing();

  // when, then - request is not yet finished (2 records' delivery to be confirmed).
  EXPECT_FALSE(testee->finished());

  // when, then - first record fails.
  const DeliveryMemento dm1 = {r1.value_.data(), 42, 0};
  EXPECT_TRUE(testee->accept(dm1));
  EXPECT_FALSE(testee->finished());

  // when, then - second record succeeds (we are going to ignore the result).
  const DeliveryMemento dm2 = {r2.value_.data(), 0, 234};
  // After all the deliveries have been confirmed, the filter is getting notified.
  EXPECT_CALL(filter_, onRequestReadyForAnswer());
  EXPECT_TRUE(testee->accept(dm2));
  EXPECT_TRUE(testee->finished());

  // when, then - answer gets computed and contains results.
  const auto answer = testee->computeAnswer();
  EXPECT_EQ(answer->metadata_.api_key_, header.api_key_);
  EXPECT_EQ(answer->metadata_.correlation_id_, header.correlation_id_);

  const auto response = std::dynamic_pointer_cast<Response<ProduceResponse>>(answer);
  ASSERT_TRUE(response);
  const std::vector<TopicProduceResponse> responses = response->data_.responses_;
  EXPECT_EQ(responses.size(), 1);
  EXPECT_EQ(responses[0].partition_responses_[0].error_code_, dm1.error_code_);
}

// As with current version of Kafka library we have no capability of linking producer's notification
// to sent record (other than data address), the owner of this request is going to do a check across
// all owned requests. What means sometimes we might get asked to accept a memento of record that
// did not originate in this request, so it should be ignored.
TEST_F(ProduceUnitTest, ShouldIgnoreMementoFromAnotherRequest) {
  // given
  const OutboundRecord r1 = {"t1", 13, "aaa", "bbb", {}};
  const std::vector<OutboundRecord> records = {r1};
  EXPECT_CALL(extractor_, extractRecords(_)).WillOnce(Return(records));

  const RequestHeader header = {0, 0, 0, absl::nullopt};
  const ProduceRequest data = {0, 0, {}};
  const auto message = std::make_shared<Request<ProduceRequest>>(header, data);
  std::shared_ptr<ProduceRequestHolder> testee =
      std::make_shared<ProduceRequestHolder>(filter_, upstream_kafka_facade_, extractor_, message);

  // when, then - this record will not match anything.
  const DeliveryMemento dm = {nullptr, 0, 42};
  EXPECT_FALSE(testee->accept(dm));
  EXPECT_FALSE(testee->finished());
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/command_handlers/api_versions.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockAbstractRequestListener : public AbstractRequestListener {
public:
  MOCK_METHOD(void, onRequest, (InFlightRequestSharedPtr));
  MOCK_METHOD(void, onRequestReadyForAnswer, ());
  MOCK_METHOD(Event::Dispatcher&, dispatcher, ());
};

TEST(ApiVersionsTest, shouldBeAlwaysReadyForAnswer) {
  // given
  MockAbstractRequestListener filter;
  EXPECT_CALL(filter, onRequestReadyForAnswer());
  const RequestHeader header = {API_VERSIONS_REQUEST_API_KEY, 0, 0, absl::nullopt};
  ApiVersionsRequestHolder testee = {filter, header};

  // when, then - invoking should immediately notify the filter.
  testee.startProcessing();

  // when, then - should always be considered finished.
  const bool finished = testee.finished();
  EXPECT_TRUE(finished);

  // when, then - the computed result is always contains correct data (confirmed by integration
  // tests).
  const auto answer = testee.computeAnswer();
  EXPECT_EQ(answer->metadata_.api_key_, header.api_key_);
  EXPECT_EQ(answer->metadata_.correlation_id_, header.correlation_id_);
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "produce_unit_test",
    srcs = ["produce_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh/command_handlers:produce_lib",
        "//test/mocks/network:network_mocks",
        "//test/mocks/stats:stats_mocks",
    ],
)

envoy_cc_test(
    name = "produce_record_extractor_unit_test",
    srcs = ["produce_record_extractor_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh/command_handlers:produce_record_extractor_lib",
    ],
)

envoy_cc_test(
    name = "fetch_unit_test",
    srcs = ["fetch_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh/command_handlers:fetch_lib",
        "//test/mocks/network:network_mocks",
        "//test/mocks/stats:stats_mocks",
    ],
)

envoy_cc_test(
    name = "fetch_record_converter_unit_test",
    srcs = ["fetch_record_converter_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh/command_handlers:fetch_record_converter_lib",
    ],
)

envoy_cc_test(
    name = "list_offsets_unit_test",
    srcs = ["list_offsets_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh/command_handlers:list_offsets_lib",
        "//test/mocks/network:network_mocks",
        "//test/mocks/stats:stats_mocks",
    ],
)

envoy_cc_test(
    name = "metadata_unit_test",
    srcs = ["metadata_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh/command_handlers:metadata_lib",
        "//test/mocks/network:network_mocks",
        "//test/mocks/stats:stats_mocks",
    ],
)

envoy_cc_test(
    name = "api_versions_unit_test",
    srcs = ["api_versions_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh/command_handlers:api_versions_lib",
        "//test/mocks/network:network_mocks",
        "//test/mocks/stats:stats_mocks",
    ],
)
#include "contrib/kafka/filters/network/source/mesh/command_handlers/list_offsets.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockAbstractRequestListener : public AbstractRequestListener {
public:
  MOCK_METHOD(void, onRequest, (InFlightRequestSharedPtr));
  MOCK_METHOD(void, onRequestReadyForAnswer, ());
  MOCK_METHOD(Event::Dispatcher&, dispatcher, ());
};

TEST(ListOffsetsTest, shouldBeAlwaysReadyForAnswer) {
  // given
  MockAbstractRequestListener filter;
  EXPECT_CALL(filter, onRequestReadyForAnswer());
  const RequestHeader header = {LIST_OFFSETS_REQUEST_API_KEY, 0, 0, absl::nullopt};
  const ListOffsetsRequest data = {0, {}};
  const auto message = std::make_shared<Request<ListOffsetsRequest>>(header, data);
  ListOffsetsRequestHolder testee = {filter, message};

  // when, then - invoking should immediately notify the filter.
  testee.startProcessing();

  // when, then - should always be considered finished.
  const bool finished = testee.finished();
  EXPECT_TRUE(finished);

  // when, then - the computed result is always contains correct data (confirmed by integration
  // tests).
  const auto answer = testee.computeAnswer();
  EXPECT_EQ(answer->metadata_.api_key_, header.api_key_);
  EXPECT_EQ(answer->metadata_.correlation_id_, header.correlation_id_);
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/mocks/event/mocks.h"
#include "test/test_common/thread_factory_for_test.h"

#include "absl/synchronization/blocking_counter.h"
#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_client_impl.h"
#include "contrib/kafka/filters/network/test/mesh/kafka_mocks.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::AnyNumber;
using testing::AtLeast;
using testing::NiceMock;
using testing::Return;
using testing::ReturnNull;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

class MockProduceFinishCb : public ProduceFinishCb {
public:
  MOCK_METHOD(bool, accept, (const DeliveryMemento&));
};

class UpstreamKafkaClientTest : public testing::Test {
protected:
  Event::MockDispatcher dispatcher_;
  Thread::ThreadFactory& thread_factory_ = Thread::threadFactoryForTest();
  NiceMock<MockLibRdKafkaUtils> kafka_utils_{};
  RawKafkaConfig config_ = {{"key1", "value1"}, {"key2", "value2"}};

  std::unique_ptr<MockKafkaProducer> producer_ptr_ = std::make_unique<MockKafkaProducer>();
  MockKafkaProducer& producer_ = *producer_ptr_;

  std::shared_ptr<MockProduceFinishCb> origin_ = std::make_shared<MockProduceFinishCb>();

  // Helper method - allows creation of RichKafkaProducer without problems.
  void setupConstructorExpectations() {
    EXPECT_CALL(kafka_utils_, setConfProperty(_, "key1", "value1", _))
        .WillOnce(Return(RdKafka::Conf::CONF_OK));
    EXPECT_CALL(kafka_utils_, setConfProperty(_, "key2", "value2", _))
        .WillOnce(Return(RdKafka::Conf::CONF_OK));
    EXPECT_CALL(kafka_utils_, setConfDeliveryCallback(_, _, _))
        .WillOnce(Return(RdKafka::Conf::CONF_OK));

    EXPECT_CALL(producer_, poll(_)).Times(AnyNumber());
    EXPECT_CALL(kafka_utils_, createProducer(_, _))
        .WillOnce(Return(testing::ByMove(std::move(producer_ptr_))));

    EXPECT_CALL(kafka_utils_, deleteHeaders(_)).Times(0);
  }
};

OutboundRecord makeRecord(const std::string& payload) { return {"topic", 13, payload, "key", {}}; }

TEST_F(UpstreamKafkaClientTest, ShouldConstructWithoutProblems) {
  // given
  setupConstructorExpectations();

  // when, then - producer got created without problems.
  RichKafkaProducer testee = {dispatcher_, thread_factory_, config_, kafka_utils_};
}

TEST_F(UpstreamKafkaClientTest, ShouldSendRecordsAndReceiveConfirmations) {
  // given
  setupConstructorExpectations();
  RichKafkaProducer testee = {dispatcher_, thread_factory_, config_, kafka_utils_};

  // when, then - should send request without problems.
  EXPECT_CALL(producer_, produce("topic", 13, _, _, _, _, _, _, _, _))
      .Times(3)
      .WillRepeatedly(Return(RdKafka::ERR_NO_ERROR));
  const std::vector<std::string> payloads = {"value1", "value2", "value3"};
  for (const auto& arg : payloads) {
    testee.send(origin_, makeRecord(arg));
  }
  EXPECT_EQ(testee.getUnfinishedRequestsForTest().size(), payloads.size());

  // when, then - should process confirmations.
  EXPECT_CALL(*origin_, accept(_)).Times(3).WillRepeatedly(Return(true));
  for (const auto& arg : payloads) {
    const DeliveryMemento memento = {arg.c_str(), RdKafka::ERR_NO_ERROR, 0};
    testee.processDelivery(memento);
  }
  EXPECT_EQ(testee.getUnfinishedRequestsForTest().size(), 0);
}

TEST_F(UpstreamKafkaClientTest, ShouldCheckCallbacksForDeliveries) {
  // given
  setupConstructorExpectations();
  RichKafkaProducer testee = {dispatcher_, thread_factory_, config_, kafka_utils_};

  // when, then - should send request without problems.
  EXPECT_CALL(producer_, produce("topic", 13, _, _, _, _, _, _, _, _))
      .Times(2)
      .WillRepeatedly(Return(RdKafka::ERR_NO_ERROR));
  const std::vector<std::string> payloads = {"value1", "value2"};
  auto origin1 = std::make_shared<MockProduceFinishCb>();
  auto origin2 = std::make_shared<MockProduceFinishCb>();
  testee.send(origin1, makeRecord(payloads[0]));
  testee.send(origin2, makeRecord(payloads[1]));
  EXPECT_EQ(testee.getUnfinishedRequestsForTest().size(), payloads.size());

  // when, then - should process confirmations (notice we pass second memento first).
  EXPECT_CALL(*origin1, accept(_)).WillOnce(Return(false)).WillOnce(Return(true));
  EXPECT_CALL(*origin2, accept(_)).WillOnce(Return(true));
  const DeliveryMemento memento1 = {payloads[1].c_str(), RdKafka::ERR_NO_ERROR, 0};
  testee.processDelivery(memento1);
  EXPECT_EQ(testee.getUnfinishedRequestsForTest().size(), 1);
  const DeliveryMemento memento2 = {payloads[0].c_str(), RdKafka::ERR_NO_ERROR, 0};
  testee.processDelivery(memento2);
  EXPECT_EQ(testee.getUnfinishedRequestsForTest().size(), 0);
}

TEST_F(UpstreamKafkaClientTest, ShouldHandleProduceFailures) {
  // given
  setupConstructorExpectations();
  RichKafkaProducer testee = {dispatcher_, thread_factory_, config_, kafka_utils_};

  // when, then - if there are problems while sending, notify the source immediately.
  EXPECT_CALL(producer_, produce("topic", 13, _, _, _, _, _, _, _, _))
      .WillOnce(Return(RdKafka::ERR_LEADER_NOT_AVAILABLE));
  EXPECT_CALL(kafka_utils_, deleteHeaders(_));
  EXPECT_CALL(*origin_, accept(_)).WillOnce(Return(true));
  testee.send(origin_, makeRecord("value"));
  EXPECT_EQ(testee.getUnfinishedRequestsForTest().size(), 0);
}

TEST_F(UpstreamKafkaClientTest, ShouldHandleKafkaCallback) {
  // given
  setupConstructorExpectations();
  RichKafkaProducer testee = {dispatcher_, thread_factory_, config_, kafka_utils_};
  NiceMock<MockKafkaMessage> message;

  // when, then - notification is passed to dispatcher.
  EXPECT_CALL(dispatcher_, post(_));
  testee.dr_cb(message);
}

TEST_F(UpstreamKafkaClientTest, ShouldHandleHeaderConversionFailures) {
  // given
  setupConstructorExpectations();
  EXPECT_CALL(kafka_utils_, convertHeaders(_)).WillOnce(Return(nullptr));

  RichKafkaProducer testee = {dispatcher_, thread_factory_, config_, kafka_utils_};

  // when, then - producer was not interacted with, response was sent immediately.
  EXPECT_CALL(producer_, produce(_, _, _, _, _, _, _, _, _, _)).Times(0);
  EXPECT_CALL(*origin_, accept(_)).WillOnce(Return(true));
  testee.send(origin_, makeRecord("value"));
  EXPECT_EQ(testee.getUnfinishedRequestsForTest().size(), 0);
}

// This handles situations when users pass bad config to raw producer.
TEST_F(UpstreamKafkaClientTest, ShouldThrowIfSettingPropertiesFails) {
  // given
  EXPECT_CALL(kafka_utils_, setConfProperty(_, _, _, _))
      .WillOnce(Return(RdKafka::Conf::CONF_INVALID));

  // when, then - exception gets thrown during construction.
  EXPECT_THROW(RichKafkaProducer(dispatcher_, thread_factory_, config_, kafka_utils_),
               EnvoyException);
}

TEST_F(UpstreamKafkaClientTest, ShouldThrowIfSettingDeliveryCallbackFails) {
  // given
  EXPECT_CALL(kafka_utils_, setConfProperty(_, _, _, _))
      .WillRepeatedly(Return(RdKafka::Conf::CONF_OK));
  EXPECT_CALL(kafka_utils_, setConfDeliveryCallback(_, _, _))
      .WillOnce(Return(RdKafka::Conf::CONF_INVALID));

  // when, then - exception gets thrown during construction.
  EXPECT_THROW(RichKafkaProducer(dispatcher_, thread_factory_, config_, kafka_utils_),
               EnvoyException);
}

TEST_F(UpstreamKafkaClientTest, ShouldThrowIfRawProducerConstructionFails) {
  // given
  EXPECT_CALL(kafka_utils_, setConfProperty(_, _, _, _))
      .WillRepeatedly(Return(RdKafka::Conf::CONF_OK));
  EXPECT_CALL(kafka_utils_, setConfDeliveryCallback(_, _, _))
      .WillOnce(Return(RdKafka::Conf::CONF_OK));
  EXPECT_CALL(kafka_utils_, createProducer(_, _)).WillOnce(ReturnNull());

  // when, then - exception gets thrown during construction.
  EXPECT_THROW(RichKafkaProducer(dispatcher_, thread_factory_, config_, kafka_utils_),
               EnvoyException);
}

// Rich producer's constructor starts a monitoring thread.
// We are going to wait for at least one invocation of producer 'poll', so we are confident that it
// does monitoring. Then we are going to destroy the testee, and expect the thread to finish.
TEST_F(UpstreamKafkaClientTest, ShouldPollProducerForEventsUntilShutdown) {
  // given
  setupConstructorExpectations();

  absl::BlockingCounter counter{1};
  EXPECT_CALL(producer_, poll(_)).Times(AtLeast(1)).WillOnce([&counter]() {
    counter.DecrementCount();
    return 0;
  });

  // when
  {
    std::unique_ptr<RichKafkaProducer> testee =
        std::make_unique<RichKafkaProducer>(dispatcher_, thread_factory_, config_, kafka_utils_);
    counter.Wait();
  }

  // then - the above block actually finished, what means that the monitoring thread interacted with
  // underlying Kafka producer.
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockAbstractRequestListener : public AbstractRequestListener {
public:
  MOCK_METHOD(void, onRequest, (InFlightRequestSharedPtr));
  MOCK_METHOD(void, onRequestReadyForAnswer, ());
  MOCK_METHOD(Event::Dispatcher&, dispatcher, ());
};

class Testee : public BaseInFlightRequest {
public:
  Testee(AbstractRequestListener& filter) : BaseInFlightRequest{filter} {};
  void startProcessing() override { throw "not interesting"; }
  bool finished() const override { throw "not interesting"; }
  AbstractResponseSharedPtr computeAnswer() const override { throw "not interesting"; }
  void finishRequest() { BaseInFlightRequest::notifyFilter(); }
};

TEST(AbstractCommandTest, shouldNotifyFilter) {
  // given
  MockAbstractRequestListener filter;
  EXPECT_CALL(filter, onRequestReadyForAnswer());
  Testee testee = {filter};

  // when
  testee.finishRequest();

  // then - filter got notified that a requested has finished processing.
}

TEST(AbstractCommandTest, shouldHandleBeingAbandoned) {
  // given
  MockAbstractRequestListener filter;
  Testee testee = {filter};
  testee.abandon();

  // when, then - abandoned request does not notify filter even after it finishes.
  testee.finishRequest();
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include <functional>

#include "test/test_common/thread_factory_for_test.h"

#include "absl/synchronization/mutex.h"
#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_consumer_impl.h"
#include "contrib/kafka/filters/network/test/mesh/kafka_mocks.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::AnyNumber;
using testing::AtLeast;
using testing::ByMove;
using testing::Invoke;
using testing::NiceMock;
using testing::Return;
using testing::ReturnNull;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

class MockInboundRecordProcessor : public InboundRecordProcessor {
public:
  MOCK_METHOD(void, receive, (InboundRecordSharedPtr), ());
  MOCK_METHOD(bool, waitUntilInterest, (const std::string&, const int32_t), (const));
};

class UpstreamKafkaConsumerTest : public testing::Test {
protected:
  Thread::ThreadFactory& thread_factory_ = Thread::threadFactoryForTest();
  MockLibRdKafkaUtils kafka_utils_;
  RawKafkaConfig config_ = {{"key1", "value1"}, {"key2", "value2"}};

  std::unique_ptr<MockKafkaConsumer> consumer_ptr_ = std::make_unique<MockKafkaConsumer>();
  MockKafkaConsumer& consumer_ = *consumer_ptr_;

  MockInboundRecordProcessor record_processor_;

  // Helper method - allows creation of RichKafkaConsumer without problems.
  void setupConstructorExpectations() {
    EXPECT_CALL(kafka_utils_, setConfProperty(_, "key1", "value1", _))
        .WillOnce(Return(RdKafka::Conf::CONF_OK));
    EXPECT_CALL(kafka_utils_, setConfProperty(_, "key2", "value2", _))
        .WillOnce(Return(RdKafka::Conf::CONF_OK));
    EXPECT_CALL(kafka_utils_, assignConsumerPartitions(_, "topic", 42));

    // These two methods get called in the destructor.
    EXPECT_CALL(consumer_, unassign());
    EXPECT_CALL(consumer_, close());

    EXPECT_CALL(kafka_utils_, createConsumer(_, _))
        .WillOnce(Return(ByMove(std::move(consumer_ptr_))));
  }

  // Helper method - creates the testee with all the mocks injected.
  KafkaConsumerPtr makeTestee() {
    return std::make_unique<RichKafkaConsumer>(record_processor_, thread_factory_, "topic", 42,
                                               config_, kafka_utils_);
  }
};

// This handles situations when users pass bad config to raw consumer_.
TEST_F(UpstreamKafkaConsumerTest, ShouldThrowIfSettingPropertiesFails) {
  // given
  EXPECT_CALL(kafka_utils_, setConfProperty(_, _, _, _))
      .WillOnce(Return(RdKafka::Conf::CONF_INVALID));

  // when, then - exception gets thrown during construction.
  EXPECT_THROW(makeTestee(), EnvoyException);
}

TEST_F(UpstreamKafkaConsumerTest, ShouldThrowIfRawConsumerConstructionFails) {
  // given
  EXPECT_CALL(kafka_utils_, setConfProperty(_, _, _, _))
      .WillRepeatedly(Return(RdKafka::Conf::CONF_OK));
  EXPECT_CALL(kafka_utils_, createConsumer(_, _)).WillOnce(ReturnNull());

  // when, then - exception gets thrown during construction.
  EXPECT_THROW(makeTestee(), EnvoyException);
}

// Utility class for counting invocations / capturing invocation data.
template <typename T> class Tracker {
private:
  mutable absl::Mutex mutex_;
  int invocation_count_ ABSL_GUARDED_BY(mutex_) = 0;
  T data_ ABSL_GUARDED_BY(mutex_);

public:
  // Stores the first value put inside.
  void registerInvocation(const T& arg) {
    absl::MutexLock lock{&mutex_};
    if (0 == invocation_count_) {
      data_ = arg;
    }
    invocation_count_++;
  }

  // Blocks until some value appears, and returns it.
  T awaitFirstInvocation() const {
    const auto cond = std::bind(&Tracker::hasInvocations, this, 1);
    absl::MutexLock lock{&mutex_, absl::Condition(&cond)};
    return data_;
  }

  // Blocks until N invocations have happened.
  void awaitInvocations(const int n) const {
    const auto cond = std::bind(&Tracker::hasInvocations, this, n);
    absl::MutexLock lock{&mutex_, absl::Condition(&cond)};
  }

private:
  bool hasInvocations(const int n) const ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
    return invocation_count_ >= n;
  }
};

// Utility method: creates a Kafka message with given error code.
// Results will be freed by the worker thread.
static RdKafkaMessageRawPtr makeMessage(const RdKafka::ErrorCode error_code) {
  const auto result = new NiceMock<MockKafkaMessage>();
  ON_CALL(*result, err()).WillByDefault(Return(error_code));
  return result;
}

// Expected behaviour: if there is interest, then poll for records, and pass them to processor.
TEST_F(UpstreamKafkaConsumerTest, ShouldReceiveRecordsFromKafkaConsumer) {
  // given
  setupConstructorExpectations();

  EXPECT_CALL(record_processor_, waitUntilInterest(_, _))
      .Times(AnyNumber())
      .WillRepeatedly(Return(true));

  EXPECT_CALL(consumer_, consume(_)).Times(AnyNumber()).WillRepeatedly([]() {
    return makeMessage(RdKafka::ERR_NO_ERROR);
  });

  Tracker<InboundRecordSharedPtr> tracker;
  EXPECT_CALL(record_processor_, receive(_))
      .Times(AtLeast(1))
      .WillRepeatedly(Invoke(&tracker, &Tracker<InboundRecordSharedPtr>::registerInvocation));

  // when
  const auto testee = makeTestee();

  // then - record processor got notified with a message.
  const InboundRecordSharedPtr record = tracker.awaitFirstInvocation();
  ASSERT_NE(record, nullptr);
}

// Expected behaviour: if there is no data, we send nothing to processor.
TEST_F(UpstreamKafkaConsumerTest, ShouldHandleNoDataGracefully) {
  // given
  setupConstructorExpectations();

  Tracker<void*> tracker;
  EXPECT_CALL(record_processor_, waitUntilInterest(_, _))
      .Times(AnyNumber())
      .WillRepeatedly([&tracker]() {
        tracker.registerInvocation(nullptr);
        return true;
      });

  EXPECT_CALL(consumer_, consume(_)).Times(AnyNumber()).WillRepeatedly([]() {
    return makeMessage(RdKafka::ERR__TIMED_OUT);
  });

  // We do not expect to receive any meaningful records.
  EXPECT_CALL(record_processor_, receive(_)).Times(0);

  // when
  const auto testee = makeTestee();

  // then - we have run a few loops, but the processor was never interacted with.
  tracker.awaitInvocations(13);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/mocks/event/mocks.h"
#include "test/test_common/utility.h"

#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/api_versions.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/list_offsets.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/metadata.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/produce.h"
#include "contrib/kafka/filters/network/source/mesh/request_processor.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_facade.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::NiceMock;
using testing::ReturnRef;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockAbstractRequestListener : public AbstractRequestListener {
public:
  MOCK_METHOD(void, onRequest, (InFlightRequestSharedPtr));
  MOCK_METHOD(void, onRequestReadyForAnswer, ());
  MOCK_METHOD(Event::Dispatcher&, dispatcher, ());

  MockAbstractRequestListener() {
    ON_CALL(*this, dispatcher).WillByDefault(ReturnRef(mock_dispatcher_));
  }

private:
  Event::MockDispatcher mock_dispatcher_;
};

class MockUpstreamKafkaFacade : public UpstreamKafkaFacade {
public:
  MOCK_METHOD(KafkaProducer&, getProducerForTopic, (const std::string&));
};

class MockRecordCallbackProcessor : public RecordCallbackProcessor {
public:
  MOCK_METHOD(void, processCallback, (const RecordCbSharedPtr&));
  MOCK_METHOD(void, removeCallback, (const RecordCbSharedPtr&));
};

class MockUpstreamKafkaConfiguration : public UpstreamKafkaConfiguration {
public:
  MOCK_METHOD(absl::optional<ClusterConfig>, computeClusterConfigForTopic, (const std::string&),
              (const));
  MOCK_METHOD((std::pair<std::string, int32_t>), getAdvertisedAddress, (), (const));
};

class RequestProcessorTest : public testing::Test {
protected:
  NiceMock<MockAbstractRequestListener> listener_;
  MockUpstreamKafkaConfiguration configuration_;
  MockUpstreamKafkaFacade upstream_kafka_facade_;
  MockRecordCallbackProcessor record_callback_processor_;
  RequestProcessor testee_ = {listener_, configuration_, upstream_kafka_facade_,
                              record_callback_processor_};
};

TEST_F(RequestProcessorTest, ShouldProcessProduceRequest) {
  // given
  const RequestHeader header = {PRODUCE_REQUEST_API_KEY, 0, 0, absl::nullopt};
  const ProduceRequest data = {0, 0, {}};
  const auto message = std::make_shared<Request<ProduceRequest>>(header, data);

  InFlightRequestSharedPtr capture = nullptr;
  EXPECT_CALL(listener_, onRequest(_)).WillOnce(testing::SaveArg<0>(&capture));

  // when
  testee_.onMessage(message);

  // then
  ASSERT_NE(std::dynamic_pointer_cast<ProduceRequestHolder>(capture), nullptr);
}

TEST_F(RequestProcessorTest, ShouldProcessFetchRequest) {
  // given
  const RequestHeader header = {FETCH_REQUEST_API_KEY, 0, 0, absl::nullopt};
  const FetchRequest data = {0, 0, 0, {}};
  const auto message = std::make_shared<Request<FetchRequest>>(header, data);

  InFlightRequestSharedPtr capture = nullptr;
  EXPECT_CALL(listener_, onRequest(_)).WillOnce(testing::SaveArg<0>(&capture));

  // when
  testee_.onMessage(message);

  // then
  ASSERT_NE(std::dynamic_pointer_cast<FetchRequestHolder>(capture), nullptr);
}

TEST_F(RequestProcessorTest, ShouldProcessListOffsetsRequest) {
  // given
  const RequestHeader header = {LIST_OFFSETS_REQUEST_API_KEY, 0, 0, absl::nullopt};
  const ListOffsetsRequest data = {0, {}};
  const auto message = std::make_shared<Request<ListOffsetsRequest>>(header, data);

  InFlightRequestSharedPtr capture = nullptr;
  EXPECT_CALL(listener_, onRequest(_)).WillOnce(testing::SaveArg<0>(&capture));

  // when
  testee_.onMessage(message);

  // then
  ASSERT_NE(std::dynamic_pointer_cast<ListOffsetsRequestHolder>(capture), nullptr);
}

TEST_F(RequestProcessorTest, ShouldProcessMetadataRequest) {
  // given
  const RequestHeader header = {METADATA_REQUEST_API_KEY, 0, 0, absl::nullopt};
  const MetadataRequest data = {absl::nullopt};
  const auto message = std::make_shared<Request<MetadataRequest>>(header, data);

  InFlightRequestSharedPtr capture = nullptr;
  EXPECT_CALL(listener_, onRequest(_)).WillOnce(testing::SaveArg<0>(&capture));

  // when
  testee_.onMessage(message);

  // then
  ASSERT_NE(std::dynamic_pointer_cast<MetadataRequestHolder>(capture), nullptr);
}

TEST_F(RequestProcessorTest, ShouldProcessApiVersionsRequest) {
  // given
  const RequestHeader header = {API_VERSIONS_REQUEST_API_KEY, 0, 0, absl::nullopt};
  const ApiVersionsRequest data = {};
  const auto message = std::make_shared<Request<ApiVersionsRequest>>(header, data);

  InFlightRequestSharedPtr capture = nullptr;
  EXPECT_CALL(listener_, onRequest(_)).WillOnce(testing::SaveArg<0>(&capture));

  // when
  testee_.onMessage(message);

  // then
  ASSERT_NE(std::dynamic_pointer_cast<ApiVersionsRequestHolder>(capture), nullptr);
}

TEST_F(RequestProcessorTest, ShouldHandleUnsupportedRequest) {
  // given
  const RequestHeader header = {END_TXN_REQUEST_API_KEY, 0, 0, absl::nullopt};
  const EndTxnRequest data = {"", 0, 0, false};
  const auto message = std::make_shared<Request<EndTxnRequest>>(header, data);

  // when, then - exception gets thrown.
  EXPECT_THROW_WITH_REGEX(testee_.onMessage(message), EnvoyException, "unsupported");
}

TEST_F(RequestProcessorTest, ShouldHandleUnparseableRequest) {
  // given
  const RequestHeader header = {42, 42, 42, absl::nullopt};
  const auto arg = std::make_shared<RequestParseFailure>(header);

  // when, then - exception gets thrown.
  EXPECT_THROW_WITH_REGEX(testee_.onFailedParse(arg), EnvoyException, "unknown");
}

} // anonymous namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "envoy/thread/thread.h"
#include "envoy/thread_local/thread_local.h"

#include "test/mocks/thread_local/mocks.h"
#include "test/test_common/thread_factory_for_test.h"

#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_facade.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::Return;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockUpstreamKafkaConfiguration : public UpstreamKafkaConfiguration {
public:
  MOCK_METHOD(absl::optional<ClusterConfig>, computeClusterConfigForTopic, (const std::string&),
              (const));
  MOCK_METHOD((std::pair<std::string, int32_t>), getAdvertisedAddress, (), (const));
};

class MockThreadFactory : public Thread::ThreadFactory {
public:
  MOCK_METHOD(Thread::ThreadPtr, createThread, (std::function<void()>, Thread::OptionsOptConstRef));
  MOCK_METHOD(Thread::ThreadId, currentThreadId, ());
};

TEST(UpstreamKafkaFacadeTest, shouldCreateProducerOnlyOnceForTheSameCluster) {
  // given
  const std::string topic1 = "topic1";
  const std::string topic2 = "topic2";

  MockUpstreamKafkaConfiguration configuration;
  const ClusterConfig cluster_config = {
      "cluster", 1, {{"bootstrap.servers", "localhost:9092"}}, {}};
  EXPECT_CALL(configuration, computeClusterConfigForTopic(topic1)).WillOnce(Return(cluster_config));
  EXPECT_CALL(configuration, computeClusterConfigForTopic(topic2)).WillOnce(Return(cluster_config));
  ThreadLocal::MockInstance slot_allocator;
  EXPECT_CALL(slot_allocator, allocateSlot())
      .WillOnce(Invoke(&slot_allocator, &ThreadLocal::MockInstance::allocateSlotMock));
  Thread::ThreadFactory& thread_factory = Thread::threadFactoryForTest();
  UpstreamKafkaFacadeImpl testee = {configuration, slot_allocator, thread_factory};

  // when
  auto& result1 = testee.getProducerForTopic(topic1);
  auto& result2 = testee.getProducerForTopic(topic2);

  // then
  EXPECT_EQ(&result1, &result2);
  EXPECT_EQ(testee.getProducerCountForTest(), 1);
}

TEST(UpstreamKafkaFacadeTest, shouldCreateDifferentProducersForDifferentClusters) {
  // given
  const std::string topic1 = "topic1";
  const std::string topic2 = "topic2";

  MockUpstreamKafkaConfiguration configuration;
  // Notice it's the cluster name that matters, not the producer config.
  const ClusterConfig cluster_config1 = {
      "cluster1", 1, {{"bootstrap.servers", "localhost:9092"}}, {}};
  EXPECT_CALL(configuration, computeClusterConfigForTopic(topic1))
      .WillOnce(Return(cluster_config1));
  const ClusterConfig cluster_config2 = {
      "cluster2", 1, {{"bootstrap.servers", "localhost:9092"}}, {}};
  EXPECT_CALL(configuration, computeClusterConfigForTopic(topic2))
      .WillOnce(Return(cluster_config2));
  ThreadLocal::MockInstance slot_allocator;
  EXPECT_CALL(slot_allocator, allocateSlot())
      .WillOnce(Invoke(&slot_allocator, &ThreadLocal::MockInstance::allocateSlotMock));
  Thread::ThreadFactory& thread_factory = Thread::threadFactoryForTest();
  UpstreamKafkaFacadeImpl testee = {configuration, slot_allocator, thread_factory};

  // when
  auto& result1 = testee.getProducerForTopic(topic1);
  auto& result2 = testee.getProducerForTopic(topic2);

  // then
  EXPECT_NE(&result1, &result2);
  EXPECT_EQ(testee.getProducerCountForTest(), 2);
}

TEST(UpstreamKafkaFacadeTest, shouldThrowIfThereIsNoConfigurationForGivenTopic) {
  // given
  const std::string topic = "topic1";

  MockUpstreamKafkaConfiguration configuration;
  const ClusterConfig cluster_config = {
      "cluster", 1, {{"bootstrap.servers", "localhost:9092"}}, {}};
  EXPECT_CALL(configuration, computeClusterConfigForTopic(topic)).WillOnce(Return(absl::nullopt));
  ThreadLocal::MockInstance slot_allocator;
  EXPECT_CALL(slot_allocator, allocateSlot())
      .WillOnce(Invoke(&slot_allocator, &ThreadLocal::MockInstance::allocateSlotMock));
  Thread::ThreadFactory& thread_factory = Thread::threadFactoryForTest();
  UpstreamKafkaFacadeImpl testee = {configuration, slot_allocator, thread_factory};

  // when, then - exception gets thrown.
  EXPECT_THROW(testee.getProducerForTopic(topic), EnvoyException);
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/mocks/server/factory_context.h"

#include "contrib/kafka/filters/network/source/mesh/config.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

class MockThreadFactory : public Thread::ThreadFactory {
public:
  MOCK_METHOD(Thread::ThreadPtr, createThread, (std::function<void()>, Thread::OptionsOptConstRef));
  MOCK_METHOD(Thread::ThreadId, currentThreadId, ());
};

TEST(KafkaMeshConfigFactoryUnitTest, shouldCreateFilter) {
  // given
  const std::string yaml = R"EOF(
advertised_host: "127.0.0.1"
advertised_port: 19092
upstream_clusters:
- cluster_name: kafka_c1
  bootstrap_servers: 127.0.0.1:9092
  partition_count: 1
- cluster_name: kafka_c2
  bootstrap_servers: 127.0.0.1:9093
  partition_count: 1
- cluster_name: kafka_c3
  bootstrap_servers: 127.0.0.1:9094
  partition_count: 5
  producer_config:
    acks: "1"
    linger.ms: "500"
forwarding_rules:
- target_cluster: kafka_c1
  topic_prefix: apples
- target_cluster: kafka_c2
  topic_prefix: bananas
- target_cluster: kafka_c3
  topic_prefix: cherries
  )EOF";

  KafkaMeshProtoConfig proto_config;
  TestUtility::loadFromYamlAndValidate(yaml, proto_config);

  testing::NiceMock<Server::Configuration::MockFactoryContext> context;
  testing::NiceMock<MockThreadFactory> thread_factory;
  ON_CALL(context.server_factory_context_.api_, threadFactory())
      .WillByDefault(ReturnRef(thread_factory));
  KafkaMeshConfigFactory factory;

  Network::FilterFactoryCb cb = factory.createFilterFactoryFromProto(proto_config, context);
  Network::MockConnection connection;
  EXPECT_CALL(connection, addReadFilter(_));

  // when
  cb(connection);

  // then - connection had `addFilter` invoked
}

TEST(KafkaMeshConfigFactoryUnitTest, throwsIfAdvertisedPortIsMissing) {
  // given
  const std::string yaml = R"EOF(
advertised_host: "127.0.0.1"
  )EOF";

  KafkaMeshProtoConfig proto_config;

  // when
  // then - exception gets thrown
  EXPECT_THROW(TestUtility::loadFromYamlAndValidate(yaml, proto_config), ProtoValidationException);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include <chrono>

#include "envoy/common/exception.h"
#include "envoy/thread/thread.h"

#include "contrib/kafka/filters/network/source/mesh/shared_consumer_manager_impl.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

using testing::_;
using testing::NiceMock;
using testing::Return;

class MockThreadFactory : public Thread::ThreadFactory {
public:
  MOCK_METHOD(Thread::ThreadPtr, createThread, (std::function<void()>, Thread::OptionsOptConstRef));
  MOCK_METHOD(Thread::ThreadId, currentThreadId, ());
};

class MockUpstreamKafkaConfiguration : public UpstreamKafkaConfiguration {
public:
  MOCK_METHOD(absl::optional<ClusterConfig>, computeClusterConfigForTopic, (const std::string&),
              (const));
  MOCK_METHOD((std::pair<std::string, int32_t>), getAdvertisedAddress, (), (const));
};

class MockKafkaConsumerFactory : public KafkaConsumerFactory {
public:
  MOCK_METHOD(KafkaConsumerPtr, createConsumer,
              (InboundRecordProcessor&, Thread::ThreadFactory&, const std::string&, const int32_t,
               const RawKafkaConfig&),
              (const));
};

class SharedConsumerManagerTest : public testing::Test {
protected:
  MockThreadFactory thread_factory_;
  MockUpstreamKafkaConfiguration configuration_;
  MockKafkaConsumerFactory consumer_factory_;

  std::unique_ptr<SharedConsumerManagerImpl> makeTestee() {
    return std::make_unique<SharedConsumerManagerImpl>(configuration_, thread_factory_,
                                                       consumer_factory_);
  }
};

TEST_F(SharedConsumerManagerTest, ShouldRegisterTopicOnlyOnce) {
  // given
  const std::string topic1 = "topic1";
  const std::string topic2 = "topic2";

  const ClusterConfig cluster_config = {"cluster", 1, {}, {}};
  EXPECT_CALL(configuration_, computeClusterConfigForTopic(topic1))
      .WillOnce(Return(cluster_config));
  EXPECT_CALL(configuration_, computeClusterConfigForTopic(topic2))
      .WillOnce(Return(cluster_config));

  EXPECT_CALL(consumer_factory_, createConsumer(_, _, _, _, _)).Times(2).WillRepeatedly([]() {
    return nullptr;
  });

  auto testee = makeTestee();

  // when
  for (int i = 0; i < 3; ++i) {
    testee->registerConsumerIfAbsent(topic1);
  }
  for (int i = 0; i < 3; ++i) {
    testee->registerConsumerIfAbsent(topic2);
  }

  // then
  ASSERT_EQ(testee->getConsumerCountForTest(), 2);
}

TEST_F(SharedConsumerManagerTest, ShouldHandleMissingConfig) {
  // given
  const std::string topic = "topic";

  EXPECT_CALL(configuration_, computeClusterConfigForTopic(topic)).WillOnce(Return(absl::nullopt));

  EXPECT_CALL(consumer_factory_, createConsumer(_, _, _, _, _)).Times(0);

  auto testee = makeTestee();

  // when, then - construction throws and nothing gets registered.
  EXPECT_THROW(testee->registerConsumerIfAbsent(topic), EnvoyException);
  ASSERT_EQ(testee->getConsumerCountForTest(), 0);
}

class MockRecordCb : public RecordCb {
public:
  MOCK_METHOD(CallbackReply, receive, (InboundRecordSharedPtr), ());
  MOCK_METHOD(TopicToPartitionsMap, interest, (), (const));
  MOCK_METHOD(std::string, toString, (), (const));
};

TEST_F(SharedConsumerManagerTest, ShouldProcessCallback) {
  // given
  const std::string topic1 = "topic1";
  const std::string topic2 = "topic2";

  const ClusterConfig cluster_config = {"cluster", 1, {}, {}};
  EXPECT_CALL(configuration_, computeClusterConfigForTopic(topic1))
      .WillOnce(Return(cluster_config));
  EXPECT_CALL(configuration_, computeClusterConfigForTopic(topic2))
      .WillOnce(Return(cluster_config));
  EXPECT_CALL(consumer_factory_, createConsumer(_, _, _, _, _)).Times(2);

  auto testee = makeTestee();

  const auto cb = std::make_shared<NiceMock<MockRecordCb>>();
  const TopicToPartitionsMap tp = {{topic1, {0, 1}}, {topic2, {0, 1, 2, 3, 4}}};
  EXPECT_CALL(*cb, interest).WillRepeatedly(Return(tp));

  // when
  testee->processCallback(cb);

  // then
  ASSERT_EQ(testee->getConsumerCountForTest(), 2);
}

// RecordDistributorTest

class RecordDistributorTest : public testing::Test {
protected:
  RecordDistributorPtr makeTestee() { return std::make_unique<RecordDistributor>(); }

  RecordDistributorPtr makeTestee(const PartitionMap<RecordCbSharedPtr>& callbacks) {
    return std::make_unique<RecordDistributor>(callbacks, PartitionMap<InboundRecordSharedPtr>());
  }

  RecordDistributorPtr makeTestee(const PartitionMap<InboundRecordSharedPtr>& records) {
    return std::make_unique<RecordDistributor>(PartitionMap<RecordCbSharedPtr>(), records);
  }

  std::shared_ptr<MockRecordCb> makeCb() {
    auto result = std::make_shared<NiceMock<MockRecordCb>>();
    ON_CALL(*result, receive(_)).WillByDefault(Return(CallbackReply::Rejected));
    return result;
  }

  InboundRecordSharedPtr makeRecord(const std::string& topic, const int32_t partition) {
    return std::make_shared<InboundRecord>(topic, partition, 0, absl::nullopt, absl::nullopt);
  }
};

TEST_F(RecordDistributorTest, ShouldNotWaitUntilInterestIfThereIsAny) {
  // given
  const std::string topic = "aaa";
  PartitionMap<RecordCbSharedPtr> callbacks;
  callbacks[{topic, 0}] = {makeCb()};
  callbacks[{"bbb", 0}] = {makeCb()};
  const auto testee = makeTestee(callbacks);

  // when
  const auto result = testee->waitUntilInterest(topic, 500);

  // then
  ASSERT_TRUE(result);
}

TEST_F(RecordDistributorTest, ShouldWaitUntilInterestIfThereIsNone) {
  // given
  const auto timeout_ms = 500;
  PartitionMap<RecordCbSharedPtr> callbacks;
  callbacks[{"bbb", 0}] = {makeCb()};
  const auto testee = makeTestee(callbacks);

  // when
  namespace ch = std::chrono;
  const auto start = ch::high_resolution_clock::now();
  const auto result = testee->waitUntilInterest("aaa", timeout_ms);
  const auto stop = ch::high_resolution_clock::now();

  // then
  ASSERT_FALSE(result);
  const auto duration_ms = (ch::duration_cast<ch::milliseconds>(stop - start)).count();
  ASSERT_GT(duration_ms, timeout_ms * 0.8);
}

TEST_F(RecordDistributorTest, ShouldStoreRecords) {
  // given
  const auto testee = makeTestee();

  // when
  for (int i = 0; i < 10; ++i) {
    const auto record = makeRecord("topic", 13);
    testee->receive(record);
  }

  // then
  ASSERT_EQ(testee->getRecordCountForTest("topic", 13), 10);
}

TEST_F(RecordDistributorTest, ShouldPassRecordToCallbacksAndRemoveIfFinished) {
  // given
  PartitionMap<RecordCbSharedPtr> callbacks;
  auto callback = makeCb();
  EXPECT_CALL(*callback, receive(_)).WillOnce(Return(CallbackReply::AcceptedAndFinished));
  callbacks[{"topic", 0}] = {callback};
  callbacks[{"topic", 1}] = {makeCb(), callback};
  callbacks[{"topic", 2}] = {callback, makeCb()};
  callbacks[{"topic", 3}] = {makeCb(), callback, makeCb()};
  const auto testee = makeTestee(callbacks);
  const auto record = makeRecord("topic", 2);

  // when
  testee->receive(record);

  // then
  // Record was not stored.
  ASSERT_EQ(testee->getRecordCountForTest("topic", 2), -1);
  // Our callback got removed, others stay.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), -1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 2), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 3), 2);
}

// Compared to previous test - callback does not get removed here.
TEST_F(RecordDistributorTest, ShouldPassRecordToCallbacksAndKeepThem) {
  // given
  PartitionMap<RecordCbSharedPtr> callbacks;
  auto callback = makeCb();
  EXPECT_CALL(*callback, receive(_)).WillOnce(Return(CallbackReply::AcceptedAndWantMore));
  callbacks[{"topic", 0}] = {callback};
  callbacks[{"topic", 1}] = {makeCb(), callback};
  callbacks[{"topic", 2}] = {callback, makeCb()};
  callbacks[{"topic", 3}] = {makeCb(), callback, makeCb()};
  const auto testee = makeTestee(callbacks);
  const auto record = makeRecord("topic", 2);

  // when
  testee->receive(record);

  // then
  // Record was not stored.
  ASSERT_EQ(testee->getRecordCountForTest("topic", 2), -1);
  // All records stay.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), 2);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 2), 2);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 3), 3);
}

TEST_F(RecordDistributorTest, ShouldPassRecordToCallbacksAndKeepItIfNoneAccepts) {
  // given
  PartitionMap<RecordCbSharedPtr> callbacks;
  callbacks[{"topic", 0}] = {makeCb()};
  callbacks[{"topic", 1}] = {makeCb(), makeCb()};
  const auto testee = makeTestee(callbacks);
  const auto record = makeRecord("topic", 1);

  // when
  testee->receive(record);

  // then
  // Record was stored.
  ASSERT_EQ(testee->getRecordCountForTest("topic", 1), 1);
  // No changes in callbacks.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), 2);
}

TEST_F(RecordDistributorTest, ShouldProcessCallbackAndRegisterItIfThereAreNoMessages) {
  // given
  const auto cb = makeCb();
  const TopicToPartitionsMap tp = {{"topic", {0, 1}}};
  EXPECT_CALL(*cb, interest).WillRepeatedly(Return(tp));

  const auto testee = makeTestee();

  // when
  testee->processCallback(cb);

  // then - Callback got registered.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), 1);
}

// Compared to previous tests, there were interesting records before.
TEST_F(RecordDistributorTest, ShouldRegisterCallbackAndPassItMatchingMessages) {
  // given
  const auto cb = makeCb();
  const TopicToPartitionsMap tp = {{"topic", {0, 1}}};
  EXPECT_CALL(*cb, interest).WillRepeatedly(Return(tp));
  EXPECT_CALL(*cb, receive(_)).Times(4).WillRepeatedly(Return(CallbackReply::AcceptedAndWantMore));

  PartitionMap<InboundRecordSharedPtr> records;
  records[{"topic", 0}] = {makeRecord("topic", 0), makeRecord("topic", 1)};
  records[{"topic", 1}] = {makeRecord("topic", 1), makeRecord("topic", 1)};
  const auto testee = makeTestee(records);

  // when
  testee->processCallback(cb);

  // then
  // Messages got consumed.
  ASSERT_EQ(testee->getRecordCountForTest("topic", 0), -1);
  ASSERT_EQ(testee->getRecordCountForTest("topic", 1), -1);
  // Callback got registered.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), 1);
}

TEST_F(RecordDistributorTest, ShouldNotRegisterCallbackIfItGotSatisfied) {
  // given
  const auto cb = makeCb();
  const TopicToPartitionsMap tp = {{"topic", {0, 1, 2}}};
  EXPECT_CALL(*cb, interest).WillRepeatedly(Return(tp));
  EXPECT_CALL(*cb, receive(_))
      .Times(3)
      .WillOnce(Return(CallbackReply::AcceptedAndWantMore))
      .WillOnce(Return(CallbackReply::AcceptedAndWantMore))
      .WillOnce(Return(CallbackReply::AcceptedAndFinished));

  PartitionMap<InboundRecordSharedPtr> records;
  records[{"topic", 0}] = {makeRecord("topic", 0)};
  records[{"topic", 1}] = {makeRecord("topic", 1), makeRecord("topic", 1), makeRecord("topic", 1)};
  records[{"topic", 2}] = {makeRecord("topic", 2), makeRecord("topic", 2)};
  const auto testee = makeTestee(records);

  // when
  testee->processCallback(cb);

  // then
  // Some messages got consumed.
  ASSERT_EQ(testee->getRecordCountForTest("topic", 0), -1);
  ASSERT_EQ(testee->getRecordCountForTest("topic", 1), 1);
  ASSERT_EQ(testee->getRecordCountForTest("topic", 2), 2);
  // Callback was not registered.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), -1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), -1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 2), -1);
}

// Very similar to previous one, but makes sure we clean up vectors in record map.
TEST_F(RecordDistributorTest, ShouldNotRegisterCallbackIfItGotSatisfiedWithLastRecordInPartition) {
  // given
  const auto cb = makeCb();
  const TopicToPartitionsMap tp = {{"topic", {0}}};
  EXPECT_CALL(*cb, interest).WillRepeatedly(Return(tp));
  EXPECT_CALL(*cb, receive(_))
      .Times(3)
      .WillOnce(Return(CallbackReply::AcceptedAndWantMore))
      .WillOnce(Return(CallbackReply::AcceptedAndWantMore))
      .WillOnce(Return(CallbackReply::AcceptedAndFinished));

  PartitionMap<InboundRecordSharedPtr> records;
  records[{"topic", 0}] = {makeRecord("topic", 0), makeRecord("topic", 0), makeRecord("topic", 0)};
  const auto testee = makeTestee(records);

  // when
  testee->processCallback(cb);

  // then
  // All messages got consumed.
  ASSERT_EQ(testee->getRecordCountForTest("topic", 0), -1);
  // Callback was not registered.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), -1);
}

TEST_F(RecordDistributorTest, ShouldNotRegisterCallbackIfItRejectsRecords) {
  // given
  const auto cb = makeCb();
  const TopicToPartitionsMap tp = {{"topic", {0, 1, 2}}};
  EXPECT_CALL(*cb, interest).WillRepeatedly(Return(tp));
  EXPECT_CALL(*cb, receive(_))
      .Times(3)
      .WillOnce(Return(CallbackReply::AcceptedAndWantMore))
      .WillOnce(Return(CallbackReply::AcceptedAndWantMore))
      .WillOnce(Return(CallbackReply::Rejected)); // Callback cancelled / abandoned / timed out.

  PartitionMap<InboundRecordSharedPtr> records;
  records[{"topic", 0}] = {makeRecord("topic", 0)};
  records[{"topic", 1}] = {makeRecord("topic", 1), makeRecord("topic", 1), makeRecord("topic", 1)};
  records[{"topic", 2}] = {makeRecord("topic", 2), makeRecord("topic", 2)};
  const auto testee = makeTestee(records);

  // when
  testee->processCallback(cb);

  // then
  // Some messages got consumed.
  ASSERT_EQ(testee->getRecordCountForTest("topic", 0), -1);
  ASSERT_EQ(testee->getRecordCountForTest("topic", 1), 2);
  ASSERT_EQ(testee->getRecordCountForTest("topic", 2), 2);
  // Callback was not registered.
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), -1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), -1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 2), -1);
}

TEST_F(RecordDistributorTest, ShouldRemoveCallbackProperly) {
  // given
  const auto callback = makeCb();
  PartitionMap<RecordCbSharedPtr> callbacks;
  callbacks[{"topic", 0}] = {callback};
  callbacks[{"topic", 1}] = {callback, makeCb()};
  callbacks[{"topic", 2}] = {makeCb(), callback};
  callbacks[{"topic", 3}] = {makeCb(), callback, makeCb()};
  const auto testee = makeTestee(callbacks);

  // when
  testee->removeCallback(callback);

  // then
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 0), -1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 1), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 2), 1);
  ASSERT_EQ(testee->getCallbackCountForTest("topic", 3), 2);
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/mocks/network/mocks.h"

#include "contrib/kafka/filters/network/source/mesh/filter.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::Return;
using testing::Throw;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {
namespace {

class MockRequestDecoder : public RequestDecoder {
public:
  MockRequestDecoder() : RequestDecoder{{}} {};
  MOCK_METHOD(void, onData, (Buffer::Instance&));
  MOCK_METHOD(void, reset, ());
};

using MockRequestDecoderSharedPtr = std::shared_ptr<MockRequestDecoder>;

class MockInFlightRequest : public InFlightRequest {
public:
  MOCK_METHOD(void, startProcessing, ());
  MOCK_METHOD(bool, finished, (), (const));
  MOCK_METHOD(AbstractResponseSharedPtr, computeAnswer, (), (const));
  MOCK_METHOD(void, abandon, ());
};

using Request = std::shared_ptr<MockInFlightRequest>;

class MockResponse : public AbstractResponse {
public:
  MockResponse() : AbstractResponse{ResponseMetadata{0, 0, 0}} {};
  MOCK_METHOD(uint32_t, computeSize, (), (const));
  MOCK_METHOD(uint32_t, encode, (Buffer::Instance & dst), (const));
};

class FilterUnitTest : public testing::Test {
protected:
  MockRequestDecoderSharedPtr request_decoder_ = std::make_shared<MockRequestDecoder>();
  KafkaMeshFilter testee_{request_decoder_};

  NiceMock<Network::MockReadFilterCallbacks> filter_callbacks_;

  // Helper: computed response for any kind of request.
  std::shared_ptr<MockResponse> computed_response_ = std::make_shared<NiceMock<MockResponse>>();

  void initialize() {
    testee_.initializeReadFilterCallbacks(filter_callbacks_);
    testee_.onNewConnection();
  }
};

TEST_F(FilterUnitTest, ShouldAcceptDataSentByKafkaClient) {
  // given
  Buffer::OwnedImpl data;
  EXPECT_CALL(*request_decoder_, onData(_));

  // when
  initialize();
  const auto result = testee_.onData(data, false);

  // then
  ASSERT_EQ(result, Network::FilterStatus::StopIteration);
  // Also, request_decoder got invoked.
}

TEST_F(FilterUnitTest, ShouldStopIterationIfProcessingDataFromKafkaClientFails) {
  // given
  Buffer::OwnedImpl data;
  EXPECT_CALL(*request_decoder_, onData(_)).WillOnce(Throw(EnvoyException("boom")));
  EXPECT_CALL(*request_decoder_, reset());

  // when
  initialize();
  const auto result = testee_.onData(data, false);

  // then
  ASSERT_EQ(result, Network::FilterStatus::StopIteration);
}

TEST_F(FilterUnitTest, ShouldAcceptAndAbandonRequests) {
  // given
  initialize();
  Request request1 = std::make_shared<Request::element_type>();
  testee_.getRequestsInFlightForTest().push_back(request1);
  EXPECT_CALL(*request1, abandon());
  Request request2 = std::make_shared<Request::element_type>();
  testee_.getRequestsInFlightForTest().push_back(request2);
  EXPECT_CALL(*request2, abandon());

  // when, then - requests get abandoned in destructor.
}

TEST_F(FilterUnitTest, ShouldAcceptAndAbandonRequestsOnConnectionClose) {
  // given
  initialize();
  Request request1 = std::make_shared<Request::element_type>();
  testee_.getRequestsInFlightForTest().push_back(request1);
  EXPECT_CALL(*request1, abandon());
  Request request2 = std::make_shared<Request::element_type>();
  testee_.getRequestsInFlightForTest().push_back(request2);
  EXPECT_CALL(*request2, abandon());

  // when
  testee_.onEvent(Network::ConnectionEvent::LocalClose);

  // then - requests get abandoned (only once).
}

TEST_F(FilterUnitTest, ShouldAcceptAndProcessRequests) {
  // given
  initialize();
  Request request = std::make_shared<Request::element_type>();
  EXPECT_CALL(*request, startProcessing());
  EXPECT_CALL(*request, finished()).WillOnce(Return(true));
  EXPECT_CALL(*request, computeAnswer()).WillOnce(Return(computed_response_));

  EXPECT_CALL(filter_callbacks_.connection_, write(_, false));

  // when - 1
  testee_.onRequest(request);

  // then - 1
  ASSERT_EQ(testee_.getRequestsInFlightForTest().size(), 1);

  // when - 2
  testee_.onRequestReadyForAnswer();

  // then - 2
  ASSERT_EQ(testee_.getRequestsInFlightForTest().size(), 0);
}

// This is important - we have two requests, but it is the second one that finishes processing
// first. As Kafka protocol uses sequence numbers, we need to wait until the first finishes.
TEST_F(FilterUnitTest, ShouldAcceptAndProcessRequestsInOrder) {
  // given
  initialize();
  Request request1 = std::make_shared<Request::element_type>();
  Request request2 = std::make_shared<Request::element_type>();
  testee_.getRequestsInFlightForTest().push_back(request1);
  testee_.getRequestsInFlightForTest().push_back(request2);

  EXPECT_CALL(*request1, finished()).WillOnce(Return(false)).WillOnce(Return(true));
  EXPECT_CALL(*request2, finished()).WillOnce(Return(true));
  EXPECT_CALL(*request1, computeAnswer()).WillOnce(Return(computed_response_));
  EXPECT_CALL(*request2, computeAnswer()).WillOnce(Return(computed_response_));
  EXPECT_CALL(filter_callbacks_.connection_, write(_, false)).Times(2);

  // when - 1
  testee_.onRequestReadyForAnswer();

  // then - 1
  ASSERT_EQ(testee_.getRequestsInFlightForTest().size(), 2);

  // when - 2
  testee_.onRequestReadyForAnswer();

  // then - 2
  ASSERT_EQ(testee_.getRequestsInFlightForTest().size(), 0);
}

TEST_F(FilterUnitTest, ShouldDoNothingOnBufferWatermarkEvents) {
  // given
  initialize();

  // when, then - nothing happens.
  testee_.onBelowWriteBufferLowWatermark();
  testee_.onAboveWriteBufferHighWatermark();
}

class MockUpstreamKafkaConfiguration : public UpstreamKafkaConfiguration {
public:
  MOCK_METHOD(void, onData, (Buffer::Instance&));
  MOCK_METHOD(void, reset, ());
  MOCK_METHOD(absl::optional<ClusterConfig>, computeClusterConfigForTopic,
              (const std::string& topic), (const));
  MOCK_METHOD((std::pair<std::string, int32_t>), getAdvertisedAddress, (), (const));
};

class MockUpstreamKafkaFacade : public UpstreamKafkaFacade {
public:
  MOCK_METHOD(KafkaProducer&, getProducerForTopic, (const std::string&));
};

class MockRecordCallbackProcessor : public RecordCallbackProcessor {
public:
  MOCK_METHOD(void, processCallback, (const RecordCbSharedPtr&));
  MOCK_METHOD(void, removeCallback, (const RecordCbSharedPtr&));
};

TEST(Filter, ShouldBeConstructable) {
  // given
  MockUpstreamKafkaConfiguration configuration;
  MockUpstreamKafkaFacade upstream_kafka_facade;
  MockRecordCallbackProcessor record_callback_processor;

  // when
  KafkaMeshFilter filter =
      KafkaMeshFilter(configuration, upstream_kafka_facade, record_callback_processor);

  // then - no exceptions.
}

} // namespace
} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_cc_test_library",
    "envoy_contrib_package",
)
load(
    "//bazel:envoy_internal.bzl",
    "envoy_external_dep_path",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "config_unit_test",
    srcs = ["config_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh:config_lib",
        "//test/mocks/server:factory_context_mocks",
    ],
)

envoy_cc_test(
    name = "filter_unit_test",
    srcs = ["filter_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh:filter_lib",
        "//test/mocks/network:network_mocks",
    ],
)

envoy_cc_test(
    name = "request_processor_unit_test",
    srcs = ["request_processor_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh:request_processor_lib",
        "//test/mocks/network:network_mocks",
    ],
)

envoy_cc_test(
    name = "abstract_command_unit_test",
    srcs = ["abstract_command_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh:abstract_command_lib",
    ],
)

envoy_cc_test(
    name = "upstream_kafka_facade_unit_test",
    srcs = ["upstream_kafka_facade_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh:upstream_kafka_facade_lib",
        "//test/mocks/thread_local:thread_local_mocks",
        "//test/test_common:thread_factory_for_test_lib",
    ],
)

envoy_cc_test(
    name = "upstream_kafka_client_impl_unit_test",
    srcs = ["upstream_kafka_client_impl_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        ":kafka_mocks_lib",
        "//contrib/kafka/filters/network/source/mesh:upstream_kafka_client_impl_lib",
        "//test/mocks/event:event_mocks",
        "//test/test_common:thread_factory_for_test_lib",
    ],
)

envoy_cc_test(
    name = "shared_consumer_manager_impl_unit_test",
    srcs = ["shared_consumer_manager_impl_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh:shared_consumer_manager_impl_lib",
    ],
)

envoy_cc_test(
    name = "upstream_kafka_consumer_impl_unit_test",
    srcs = ["upstream_kafka_consumer_impl_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        ":kafka_mocks_lib",
        "//contrib/kafka/filters/network/source/mesh:upstream_kafka_consumer_impl_lib",
        "//test/test_common:thread_factory_for_test_lib",
    ],
)

envoy_cc_test_library(
    name = "kafka_mocks_lib",
    srcs = [],
    hdrs = ["kafka_mocks.h"],
    tags = ["skip_on_windows"],
    deps = [
        envoy_external_dep_path("librdkafka"),
        "//contrib/kafka/filters/network/source/mesh:librdkafka_utils_lib",
    ],
)

envoy_cc_test(
    name = "upstream_config_unit_test",
    srcs = ["upstream_config_unit_test.cc"],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source/mesh:upstream_config_lib",
    ],
)
#include "source/common/protobuf/utility.h"

#include "test/test_common/utility.h"

#include "contrib/kafka/filters/network/source/mesh/upstream_config.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

TEST(UpstreamKafkaConfigurationTest, shouldThrowIfNoKafkaClusters) {
  // given
  KafkaMeshProtoConfig proto_config;

  // when
  // then - exception gets thrown
  EXPECT_THROW_WITH_REGEX(UpstreamKafkaConfigurationImpl{proto_config}, EnvoyException,
                          "at least one upstream Kafka cluster");
}

TEST(UpstreamKafkaConfigurationTest, shouldThrowIfKafkaClustersWithSameName) {
  // given
  const std::string yaml = R"EOF(
advertised_host: mock
advertised_port: 1
upstream_clusters:
- cluster_name: REPEATEDNAME
  bootstrap_servers: mock
  partition_count : 1
- cluster_name: REPEATEDNAME
  bootstrap_servers: mock
  partition_count : 1
forwarding_rules:
  )EOF";
  KafkaMeshProtoConfig proto_config;
  TestUtility::loadFromYamlAndValidate(yaml, proto_config);

  // when
  // then - exception gets thrown
  EXPECT_THROW_WITH_REGEX(UpstreamKafkaConfigurationImpl{proto_config}, EnvoyException,
                          "multiple Kafka clusters referenced by the same name");
}

TEST(UpstreamKafkaConfigurationTest, shouldThrowIfNoForwardingRules) {
  // given
  const std::string yaml = R"EOF(
advertised_host: mock_host
advertised_port: 42
upstream_clusters:
- cluster_name: mock
  bootstrap_servers: mock
  partition_count : 1
forwarding_rules:
  )EOF";
  KafkaMeshProtoConfig proto_config;
  TestUtility::loadFromYamlAndValidate(yaml, proto_config);

  // when
  // then - exception gets thrown
  EXPECT_THROW_WITH_REGEX(UpstreamKafkaConfigurationImpl{proto_config}, EnvoyException,
                          "at least one forwarding rule");
}

TEST(UpstreamKafkaConfigurationTest, shouldThrowIfForwardingRuleWithUnknownTarget) {
  // given
  const std::string yaml = R"EOF(
advertised_host: mock_host
advertised_port: 42
upstream_clusters:
- cluster_name: mock
  bootstrap_servers: mock
  partition_count : 1
forwarding_rules:
- target_cluster: BADNAME
  topic_prefix: mock
  )EOF";
  KafkaMeshProtoConfig proto_config;
  TestUtility::loadFromYamlAndValidate(yaml, proto_config);

  // when
  // then - exception gets thrown
  EXPECT_THROW_WITH_REGEX(UpstreamKafkaConfigurationImpl{proto_config}, EnvoyException,
                          "forwarding rule is referencing unknown upstream Kafka cluster");
}

TEST(UpstreamKafkaConfigurationTest, shouldBehaveProperly) {
  // given
  const std::string yaml = R"EOF(
advertised_host: mock_host
advertised_port: 42
upstream_clusters:
- cluster_name: cluster1
  bootstrap_servers: s1
  partition_count : 1
- cluster_name: cluster2
  bootstrap_servers: s2
  partition_count : 2
forwarding_rules:
- target_cluster: cluster1
  topic_prefix: prefix1
- target_cluster: cluster2
  topic_prefix: prefix2
  )EOF";
  KafkaMeshProtoConfig proto_config;
  TestUtility::loadFromYamlAndValidate(yaml, proto_config);
  const UpstreamKafkaConfiguration& testee = UpstreamKafkaConfigurationImpl{proto_config};

  const ClusterConfig cluster1 = {"cluster1",
                                  1,
                                  {{"bootstrap.servers", "s1"}},
                                  {{"bootstrap.servers", "s1"}, {"group.id", "envoy"}}};
  const ClusterConfig cluster2 = {"cluster2",
                                  2,
                                  {{"bootstrap.servers", "s2"}},
                                  {{"bootstrap.servers", "s2"}, {"group.id", "envoy"}}};

  // when, then (advertised address is returned properly)
  const auto address = testee.getAdvertisedAddress();
  EXPECT_EQ(address.first, "mock_host");
  EXPECT_EQ(address.second, 42);

  // when, then (matching prefix with something more)
  const auto res1 = testee.computeClusterConfigForTopic("prefix1somethingmore");
  ASSERT_TRUE(res1.has_value());
  EXPECT_EQ(*res1, cluster1);

  // when, then (matching prefix alone)
  const auto res2 = testee.computeClusterConfigForTopic("prefix1");
  ASSERT_TRUE(res2.has_value());
  EXPECT_EQ(*res2, cluster1);

  // when, then (failing to match first rule, but then matching the second one)
  const auto res3 = testee.computeClusterConfigForTopic("prefix2somethingmore");
  ASSERT_TRUE(res3.has_value());
  EXPECT_EQ(*res3, cluster2);

  // when, then (no rules match)
  const auto res4 = testee.computeClusterConfigForTopic("someotherthing");
  EXPECT_FALSE(res4.has_value());
}

TEST(UpstreamKafkaConfigurationTest, shouldBehaveProperlyWithCustomConfigs) {
  // given
  const std::string yaml = R"EOF(
advertised_host: mock_host
advertised_port: 42
upstream_clusters:
- cluster_name: cluster1
  bootstrap_servers: s1
  partition_count : 1
  producer_config:
    p1: "111"
- cluster_name: cluster2
  bootstrap_servers: s2
  partition_count : 2
  consumer_config:
    p2: "222"
    group.id: "custom-value"
forwarding_rules:
- target_cluster: cluster1
  topic_prefix: prefix1
- target_cluster: cluster2
  topic_prefix: prefix2
  )EOF";
  KafkaMeshProtoConfig proto_config;
  TestUtility::loadFromYamlAndValidate(yaml, proto_config);
  const UpstreamKafkaConfiguration& testee = UpstreamKafkaConfigurationImpl{proto_config};

  const ClusterConfig cluster1 = {"cluster1",
                                  1,
                                  {{"bootstrap.servers", "s1"}, {"p1", "111"}},
                                  {{"bootstrap.servers", "s1"}, {"group.id", "envoy"}}};
  const ClusterConfig cluster2 = {
      "cluster2",
      2,
      {{"bootstrap.servers", "s2"}},
      {{"bootstrap.servers", "s2"}, {"group.id", "custom-value"}, {"p2", "222"}}};

  // when, then (advertised address is returned properly)
  const auto address = testee.getAdvertisedAddress();
  EXPECT_EQ(address.first, "mock_host");
  EXPECT_EQ(address.second, 42);

  // when, then (matching prefix with something more)
  const auto res1 = testee.computeClusterConfigForTopic("prefix1somethingmore");
  ASSERT_TRUE(res1.has_value());
  EXPECT_EQ(*res1, cluster1);

  // when, then (matching prefix alone)
  const auto res2 = testee.computeClusterConfigForTopic("prefix1");
  ASSERT_TRUE(res2.has_value());
  EXPECT_EQ(*res2, cluster1);

  // when, then (failing to match first rule, but then matching the second one)
  const auto res3 = testee.computeClusterConfigForTopic("prefix2somethingmore");
  ASSERT_TRUE(res3.has_value());
  EXPECT_EQ(*res3, cluster2);

  // when, then (no rules match)
  const auto res4 = testee.computeClusterConfigForTopic("someotherthing");
  EXPECT_FALSE(res4.has_value());
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils.h"
#include "gmock/gmock.h"
#include "librdkafka/rdkafkacpp.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// This file defines all librdkafka-related mocks.

class MockConsumerAssignment : public ConsumerAssignment {};

class MockLibRdKafkaUtils : public LibRdKafkaUtils {
public:
  MOCK_METHOD(RdKafka::Conf::ConfResult, setConfProperty,
              (RdKafka::Conf&, const std::string&, const std::string&, std::string&), (const));
  MOCK_METHOD(RdKafka::Conf::ConfResult, setConfDeliveryCallback,
              (RdKafka::Conf&, RdKafka::DeliveryReportCb*, std::string&), (const));
  MOCK_METHOD((std::unique_ptr<RdKafka::Producer>), createProducer,
              (RdKafka::Conf*, std::string& errstr), (const));
  MOCK_METHOD((std::unique_ptr<RdKafka::KafkaConsumer>), createConsumer,
              (RdKafka::Conf*, std::string& errstr), (const));
  MOCK_METHOD(RdKafka::Headers*, convertHeaders,
              ((const std::vector<std::pair<absl::string_view, absl::string_view>>&)), (const));
  MOCK_METHOD(void, deleteHeaders, (RdKafka::Headers * librdkafka_headers), (const));
  MOCK_METHOD(ConsumerAssignmentConstPtr, assignConsumerPartitions,
              (RdKafka::KafkaConsumer&, const std::string&, int32_t), (const));

  MockLibRdKafkaUtils() {
    ON_CALL(*this, convertHeaders(testing::_))
        .WillByDefault(testing::Return(headers_holder_.get()));
  }

private:
  std::unique_ptr<RdKafka::Headers> headers_holder_{RdKafka::Headers::create()};
};

// Base class for librdkafka objects.
class MockKafkaHandle : public virtual RdKafka::Handle {
public:
  MOCK_METHOD(std::string, name, (), (const));
  MOCK_METHOD(std::string, memberid, (), (const));
  MOCK_METHOD(int, poll, (int), ());
  MOCK_METHOD(int, outq_len, (), ());
  MOCK_METHOD(RdKafka::ErrorCode, metadata,
              (bool, const RdKafka::Topic*, RdKafka::Metadata**, int timout_ms), ());
  MOCK_METHOD(RdKafka::ErrorCode, pause, (std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, resume, (std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, query_watermark_offsets,
              (const std::string&, int32_t, int64_t*, int64_t*, int), ());
  MOCK_METHOD(RdKafka::ErrorCode, get_watermark_offsets,
              (const std::string&, int32_t, int64_t*, int64_t*), ());
  MOCK_METHOD(RdKafka::ErrorCode, offsetsForTimes, (std::vector<RdKafka::TopicPartition*>&, int),
              ());
  MOCK_METHOD(RdKafka::Queue*, get_partition_queue, (const RdKafka::TopicPartition*), ());
  MOCK_METHOD(RdKafka::ErrorCode, set_log_queue, (RdKafka::Queue*), ());
  MOCK_METHOD(void, yield, (), ());
  MOCK_METHOD(std::string, clusterid, (int), ());
  MOCK_METHOD(struct rd_kafka_s*, c_ptr, (), ());
  MOCK_METHOD(int32_t, controllerid, (int), ());
  MOCK_METHOD(RdKafka::ErrorCode, fatal_error, (std::string&), (const));
  MOCK_METHOD(RdKafka::ErrorCode, oauthbearer_set_token,
              (const std::string&, int64_t, const std::string&, const std::list<std::string>&,
               std::string&),
              ());
  MOCK_METHOD(RdKafka::ErrorCode, oauthbearer_set_token_failure, (const std::string&), ());
  MOCK_METHOD(void*, mem_malloc, (size_t), ());
  MOCK_METHOD(void, mem_free, (void*), ());
};

class MockKafkaProducer : public RdKafka::Producer, public MockKafkaHandle {
public:
  MOCK_METHOD(RdKafka::ErrorCode, produce,
              (RdKafka::Topic*, int32_t, int, void*, size_t, const std::string*, void*), ());
  MOCK_METHOD(RdKafka::ErrorCode, produce,
              (RdKafka::Topic*, int32_t, int, void*, size_t, const void*, size_t, void*), ());
  MOCK_METHOD(RdKafka::ErrorCode, produce,
              (const std::string, int32_t, int, void*, size_t, const void*, size_t, int64_t, void*),
              ());
  MOCK_METHOD(RdKafka::ErrorCode, produce,
              (const std::string, int32_t, int, void*, size_t, const void*, size_t, int64_t,
               RdKafka::Headers*, void*),
              ());
  MOCK_METHOD(RdKafka::ErrorCode, produce,
              (RdKafka::Topic*, int32_t, const std::vector<char>*, const std::vector<char>*, void*),
              ());
  MOCK_METHOD(RdKafka::ErrorCode, flush, (int), ());
  MOCK_METHOD(RdKafka::ErrorCode, purge, (int), ());
  MOCK_METHOD(RdKafka::Error*, init_transactions, (int), ());
  MOCK_METHOD(RdKafka::Error*, begin_transaction, (), ());
  MOCK_METHOD(RdKafka::Error*, send_offsets_to_transaction,
              (const std::vector<RdKafka::TopicPartition*>&, const RdKafka::ConsumerGroupMetadata*,
               int),
              ());
  MOCK_METHOD(RdKafka::Error*, commit_transaction, (int), ());
  MOCK_METHOD(RdKafka::Error*, abort_transaction, (int), ());
  MOCK_METHOD(RdKafka::Error*, sasl_background_callbacks_enable, (), ());
  MOCK_METHOD(RdKafka::Queue*, get_sasl_queue, (), ());
  MOCK_METHOD(RdKafka::Queue*, get_background_queue, (), ());
  MOCK_METHOD(RdKafka::Error*, sasl_set_credentials, (const std::string&, const std::string&), ());
};

class MockKafkaMessage : public RdKafka::Message {
public:
  MOCK_METHOD(std::string, errstr, (), (const));
  MOCK_METHOD(RdKafka::ErrorCode, err, (), (const));
  MOCK_METHOD(RdKafka::Topic*, topic, (), (const));
  MOCK_METHOD(std::string, topic_name, (), (const));
  MOCK_METHOD(int32_t, partition, (), (const));
  MOCK_METHOD(void*, payload, (), (const));
  MOCK_METHOD(size_t, len, (), (const));
  MOCK_METHOD(const std::string*, key, (), (const));
  MOCK_METHOD(const void*, key_pointer, (), (const));
  MOCK_METHOD(size_t, key_len, (), (const));
  MOCK_METHOD(int64_t, offset, (), (const));
  MOCK_METHOD(RdKafka::MessageTimestamp, timestamp, (), (const));
  MOCK_METHOD(void*, msg_opaque, (), (const));
  MOCK_METHOD(int64_t, latency, (), (const));
  MOCK_METHOD(struct rd_kafka_message_s*, c_ptr, ());
  MOCK_METHOD(RdKafka::Message::Status, status, (), (const));
  MOCK_METHOD(RdKafka::Headers*, headers, ());
  MOCK_METHOD(RdKafka::Headers*, headers, (RdKafka::ErrorCode*));
  MOCK_METHOD(int32_t, broker_id, (), (const));
  MOCK_METHOD(int32_t, leader_epoch, (), (const));
  MOCK_METHOD(RdKafka::Error*, offset_store, ());
};

class MockKafkaConsumer : public RdKafka::KafkaConsumer, public MockKafkaHandle {
public:
  MOCK_METHOD(RdKafka::ErrorCode, assignment, (std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, subscription, (std::vector<std::string>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, subscribe, (const std::vector<std::string>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, unsubscribe, (), ());
  MOCK_METHOD(RdKafka::ErrorCode, assign, (const std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, unassign, (), ());
  MOCK_METHOD(RdKafka::Message*, consume, (int), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitSync, (), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitAsync, (), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitSync, (RdKafka::Message*), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitAsync, (RdKafka::Message*), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitSync, (std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitAsync, (const std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitSync, (RdKafka::OffsetCommitCb*), ());
  MOCK_METHOD(RdKafka::ErrorCode, commitSync,
              (std::vector<RdKafka::TopicPartition*>&, RdKafka::OffsetCommitCb*), ());
  MOCK_METHOD(RdKafka::ErrorCode, committed, (std::vector<RdKafka::TopicPartition*>&, int), ());
  MOCK_METHOD(RdKafka::ErrorCode, position, (std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ErrorCode, close, (), ());
  MOCK_METHOD(RdKafka::Error*, close, (RdKafka::Queue*), ());
  MOCK_METHOD(bool, closed, (), ());
  MOCK_METHOD(RdKafka::ErrorCode, seek, (const RdKafka::TopicPartition&, int), ());
  MOCK_METHOD(RdKafka::ErrorCode, offsets_store, (std::vector<RdKafka::TopicPartition*>&), ());
  MOCK_METHOD(RdKafka::ConsumerGroupMetadata*, groupMetadata, (), ());
  MOCK_METHOD(bool, assignment_lost, (), ());
  MOCK_METHOD(std::string, rebalance_protocol, (), ());
  MOCK_METHOD(RdKafka::Error*, incremental_assign, (const std::vector<RdKafka::TopicPartition*>&),
              ());
  MOCK_METHOD(RdKafka::Error*, incremental_unassign, (const std::vector<RdKafka::TopicPartition*>&),
              ());
  MOCK_METHOD(RdKafka::Error*, sasl_background_callbacks_enable, (), ());
  MOCK_METHOD(RdKafka::Queue*, get_sasl_queue, (), ());
  MOCK_METHOD(RdKafka::Queue*, get_background_queue, (), ());
  MOCK_METHOD(RdKafka::Error*, sasl_set_credentials, (const std::string&, const std::string&), ());
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/test_common/utility.h"

#include "contrib/kafka/filters/network/source/kafka_response_parser.h"
#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "contrib/kafka/filters/network/test/serialization_utilities.h"
#include "gmock/gmock.h"

using testing::_;
using testing::ContainsRegex;
using testing::Return;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace KafkaResponseParserTest {

const int32_t FAILED_DESERIALIZER_STEP = 13;

class KafkaResponseParserTest : public testing::Test, public BufferBasedTest {};

class MockResponseParserResolver : public ResponseParserResolver {
public:
  MockResponseParserResolver() = default;
  MOCK_METHOD(ResponseParserSharedPtr, createParser, (ResponseContextSharedPtr), (const));
};

class MockParser : public ResponseParser {
public:
  ResponseParseResponse parse(absl::string_view&) override {
    throw EnvoyException("should not be invoked");
  }
};

TEST_F(KafkaResponseParserTest, ResponseHeaderParserShouldExtractHeaderAndResolveNextParser) {
  // given
  const int32_t payload_length = 100;
  const int32_t correlation_id = 1234;
  uint32_t header_len = 0;
  header_len += putIntoBuffer(payload_length);
  header_len += putIntoBuffer(correlation_id); // Insert correlation id.

  const absl::string_view orig_data = putGarbageIntoBuffer();
  absl::string_view data = orig_data;

  const int16_t api_key = 42;
  const int16_t api_version = 123;

  ExpectedResponsesSharedPtr expected_responses = std::make_shared<ExpectedResponses>();
  (*expected_responses)[correlation_id] = {api_key, api_version};

  const MockResponseParserResolver parser_resolver;
  const ResponseParserSharedPtr parser{new MockParser{}};
  EXPECT_CALL(parser_resolver, createParser(_)).WillOnce(Return(parser));

  ResponseHeaderParser testee{expected_responses, parser_resolver};

  // when
  const ResponseParseResponse result = testee.parse(data);

  // then
  ASSERT_EQ(result.hasData(), true);
  ASSERT_EQ(result.next_parser_, parser);
  ASSERT_EQ(result.message_, nullptr);
  ASSERT_EQ(result.failure_data_, nullptr);

  const auto context = testee.contextForTest();
  ASSERT_EQ(context->remaining_response_size_, payload_length - sizeof(correlation_id));
  ASSERT_EQ(context->correlation_id_, correlation_id);
  ASSERT_EQ(context->api_key_, api_key);
  ASSERT_EQ(context->api_version_, api_version);

  ASSERT_EQ(expected_responses->size(), 0);

  assertStringViewIncrement(data, orig_data, header_len);
}

TEST_F(KafkaResponseParserTest, ResponseHeaderParserShouldThrowIfThereIsUnexpectedResponse) {
  // given
  const int32_t payload_length = 100;
  const int32_t correlation_id = 1234;
  putIntoBuffer(payload_length);
  putIntoBuffer(correlation_id); // Insert correlation id.

  absl::string_view data = putGarbageIntoBuffer();

  ExpectedResponsesSharedPtr expected_responses = std::make_shared<ExpectedResponses>();
  const MockResponseParserResolver parser_resolver;
  const ResponseParserSharedPtr parser{new MockParser{}};
  EXPECT_CALL(parser_resolver, createParser(_)).Times(0);

  ResponseHeaderParser testee{expected_responses, parser_resolver};

  // when
  // then - exception gets thrown.
  EXPECT_THAT_THROWS_MESSAGE(testee.parse(data), EnvoyException,
                             AllOf(ContainsRegex("no response metadata registered"),
                                   ContainsRegex(std::to_string(correlation_id))));
}

TEST_F(KafkaResponseParserTest, ResponseDataParserShoulRethrowDeserializerExceptionsDuringFeeding) {
  // given

  // This deserializer throws during feeding.
  class ThrowingDeserializer : public Deserializer<int32_t> {
  public:
    uint32_t feed(absl::string_view&) override {
      // Move some pointers to simulate data consumption.
      throw EnvoyException("deserializer-failure");
    };

    bool ready() const override { throw std::runtime_error("should not be invoked at all"); };

    int32_t get() const override { throw std::runtime_error("should not be invoked at all"); };
  };

  ResponseContextSharedPtr context = std::make_shared<ResponseContext>();
  ResponseDataParser<int32_t, ThrowingDeserializer> testee{context};

  absl::string_view data = putGarbageIntoBuffer();

  // when
  // then - exception gets thrown.
  EXPECT_THROW_WITH_MESSAGE(testee.parse(data), EnvoyException, "deserializer-failure");
}

// This deserializer consumes FAILED_DESERIALIZER_STEP bytes and returns 0
class SomeBytesDeserializer : public Deserializer<int32_t> {
public:
  uint32_t feed(absl::string_view& data) override {
    data = {data.data() + FAILED_DESERIALIZER_STEP, data.size() - FAILED_DESERIALIZER_STEP};
    return FAILED_DESERIALIZER_STEP;
  };

  bool ready() const override { return true; };

  int32_t get() const override { return 0; };
};

TEST_F(KafkaResponseParserTest,
       ResponseDataParserShouldHandleDeserializerReturningReadyButLeavingData) {
  // given
  const int32_t message_size = 1024; // There are still 1024 bytes to read to complete the message.
  ResponseContextSharedPtr context = std::make_shared<ResponseContext>();
  context->remaining_response_size_ = message_size;

  ResponseDataParser<int32_t, SomeBytesDeserializer> testee{context};

  const absl::string_view orig_data = putGarbageIntoBuffer();
  absl::string_view data = orig_data;

  // when
  const ResponseParseResponse result = testee.parse(data);

  // then
  ASSERT_EQ(result.hasData(), true);
  ASSERT_NE(std::dynamic_pointer_cast<SentinelResponseParser>(result.next_parser_), nullptr);
  ASSERT_EQ(result.message_, nullptr);
  ASSERT_EQ(result.failure_data_, nullptr);

  ASSERT_EQ(testee.contextForTest()->remaining_response_size_,
            message_size - FAILED_DESERIALIZER_STEP);

  assertStringViewIncrement(data, orig_data, FAILED_DESERIALIZER_STEP);
}

TEST_F(KafkaResponseParserTest, SentinelResponseParserShouldConsumeDataUntilEndOfMessage) {
  // given
  const int32_t response_len = 1000;
  ResponseContextSharedPtr context = std::make_shared<ResponseContext>();
  context->remaining_response_size_ = response_len;
  SentinelResponseParser testee{context};

  const absl::string_view orig_data = putGarbageIntoBuffer(response_len * 2);
  absl::string_view data = orig_data;

  // when
  const ResponseParseResponse result = testee.parse(data);

  // then
  ASSERT_EQ(result.hasData(), true);
  ASSERT_EQ(result.next_parser_, nullptr);
  ASSERT_EQ(result.message_, nullptr);
  ASSERT_NE(std::dynamic_pointer_cast<ResponseMetadata>(result.failure_data_), nullptr);

  ASSERT_EQ(testee.contextForTest()->remaining_response_size_, 0);

  assertStringViewIncrement(data, orig_data, response_len);
}

} // namespace KafkaResponseParserTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
clientPort={{ data['zk_port'] }}
dataDir={{ data['data_dir'] }}
maxClientCnxns=0
# ZK 3.5 tries to bind 8080 for introspection capacility - we do not need that.
admin.enableServer=false
broker.id=0
listeners=PLAINTEXT://127.0.0.1:{{ data['kafka_real_port'] }}
advertised.listeners=PLAINTEXT://127.0.0.1:{{ data['kafka_real_port'] }}

num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

log.dirs={{ data['data_dir'] }}
num.partitions=1
num.recovery.threads.per.data.dir=1

offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000

zookeeper.connect=127.0.0.1:{{ data['zk_port'] }}
zookeeper.connection.timeout.ms=6000

group.initial.rebalance.delay.ms=0

# The number of __consumer_offsets partitions is reduced to make logs a bit more readable.
offsets.topic.num.partitions=5
#!/usr/bin/python

import random
import os
import shutil
import socket
import subprocess
import tempfile
from threading import Thread, Semaphore
import time
import unittest

from kafka import KafkaAdminClient, KafkaConsumer, KafkaProducer, TopicPartition
from kafka.admin import ConfigResource, ConfigResourceType, NewPartitions, NewTopic
import urllib.request


class KafkaBrokerIntegrationTest(unittest.TestCase):
    """
  All tests in this class depend on Envoy/Zookeeper/Kafka running.
  For each of these tests we are going to create Kafka consumers/producers/admins and point them
  to Envoy (that proxies Kafka).
  We expect every operation to succeed (as they should reach Kafka) and the corresponding metrics
  to increase on Envoy side (to show that messages were received and forwarded successfully).
  """

    services = None

    @classmethod
    def setUpClass(cls):
        KafkaBrokerIntegrationTest.services = ServicesHolder()
        KafkaBrokerIntegrationTest.services.start()

    @classmethod
    def tearDownClass(cls):
        KafkaBrokerIntegrationTest.services.shut_down()

    def setUp(self):
        # We want to check if our services are okay before running any kind of test.
        KafkaBrokerIntegrationTest.services.check_state()
        self.metrics = MetricsHolder(self)

    def tearDown(self):
        # We want to check if our services are okay after running any test.
        KafkaBrokerIntegrationTest.services.check_state()

    @classmethod
    def kafka_address(cls):
        return '127.0.0.1:%s' % KafkaBrokerIntegrationTest.services.kafka_envoy_port

    @classmethod
    def envoy_stats_address(cls):
        return 'http://127.0.0.1:%s/stats' % KafkaBrokerIntegrationTest.services.envoy_monitoring_port

    def test_kafka_consumer_with_no_messages_received(self):
        """
    This test verifies that consumer sends fetches correctly, and receives nothing.
    """

        consumer = KafkaConsumer(
            bootstrap_servers=KafkaBrokerIntegrationTest.kafka_address(), fetch_max_wait_ms=500)
        consumer.assign([TopicPartition('test_kafka_consumer_with_no_messages_received', 0)])
        for _ in range(10):
            records = consumer.poll(timeout_ms=1000)
            self.assertEqual(len(records), 0)

        self.metrics.collect_final_metrics()
        # 'consumer.poll()' can translate into 0 or more fetch requests.
        # We have set API timeout to 1000ms, while fetch_max_wait is 500ms.
        # This means that consumer will send roughly 2 (1000/500) requests per API call (so 20 total).
        # So increase of 10 (half of that value) should be safe enough to test.
        self.metrics.assert_metric_increase('fetch', 10)
        # Metadata is used by consumer to figure out current partition leader.
        self.metrics.assert_metric_increase('metadata', 1)

    def test_kafka_producer_and_consumer(self):
        """
    This test verifies that producer can send messages, and consumer can receive them.
    """

        messages_to_send = 100
        partition = TopicPartition('test_kafka_producer_and_consumer', 0)

        producer = KafkaProducer(bootstrap_servers=KafkaBrokerIntegrationTest.kafka_address())
        for _ in range(messages_to_send):
            future = producer.send(
                value=b'some_message_bytes', topic=partition.topic, partition=partition.partition)
            send_status = future.get()
            self.assertTrue(send_status.offset >= 0)

        consumer = KafkaConsumer(
            bootstrap_servers=KafkaBrokerIntegrationTest.kafka_address(),
            auto_offset_reset='earliest',
            fetch_max_bytes=100)
        consumer.assign([partition])
        received_messages = []
        while (len(received_messages) < messages_to_send):
            poll_result = consumer.poll(timeout_ms=1000)
            received_messages += poll_result[partition]

        self.metrics.collect_final_metrics()
        self.metrics.assert_metric_increase('metadata', 2)
        self.metrics.assert_metric_increase('produce', 100)
        # 'fetch_max_bytes' was set to a very low value, so client will need to send a FetchRequest
        # multiple times to broker to get all 100 messages (otherwise all 100 records could have been
        # received in one go).
        self.metrics.assert_metric_increase('fetch', 20)
        # Both producer & consumer had to fetch cluster metadata.
        self.metrics.assert_metric_increase('metadata', 2)

    def test_consumer_with_consumer_groups(self):
        """
    This test verifies that multiple consumers can form a Kafka consumer group.
    """

        consumer_count = 10
        consumers = []
        for id in range(consumer_count):
            consumer = KafkaConsumer(
                bootstrap_servers=KafkaBrokerIntegrationTest.kafka_address(),
                group_id='test',
                client_id='test-%s' % id)
            consumer.subscribe(['test_consumer_with_consumer_groups'])
            consumers.append(consumer)

        worker_threads = []
        for consumer in consumers:
            thread = Thread(target=KafkaBrokerIntegrationTest.worker, args=(consumer,))
            thread.start()
            worker_threads.append(thread)

        for thread in worker_threads:
            thread.join()

        for consumer in consumers:
            consumer.close()

        self.metrics.collect_final_metrics()
        self.metrics.assert_metric_increase('api_versions', consumer_count)
        self.metrics.assert_metric_increase('metadata', consumer_count)
        self.metrics.assert_metric_increase('join_group', consumer_count)
        self.metrics.assert_metric_increase('find_coordinator', consumer_count)
        self.metrics.assert_metric_increase('leave_group', consumer_count)

    @staticmethod
    def worker(consumer):
        """
    Worker thread for Kafka consumer.
    Multiple poll-s are done here, so that the group can safely form.
    """

        poll_operations = 10
        for i in range(poll_operations):
            consumer.poll(timeout_ms=1000)

    def test_admin_client(self):
        """
    This test verifies that Kafka Admin Client can still be used to manage Kafka.
    """

        admin_client = KafkaAdminClient(
            bootstrap_servers=KafkaBrokerIntegrationTest.kafka_address())

        # Create a topic with 3 partitions.
        new_topic_spec = NewTopic(name='test_admin_client', num_partitions=3, replication_factor=1)
        create_response = admin_client.create_topics([new_topic_spec])
        error_data = create_response.topic_errors
        self.assertEqual(len(error_data), 1)
        self.assertEqual(error_data[0], (new_topic_spec.name, 0, None))

        # Alter topic (change some Kafka-level property).
        config_resource = ConfigResource(
            ConfigResourceType.TOPIC, new_topic_spec.name, {'flush.messages': 42})
        alter_response = admin_client.alter_configs([config_resource])
        error_data = alter_response.resources
        self.assertEqual(len(error_data), 1)
        self.assertEqual(error_data[0][0], 0)

        # Add 2 more partitions to topic.
        new_partitions_spec = {new_topic_spec.name: NewPartitions(5)}
        new_partitions_response = admin_client.create_partitions(new_partitions_spec)
        error_data = create_response.topic_errors
        self.assertEqual(len(error_data), 1)
        self.assertEqual(error_data[0], (new_topic_spec.name, 0, None))

        # Delete a topic.
        delete_response = admin_client.delete_topics([new_topic_spec.name])
        error_data = create_response.topic_errors
        self.assertEqual(len(error_data), 1)
        self.assertEqual(error_data[0], (new_topic_spec.name, 0, None))

        self.metrics.collect_final_metrics()
        self.metrics.assert_metric_increase('create_topics', 1)
        self.metrics.assert_metric_increase('alter_configs', 1)
        self.metrics.assert_metric_increase('create_partitions', 1)
        self.metrics.assert_metric_increase('delete_topics', 1)


class MetricsHolder:
    """
  Utility for storing Envoy metrics.
  Expected to be created before the test (to get initial metrics), and then to collect them at the
  end of test, so the expected increases can be verified.
  """

    def __init__(self, owner):
        self.owner = owner
        self.initial_requests, self.inital_responses = MetricsHolder.get_envoy_stats()
        self.final_requests = None
        self.final_responses = None

    def collect_final_metrics(self):
        self.final_requests, self.final_responses = MetricsHolder.get_envoy_stats()

    def assert_metric_increase(self, message_type, count):
        request_type = message_type + '_request'
        response_type = message_type + '_response'

        initial_request_value = self.initial_requests.get(request_type, 0)
        final_request_value = self.final_requests.get(request_type, 0)
        self.owner.assertGreaterEqual(final_request_value, initial_request_value + count)

        initial_response_value = self.inital_responses.get(response_type, 0)
        final_response_value = self.final_responses.get(response_type, 0)
        self.owner.assertGreaterEqual(final_response_value, initial_response_value + count)

    @staticmethod
    def get_envoy_stats():
        """
    Grab request/response metrics from envoy's stats interface.
    """

        stats_url = KafkaBrokerIntegrationTest.envoy_stats_address()
        requests = {}
        responses = {}
        with urllib.request.urlopen(stats_url) as remote_metrics_url:
            payload = remote_metrics_url.read().decode()
            lines = payload.splitlines()
            for line in lines:
                request_prefix = 'kafka.testfilter.request.'
                response_prefix = 'kafka.testfilter.response.'
                if line.startswith(request_prefix):
                    data = line[len(request_prefix):].split(': ')
                    requests[data[0]] = int(data[1])
                    pass
                if line.startswith(response_prefix) and '_response:' in line:
                    data = line[len(response_prefix):].split(': ')
                    responses[data[0]] = int(data[1])
        return [requests, responses]


class ServicesHolder:
    """
  Utility class for setting up our external dependencies: Envoy, Zookeeper & Kafka.
  """

    def __init__(self):
        self.kafka_tmp_dir = None

        self.envoy_worker = None
        self.zk_worker = None
        self.kafka_worker = None

    @staticmethod
    def get_random_listener_port():
        """
    Here we count on OS to give us some random socket.
    Obviously this method will need to be invoked in a try loop anyways, as in degenerate scenario
    someone else might have bound to it after we had closed the socket and before the service
    that's supposed to use it binds to it.
    """

        import socket
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:
            server_socket.bind(('0.0.0.0', 0))
            socket_port = server_socket.getsockname()[1]
            print('returning %s' % socket_port)
            return socket_port

    def start(self):
        """
    Starts all the services we need for integration tests.
    """

        # Find java installation that we are going to use to start Zookeeper & Kafka.
        java_directory = ServicesHolder.find_java()

        launcher_environment = os.environ.copy()
        # Make `java` visible to build script:
        # https://github.com/apache/kafka/blob/2.2.0/bin/kafka-run-class.sh#L226
        new_path = os.path.abspath(java_directory) + os.pathsep + launcher_environment['PATH']
        launcher_environment['PATH'] = new_path
        # Both ZK & Kafka use Kafka launcher script.
        # By default it sets up JMX options:
        # https://github.com/apache/kafka/blob/2.2.0/bin/kafka-run-class.sh#L167
        # But that forces the JVM to load file that is not present due to:
        # https://docs.oracle.com/javase/9/management/monitoring-and-management-using-jmx-technology.htm
        # Let's make it simple and just disable JMX.
        launcher_environment['KAFKA_JMX_OPTS'] = ' '

        # Setup a temporary directory, which will be used by Kafka & Zookeeper servers.
        self.kafka_tmp_dir = tempfile.mkdtemp()
        print('Temporary directory used for tests: ' + self.kafka_tmp_dir)

        # This directory will store the configuration files fed to services.
        config_dir = self.kafka_tmp_dir + '/config'
        os.mkdir(config_dir)
        # This directory will store Zookeeper's data (== Kafka server metadata).
        zookeeper_store_dir = self.kafka_tmp_dir + '/zookeeper_data'
        os.mkdir(zookeeper_store_dir)
        # This directory will store Kafka's data (== partitions).
        kafka_store_dir = self.kafka_tmp_dir + '/kafka_data'
        os.mkdir(kafka_store_dir)

        # Find the Kafka server 'bin' directory.
        kafka_bin_dir = os.path.join('.', 'external', 'kafka_server_binary', 'bin')

        # Main initialization block:
        # - generate random ports,
        # - render configuration with these ports,
        # - start services and check if they are running okay,
        # - if anything is having problems, kill everything and start again.
        while True:

            # Generate random ports.
            zk_port = ServicesHolder.get_random_listener_port()
            kafka_real_port = ServicesHolder.get_random_listener_port()
            kafka_envoy_port = ServicesHolder.get_random_listener_port()
            envoy_monitoring_port = ServicesHolder.get_random_listener_port()

            # These ports need to be exposed to tests.
            self.kafka_envoy_port = kafka_envoy_port
            self.envoy_monitoring_port = envoy_monitoring_port

            # Render config file for Envoy.
            template = RenderingHelper.get_template('envoy_config_yaml.j2')
            contents = template.render(
                data={
                    'kafka_real_port': kafka_real_port,
                    'kafka_envoy_port': kafka_envoy_port,
                    'envoy_monitoring_port': envoy_monitoring_port
                })
            envoy_config_file = os.path.join(config_dir, 'envoy_config.yaml')
            with open(envoy_config_file, 'w') as fd:
                fd.write(contents)
                print('Envoy config file rendered at: ' + envoy_config_file)

            # Render config file for Zookeeper.
            template = RenderingHelper.get_template('zookeeper_properties.j2')
            contents = template.render(data={'data_dir': zookeeper_store_dir, 'zk_port': zk_port})
            zookeeper_config_file = os.path.join(config_dir, 'zookeeper.properties')
            with open(zookeeper_config_file, 'w') as fd:
                fd.write(contents)
                print('Zookeeper config file rendered at: ' + zookeeper_config_file)

            # Render config file for Kafka.
            template = RenderingHelper.get_template('kafka_server_properties.j2')
            contents = template.render(
                data={
                    'data_dir': kafka_store_dir,
                    'zk_port': zk_port,
                    'kafka_real_port': kafka_real_port,
                    'kafka_envoy_port': kafka_envoy_port
                })
            kafka_config_file = os.path.join(config_dir, 'kafka_server.properties')
            with open(kafka_config_file, 'w') as fd:
                fd.write(contents)
                print('Kafka config file rendered at: ' + kafka_config_file)

            # Start the services now.
            try:

                # Start Envoy in the background, pointing to rendered config file.
                envoy_binary = ServicesHolder.find_envoy()
                # --base-id is added to allow multiple Envoy instances to run at the same time.
                envoy_args = [
                    os.path.abspath(envoy_binary), '-c', envoy_config_file, '--base-id',
                    str(random.randint(1, 999999))
                ]
                envoy_handle = subprocess.Popen(
                    envoy_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.envoy_worker = ProcessWorker(
                    envoy_handle, 'Envoy', 'starting main dispatch loop')
                self.envoy_worker.await_startup()

                # Start Zookeeper in background, pointing to rendered config file.
                zk_binary = os.path.join(kafka_bin_dir, 'zookeeper-server-start.sh')
                zk_args = [os.path.abspath(zk_binary), zookeeper_config_file]
                zk_handle = subprocess.Popen(
                    zk_args,
                    env=launcher_environment,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE)
                self.zk_worker = ProcessWorker(zk_handle, 'Zookeeper', 'binding to port')
                self.zk_worker.await_startup()

                # Start Kafka in background, pointing to rendered config file.
                kafka_binary = os.path.join(kafka_bin_dir, 'kafka-server-start.sh')
                kafka_args = [os.path.abspath(kafka_binary), kafka_config_file]
                kafka_handle = subprocess.Popen(
                    kafka_args,
                    env=launcher_environment,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE)
                self.kafka_worker = ProcessWorker(
                    kafka_handle, 'Kafka', '[KafkaServer id=0] started')
                self.kafka_worker.await_startup()

                # All services have started without problems - now we can finally finish.
                break

            except Exception as e:
                print('Could not start services, will try again', e)

                if self.kafka_worker:
                    self.kafka_worker.kill()
                    self.kafka_worker = None
                if self.zk_worker:
                    self.zk_worker.kill()
                    self.zk_worker = None
                if self.envoy_worker:
                    self.envoy_worker.kill()
                    self.envoy_worker = None

    @staticmethod
    def find_java():
        """
    This method just locates the Java installation in current directory.
    We cannot hardcode the name, as the dirname changes as per:
    https://github.com/bazelbuild/bazel/blob/master/tools/jdk/BUILD#L491
    """

        external_dir = os.path.join('.', 'external')
        for directory in os.listdir(external_dir):
            if 'remotejdk11' in directory:
                result = os.path.join(external_dir, directory, 'bin')
                print('Using Java: ' + result)
                return result
        raise Exception('Could not find Java in: ' + external_dir)

    @staticmethod
    def find_envoy():
        """
        This method locates envoy binary.
        It's present at ./source/exe/envoy-static (at least for mac/bazel-asan/bazel-tsan),
        or at ./external/envoy/source/exe/envoy-static (for bazel-compile_time_options).
        """

        candidate = os.path.join('.', 'contrib', 'exe', 'envoy-static')
        if os.path.isfile(candidate):
            return candidate
        candidate = os.path.join('.', 'external', 'envoy', 'contrib', 'exe', 'envoy-static')
        if os.path.isfile(candidate):
            return candidate
        raise Exception("Could not find Envoy")

    def shut_down(self):
        # Teardown - kill Kafka, Zookeeper, and Envoy. Then delete their data directory.
        print('Cleaning up')

        if self.kafka_worker:
            self.kafka_worker.kill()

        if self.zk_worker:
            self.zk_worker.kill()

        if self.envoy_worker:
            self.envoy_worker.kill()

        if self.kafka_tmp_dir:
            print('Removing temporary directory: ' + self.kafka_tmp_dir)
            shutil.rmtree(self.kafka_tmp_dir)

    def check_state(self):
        self.envoy_worker.check_state()
        self.zk_worker.check_state()
        self.kafka_worker.check_state()


class ProcessWorker:
    """
  Helper class that wraps the external service process.
  Provides ability to wait until service is ready to use (this is done by tracing logs) and
  printing service's output to stdout.
  """

    # Service is considered to be properly initialized after it has logged its startup message
    # and has been alive for INITIALIZATION_WAIT_SECONDS after that message has been seen.
    # This (clunky) design is needed because Zookeeper happens to log "binding to port" and then
    # might fail to bind.
    INITIALIZATION_WAIT_SECONDS = 3

    def __init__(self, process_handle, name, startup_message):
        # Handle to process and pretty name.
        self.process_handle = process_handle
        self.name = name

        self.startup_message = startup_message
        self.startup_message_ts = None

        # Semaphore raised when startup has finished and information regarding startup's success.
        self.initialization_semaphore = Semaphore(value=0)
        self.initialization_ok = False

        self.state_worker = Thread(target=ProcessWorker.initialization_worker, args=(self,))
        self.state_worker.start()
        self.out_worker = Thread(
            target=ProcessWorker.pipe_handler, args=(self, self.process_handle.stdout, 'out'))
        self.out_worker.start()
        self.err_worker = Thread(
            target=ProcessWorker.pipe_handler, args=(self, self.process_handle.stderr, 'err'))
        self.err_worker.start()

    @staticmethod
    def initialization_worker(owner):
        """
    Worker thread.
    Responsible for detecting if service died during initialization steps and ensuring if enough
    time has passed since the startup message has been seen.
    When either of these happens, we just raise the initialization semaphore.
    """

        while True:
            status = owner.process_handle.poll()
            if status:
                # Service died.
                print('%s did not initialize properly - finished with: %s' % (owner.name, status))
                owner.initialization_ok = False
                owner.initialization_semaphore.release()
                break
            else:
                # Service is still running.
                startup_message_ts = owner.startup_message_ts
                if startup_message_ts:
                    # The log message has been registered (by pipe_handler thread), let's just ensure that
                    # some time has passed and mark the service as running.
                    current_time = int(round(time.time()))
                    if current_time - startup_message_ts >= ProcessWorker.INITIALIZATION_WAIT_SECONDS:
                        print(
                            'Startup message seen %s seconds ago, and service is still running' %
                            (ProcessWorker.INITIALIZATION_WAIT_SECONDS),
                            flush=True)
                        owner.initialization_ok = True
                        owner.initialization_semaphore.release()
                        break
            time.sleep(1)
        print('Initialization worker for %s has finished' % (owner.name))

    @staticmethod
    def pipe_handler(owner, pipe, pipe_name):
        """
    Worker thread.
    If a service startup message is seen, then it just registers the timestamp of its appearance.
    Also prints every received message.
    """

        try:
            for raw_line in pipe:
                line = raw_line.decode().rstrip()
                print('%s(%s):' % (owner.name, pipe_name), line, flush=True)
                if owner.startup_message in line:
                    print(
                        '%s initialization message [%s] has been logged' %
                        (owner.name, owner.startup_message))
                    owner.startup_message_ts = int(round(time.time()))
        finally:
            pipe.close()
        print('Pipe handler for %s(%s) has finished' % (owner.name, pipe_name))

    def await_startup(self):
        """
    Awaits on initialization semaphore, and then verifies the initialization state.
    If everything is okay, we just continue (we can use the service), otherwise throw.
    """

        print('Waiting for %s to start...' % (self.name))
        self.initialization_semaphore.acquire()
        try:
            if self.initialization_ok:
                print('Service %s started successfully' % (self.name))
            else:
                raise Exception('%s could not start' % (self.name))
        finally:
            self.initialization_semaphore.release()

    def check_state(self):
        """
    Verifies if the service is still running. Throws if it is not.
    """

        status = self.process_handle.poll()
        if status:
            raise Exception('%s died with: %s' % (self.name, str(status)))

    def kill(self):
        """
    Utility method to kill the main service thread and all related workers.
    """

        print('Stopping service %s' % self.name)

        # Kill the real process.
        self.process_handle.kill()
        self.process_handle.wait()

        # The sub-workers are going to finish on their own, as they will detect main thread dying
        # (through pipes closing, or .poll() returning a non-null value).
        self.state_worker.join()
        self.out_worker.join()
        self.err_worker.join()

        print('Service %s has been stopped' % self.name)


class RenderingHelper:
    """
  Helper for jinja templates.
  """

    @staticmethod
    def get_template(template):
        import jinja2
        import os
        import sys
        # Templates are resolved relatively to main start script, due to main & test templates being
        # stored in different directories.
        env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(searchpath=os.path.dirname(os.path.abspath(__file__))))
        return env.get_template(template)


if __name__ == '__main__':
    unittest.main()
Kafka broker integration test
=============================

The code in this directory provides `kafka_broker_integration_test.py`
which is used to launch full integration test for Kafka broker.

The Python script allocates starts Envoy, Zookeeper, and Kafka as separate
processes, all of them listening on randomly-allocated ports.
Afterwards, the Python Kafka consumers and producers are initialized and
do run the traffic through Kafka.

The tests verify if:
- Kafka operations behave properly (get expected results, no exceptions),
- Kafka metrics in Envoy show proper increases.

**Right now this test is not executed as a part of normal build, and needs to be invoked manually.**

**Please re-run this test if you are making any changes to Kafka-related code:**

```
bazel test \
	//contrib/kafka/filters/network/test/broker/integration_test:kafka_broker_integration_test \
	--runs_per_test 1000
```
static_resources:
  listeners:
  - address:
      socket_address:
        address: 127.0.0.1
        port_value: {{ data['kafka_envoy_port'] }}
    filter_chains:
    - filters:
      - name: kafka
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.network.kafka_broker.v3.KafkaBroker
          stat_prefix: testfilter
          id_based_broker_address_rewrite_spec:
            rules:
            - id: 0
              host: 127.0.0.1
              port: {{ data['kafka_envoy_port'] }}
            # More ids go here if we add brokers to the test cluster.
      - name: tcp
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy
          stat_prefix: ingress_tcp
          cluster: localinstallation
  clusters:
  - name: localinstallation
    connect_timeout: 0.25s
    load_assignment:
      cluster_name: localinstallation
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: 127.0.0.1
                port_value: {{ data['kafka_real_port'] }}
admin:
  profile_path: /dev/null
  address:
    socket_address: { address: 127.0.0.1, port_value: {{ data['envoy_monitoring_port'] }} }
load("@base_pip3//:requirements.bzl", "requirement")
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_contrib_package",
    "envoy_py_test",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

# This test sets up multiple services, and this can take variable amount of time (30-60 seconds).
envoy_py_test(
    name = "kafka_broker_integration_test",
    srcs = [
        "kafka_broker_integration_test.py",
        "@kafka_python_client//:all",
    ],
    data = [
        "//bazel:remote_jdk11",
        "//contrib/exe:envoy-static",
        "@kafka_server_binary//:all",
    ] + glob(["*.j2"]),
    flaky = True,
    deps = [
        requirement("Jinja2"),
        requirement("MarkupSafe"),
    ],
)
#pragma once

#include "contrib/kafka/filters/network/source/broker/filter_config.h"
#include "gmock/gmock.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Broker {

class MockBrokerFilterConfig : public BrokerFilterConfig {
public:
  MOCK_METHOD((const std::string&), stat_prefix, (), (const));
  MOCK_METHOD(bool, needsResponseRewrite, (), (const));
  MOCK_METHOD((absl::optional<HostAndPort>), findBrokerAddressOverride, (const uint32_t), (const));
  MockBrokerFilterConfig() : BrokerFilterConfig{"prefix", false, {}} {};
};

} // namespace Broker
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
/**
 * Tests in this file verify whether Kafka broker filter instance is capable of processing protocol
 * messages properly.
 */

#include "source/common/common/utility.h"
#include "source/common/stats/isolated_store_impl.h"

#include "test/common/stats/stat_test_utility.h"
#include "test/test_common/test_time.h"

#include "contrib/kafka/filters/network/source/broker/filter.h"
#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/test/broker/mock_filter_config.h"
#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "contrib/kafka/filters/network/test/message_utilities.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Broker {

using RequestB = MessageBasedTest<RequestEncoder>;
using ResponseB = MessageBasedTest<ResponseEncoder>;

// Message size for all kind of broken messages (we are not going to process all the bytes).
constexpr static int32_t BROKEN_MESSAGE_SIZE = std::numeric_limits<int32_t>::max();

class KafkaBrokerFilterProtocolTest : public testing::Test,
                                      protected RequestB,
                                      protected ResponseB {
protected:
  Stats::TestUtil::TestStore store_;
  Stats::Scope& scope_{*store_.rootScope()};
  Event::TestRealTimeSystem time_source_;
  KafkaBrokerFilter testee_{scope_, time_source_, BrokerFilterConfig{"prefix", false, {}}};

  Network::FilterStatus consumeRequestFromBuffer() {
    return testee_.onData(RequestB::buffer_, false);
  }

  Network::FilterStatus consumeResponseFromBuffer() {
    return testee_.onWrite(ResponseB::buffer_, false);
  }
};

TEST_F(KafkaBrokerFilterProtocolTest, ShouldHandleUnknownRequestAndResponseWithoutBreaking) {
  // given
  const int16_t unknown_api_key = std::numeric_limits<int16_t>::max();

  const RequestHeader request_header = {unknown_api_key, 0, 0, "client-id"};
  const ProduceRequest request_data = {0, 0, {}};
  const Request<ProduceRequest> produce_request = {request_header, request_data};
  RequestB::putMessageIntoBuffer(produce_request);

  const ResponseMetadata response_metadata = {unknown_api_key, 0, 0};
  const ProduceResponse response_data = {{}};
  const Response<ProduceResponse> produce_response = {response_metadata, response_data};
  ResponseB::putMessageIntoBuffer(produce_response);

  // when
  const Network::FilterStatus result1 = consumeRequestFromBuffer();
  const Network::FilterStatus result2 = consumeResponseFromBuffer();

  // then
  ASSERT_EQ(result1, Network::FilterStatus::Continue);
  ASSERT_EQ(result2, Network::FilterStatus::Continue);
  ASSERT_EQ(store_.counter("kafka.prefix.request.unknown").value(), 1);
  ASSERT_EQ(store_.counter("kafka.prefix.response.unknown").value(), 1);
}

TEST_F(KafkaBrokerFilterProtocolTest, ShouldHandleBrokenRequestPayload) {
  // given

  // Encode broken request into buffer.
  // We will put invalid length of nullable string passed as client-id (length < -1).
  RequestB::putIntoBuffer(BROKEN_MESSAGE_SIZE);
  RequestB::putIntoBuffer(static_cast<int16_t>(0)); // Api key.
  RequestB::putIntoBuffer(static_cast<int16_t>(0)); // Api version.
  RequestB::putIntoBuffer(static_cast<int32_t>(0)); // Correlation-id.
  RequestB::putIntoBuffer(static_cast<int16_t>(std::numeric_limits<int16_t>::min())); // Client-id.

  // when
  const Network::FilterStatus result = consumeRequestFromBuffer();

  // then
  ASSERT_EQ(result, Network::FilterStatus::StopIteration);
  ASSERT_EQ(testee_.getRequestDecoderForTest()->getCurrentParserForTest(), nullptr);
}

TEST_F(KafkaBrokerFilterProtocolTest, ShouldHandleBrokenResponsePayload) {
  // given

  const int32_t correlation_id = 42;
  // Encode broken response into buffer.
  // Produce response v0 is a nullable array of TopicProduceResponses.
  // Encoding invalid length (< -1) of this nullable array is going to break the parser.
  ResponseB::putIntoBuffer(BROKEN_MESSAGE_SIZE);
  ResponseB::putIntoBuffer(correlation_id); // Correlation-id.
  ResponseB::putIntoBuffer(static_cast<int32_t>(std::numeric_limits<int32_t>::min())); // Array.

  testee_.getResponseDecoderForTest()->expectResponse(correlation_id, 0, 0);

  // when
  const Network::FilterStatus result = consumeResponseFromBuffer();

  // then
  ASSERT_EQ(result, Network::FilterStatus::StopIteration);
  ASSERT_EQ(testee_.getResponseDecoderForTest()->getCurrentParserForTest(), nullptr);
}

TEST_F(KafkaBrokerFilterProtocolTest, ShouldAbortOnUnregisteredResponse) {
  // given
  const ResponseMetadata response_metadata = {0, 0, 0};
  const ProduceResponse response_data = {{}};
  const Response<ProduceResponse> produce_response = {response_metadata, response_data};
  ResponseB::putMessageIntoBuffer(produce_response);

  // when
  const Network::FilterStatus result = consumeResponseFromBuffer();

  // then
  ASSERT_EQ(result, Network::FilterStatus::StopIteration);
}

TEST_F(KafkaBrokerFilterProtocolTest, ShouldProcessMessages) {
  // given
  // For every request/response type & version, put a corresponding request into the buffer.
  for (const AbstractRequestSharedPtr& message : MessageUtilities::makeAllRequests()) {
    RequestB::putMessageIntoBuffer(*message);
  }
  for (const AbstractResponseSharedPtr& message : MessageUtilities::makeAllResponses()) {
    ResponseB::putMessageIntoBuffer(*message);
  }

  // when
  const Network::FilterStatus result1 = consumeRequestFromBuffer();
  const Network::FilterStatus result2 = consumeResponseFromBuffer();

  // then
  ASSERT_EQ(result1, Network::FilterStatus::Continue);
  ASSERT_EQ(result2, Network::FilterStatus::Continue);

  // Also, assert that every message type has been processed properly.
  for (const int16_t i : MessageUtilities::apiKeys()) {
    // We should have received one request per api version.
    const Stats::Counter& request_counter = store_.counter(MessageUtilities::requestMetric(i));
    ASSERT_EQ(request_counter.value(), MessageUtilities::requestApiVersions(i));
    // We should have received one response per api version.
    const Stats::Counter& response_counter = store_.counter(MessageUtilities::responseMetric(i));
    ASSERT_EQ(response_counter.value(), MessageUtilities::responseApiVersions(i));
  }
}

} // namespace Broker
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/mocks/server/factory_context.h"

#include "contrib/kafka/filters/network/source/broker/config.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Broker {

TEST(KafkaConfigFactoryUnitTest, shouldCreateFilter) {
  // given
  const std::string yaml = R"EOF(
stat_prefix: test_prefix
  )EOF";

  KafkaBrokerProtoConfig proto_config;
  TestUtility::loadFromYamlAndValidate(yaml, proto_config);

  testing::NiceMock<Server::Configuration::MockFactoryContext> context;
  KafkaConfigFactory factory;

  Network::FilterFactoryCb cb = factory.createFilterFactoryFromProto(proto_config, context);
  Network::MockConnection connection;
  EXPECT_CALL(connection, addFilter(_));

  // when
  cb(connection);

  // then - connection had `addFilter` invoked
}

TEST(KafkaConfigFactoryUnitTest, shouldThrowOnInvalidStatPrefix) {
  // given
  const std::string yaml = R"EOF(
stat_prefix: ""
  )EOF";

  KafkaBrokerProtoConfig proto_config;

  // when
  // then - exception gets thrown
  EXPECT_THROW(TestUtility::loadFromYamlAndValidate(yaml, proto_config), ProtoValidationException);
}

} // namespace Broker
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "source/common/buffer/buffer_impl.h"

#include "contrib/kafka/filters/network/source/broker/filter_config.h"
#include "contrib/kafka/filters/network/source/broker/rewriter.h"
#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/test/broker/mock_filter_config.h"
#include "gtest/gtest.h"

using testing::_;
using testing::Return;
using testing::Throw;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Broker {

static void putBytesIntoBuffer(Buffer::Instance& buffer, const uint32_t size) {
  std::vector<char> data(size, 42);
  absl::string_view sv = {data.data(), data.size()};
  buffer.add(sv);
}

static Buffer::InstancePtr makeRandomBuffer(const uint32_t size) {
  Buffer::InstancePtr result = std::make_unique<Buffer::OwnedImpl>();
  putBytesIntoBuffer(*result, size);
  return result;
}

class FakeResponse : public AbstractResponse {
public:
  FakeResponse(const size_t size) : AbstractResponse{ResponseMetadata{-1, 0, 0}}, size_{size} {}

  uint32_t computeSize() const override { return size_; };

  virtual uint32_t encode(Buffer::Instance& dst) const override {
    putBytesIntoBuffer(dst, size_);
    return size_;
  };

private:
  size_t size_;
};

TEST(ResponseRewriterImplUnitTest, ShouldRewriteBuffer) {
  // given
  ResponseRewriterImpl testee{MockBrokerFilterConfig{}};

  auto response1 = std::make_shared<FakeResponse>(7);
  auto response2 = std::make_shared<FakeResponse>(13);
  auto response3 = std::make_shared<FakeResponse>(42);

  // when - 1
  testee.onMessage(response1);
  testee.onMessage(response2);
  testee.onMessage(response3);

  // then - 1
  ASSERT_EQ(testee.getStoredResponseCountForTest(), 3);

  // when - 2
  auto buffer = makeRandomBuffer(4242);
  testee.process(*buffer);

  // then - 2
  ASSERT_EQ(testee.getStoredResponseCountForTest(), 0);
  ASSERT_EQ(buffer->length(), (3 * 4) + 7 + 13 + 42); // 4 bytes for message length
}

template <typename T>
static void assertAddress(const T& arg, const std::string& host, const int32_t port) {
  ASSERT_EQ(arg.host_, host);
  ASSERT_EQ(arg.port_, port);
}

TEST(ResponseRewriterImplUnitTest, ShouldRewriteMetadataResponse) {
  // given
  MetadataResponseBroker b1 = {13, "host1", 1111};
  MetadataResponseBroker b2 = {42, "host2", 2222};
  MetadataResponseBroker b3 = {77, "host3", 3333};
  std::vector<MetadataResponseBroker> brokers = {b1, b2, b3};
  MetadataResponse mr = {brokers, {}};

  MockBrokerFilterConfig config;
  absl::optional<HostAndPort> r1 = {{"nh1", 4444}};
  EXPECT_CALL(config, findBrokerAddressOverride(b1.node_id_)).WillOnce(Return(r1));
  absl::optional<HostAndPort> r2 = absl::nullopt;
  EXPECT_CALL(config, findBrokerAddressOverride(b2.node_id_)).WillOnce(Return(r2));
  absl::optional<HostAndPort> r3 = {{"nh3", 6666}};
  EXPECT_CALL(config, findBrokerAddressOverride(b3.node_id_)).WillOnce(Return(r3));
  ResponseRewriterImpl testee{config};

  // when
  testee.updateMetadataBrokerAddresses(mr);

  // then
  assertAddress(mr.brokers_[0], r1->first, r1->second);
  assertAddress(mr.brokers_[1], b2.host_, b2.port_);
  assertAddress(mr.brokers_[2], r3->first, r3->second);
}

TEST(ResponseRewriterImplUnitTest, ShouldRewriteFindCoordinatorResponse) {
  // given

  FindCoordinatorResponse fcr = {0, 13, "host1", 1111};
  Coordinator c1 = {"k1", 1, "ch1", 2222, 0, {}, {}};
  Coordinator c2 = {"k2", 2, "ch2", 3333, 0, {}, {}};
  Coordinator c3 = {"k3", 3, "ch3", 4444, 0, {}, {}};
  fcr.coordinators_ = {c1, c2, c3};

  MockBrokerFilterConfig config;
  absl::optional<HostAndPort> fcrhp = {{"nh1", 4444}};
  EXPECT_CALL(config, findBrokerAddressOverride(fcr.node_id_)).WillOnce(Return(fcrhp));
  absl::optional<HostAndPort> cr1 = {{"nh1", 4444}};
  EXPECT_CALL(config, findBrokerAddressOverride(c1.node_id_)).WillOnce(Return(cr1));
  absl::optional<HostAndPort> cr2 = absl::nullopt;
  EXPECT_CALL(config, findBrokerAddressOverride(c2.node_id_)).WillOnce(Return(cr2));
  absl::optional<HostAndPort> cr3 = {{"nh3", 6666}};
  EXPECT_CALL(config, findBrokerAddressOverride(c3.node_id_)).WillOnce(Return(cr3));
  ResponseRewriterImpl testee{config};

  // when
  testee.updateFindCoordinatorBrokerAddresses(fcr);

  // then
  assertAddress(fcr, fcrhp->first, fcrhp->second);
  assertAddress(fcr.coordinators_[0], cr1->first, cr1->second);
  assertAddress(fcr.coordinators_[1], c2.host_, c2.port_);
  assertAddress(fcr.coordinators_[2], cr3->first, cr3->second);
}

TEST(ResponseRewriterImplUnitTest, ShouldRewriteDescribeClusterResponse) {
  // given
  DescribeClusterBroker b1 = {13, "host1", 1111, absl::nullopt, {}};
  DescribeClusterBroker b2 = {42, "host2", 2222, absl::nullopt, {}};
  DescribeClusterBroker b3 = {77, "host3", 3333, absl::nullopt, {}};
  std::vector<DescribeClusterBroker> brokers = {b1, b2, b3};
  DescribeClusterResponse dcr = {0, 0, absl::nullopt, "", 0, brokers, 0, {}};

  MockBrokerFilterConfig config;
  absl::optional<HostAndPort> cr1 = {{"nh1", 4444}};
  EXPECT_CALL(config, findBrokerAddressOverride(b1.broker_id_)).WillOnce(Return(cr1));
  absl::optional<HostAndPort> cr2 = absl::nullopt;
  EXPECT_CALL(config, findBrokerAddressOverride(b2.broker_id_)).WillOnce(Return(cr2));
  absl::optional<HostAndPort> cr3 = {{"nh3", 6666}};
  EXPECT_CALL(config, findBrokerAddressOverride(b3.broker_id_)).WillOnce(Return(cr3));
  ResponseRewriterImpl testee{config};

  // when
  testee.updateDescribeClusterBrokerAddresses(dcr);

  // then
  assertAddress(dcr.brokers_[0], cr1->first, cr1->second);
  assertAddress(dcr.brokers_[1], b2.host_, b2.port_);
  assertAddress(dcr.brokers_[2], cr3->first, cr3->second);
}

TEST(ResponseRewriterUnitTest, ShouldCreateProperRewriter) {
  MockBrokerFilterConfig c1;
  EXPECT_CALL(c1, needsResponseRewrite()).WillOnce(Return(true));
  ResponseRewriterSharedPtr r1 = createRewriter(c1);
  ASSERT_NE(std::dynamic_pointer_cast<ResponseRewriterImpl>(r1), nullptr);

  MockBrokerFilterConfig c2;
  EXPECT_CALL(c2, needsResponseRewrite()).WillOnce(Return(false));
  ResponseRewriterSharedPtr r2 = createRewriter(c2);
  ASSERT_NE(std::dynamic_pointer_cast<DoNothingRewriter>(r2), nullptr);
}

} // namespace Broker
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "envoy/event/timer.h"

#include "test/mocks/network/mocks.h"
#include "test/mocks/stats/mocks.h"

#include "contrib/kafka/filters/network/source/broker/filter.h"
#include "contrib/kafka/filters/network/source/broker/filter_config.h"
#include "contrib/kafka/filters/network/source/external/requests.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::Return;
using testing::Throw;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Broker {

// Mocks.

class MockKafkaMetricsFacade : public KafkaMetricsFacade {
public:
  MOCK_METHOD(void, onMessage, (AbstractRequestSharedPtr));
  MOCK_METHOD(void, onMessage, (AbstractResponseSharedPtr));
  MOCK_METHOD(void, onFailedParse, (RequestParseFailureSharedPtr));
  MOCK_METHOD(void, onFailedParse, (ResponseMetadataSharedPtr));
  MOCK_METHOD(void, onRequestException, ());
  MOCK_METHOD(void, onResponseException, ());
};

using MockKafkaMetricsFacadeSharedPtr = std::shared_ptr<MockKafkaMetricsFacade>;

class MockResponseRewriter : public ResponseRewriter {
public:
  MOCK_METHOD(void, onMessage, (AbstractResponseSharedPtr));
  MOCK_METHOD(void, onFailedParse, (ResponseMetadataSharedPtr));
  MOCK_METHOD(void, process, (Buffer::Instance&));
};

using MockResponseRewriterSharedPtr = std::shared_ptr<MockResponseRewriter>;

class MockResponseDecoder : public ResponseDecoder {
public:
  MockResponseDecoder() : ResponseDecoder{{}} {};
  MOCK_METHOD(void, onData, (Buffer::Instance&));
  MOCK_METHOD(void, expectResponse, (const int32_t, const int16_t, const int16_t));
  MOCK_METHOD(void, reset, ());
};

using MockResponseDecoderSharedPtr = std::shared_ptr<MockResponseDecoder>;

class MockRequestDecoder : public RequestDecoder {
public:
  MockRequestDecoder() : RequestDecoder{{}} {};
  MOCK_METHOD(void, onData, (Buffer::Instance&));
  MOCK_METHOD(void, reset, ());
};

using MockRequestDecoderSharedPtr = std::shared_ptr<MockRequestDecoder>;

class MockTimeSource : public TimeSource {
public:
  MOCK_METHOD(SystemTime, systemTime, ());
  MOCK_METHOD(MonotonicTime, monotonicTime, ());
};

class MockRichRequestMetrics : public RichRequestMetrics {
public:
  MOCK_METHOD(void, onRequest, (const int16_t));
  MOCK_METHOD(void, onUnknownRequest, ());
  MOCK_METHOD(void, onBrokenRequest, ());
};

class MockRichResponseMetrics : public RichResponseMetrics {
public:
  MOCK_METHOD(void, onResponse, (const int16_t, const long long duration));
  MOCK_METHOD(void, onUnknownResponse, ());
  MOCK_METHOD(void, onBrokenResponse, ());
};

class MockRequest : public AbstractRequest {
public:
  MockRequest(const int16_t api_key, const int16_t api_version, const int32_t correlation_id)
      : AbstractRequest{{api_key, api_version, correlation_id, ""}} {};
  uint32_t computeSize() const override { return 0; };
  uint32_t encode(Buffer::Instance&) const override { return 0; };
};

class MockResponse : public AbstractResponse {
public:
  MockResponse(const int16_t api_key, const int32_t correlation_id)
      : AbstractResponse{{api_key, 0, correlation_id}} {};
  uint32_t computeSize() const override { return 0; };
  uint32_t encode(Buffer::Instance&) const override { return 0; };
};

// Tests.

class KafkaBrokerFilterUnitTest : public testing::Test {
protected:
  MockKafkaMetricsFacadeSharedPtr metrics_{std::make_shared<MockKafkaMetricsFacade>()};
  MockResponseRewriterSharedPtr response_rewriter_{std::make_shared<MockResponseRewriter>()};
  MockResponseDecoderSharedPtr response_decoder_{std::make_shared<MockResponseDecoder>()};
  MockRequestDecoderSharedPtr request_decoder_{std::make_shared<MockRequestDecoder>()};

  NiceMock<Network::MockReadFilterCallbacks> filter_callbacks_;

  KafkaBrokerFilter testee_{metrics_, response_rewriter_, response_decoder_, request_decoder_};

  void initialize() {
    testee_.initializeReadFilterCallbacks(filter_callbacks_);
    testee_.onNewConnection();
  }
};

TEST_F(KafkaBrokerFilterUnitTest, ShouldAcceptDataSentByKafkaClient) {
  // given
  Buffer::OwnedImpl data;
  EXPECT_CALL(*request_decoder_, onData(_));

  // when
  initialize();
  const auto result = testee_.onData(data, false);

  // then
  ASSERT_EQ(result, Network::FilterStatus::Continue);
  // Also, request_decoder got invoked.
}

TEST_F(KafkaBrokerFilterUnitTest, ShouldStopIterationIfProcessingDataFromKafkaClientFails) {
  // given
  Buffer::OwnedImpl data;
  EXPECT_CALL(*request_decoder_, onData(_)).WillOnce(Throw(EnvoyException("boom")));
  EXPECT_CALL(*request_decoder_, reset());
  EXPECT_CALL(*metrics_, onRequestException());

  // when
  initialize();
  const auto result = testee_.onData(data, false);

  // then
  ASSERT_EQ(result, Network::FilterStatus::StopIteration);
}

TEST_F(KafkaBrokerFilterUnitTest, ShouldAcceptDataSentByKafkaBroker) {
  // given
  Buffer::OwnedImpl data;
  EXPECT_CALL(*response_decoder_, onData(_));
  EXPECT_CALL(*response_rewriter_, process(_));

  // when
  initialize();
  const auto result = testee_.onWrite(data, false);

  // then
  ASSERT_EQ(result, Network::FilterStatus::Continue);
  // Also, request_decoder got invoked.
}

TEST_F(KafkaBrokerFilterUnitTest, ShouldStopIterationIfProcessingDataFromKafkaBrokerFails) {
  // given
  Buffer::OwnedImpl data;
  EXPECT_CALL(*response_decoder_, onData(_)).WillOnce(Throw(EnvoyException("boom")));
  EXPECT_CALL(*response_decoder_, reset());
  EXPECT_CALL(*metrics_, onResponseException());

  // when
  initialize();
  const auto result = testee_.onWrite(data, false);

  // then
  ASSERT_EQ(result, Network::FilterStatus::StopIteration);
}

class ForwarderUnitTest : public testing::Test {
protected:
  MockResponseDecoderSharedPtr response_decoder_{std::make_shared<MockResponseDecoder>()};
  Forwarder testee_{*response_decoder_};
};

TEST_F(ForwarderUnitTest, ShouldUpdateResponseDecoderState) {
  // given
  const int16_t api_key = 42;
  const int16_t api_version = 13;
  const int32_t correlation_id = 1234;
  AbstractRequestSharedPtr request =
      std::make_shared<MockRequest>(api_key, api_version, correlation_id);

  EXPECT_CALL(*response_decoder_, expectResponse(correlation_id, api_key, api_version));

  // when
  testee_.onMessage(request);

  // then - response_decoder_ had a new expected response registered.
}

TEST_F(ForwarderUnitTest, ShouldUpdateResponseDecoderStateOnFailedParse) {
  // given
  const int16_t api_key = 42;
  const int16_t api_version = 13;
  const int32_t correlation_id = 1234;
  RequestHeader header = {api_key, api_version, correlation_id, ""};
  RequestParseFailureSharedPtr parse_failure = std::make_shared<RequestParseFailure>(header);

  EXPECT_CALL(*response_decoder_, expectResponse(correlation_id, api_key, api_version));

  // when
  testee_.onFailedParse(parse_failure);

  // then - response_decoder_ had a new expected response registered.
}

class KafkaMetricsFacadeImplUnitTest : public testing::Test {
protected:
  MockTimeSource time_source_;
  std::shared_ptr<MockRichRequestMetrics> request_metrics_ =
      std::make_shared<MockRichRequestMetrics>();
  std::shared_ptr<MockRichResponseMetrics> response_metrics_ =
      std::make_shared<MockRichResponseMetrics>();
  KafkaMetricsFacadeImpl testee_{time_source_, request_metrics_, response_metrics_};
};

TEST_F(KafkaMetricsFacadeImplUnitTest, ShouldRegisterRequest) {
  // given
  const int16_t api_key = 42;
  const int32_t correlation_id = 1234;
  AbstractRequestSharedPtr request = std::make_shared<MockRequest>(api_key, 0, correlation_id);

  EXPECT_CALL(*request_metrics_, onRequest(api_key));

  MonotonicTime time_point{Event::TimeSystem::Milliseconds(1234)};
  EXPECT_CALL(time_source_, monotonicTime()).WillOnce(Return(time_point));

  // when
  testee_.onMessage(request);

  // then
  const auto& request_arrivals = testee_.getRequestArrivalsForTest();
  ASSERT_EQ(request_arrivals.at(correlation_id), time_point);
}

TEST_F(KafkaMetricsFacadeImplUnitTest, ShouldRegisterUnknownRequest) {
  // given
  RequestHeader header = {0, 0, 0, ""};
  RequestParseFailureSharedPtr unknown_request = std::make_shared<RequestParseFailure>(header);

  EXPECT_CALL(*request_metrics_, onUnknownRequest());

  // when
  testee_.onFailedParse(unknown_request);

  // then - request_metrics_ is updated.
}

TEST_F(KafkaMetricsFacadeImplUnitTest, ShouldRegisterResponse) {
  // given
  const int16_t api_key = 42;
  const int32_t correlation_id = 1234;
  AbstractResponseSharedPtr response = std::make_shared<MockResponse>(api_key, correlation_id);

  MonotonicTime request_time_point{Event::TimeSystem::Milliseconds(1234)};
  testee_.getRequestArrivalsForTest()[correlation_id] = request_time_point;

  MonotonicTime response_time_point{Event::TimeSystem::Milliseconds(2345)};

  EXPECT_CALL(*response_metrics_, onResponse(api_key, 1111));
  EXPECT_CALL(time_source_, monotonicTime()).WillOnce(Return(response_time_point));

  // when
  testee_.onMessage(response);

  // then
  const auto& request_arrivals = testee_.getRequestArrivalsForTest();
  ASSERT_EQ(request_arrivals.find(correlation_id), request_arrivals.end());
}

TEST_F(KafkaMetricsFacadeImplUnitTest, ShouldRegisterUnknownResponse) {
  // given
  ResponseMetadataSharedPtr unknown_response = std::make_shared<ResponseMetadata>(0, 0, 0);

  EXPECT_CALL(*response_metrics_, onUnknownResponse());

  // when
  testee_.onFailedParse(unknown_response);

  // then - response_metrics_ is updated.
}

} // namespace Broker
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_cc_test_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test(
    name = "config_unit_test",
    srcs = ["config_unit_test.cc"],
    deps = [
        "//contrib/kafka/filters/network/source/broker:config_lib",
        "//test/mocks/server:factory_context_mocks",
    ],
)

envoy_cc_test(
    name = "filter_unit_test",
    srcs = ["filter_unit_test.cc"],
    deps = [
        "//contrib/kafka/filters/network/source/broker:filter_lib",
        "//envoy/event:timer_interface",
        "//test/mocks/network:network_mocks",
        "//test/mocks/stats:stats_mocks",
    ],
)

envoy_cc_test(
    name = "filter_protocol_test",
    srcs = ["filter_protocol_test.cc"],
    deps = [
        ":mock_filter_config_test_lib",
        "//contrib/kafka/filters/network/source/broker:filter_lib",
        "//contrib/kafka/filters/network/test:buffer_based_test_lib",
        "//contrib/kafka/filters/network/test:message_utilities",
        "//test/common/stats:stat_test_utility_lib",
        "//test/test_common:test_time_lib",
    ],
)

envoy_cc_test(
    name = "rewriter_unit_test",
    srcs = ["rewriter_unit_test.cc"],
    deps = [
        ":mock_filter_config_test_lib",
        "//contrib/kafka/filters/network/source/broker:rewriter_lib",
        "//source/common/buffer:buffer_lib",
    ],
)

envoy_cc_test_library(
    name = "mock_filter_config_test_lib",
    srcs = [],
    hdrs = ["mock_filter_config.h"],
    deps = [
        "//contrib/kafka/filters/network/source/broker:filter_config_lib",
    ],
)
#include "test/test_common/utility.h"

#include "contrib/kafka/filters/network/source/tagged_fields.h"
#include "contrib/kafka/filters/network/test/serialization_utilities.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace SerializationTest {

/**
 * Tests in this file are supposed to check whether serialization operations
 * on Kafka-primitive types (ints, strings, arrays) are behaving correctly.
 */

// Freshly created deserializers should not be ready.
#define TEST_EmptyDeserializerShouldNotBeReady(DeserializerClass)                                  \
  TEST(DeserializerClass, EmptyBufferShouldNotBeReady) {                                           \
    const DeserializerClass testee{};                                                              \
    ASSERT_EQ(testee.ready(), false);                                                              \
  }

TEST_EmptyDeserializerShouldNotBeReady(Int8Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(Int16Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(UInt16Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(Int32Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(UInt32Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(Int64Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(Float64Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(BooleanDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(VarUInt32Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(VarInt32Deserializer);
TEST_EmptyDeserializerShouldNotBeReady(VarInt64Deserializer);

TEST_EmptyDeserializerShouldNotBeReady(StringDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(CompactStringDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(NullableStringDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(NullableCompactStringDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(BytesDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(CompactBytesDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(NullableBytesDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(NullableCompactBytesDeserializer);
TEST_EmptyDeserializerShouldNotBeReady(UuidDeserializer);

TEST(ArrayDeserializer, EmptyBufferShouldNotBeReady) {
  // given
  const ArrayDeserializer<Int8Deserializer> testee{};
  // when, then
  ASSERT_EQ(testee.ready(), false);
}

TEST(CompactArrayDeserializer, EmptyBufferShouldNotBeReady) {
  // given
  const CompactArrayDeserializer<Int32Deserializer> testee{};
  // when, then
  ASSERT_EQ(testee.ready(), false);
}

TEST(NullableArrayDeserializer, EmptyBufferShouldNotBeReady) {
  // given
  const NullableArrayDeserializer<Int8Deserializer> testee{};
  // when, then
  ASSERT_EQ(testee.ready(), false);
}

TEST(NullableCompactArrayDeserializer, EmptyBufferShouldNotBeReady) {
  // given
  const NullableCompactArrayDeserializer<Int32Deserializer> testee{};
  // when, then
  ASSERT_EQ(testee.ready(), false);
}

// Extracted test for numeric buffers.
#define TEST_DeserializerShouldDeserialize(BufferClass, DataClass, Value)                          \
  TEST(DataClass, ShouldConsumeCorrectAmountOfData) {                                              \
    /* given */                                                                                    \
    const DataClass value = Value;                                                                 \
    serializeThenDeserializeAndCheckEquality<BufferClass>(value);                                  \
  }

TEST_DeserializerShouldDeserialize(Int8Deserializer, int8_t, 42);
TEST_DeserializerShouldDeserialize(Int16Deserializer, int16_t, 42);
TEST_DeserializerShouldDeserialize(UInt16Deserializer, uint16_t, 42);
TEST_DeserializerShouldDeserialize(Int32Deserializer, int32_t, 42);
TEST_DeserializerShouldDeserialize(UInt32Deserializer, uint32_t, 42);
TEST_DeserializerShouldDeserialize(Int64Deserializer, int64_t, 42);
TEST_DeserializerShouldDeserialize(Float64Deserializer, double, 13.25);
TEST_DeserializerShouldDeserialize(BooleanDeserializer, bool, true);

EncodingContext encoder{-1}; // Provided api_version does not matter for primitive types.

// Variable-length uint32_t tests.

TEST(VarUInt32Deserializer, ShouldDeserialize) {
  const uint32_t value = 0;
  serializeCompactThenDeserializeAndCheckEquality<VarUInt32Deserializer>(value);
}

TEST(VarUInt32Deserializer, ShouldDeserializeMaxUint32) {
  const uint32_t value = std::numeric_limits<uint32_t>::max();
  serializeCompactThenDeserializeAndCheckEquality<VarUInt32Deserializer>(value);
}

TEST(VarUInt32Deserializer, ShouldDeserializeEdgeValues) {
  // Each of these values should fit in 1, 2, 3, 4 bytes.
  std::vector<uint32_t> values = {0x7f, 0x3fff, 0x1fffff, 0xfffffff};
  for (auto i = 0; i < static_cast<int>(values.size()); ++i) {
    // given
    Buffer::OwnedImpl buffer;

    // when
    const uint32_t expected_size = encoder.computeCompactSize(values[i]);
    const uint32_t written = encoder.encodeCompact(values[i], buffer);

    // then
    ASSERT_EQ(written, i + 1);
    ASSERT_EQ(written, expected_size);
    absl::string_view data = {getRawData(buffer), 1024};
    // All bits in lower bytes need to be set.
    for (auto j = 0; j + 1 < i; ++j) {
      ASSERT_EQ(static_cast<uint8_t>(data[j]), 0xFF);
    }
    // Highest bit in last byte needs to be clear (end marker).
    ASSERT_EQ(static_cast<uint8_t>(data[i]), 0x7F);
  }
}

TEST(VarUInt32Deserializer, ShouldSerializeMaxUint32Properly) {
  // given
  Buffer::OwnedImpl buffer;

  // when
  const uint32_t value = std::numeric_limits<uint32_t>::max();
  const uint32_t result = encoder.encodeCompact(value, buffer);

  // then
  ASSERT_EQ(result, 5);
  absl::string_view data = {getRawData(buffer), 1024};
  ASSERT_EQ(static_cast<uint8_t>(data[0]), 0xFF); // Bits 1-7 (starting at 1).
  ASSERT_EQ(static_cast<uint8_t>(data[1]), 0xFF); // Bits 8-14.
  ASSERT_EQ(static_cast<uint8_t>(data[2]), 0xFF); // Bits 15-21.
  ASSERT_EQ(static_cast<uint8_t>(data[3]), 0xFF); // Bits 22-28.
  ASSERT_EQ(static_cast<uint8_t>(data[4]), 0x0F); // Bits 29-32.
}

TEST(VarUInt32Deserializer, ShouldThrowIfNoEndWith5Bytes) {
  // given
  VarUInt32Deserializer testee;
  Buffer::OwnedImpl buffer;

  // The buffer makes no sense, it's 5 times 0xFF, while varint encoding ensures that in the worst
  // case 5th byte has the highest bit clear.
  for (int i = 0; i < 5; ++i) {
    const uint8_t all_bits_set = 0xFF;
    buffer.add(&all_bits_set, sizeof(all_bits_set));
  }

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW_WITH_REGEX(testee.feed(data), EnvoyException, "is too long");
}

// Variable-length int32_t tests.

TEST(VarInt32Deserializer, ShouldDeserialize) {
  Buffer::OwnedImpl buffer;
  const char input[1] = {0};
  buffer.add(absl::string_view(input, sizeof(input)));
  const int32_t expected_value = 0;
  deserializeCompactAndCheckEquality<VarInt32Deserializer>(buffer, expected_value);
}

TEST(VarInt32Deserializer, ShouldDeserializeMinInt32) {
  Buffer::OwnedImpl buffer;
  const uint8_t input[5] = {0xFF, 0xFF, 0xFF, 0xFF, 0x0F};
  buffer.add(absl::string_view(reinterpret_cast<const char*>(input), sizeof(input)));
  const int32_t expected_value = std::numeric_limits<int32_t>::min();
  deserializeCompactAndCheckEquality<VarInt32Deserializer>(buffer, expected_value);
}

TEST(VarInt32Deserializer, ShouldDeserializeMaxInt32) {
  Buffer::OwnedImpl buffer;
  const uint8_t input[5] = {0xFE, 0xFF, 0xFF, 0xFF, 0x0F};
  buffer.add(absl::string_view(reinterpret_cast<const char*>(input), sizeof(input)));
  const int32_t expected_value = std::numeric_limits<int32_t>::max();
  deserializeCompactAndCheckEquality<VarInt32Deserializer>(buffer, expected_value);
}

// Variable-length int64_t tests.

TEST(VarInt64Deserializer, ShouldDeserialize) {
  Buffer::OwnedImpl buffer;
  const char input[1] = {0};
  buffer.add(absl::string_view(input, sizeof(input)));
  const int64_t expected_value = 0;
  deserializeCompactAndCheckEquality<VarInt64Deserializer>(buffer, expected_value);
}

TEST(VarInt64Deserializer, ShouldDeserializeMinInt64) {
  Buffer::OwnedImpl buffer;
  const uint8_t input[10] = {0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x0F};
  buffer.add(absl::string_view(reinterpret_cast<const char*>(input), sizeof(input)));
  const int64_t expected_value = std::numeric_limits<int64_t>::min();
  deserializeCompactAndCheckEquality<VarInt64Deserializer>(buffer, expected_value);
}

TEST(VarInt64Deserializer, ShouldDeserializeMaxInt64) {
  Buffer::OwnedImpl buffer;
  const uint8_t input[10] = {0xFE, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x0F};
  buffer.add(absl::string_view(reinterpret_cast<const char*>(input), sizeof(input)));
  const int64_t expected_value = std::numeric_limits<int64_t>::max();
  deserializeCompactAndCheckEquality<VarInt64Deserializer>(buffer, expected_value);
}

TEST(VarInt64Deserializer, ShouldThrowIfNoEndWith10Bytes) {
  // given
  VarInt64Deserializer testee;
  Buffer::OwnedImpl buffer;

  // The buffer makes no sense, it's 10 times 0xFF, while varint encoding ensures that in the worst
  // case 10th byte has the highest bit clear.
  for (int i = 0; i < 10; ++i) {
    const uint8_t all_bits_set = 0xFF;
    buffer.add(&all_bits_set, sizeof(all_bits_set));
  }

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW_WITH_REGEX(testee.feed(data), EnvoyException, "is too long");
}

// String tests.

TEST(StringDeserializer, ShouldDeserialize) {
  const std::string value = "sometext";
  serializeThenDeserializeAndCheckEquality<StringDeserializer>(value);
}

TEST(StringDeserializer, ShouldDeserializeEmptyString) {
  const std::string value = "";
  serializeThenDeserializeAndCheckEquality<StringDeserializer>(value);
}

TEST(StringDeserializer, ShouldThrowOnInvalidLength) {
  // given
  StringDeserializer testee;
  Buffer::OwnedImpl buffer;

  int16_t len = -1; // STRING accepts length >= 0.
  encoder.encode(len, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Compact string tests.

TEST(CompactStringDeserializer, ShouldDeserialize) {
  const std::string value = "sometext";
  serializeCompactThenDeserializeAndCheckEquality<CompactStringDeserializer>(value);
}

TEST(CompactStringDeserializer, ShouldDeserializeEmptyString) {
  const std::string value = "";
  serializeCompactThenDeserializeAndCheckEquality<CompactStringDeserializer>(value);
}

TEST(CompactStringDeserializer, ShouldThrowOnInvalidLength) {
  // given
  CompactStringDeserializer testee;
  Buffer::OwnedImpl buffer;

  const uint32_t len = 0; // COMPACT_STRING requires length >= 1.
  encoder.encodeCompact(len, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Nullable string tests.

TEST(NullableStringDeserializer, ShouldDeserializeString) {
  // given
  const NullableString value{"sometext"};
  serializeThenDeserializeAndCheckEquality<NullableStringDeserializer>(value);
}

TEST(NullableStringDeserializer, ShouldDeserializeEmptyString) {
  // given
  const NullableString value{""};
  serializeThenDeserializeAndCheckEquality<NullableStringDeserializer>(value);
}

TEST(NullableStringDeserializer, ShouldDeserializeAbsentString) {
  // given
  const NullableString value = absl::nullopt;
  serializeThenDeserializeAndCheckEquality<NullableStringDeserializer>(value);
}

TEST(NullableStringDeserializer, ShouldThrowOnInvalidLength) {
  // given
  NullableStringDeserializer testee;
  Buffer::OwnedImpl buffer;

  int16_t len = -2; // -1 is OK for NULLABLE_STRING.
  encoder.encode(len, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Nullable compact string tests.

TEST(NullableCompactStringDeserializer, ShouldDeserializeString) {
  // given
  const NullableString value{"sometext"};
  serializeCompactThenDeserializeAndCheckEquality<NullableCompactStringDeserializer>(value);
}

TEST(NullableCompactStringDeserializer, ShouldDeserializeEmptyString) {
  // given
  const NullableString value{""};
  serializeCompactThenDeserializeAndCheckEquality<NullableCompactStringDeserializer>(value);
}

TEST(NullableCompactStringDeserializer, ShouldDeserializeAbsentString) {
  // given
  const NullableString value = absl::nullopt;
  serializeCompactThenDeserializeAndCheckEquality<NullableCompactStringDeserializer>(value);
}

// Byte array tests.

TEST(BytesDeserializer, ShouldDeserialize) {
  const Bytes value{'a', 'b', 'c', 'd'};
  serializeThenDeserializeAndCheckEquality<BytesDeserializer>(value);
}

TEST(BytesDeserializer, ShouldDeserializeEmptyBytes) {
  const Bytes value{};
  serializeThenDeserializeAndCheckEquality<BytesDeserializer>(value);
}

TEST(BytesDeserializer, ShouldThrowOnInvalidLength) {
  // given
  BytesDeserializer testee;
  Buffer::OwnedImpl buffer;

  const int32_t bytes_length = -1; // BYTES accepts length >= 0.
  encoder.encode(bytes_length, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Compact byte array tests.

TEST(CompactBytesDeserializer, ShouldDeserialize) {
  const Bytes value{'a', 'b', 'c', 'd'};
  serializeCompactThenDeserializeAndCheckEquality<CompactBytesDeserializer>(value);
}

TEST(CompactBytesDeserializer, ShouldDeserializeEmptyBytes) {
  const Bytes value{};
  serializeCompactThenDeserializeAndCheckEquality<CompactBytesDeserializer>(value);
}

TEST(CompactBytesDeserializer, ShouldThrowOnInvalidLength) {
  // given
  CompactBytesDeserializer testee;
  Buffer::OwnedImpl buffer;

  const uint32_t bytes_length = 0; // COMPACT_BYTES requires length >= 1.
  encoder.encodeCompact(bytes_length, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Nullable byte array tests.

TEST(NullableBytesDeserializer, ShouldDeserialize) {
  const NullableBytes value{{'a', 'b', 'c', 'd'}};
  serializeThenDeserializeAndCheckEquality<NullableBytesDeserializer>(value);
}

TEST(NullableBytesDeserializer, ShouldDeserializeEmptyBytes) {
  // gcc refuses to initialize optional with empty vector with value{{}}
  const NullableBytes value = {{}};
  serializeThenDeserializeAndCheckEquality<NullableBytesDeserializer>(value);
}

TEST(NullableBytesDeserializer, ShouldDeserializeNullBytes) {
  const NullableBytes value = absl::nullopt;
  serializeThenDeserializeAndCheckEquality<NullableBytesDeserializer>(value);
}

TEST(NullableBytesDeserializer, ShouldThrowOnInvalidLength) {
  // given
  NullableBytesDeserializer testee;
  Buffer::OwnedImpl buffer;

  const int32_t bytes_length = -2; // -1 is OK for NULLABLE_BYTES.
  encoder.encode(bytes_length, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Nullable compact byte-array tests.

TEST(NullableCompactBytesDeserializer, ShouldDeserialize) {
  const NullableBytes value{{'a', 'b', 'c', 'd'}};
  serializeCompactThenDeserializeAndCheckEquality<NullableCompactBytesDeserializer>(value);
}

TEST(NullableCompactBytesDeserializer, ShouldDeserializeEmptyBytes) {
  const NullableBytes value = {{}};
  serializeCompactThenDeserializeAndCheckEquality<NullableCompactBytesDeserializer>(value);
}

TEST(NullableCompactBytesDeserializer, ShouldDeserializeNullBytes) {
  const NullableBytes value = absl::nullopt;
  serializeCompactThenDeserializeAndCheckEquality<NullableCompactBytesDeserializer>(value);
}

// Generic-array tests.

TEST(ArrayDeserializer, ShouldConsumeCorrectAmountOfData) {
  const std::vector<std::string> value{{"aaa", "bbbbb", "cc", "d", "e", "ffffffff"}};
  serializeThenDeserializeAndCheckEquality<ArrayDeserializer<StringDeserializer>>(value);
}

TEST(ArrayDeserializer, ShouldThrowOnInvalidLength) {
  // given
  ArrayDeserializer<StringDeserializer> testee;
  Buffer::OwnedImpl buffer;

  const int32_t len = -1; // ARRAY accepts length >= 0.
  encoder.encode(len, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Compact generic-array tests.

TEST(CompactArrayDeserializer, ShouldConsumeCorrectAmountOfData) {
  const std::vector<int32_t> value{{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}};
  serializeCompactThenDeserializeAndCheckEquality<CompactArrayDeserializer<Int32Deserializer>>(
      value);
}

TEST(CompactArrayDeserializer, ShouldThrowOnInvalidLength) {
  // given
  CompactArrayDeserializer<Int8Deserializer> testee;
  Buffer::OwnedImpl buffer;

  const uint32_t len = 0; // COMPACT_ARRAY accepts length >= 1.
  encoder.encodeCompact(len, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Nullable generic-array tests.

TEST(NullableArrayDeserializer, ShouldConsumeCorrectAmountOfData) {
  const NullableArray<std::string> value{{"aaa", "bbbbb", "cc", "d", "e", "ffffffff"}};
  serializeThenDeserializeAndCheckEquality<NullableArrayDeserializer<StringDeserializer>>(value);
}

TEST(NullableArrayDeserializer, ShouldConsumeNullArray) {
  const NullableArray<std::string> value = absl::nullopt;
  serializeThenDeserializeAndCheckEquality<NullableArrayDeserializer<StringDeserializer>>(value);
}

TEST(NullableArrayDeserializer, ShouldThrowOnInvalidLength) {
  // given
  NullableArrayDeserializer<StringDeserializer> testee;
  Buffer::OwnedImpl buffer;

  const int32_t len = -2; // -1 is OK for NULLABLE_ARRAY.
  encoder.encode(len, buffer);

  absl::string_view data = {getRawData(buffer), 1024};

  // when
  // then
  EXPECT_THROW(testee.feed(data), EnvoyException);
}

// Nullable compact generic-array tests.

TEST(NullableCompactArrayDeserializer, ShouldConsumeCorrectAmountOfData) {
  const NullableArray<int32_t> value{{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}};
  serializeCompactThenDeserializeAndCheckEquality<
      NullableCompactArrayDeserializer<Int32Deserializer>>(value);
}

TEST(NullableCompactArrayDeserializer, ShouldConsumeNullArray) {
  const NullableArray<int32_t> value = absl::nullopt;
  serializeCompactThenDeserializeAndCheckEquality<
      NullableCompactArrayDeserializer<Int32Deserializer>>(value);
}

TEST(NullableCompactArrayDeserializer, ShouldConsumeCorrectAmountOfDataForLargeInput) {
  std::vector<int32_t> raw;
  raw.reserve(4096);
  for (int32_t i = 0; i < 4096; ++i) {
    raw.push_back(i);
  }
  const NullableArray<int32_t> value{raw};
  serializeCompactThenDeserializeAndCheckEquality<
      NullableCompactArrayDeserializer<Int32Deserializer>>(value);
}

// UUID.

TEST(UuidDeserializer, ShouldDeserialize) {
  const Uuid value = {13, 42};
  serializeThenDeserializeAndCheckEquality<UuidDeserializer>(value);
}

// Tagged fields.

TEST(TaggedFieldDeserializer, ShouldConsumeCorrectAmountOfData) {
  const TaggedField value{200, Bytes{1, 2, 3, 4, 5, 6}};
  serializeCompactThenDeserializeAndCheckEquality<TaggedFieldDeserializer>(value);
}

TEST(TaggedFieldsDeserializer, ShouldConsumeCorrectAmountOfData) {
  std::vector<TaggedField> fields;
  for (uint32_t i = 0; i < 200; ++i) {
    const TaggedField tagged_field = {i, Bytes{1, 2, 3, 4}};
    fields.push_back(tagged_field);
  }
  const TaggedFields value{fields};
  serializeCompactThenDeserializeAndCheckEquality<TaggedFieldsDeserializer>(value);
}

// Just a helper to write shorter tests.
template <typename T> Bytes toBytes(uint32_t fn(const T arg, Bytes& out), const T arg) {
  Bytes res;
  fn(arg, res);
  return res;
}

TEST(VarlenUtils, ShouldEncodeUnsignedVarInt) {
  const auto testee = VarlenUtils::writeUnsignedVarint;
  ASSERT_EQ(toBytes<uint32_t>(testee, 0), Bytes({0x00}));
  ASSERT_EQ(toBytes<uint32_t>(testee, 1), Bytes({0x01}));
  ASSERT_EQ(toBytes<uint32_t>(testee, 127), Bytes({0x7f}));
  ASSERT_EQ(toBytes<uint32_t>(testee, 128), Bytes({0x80, 0x01}));
  ASSERT_EQ(toBytes<uint32_t>(testee, 2147483647), Bytes({0xFF, 0xFF, 0xFF, 0xFF, 0x07}));
  ASSERT_EQ(toBytes<uint32_t>(testee, std::numeric_limits<uint32_t>::max()),
            Bytes({0xFF, 0xFF, 0xFF, 0xFF, 0x0F}));
}

TEST(VarlenUtils, ShouldEncodeSignedVarInt) {
  const auto testee = VarlenUtils::writeVarint;
  ASSERT_EQ(toBytes<int32_t>(testee, 0), Bytes({0x00}));
  ASSERT_EQ(toBytes<int32_t>(testee, 1), Bytes({0x02}));
  ASSERT_EQ(toBytes<int32_t>(testee, 63), Bytes({0x7e}));
  ASSERT_EQ(toBytes<int32_t>(testee, 64), Bytes({0x80, 0x01}));
  ASSERT_EQ(toBytes<int32_t>(testee, -1), Bytes({0x01}));
  ASSERT_EQ(toBytes<int32_t>(testee, std::numeric_limits<int32_t>::min()),
            Bytes({0xFF, 0xFF, 0xFF, 0xFF, 0x0F}));
  ASSERT_EQ(toBytes<int32_t>(testee, std::numeric_limits<int32_t>::max()),
            Bytes({0xFE, 0xFF, 0xFF, 0xFF, 0x0F}));
}

TEST(VarlenUtils, ShouldEncodeVarLong) {
  const auto testee = VarlenUtils::writeVarlong;
  ASSERT_EQ(toBytes<int64_t>(testee, 0), Bytes({0x00}));
  ASSERT_EQ(toBytes<int64_t>(testee, 1), Bytes({0x02}));
  ASSERT_EQ(toBytes<int64_t>(testee, 63), Bytes({0x7e}));
  ASSERT_EQ(toBytes<int64_t>(testee, 64), Bytes({0x80, 0x01}));
  ASSERT_EQ(toBytes<int64_t>(testee, -1), Bytes({0x01}));
  ASSERT_EQ(toBytes<int64_t>(testee, std::numeric_limits<int64_t>::min()),
            Bytes({0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x01}));
  ASSERT_EQ(toBytes<int64_t>(testee, std::numeric_limits<int64_t>::max()),
            Bytes({0xFE, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x01}));
}

} // namespace SerializationTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/test/serialization_utilities.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {

void assertStringViewIncrement(const absl::string_view incremented,
                               const absl::string_view original, const size_t difference) {

  ASSERT_EQ(incremented.data(), original.data() + difference);
  ASSERT_EQ(incremented.size(), original.size() - difference);
}

const char* getRawData(const Buffer::Instance& buffer) {
  Buffer::RawSliceVector slices = buffer.getRawSlices(1);
  ASSERT(slices.size() == 1);
  return reinterpret_cast<const char*>((slices[0]).mem_);
}

} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
{#
  Creates 'serialization_composite_test.cc'.

  Template for composite serializer tests (the CompositeDeserializerWith_N_Delegates classes).
  Covers the corner case of 0 delegates, and then uses templating to create tests for 1..N cases.
#}

#include "contrib/kafka/filters/network/source/external/serialization_composite.h"

#include "contrib/kafka/filters/network/test/serialization_utilities.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {

/**
 * Tests in this class are supposed to check whether serialization operations on composite
 * deserializers are correct.
 */

// Tests for composite deserializer with 0 fields (corner case).

struct CompositeResultWith0Fields {
  uint32_t encode(Buffer::Instance&, EncodingContext&) const { return 0; }
  bool operator==(const CompositeResultWith0Fields&) const { return true; }
};

using TestCompositeDeserializer0 = CompositeDeserializerWith0Delegates<CompositeResultWith0Fields>;

// Composite with 0 delegates is special case: it's always ready.
TEST(CompositeDeserializerWith0Delegates, EmptyBufferShouldBeReady) {
  // given
  const TestCompositeDeserializer0 testee{};
  // when, then
  ASSERT_EQ(testee.ready(), true);
}

TEST(CompositeDeserializerWith0Delegates, ShouldDeserialize) {
  const CompositeResultWith0Fields expected{};
  serializeThenDeserializeAndCheckEquality<TestCompositeDeserializer0>(expected);
}

// Tests for composite deserializer with N+ fields.

{% for field_count in counts %}
struct CompositeResultWith{{ field_count }}Fields {
  {% for field in range(1, field_count + 1) %}
  const std::string field{{ field }}_;
  {% endfor %}

  uint32_t encode(Buffer::Instance& dst, EncodingContext& encoder) const {
    uint32_t written{0};
    {% for field in range(1, field_count + 1) %}
    written += encoder.encode(field{{ field }}_, dst);
    {% endfor %}
    return written;
  }

  bool operator==(const CompositeResultWith{{ field_count }}Fields& rhs) const {
    return true
    {% for field in range(1, field_count + 1) %} && field{{ field }}_ == rhs.field{{ field }}_
    {% endfor %};
  }
};

using TestCompositeDeserializer{{ field_count }} =
  CompositeDeserializerWith{{ field_count }}Delegates<
  CompositeResultWith{{ field_count }}Fields
  {% for field in range(1, field_count + 1) %}, StringDeserializer{% endfor %}>;

TEST(CompositeDeserializerWith{{ field_count }}Delegates, EmptyBufferShouldNotBeReady) {
  // given
  const TestCompositeDeserializer{{ field_count }} testee{};
  // when, then
  ASSERT_EQ(testee.ready(), false);
}

TEST(CompositeDeserializerWith{{ field_count }}Delegates, ShouldDeserialize) {
  const CompositeResultWith{{ field_count }}Fields expected{
    {% for field in range(1, field_count + 1) %}"s{{ field }}", {% endfor %}
  };
  serializeThenDeserializeAndCheckEquality<TestCompositeDeserializer{{ field_count }}>(expected);
}
{% endfor %}

} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#!/usr/bin/python

# Launcher for generating composite serializer tests.

import contrib.kafka.filters.network.source.serialization.generator as generator
import sys
import os


def main():
    """
  Serialization composite test generator
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  Generates test source files for composite deserializers.
  The files are generated, as they are extremely repetitive (tests for composite deserializer
  for 0..9 sub-deserializers).

  Usage:
    launcher.py LOCATION_OF_OUTPUT_FILE
  where:
  LOCATION_OF_OUTPUT_FILE : location of 'serialization_composite_test.cc'.

  Creates 'serialization_composite_test.cc' - tests composite deserializers.

  Template used is 'serialization_composite_test_cc.j2'.
  """
    serialization_composite_test_cc_file = os.path.abspath(sys.argv[1])
    generator.generate_test_code(serialization_composite_test_cc_file)


if __name__ == "__main__":
    main()
{#
  Template for 'request_codec_request_test.cc'.

  Provides integration tests using Kafka codec.
  The tests do the following:
  - create the message,
  - serialize the message into buffer,
  - pass the buffer to the codec,
  - capture messages received in callback,
  - verify that captured messages are identical to the ones sent.
#}
#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/request_codec.h"

#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "contrib/kafka/filters/network/test/serialization_utilities.h"

#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace RequestCodecRequestTest {

class RequestCodecRequestTest : public testing::Test, public MessageBasedTest<RequestEncoder> {};

using RequestCapturingCallback = CapturingCallback<RequestCallback, AbstractRequestSharedPtr,
  RequestParseFailureSharedPtr>;

{% for message_type in message_types %}

// Integration test for {{ message_type.name }} messages.

TEST_F(RequestCodecRequestTest, ShouldHandle{{ message_type.name }}Messages) {
  // given
  using RequestUnderTest = Request<{{ message_type.name }}>;

  std::vector<RequestUnderTest> sent;
  int32_t correlation = 0;

  {% for field_list in message_type.compute_field_lists() %}
  for (int i = 0; i < 100; ++i ) {
    {# Request header cannot contain tagged fields if request does not support them. #}
    const TaggedFields tagged_fields = requestUsesTaggedFieldsInHeader(
      {{ message_type.get_extra('api_key') }}, {{ field_list.version }}) ?
        TaggedFields{ { TaggedField{ 10, Bytes{1, 2, 3, 4} } } }:
        TaggedFields({});
    const RequestHeader header = {
      {{ message_type.get_extra('api_key') }},
      {{ field_list.version }},
      correlation++,
      "id",
      tagged_fields
    };
    const {{ message_type.name }} data = { {{ field_list.example_value() }} };
    const RequestUnderTest request = {header, data};
    putMessageIntoBuffer(request);
    sent.push_back(request);
  }
  {% endfor %}

  const InitialParserFactory& initial_parser_factory = InitialParserFactory::getDefaultInstance();
  const RequestParserResolver& request_parser_resolver =
    RequestParserResolver::getDefaultInstance();
  const auto callback = std::make_shared<RequestCapturingCallback>();

  RequestDecoder testee{initial_parser_factory, request_parser_resolver, {callback}};

  // when
  testee.onData(buffer_);

  // then
  const std::vector<AbstractRequestSharedPtr>& received = callback->getCapturedMessages();
  ASSERT_EQ(received.size(), sent.size());
  ASSERT_EQ(received.size(), correlation);

  for (size_t i = 0; i < received.size(); ++i) {
    const std::shared_ptr<RequestUnderTest> request =
      std::dynamic_pointer_cast<RequestUnderTest>(received[i]);
    ASSERT_NE(request, nullptr);
    ASSERT_EQ(*request, sent[i]);
  }
}
{% endfor %}

} // namespace RequestCodecRequestTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
{#
  Template for 'response_utilities.cc'.
  This file contains implementation of response-related methods contained in 'message_utilities.h'.
#}

#include <algorithm>

#include "contrib/kafka/filters/network/test/message_utilities.h"

#include "contrib/kafka/filters/network/source/external/responses.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {

int16_t MessageUtilities::responseApiVersions(const int16_t api_key) {
  switch (api_key) {
  {% for message_type in message_types %}
  case {{ message_type.get_extra('api_key') }} :
    return {{ message_type.compute_field_lists() | length }} ;
  {% endfor %}
  default:
    throw EnvoyException("unsupported api key used in test code");
  }
}

std::vector<AbstractResponseSharedPtr> MessageUtilities::makeResponses(
  const int16_t api_key, int32_t& correlation_id) {

  const std::vector<int16_t> api_keys = apiKeys();
  if (std::find(api_keys.begin(), api_keys.end(), api_key) == api_keys.end()) {
    throw EnvoyException("unsupported api key used in test code");
  }

  std::vector<AbstractResponseSharedPtr> result;
  {% for message_type in message_types %}
  if ({{ message_type.get_extra('api_key') }} == api_key) {
    {% for field_list in message_type.compute_field_lists() %}
    {
      const ResponseMetadata metadata = {
          {{ message_type.get_extra('api_key') }}, {{ field_list.version }}, correlation_id++ };
      const {{ message_type.name }} data = { {{ field_list.example_value() }} };
      const AbstractResponseSharedPtr response =
          std::make_shared<Response<{{ message_type.name }}>>(metadata, data);
      result.push_back(response);
    }
    {% endfor %}
  }
  {% endfor %}
  return result;
}

std::vector<AbstractResponseSharedPtr> MessageUtilities::makeAllResponses() {
  std::vector<AbstractResponseSharedPtr> result;
  int32_t correlation_id = 0;
  for (const int16_t i : MessageUtilities::apiKeys()) {
    const std::vector<AbstractResponseSharedPtr> tmp =
        MessageUtilities::makeResponses(i, correlation_id);
    result.insert(result.end(), tmp.begin(), tmp.end());
  }
  return result;
}

std::string MessageUtilities::responseMetric(const int16_t api_key) {
  switch (api_key) {
  {% for message_type in message_types %}
  case {{ message_type.get_extra('api_key') }} :
    return "kafka.prefix.response.{{ message_type.name_in_c_case() }}" ;
  {% endfor %}
  default:
    throw EnvoyException("unsupported api key used in test code");
  }
}

} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
{#
  Template for request serialization/deserialization tests.
  For every request, we want to check if it can be serialized and deserialized properly.
#}

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/request_codec.h"

#include "contrib/kafka/filters/network/test/buffer_based_test.h"

#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace RequestTest {

class RequestTest : public testing::Test, public MessageBasedTest<RequestEncoder> {
protected:
  template <typename T> std::shared_ptr<T> serializeAndDeserialize(T message);
};

class MockMessageListener : public RequestCallback {
public:
  MOCK_METHOD(void, onMessage, (AbstractRequestSharedPtr));
  MOCK_METHOD(void, onFailedParse, (RequestParseFailureSharedPtr));
};

/**
 * Helper method.
 * Takes an instance of a request, serializes it, then deserializes it.
 * This method gets executed for every request * version pair.
 */
template <typename T> std::shared_ptr<T> RequestTest::serializeAndDeserialize(T message) {
  putMessageIntoBuffer(message);

  std::shared_ptr<MockMessageListener> mock_listener = std::make_shared<MockMessageListener>();
  RequestDecoder testee{ {mock_listener} };

  AbstractRequestSharedPtr received_message;
  EXPECT_CALL(*mock_listener, onMessage(testing::_))
    .WillOnce(testing::SaveArg<0>(&received_message));

  testee.onData(buffer_);

  return std::dynamic_pointer_cast<T>(received_message);
};

{#
  Concrete tests for each message_type and version (field_list).
  Each request is naively constructed using some default values
  (put "string" as std::string, 32 as uint32_t, etc.).
#}
{% for message_type in message_types %}{% for field_list in message_type.compute_field_lists() %}
TEST_F(RequestTest, ShouldParse{{ message_type.name }}V{{ field_list.version }}) {
  // given
  {{ message_type.name }} data = { {{ field_list.example_value() }} };
  Request<{{ message_type.name }}> message = { {
    {{ message_type.get_extra('api_key') }}, {{ field_list.version }}, 0, absl::nullopt }, data };

  // when
  auto received = serializeAndDeserialize(message);

  // then
  ASSERT_NE(received, nullptr);
  ASSERT_EQ(*received, message);
}
{% endfor %}{% endfor %}

} // namespace RequestTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
{#
  Template for response serialization/deserialization tests.
  For every response, we want to check if it can be serialized and deserialized properly.
#}

#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/source/response_codec.h"

#include "contrib/kafka/filters/network/test/buffer_based_test.h"

#include "gmock/gmock.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace ResponseTest {

class ResponseTest : public testing::Test, public MessageBasedTest<ResponseEncoder> {
protected:
  template <typename T> std::shared_ptr<T> serializeAndDeserialize(T message);
};

class MockMessageListener : public ResponseCallback {
public:
  MOCK_METHOD(void, onMessage, (AbstractResponseSharedPtr));
  MOCK_METHOD(void, onFailedParse, (ResponseMetadataSharedPtr));
};

/**
 * Helper method.
 * Takes an instance of a response, serializes it, then deserializes it.
 * This method gets executed for every response * version pair.
 */
template <typename T> std::shared_ptr<T> ResponseTest::serializeAndDeserialize(T message) {
  putMessageIntoBuffer(message);

  std::shared_ptr<MockMessageListener> mock_listener = std::make_shared<MockMessageListener>();
  ResponseDecoder testee{ {mock_listener} };
  const ResponseMetadata& metadata = message.metadata_;
  testee.expectResponse(metadata.correlation_id_, metadata.api_key_, metadata.api_version_);

  AbstractResponseSharedPtr received_message;
  EXPECT_CALL(*mock_listener, onMessage(testing::_))
    .WillOnce(testing::SaveArg<0>(&received_message));

  testee.onData(buffer_);

  return std::dynamic_pointer_cast<T>(received_message);
};

{#
  Concrete tests for each message_type and version (field_list).
  Each response is naively constructed using some default values
  (put "string" as std::string, 32 as uint32_t, etc.).
#}
{% for message_type in message_types %}{% for field_list in message_type.compute_field_lists() %}
TEST_F(ResponseTest, ShouldParse{{ message_type.name }}V{{ field_list.version }}) {
  // given
  {{ message_type.name }} data = { {{ field_list.example_value() }} };
  Response<{{ message_type.name }}> message = { {
    {{ message_type.get_extra('api_key') }}, {{ field_list.version }}, 0 }, data };

  // when
  auto received = serializeAndDeserialize(message);

  // then
  ASSERT_NE(received, nullptr);
  ASSERT_EQ(*received, message);
}
{% endfor %}{% endfor %}

} // namespace ResponseTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
{#
  Template for 'response_codec_response_test.cc'.

  Provides integration tests using Kafka codec.
  The tests do the following:
  - create the message,
  - serialize the message into buffer,
  - pass the buffer to the codec,
  - capture messages received in callback,
  - verify that captured messages are identical to the ones sent.
#}
#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/source/response_codec.h"

#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "contrib/kafka/filters/network/test/serialization_utilities.h"

#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace ResponseCodecResponseTest {

class ResponseCodecResponseTest : public testing::Test, public MessageBasedTest<ResponseEncoder> {};

using ResponseCapturingCallback = CapturingCallback<ResponseCallback, AbstractResponseSharedPtr,
  ResponseMetadataSharedPtr>;

{% for message_type in message_types %}

// Integration test for {{ message_type.name }} messages.

TEST_F(ResponseCodecResponseTest, ShouldHandle{{ message_type.name }}Messages) {
  // given
  const auto callback = std::make_shared<ResponseCapturingCallback>();
  ResponseDecoder testee{ {callback} };

  using ResponseUnderTest = Response<{{ message_type.name }}>;

  std::vector<ResponseUnderTest> sent;
  int32_t correlation_id = 0;

  {% for field_list in message_type.compute_field_lists() %}
  for (int i = 0; i < 100; ++i ) {
    {# Response header cannot contain tagged fields if response does not support them. #}
    const TaggedFields tagged_fields = responseUsesTaggedFieldsInHeader(
      {{ message_type.get_extra('api_key') }}, {{ field_list.version }}) ?
        TaggedFields{ { TaggedField{ 10, Bytes{1, 2, 3, 4} } } }:
        TaggedFields({});
    const ResponseMetadata metadata = {
      {{ message_type.get_extra('api_key') }},
      {{ field_list.version }},
      ++correlation_id,
      tagged_fields,
    };
    const {{ message_type.name }} data = { {{ field_list.example_value() }} };
    const ResponseUnderTest response = {metadata, data};
    putMessageIntoBuffer(response);
    testee.expectResponse(
      correlation_id, {{ message_type.get_extra('api_key') }}, {{ field_list.version }});
    sent.push_back(response);
  }
  {% endfor %}

  // when
  testee.onData(buffer_);

  // then
  const std::vector<AbstractResponseSharedPtr>& received = callback->getCapturedMessages();
  ASSERT_EQ(received.size(), sent.size());
  ASSERT_EQ(received.size(), correlation_id);

  for (size_t i = 0; i < received.size(); ++i) {
    const std::shared_ptr<ResponseUnderTest> response =
      std::dynamic_pointer_cast<ResponseUnderTest>(received[i]);
    ASSERT_NE(response, nullptr);
    ASSERT_EQ(*response, sent[i]);
  }
}
{% endfor %}

} // namespace ResponseCodecResponseTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
{#
  Template for 'request_utilities.cc'.
  This file contains implementation of request-related methods contained in 'message_utilities.h'.
#}

#include <algorithm>

#include "contrib/kafka/filters/network/test/message_utilities.h"

#include "contrib/kafka/filters/network/source/external/requests.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {

std::vector<int16_t> MessageUtilities::apiKeys() {
  std::vector<int16_t> result;
  {% for message_type in message_types %}
  result.push_back({{ message_type.get_extra('api_key') }});
  {% endfor %}
  return result;
}

int16_t MessageUtilities::requestApiVersions(const int16_t api_key) {
  switch (api_key) {
  {% for message_type in message_types %}
  case {{ message_type.get_extra('api_key') }} :
    return {{ message_type.compute_field_lists() | length }} ;
  {% endfor %}
  default:
    throw EnvoyException("unsupported api key used in test code");
  }
}

std::vector<AbstractRequestSharedPtr> MessageUtilities::makeRequests(
  const int16_t api_key, int32_t& correlation_id) {

  const std::vector<int16_t> api_keys = apiKeys();
  if (std::find(api_keys.begin(), api_keys.end(), api_key) == api_keys.end()) {
    throw EnvoyException("unsupported api key used in test code");
  }

  std::vector<AbstractRequestSharedPtr> result;
  {% for message_type in message_types %}
  if ({{ message_type.get_extra('api_key') }} == api_key) {
    {% for field_list in message_type.compute_field_lists() %}
    {
      const RequestHeader header = {
          {{ message_type.get_extra('api_key') }}, {{ field_list.version }}, correlation_id++,
          "id" };
      const {{ message_type.name }} data = { {{ field_list.example_value() }} };
      const AbstractRequestSharedPtr request = std::make_shared<Request<{{ message_type.name }}>>(
          header, data);
      result.push_back(request);
    }
    {% endfor %}
  }
  {% endfor %}
  return result;
}

std::vector<AbstractRequestSharedPtr> MessageUtilities::makeAllRequests() {
  std::vector<AbstractRequestSharedPtr> result;
  int32_t correlation_id = 0;
  for (const int16_t i : MessageUtilities::apiKeys()) {
    const std::vector<AbstractRequestSharedPtr> tmp =
        MessageUtilities::makeRequests(i, correlation_id);
    result.insert(result.end(), tmp.begin(), tmp.end());
  }
  return result;
}

std::string MessageUtilities::requestMetric(const int16_t api_key) {
  switch (api_key) {
  {% for message_type in message_types %}
  case {{ message_type.get_extra('api_key') }} :
    return "kafka.prefix.request.{{ message_type.name_in_c_case() }}" ;
  {% endfor %}
  default:
    throw EnvoyException("unsupported api key used in test code");
  }
}

} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#!/usr/bin/python

# Launcher for generating Kafka protocol tests.

import contrib.kafka.filters.network.source.protocol.generator as generator
import sys
import os


def main():
    """
  Kafka test generator script
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~
  Generates tests from Kafka protocol specification.

  Usage:
    launcher.py MESSAGE_TYPE OUTPUT_FILES INPUT_FILES
  where:
  MESSAGE_TYPE : 'request' or 'response'
  OUTPUT_FILES : location of 'requests_test.cc'/'responses_test.cc',
                 'request_codec_request_test.cc' / 'response_codec_response_test.cc',
                 'request_utilities.cc.cc' / 'response_utilities.cc'.
  INPUT_FILES: Kafka protocol json files to be processed.

  Kafka spec files are provided in Kafka clients jar file.

  Files created are:
    - ${MESSAGE_TYPE}s_test.cc - serialization/deserialization tests for kafka structures,
    - ${MESSAGE_TYPE}_codec_${MESSAGE_TYPE}_test.cc - integration tests involving codec for all
      request/response operations,
    - ${MESSAGE_TYPE}_utilities.cc - utilities for creating sample messages of given type.

  Templates used are:
  - to create '${MESSAGE_TYPE}s_test.cc': ${MESSAGE_TYPE}s_test_cc.j2,
  - to create '${MESSAGE_TYPE}_codec_${MESSAGE_TYPE}_test.cc' -
      ${MESSAGE_TYPE}_codec_${MESSAGE_TYPE}_test_cc.j2,
  - to create '${MESSAGE_TYPE}_utilities.cc' - ${MESSAGE_TYPE}_utilities_cc.j2.
  """
    type = sys.argv[1]
    header_test_cc_file = os.path.abspath(sys.argv[2])
    codec_test_cc_file = os.path.abspath(sys.argv[3])
    utilities_cc_file = os.path.abspath(sys.argv[4])
    input_files = sys.argv[5:]
    generator.generate_test_code(
        type, header_test_cc_file, codec_test_cc_file, utilities_cc_file, input_files)


if __name__ == "__main__":
    main()
#include "contrib/kafka/filters/network/source/request_codec.h"
#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "contrib/kafka/filters/network/test/serialization_utilities.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace RequestCodecIntegrationTest {

class RequestCodecIntegrationTest : public testing::Test,
                                    public MessageBasedTest<RequestEncoder> {};

using RequestCapturingCallback =
    CapturingCallback<RequestCallback, AbstractRequestSharedPtr, RequestParseFailureSharedPtr>;

// Other request types are tested in (generated) 'request_codec_request_test.cc'.
TEST_F(RequestCodecIntegrationTest, ShouldProduceAbortedMessageOnUnknownData) {
  // given
  // As real api keys have values below 100, the messages generated in this loop should not be
  // recognized by the codec.
  const int16_t base_api_key = 100;
  std::vector<RequestHeader> sent_headers;
  for (int16_t i = 0; i < 1000; ++i) {
    const int16_t api_key = static_cast<int16_t>(base_api_key + i);
    const RequestHeader header = {api_key, 0, 0, "client-id"};
    const std::vector<unsigned char> data = std::vector<unsigned char>(1024);
    const auto message = Request<std::vector<unsigned char>>{header, data};
    putMessageIntoBuffer(message);
    sent_headers.push_back(header);
  }

  const InitialParserFactory& initial_parser_factory = InitialParserFactory::getDefaultInstance();
  const RequestParserResolver& request_parser_resolver =
      RequestParserResolver::getDefaultInstance();
  const auto request_callback = std::make_shared<RequestCapturingCallback>();

  RequestDecoder testee{initial_parser_factory, request_parser_resolver, {request_callback}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(request_callback->getCapturedMessages().size(), 0);

  const std::vector<RequestParseFailureSharedPtr>& parse_failures =
      request_callback->getParseFailures();
  ASSERT_EQ(parse_failures.size(), sent_headers.size());

  for (size_t i = 0; i < parse_failures.size(); ++i) {
    const std::shared_ptr<RequestParseFailure> failure_data =
        std::dynamic_pointer_cast<RequestParseFailure>(parse_failures[i]);
    ASSERT_NE(failure_data, nullptr);
    ASSERT_EQ(failure_data->request_header_, sent_headers[i]);
  }
}

} // namespace RequestCodecIntegrationTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/response_codec.h"
#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "contrib/kafka/filters/network/test/serialization_utilities.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace ResponseCodecIntegrationTest {

class ResponseCodecIntegrationTest : public testing::Test,
                                     public MessageBasedTest<ResponseEncoder> {};

using ResponseCapturingCallback =
    CapturingCallback<ResponseCallback, AbstractResponseSharedPtr, ResponseMetadataSharedPtr>;

// Other response types are tested in (generated) 'response_codec_response_test.cc'.
TEST_F(ResponseCodecIntegrationTest, ShouldProduceAbortedMessageOnUnknownData) {
  // given
  const auto callback = std::make_shared<ResponseCapturingCallback>();
  ResponseDecoder testee{{callback}};

  // As real api keys have values below 100, the messages generated in this loop should not be
  // recognized by the codec.
  const int16_t base_api_key = 100;
  const int32_t base_correlation_id = 0;
  std::vector<ResponseMetadata> sent;

  for (int16_t i = 0; i < 1000; ++i) {
    const int16_t api_key = static_cast<int16_t>(base_api_key + i);
    const int16_t api_version = 0;
    const int32_t correlation_id = base_correlation_id + i;

    const ResponseMetadata metadata = {api_key, api_version, correlation_id};
    const std::vector<unsigned char> data = std::vector<unsigned char>(1024);
    const auto message = Response<std::vector<unsigned char>>{metadata, data};
    putMessageIntoBuffer(message);
    sent.push_back(metadata);
    // We need to register the response, so the parser knows what to expect.
    testee.expectResponse(correlation_id, api_key, api_version);
  }

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(callback->getCapturedMessages().size(), 0);

  const std::vector<ResponseMetadataSharedPtr>& parse_failures = callback->getParseFailures();
  ASSERT_EQ(parse_failures.size(), sent.size());
  for (size_t i = 0; i < parse_failures.size(); ++i) {
    ASSERT_EQ(*(parse_failures[i]), sent[i]);
  }
}

TEST_F(ResponseCodecIntegrationTest, ShouldThrowIfAttemptingToParseResponseButNothingIsExpected) {
  // given
  const auto callback = std::make_shared<ResponseCapturingCallback>();
  ResponseDecoder testee{{callback}};

  putGarbageIntoBuffer();

  // when
  bool caught = false;
  try {
    testee.onData(buffer_);
  } catch (EnvoyException& e) {
    caught = true;
  }

  // then
  ASSERT_EQ(caught, true);
}

} // namespace ResponseCodecIntegrationTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/request_codec.h"
#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::AnyNumber;
using testing::Invoke;
using testing::Return;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace RequestCodecUnitTest {

class MockParserFactory : public InitialParserFactory {
public:
  MOCK_METHOD(RequestParserSharedPtr, create, (const RequestParserResolver&), (const));
};

class MockParser : public RequestParser {
public:
  MOCK_METHOD(RequestParseResponse, parse, (absl::string_view&));
};

using MockParserSharedPtr = std::shared_ptr<MockParser>;

class MockRequestParserResolver : public RequestParserResolver {
public:
  MockRequestParserResolver() : RequestParserResolver({}){};
  MOCK_METHOD(RequestParserSharedPtr, createParser, (int16_t, int16_t, RequestContextSharedPtr),
              (const));
};

class MockRequestCallback : public RequestCallback {
public:
  MOCK_METHOD(void, onMessage, (AbstractRequestSharedPtr));
  MOCK_METHOD(void, onFailedParse, (RequestParseFailureSharedPtr));
};

using MockRequestCallbackSharedPtr = std::shared_ptr<MockRequestCallback>;

class RequestCodecUnitTest : public testing::Test, public BufferBasedTest {
protected:
  MockParserFactory initial_parser_factory_{};
  MockRequestParserResolver parser_resolver_{};
  MockRequestCallbackSharedPtr callback_{std::make_shared<MockRequestCallback>()};
};

RequestParseResponse consumeOneByte(absl::string_view& data) {
  data = {data.data() + 1, data.size() - 1};
  return RequestParseResponse::stillWaiting();
}

TEST_F(RequestCodecUnitTest, ShouldDoNothingIfParserReturnsWaiting) {
  // given
  putGarbageIntoBuffer();

  MockParserSharedPtr parser = std::make_shared<MockParser>();
  EXPECT_CALL(*parser, parse(_)).Times(AnyNumber()).WillRepeatedly(Invoke(consumeOneByte));

  EXPECT_CALL(initial_parser_factory_, create(_)).WillOnce(Return(parser));

  EXPECT_CALL(*callback_, onMessage(_)).Times(0);
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  RequestDecoder testee{initial_parser_factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  // There were no interactions with `callback_`.
}

TEST_F(RequestCodecUnitTest, ShouldUseNewParserAsResponse) {
  // given
  putGarbageIntoBuffer();

  MockParserSharedPtr parser1 = std::make_shared<MockParser>();
  MockParserSharedPtr parser2 = std::make_shared<MockParser>();
  MockParserSharedPtr parser3 = std::make_shared<MockParser>();
  EXPECT_CALL(*parser1, parse(_)).WillOnce(Return(RequestParseResponse::nextParser(parser2)));
  EXPECT_CALL(*parser2, parse(_)).WillOnce(Return(RequestParseResponse::nextParser(parser3)));
  EXPECT_CALL(*parser3, parse(_)).Times(AnyNumber()).WillRepeatedly(Invoke(consumeOneByte));

  EXPECT_CALL(initial_parser_factory_, create(_)).WillOnce(Return(parser1));
  EXPECT_CALL(parser_resolver_, createParser(_, _, _)).Times(0);

  EXPECT_CALL(*callback_, onMessage(_)).Times(0);
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  RequestDecoder testee{initial_parser_factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), parser3);
  // Also, there were no interactions with `callback_`.
}

TEST_F(RequestCodecUnitTest, ShouldPassParsedMessageToCallback) {
  // given
  putGarbageIntoBuffer();

  const AbstractRequestSharedPtr parsed_message =
      std::make_shared<Request<int32_t>>(RequestHeader{0, 0, 0, ""}, 0);

  MockParserSharedPtr all_consuming_parser = std::make_shared<MockParser>();
  auto consume_and_return = [&parsed_message](absl::string_view& data) -> RequestParseResponse {
    data = {data.data() + data.size(), 0};
    return RequestParseResponse::parsedMessage(parsed_message);
  };
  EXPECT_CALL(*all_consuming_parser, parse(_)).WillOnce(Invoke(consume_and_return));

  EXPECT_CALL(initial_parser_factory_, create(_)).WillOnce(Return(all_consuming_parser));
  EXPECT_CALL(parser_resolver_, createParser(_, _, _)).Times(0);

  EXPECT_CALL(*callback_, onMessage(parsed_message));
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  RequestDecoder testee{initial_parser_factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), nullptr);
  // Also, `callback_` had `onMessage` invoked once with matching argument.
}

TEST_F(RequestCodecUnitTest, ShouldPassParsedMessageToCallbackAndInitializeNextParser) {
  // given
  putGarbageIntoBuffer();

  const AbstractRequestSharedPtr parsed_message =
      std::make_shared<Request<int32_t>>(RequestHeader{0, 0, 0, absl::nullopt}, 0);

  MockParserSharedPtr parser1 = std::make_shared<MockParser>();
  EXPECT_CALL(*parser1, parse(_))
      .WillOnce(Return(RequestParseResponse::parsedMessage(parsed_message)));

  MockParserSharedPtr parser2 = std::make_shared<MockParser>();
  EXPECT_CALL(*parser2, parse(_)).Times(AnyNumber()).WillRepeatedly(Invoke(consumeOneByte));

  EXPECT_CALL(initial_parser_factory_, create(_))
      .WillOnce(Return(parser1))
      .WillOnce(Return(parser2));

  EXPECT_CALL(*callback_, onMessage(parsed_message));
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  RequestDecoder testee{initial_parser_factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), parser2);
  // Also, `callback_` had `onMessage` invoked once with matching argument.
}

TEST_F(RequestCodecUnitTest, ShouldPassParseFailureDataToCallback) {
  // given
  putGarbageIntoBuffer();

  const RequestParseFailureSharedPtr failure_data =
      std::make_shared<RequestParseFailure>(RequestHeader{0, 0, 0, absl::nullopt});

  MockParserSharedPtr parser = std::make_shared<MockParser>();
  auto consume_and_return = [&failure_data](absl::string_view& data) -> RequestParseResponse {
    data = {data.data() + data.size(), 0};
    return RequestParseResponse::parseFailure(failure_data);
  };
  EXPECT_CALL(*parser, parse(_)).WillOnce(Invoke(consume_and_return));

  EXPECT_CALL(initial_parser_factory_, create(_)).WillOnce(Return(parser));
  EXPECT_CALL(parser_resolver_, createParser(_, _, _)).Times(0);

  EXPECT_CALL(*callback_, onMessage(_)).Times(0);
  EXPECT_CALL(*callback_, onFailedParse(failure_data));

  RequestDecoder testee{initial_parser_factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), nullptr);
  // Also, `callback_` had `onFailedParse` invoked once with matching argument.
}

} // namespace RequestCodecUnitTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/kafka_request_parser.h"
#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "contrib/kafka/filters/network/test/serialization_utilities.h"
#include "gmock/gmock.h"

using testing::_;
using testing::Return;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace KafkaRequestParserTest {

const int32_t FAILED_DESERIALIZER_STEP = 13;

class KafkaRequestParserTest : public testing::Test, public BufferBasedTest {};

class MockRequestParserResolver : public RequestParserResolver {
public:
  MockRequestParserResolver() = default;
  MOCK_METHOD(RequestParserSharedPtr, createParser, (int16_t, int16_t, RequestContextSharedPtr),
              (const));
};

TEST_F(KafkaRequestParserTest, RequestStartParserTestShouldReturnRequestHeaderParser) {
  // given
  MockRequestParserResolver resolver{};
  RequestStartParser testee{resolver};

  int32_t request_len = 1234;
  putIntoBuffer(request_len);

  const absl::string_view orig_data = {getBytes(), 1024};
  absl::string_view data = orig_data;

  // when
  const RequestParseResponse result = testee.parse(data);

  // then
  ASSERT_EQ(result.hasData(), true);
  ASSERT_NE(std::dynamic_pointer_cast<RequestHeaderParser>(result.next_parser_), nullptr);
  ASSERT_EQ(result.message_, nullptr);
  ASSERT_EQ(result.failure_data_, nullptr);
  ASSERT_EQ(testee.contextForTest()->remaining_request_size_, request_len);
  assertStringViewIncrement(data, orig_data, sizeof(int32_t));
}

class MockParser : public RequestParser {
public:
  RequestParseResponse parse(absl::string_view&) override {
    throw EnvoyException("should not be invoked");
  }
};

TEST_F(KafkaRequestParserTest, RequestHeaderParserShouldExtractHeaderAndResolveNextParser) {
  // given
  const MockRequestParserResolver parser_resolver;
  const RequestParserSharedPtr parser{new MockParser{}};
  EXPECT_CALL(parser_resolver, createParser(_, _, _)).WillOnce(Return(parser));

  const int32_t request_len = 1000;
  RequestContextSharedPtr context{new RequestContext()};
  context->remaining_request_size_ = request_len;
  RequestHeaderParser testee{parser_resolver, context};

  const int16_t api_key{1};
  const int16_t api_version{2};
  const int32_t correlation_id{10};
  const NullableString client_id{"aaa"};
  uint32_t header_len = 0;
  header_len += putIntoBuffer(api_key);
  header_len += putIntoBuffer(api_version);
  header_len += putIntoBuffer(correlation_id);
  header_len += putIntoBuffer(client_id);

  const absl::string_view orig_data = putGarbageIntoBuffer();
  absl::string_view data = orig_data;

  // when
  const RequestParseResponse result = testee.parse(data);

  // then
  ASSERT_EQ(result.hasData(), true);
  ASSERT_EQ(result.next_parser_, parser);
  ASSERT_EQ(result.message_, nullptr);
  ASSERT_EQ(result.failure_data_, nullptr);

  const RequestHeader expected_header{api_key, api_version, correlation_id, client_id};
  ASSERT_EQ(testee.contextForTest()->request_header_, expected_header);
  ASSERT_EQ(testee.contextForTest()->remaining_request_size_, request_len - header_len);

  assertStringViewIncrement(data, orig_data, header_len);
}

TEST_F(KafkaRequestParserTest, RequestDataParserShouldHandleDeserializerExceptionsDuringFeeding) {
  // given

  // This deserializer throws during feeding.
  class ThrowingDeserializer : public Deserializer<int32_t> {
  public:
    uint32_t feed(absl::string_view&) override {
      // Move some pointers to simulate data consumption.
      throw EnvoyException("feed");
    };

    bool ready() const override { throw std::runtime_error("should not be invoked at all"); };

    int32_t get() const override { throw std::runtime_error("should not be invoked at all"); };
  };

  RequestContextSharedPtr request_context{new RequestContext{1024, {0, 0, 0, absl::nullopt}}};
  RequestDataParser<int32_t, ThrowingDeserializer> testee{request_context};

  absl::string_view data = putGarbageIntoBuffer();

  // when
  bool caught = false;
  try {
    testee.parse(data);
  } catch (EnvoyException& e) {
    caught = true;
  }

  // then
  ASSERT_EQ(caught, true);
}

// This deserializer consumes FAILED_DESERIALIZER_STEP bytes and returns 0
class SomeBytesDeserializer : public Deserializer<int32_t> {
public:
  uint32_t feed(absl::string_view& data) override {
    data = {data.data() + FAILED_DESERIALIZER_STEP, data.size() - FAILED_DESERIALIZER_STEP};
    return FAILED_DESERIALIZER_STEP;
  };

  bool ready() const override { return true; };

  int32_t get() const override { return 0; };
};

TEST_F(KafkaRequestParserTest,
       RequestDataParserShouldHandleDeserializerReturningReadyButLeavingData) {
  // given
  const int32_t request_size = 1024; // There are still 1024 bytes to read to complete the request.
  RequestContextSharedPtr request_context{
      new RequestContext{request_size, {0, 0, 0, absl::nullopt}}};

  RequestDataParser<int32_t, SomeBytesDeserializer> testee{request_context};

  const absl::string_view orig_data = putGarbageIntoBuffer();
  absl::string_view data = orig_data;

  // when
  const RequestParseResponse result = testee.parse(data);

  // then
  ASSERT_EQ(result.hasData(), true);
  ASSERT_NE(std::dynamic_pointer_cast<SentinelParser>(result.next_parser_), nullptr);
  ASSERT_EQ(result.message_, nullptr);
  ASSERT_EQ(result.failure_data_, nullptr);

  ASSERT_EQ(testee.contextForTest()->remaining_request_size_,
            request_size - FAILED_DESERIALIZER_STEP);

  assertStringViewIncrement(data, orig_data, FAILED_DESERIALIZER_STEP);
}

TEST_F(KafkaRequestParserTest, SentinelParserShouldConsumeDataUntilEndOfRequest) {
  // given
  const int32_t request_len = 1000;
  RequestContextSharedPtr context{new RequestContext()};
  context->remaining_request_size_ = request_len;
  SentinelParser testee{context};

  const absl::string_view orig_data = putGarbageIntoBuffer(request_len * 2);
  absl::string_view data = orig_data;

  // when
  const RequestParseResponse result = testee.parse(data);

  // then
  ASSERT_EQ(result.hasData(), true);
  ASSERT_EQ(result.next_parser_, nullptr);
  ASSERT_EQ(result.message_, nullptr);
  ASSERT_NE(std::dynamic_pointer_cast<RequestParseFailure>(result.failure_data_), nullptr);

  ASSERT_EQ(testee.contextForTest()->remaining_request_size_, 0);

  assertStringViewIncrement(data, orig_data, request_len);
}

} // namespace KafkaRequestParserTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/response_codec.h"
#include "contrib/kafka/filters/network/test/buffer_based_test.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using testing::_;
using testing::AnyNumber;
using testing::Invoke;
using testing::Return;

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace ResponseCodecUnitTest {

class MockResponseInitialParserFactory : public ResponseInitialParserFactory {
public:
  MOCK_METHOD(ResponseParserSharedPtr, create,
              (ExpectedResponsesSharedPtr, const ResponseParserResolver&), (const));
};

class MockParser : public ResponseParser {
public:
  MOCK_METHOD(ResponseParseResponse, parse, (absl::string_view&));
};

using MockParserSharedPtr = std::shared_ptr<MockParser>;

class MockResponseParserResolver : public ResponseParserResolver {
public:
  MockResponseParserResolver() : ResponseParserResolver({}){};
  MOCK_METHOD(ResponseParserSharedPtr, createParser, (ResponseContextSharedPtr), (const));
};

class MockResponseCallback : public ResponseCallback {
public:
  MOCK_METHOD(void, onMessage, (AbstractResponseSharedPtr));
  MOCK_METHOD(void, onFailedParse, (ResponseMetadataSharedPtr));
};

using MockResponseCallbackSharedPtr = std::shared_ptr<MockResponseCallback>;

class ResponseCodecUnitTest : public testing::Test, public BufferBasedTest {
protected:
  MockResponseInitialParserFactory factory_{};
  MockResponseParserResolver parser_resolver_{};
  MockResponseCallbackSharedPtr callback_{std::make_shared<MockResponseCallback>()};
};

ResponseParseResponse consumeOneByte(absl::string_view& data) {
  data = {data.data() + 1, data.size() - 1};
  return ResponseParseResponse::stillWaiting();
}

TEST_F(ResponseCodecUnitTest, ShouldDoNothingIfParserReturnsWaiting) {
  // given
  putGarbageIntoBuffer();

  MockParserSharedPtr parser = std::make_shared<MockParser>();
  EXPECT_CALL(*parser, parse(_)).Times(AnyNumber()).WillRepeatedly(Invoke(consumeOneByte));

  EXPECT_CALL(factory_, create(_, _)).WillOnce(Return(parser));
  EXPECT_CALL(parser_resolver_, createParser(_)).Times(0);

  EXPECT_CALL(*callback_, onMessage(_)).Times(0);
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  ResponseDecoder testee{factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  // There were no interactions with `callback_`.
}

TEST_F(ResponseCodecUnitTest, ShouldUseNewParserAsResponse) {
  // given
  putGarbageIntoBuffer();

  MockParserSharedPtr parser1 = std::make_shared<MockParser>();
  MockParserSharedPtr parser2 = std::make_shared<MockParser>();
  MockParserSharedPtr parser3 = std::make_shared<MockParser>();
  EXPECT_CALL(*parser1, parse(_)).WillOnce(Return(ResponseParseResponse::nextParser(parser2)));
  EXPECT_CALL(*parser2, parse(_)).WillOnce(Return(ResponseParseResponse::nextParser(parser3)));
  EXPECT_CALL(*parser3, parse(_)).Times(AnyNumber()).WillRepeatedly(Invoke(consumeOneByte));

  EXPECT_CALL(factory_, create(_, _)).WillOnce(Return(parser1));
  EXPECT_CALL(parser_resolver_, createParser(_)).Times(0);

  EXPECT_CALL(*callback_, onMessage(_)).Times(0);
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  ResponseDecoder testee{factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), parser3);
  // Also, there were no interactions with `callback_`.
}

TEST_F(ResponseCodecUnitTest, ShouldPassParsedMessageToCallback) {
  // given
  putGarbageIntoBuffer();

  const AbstractResponseSharedPtr parsed_message =
      std::make_shared<Response<int32_t>>(ResponseMetadata{0, 0, 0}, 0);

  MockParserSharedPtr all_consuming_parser = std::make_shared<MockParser>();
  auto consume_and_return = [&parsed_message](absl::string_view& data) -> ResponseParseResponse {
    data = {data.data() + data.size(), 0};
    return ResponseParseResponse::parsedMessage(parsed_message);
  };
  EXPECT_CALL(*all_consuming_parser, parse(_)).WillOnce(Invoke(consume_and_return));

  EXPECT_CALL(factory_, create(_, _)).WillOnce(Return(all_consuming_parser));
  EXPECT_CALL(parser_resolver_, createParser(_)).Times(0);

  EXPECT_CALL(*callback_, onMessage(parsed_message));
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  ResponseDecoder testee{factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), nullptr);
  // Also, `callback_` had `onMessage` invoked once with matching argument.
}

TEST_F(ResponseCodecUnitTest, ShouldPassParsedMessageToCallbackAndInitializeNextParser) {
  // given
  putGarbageIntoBuffer();

  const AbstractResponseSharedPtr parsed_message =
      std::make_shared<Response<int32_t>>(ResponseMetadata{0, 0, 0}, 0);

  MockParserSharedPtr parser1 = std::make_shared<MockParser>();
  EXPECT_CALL(*parser1, parse(_))
      .WillOnce(Return(ResponseParseResponse::parsedMessage(parsed_message)));

  MockParserSharedPtr parser2 = std::make_shared<MockParser>();
  EXPECT_CALL(*parser2, parse(_)).Times(AnyNumber()).WillRepeatedly(Invoke(consumeOneByte));

  EXPECT_CALL(factory_, create(_, _)).WillOnce(Return(parser1)).WillOnce(Return(parser2));

  EXPECT_CALL(*callback_, onMessage(parsed_message));
  EXPECT_CALL(*callback_, onFailedParse(_)).Times(0);

  ResponseDecoder testee{factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), parser2);
  // Also, `callback_` had `onMessage` invoked once with matching argument.
}

TEST_F(ResponseCodecUnitTest, ShouldPassParseFailureDataToCallback) {
  // given
  putGarbageIntoBuffer();

  const ResponseMetadataSharedPtr failure_data = std::make_shared<ResponseMetadata>(0, 0, 0);

  MockParserSharedPtr parser = std::make_shared<MockParser>();
  auto consume_and_return = [&failure_data](absl::string_view& data) -> ResponseParseResponse {
    data = {data.data() + data.size(), 0};
    return ResponseParseResponse::parseFailure(failure_data);
  };
  EXPECT_CALL(*parser, parse(_)).WillOnce(Invoke(consume_and_return));

  EXPECT_CALL(factory_, create(_, _)).WillOnce(Return(parser));
  EXPECT_CALL(parser_resolver_, createParser(_)).Times(0);

  EXPECT_CALL(*callback_, onMessage(_)).Times(0);
  EXPECT_CALL(*callback_, onFailedParse(failure_data));

  ResponseDecoder testee{factory_, parser_resolver_, {callback_}};

  // when
  testee.onData(buffer_);

  // then
  ASSERT_EQ(testee.getCurrentParserForTest(), nullptr);
  // Also, `callback_` had `onFailedParse` invoked once with matching argument.
}

} // namespace ResponseCodecUnitTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
load("@base_pip3//:requirements.bzl", "requirement")
load("@rules_python//python:defs.bzl", "py_binary")
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_test",
    "envoy_cc_test_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

envoy_cc_test_library(
    name = "buffer_based_test_lib",
    srcs = [],
    hdrs = ["buffer_based_test.h"],
    deps = [
        "//contrib/kafka/filters/network/source:serialization_lib",
        "//source/common/buffer:buffer_lib",
    ],
)

envoy_cc_test_library(
    name = "serialization_utilities_lib",
    srcs = ["serialization_utilities.cc"],
    hdrs = ["serialization_utilities.h"],
    deps = [
        "//contrib/kafka/filters/network/source:serialization_lib",
        "//source/common/buffer:buffer_lib",
    ],
)

envoy_cc_test(
    name = "serialization_test",
    srcs = ["serialization_test.cc"],
    deps = [
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:serialization_lib",
        "//contrib/kafka/filters/network/source:tagged_fields_lib",
        "//test/mocks/server:server_mocks",
    ],
)

envoy_cc_test(
    name = "serialization_composite_test",
    srcs = ["external/serialization_composite_test.cc"],
    deps = [
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:serialization_lib",
        "//test/mocks/server:server_mocks",
    ],
)

genrule(
    name = "serialization_composite_generated_tests",
    srcs = [],
    outs = ["external/serialization_composite_test.cc"],
    cmd = """
        ./$(location :serialization_composite_test_generator_bin) \
        $(location external/serialization_composite_test.cc)
    """,
    tools = [
        ":serialization_composite_test_generator_bin",
    ],
)

py_binary(
    name = "serialization_composite_test_generator_bin",
    srcs = ["serialization/launcher.py"],
    data = glob(["serialization/*.j2"]),
    main = "serialization/launcher.py",
    deps = [
        "//contrib/kafka/filters/network/source:serialization_composite_generator_lib",
        requirement("Jinja2"),
        requirement("MarkupSafe"),
    ],
)

envoy_cc_test_library(
    name = "message_utilities",
    srcs = [
        "external/request_utilities.cc",
        "external/response_utilities.cc",
    ],
    hdrs = ["message_utilities.h"],
    deps = [
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
    ],
)

envoy_cc_test(
    name = "kafka_request_parser_test",
    srcs = ["kafka_request_parser_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
    ],
)

envoy_cc_test(
    name = "request_codec_unit_test",
    srcs = ["request_codec_unit_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        "//contrib/kafka/filters/network/source:kafka_request_codec_lib",
    ],
)

envoy_cc_test(
    name = "request_codec_integration_test",
    srcs = ["request_codec_integration_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:kafka_request_codec_lib",
    ],
)

envoy_cc_test(
    name = "request_codec_request_test",
    srcs = ["external/request_codec_request_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:kafka_request_codec_lib",
    ],
)

envoy_cc_test(
    name = "requests_test",
    srcs = ["external/requests_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        "//contrib/kafka/filters/network/source:kafka_request_codec_lib",
    ],
)

genrule(
    name = "request_generated_tests",
    srcs = [
        "@kafka_source//:request_protocol_files",
    ],
    outs = [
        "external/requests_test.cc",
        "external/request_codec_request_test.cc",
        "external/request_utilities.cc",
    ],
    cmd = """
        ./$(location :kafka_protocol_test_generator_bin) request \
        $(location external/requests_test.cc) \
        $(location external/request_codec_request_test.cc) \
        $(location external/request_utilities.cc) \
        $(SRCS)
    """,
    tools = [
        ":kafka_protocol_test_generator_bin",
    ],
)

envoy_cc_test(
    name = "kafka_response_parser_test",
    srcs = ["kafka_response_parser_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
    ],
)

envoy_cc_test(
    name = "response_codec_unit_test",
    srcs = ["response_codec_unit_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        "//contrib/kafka/filters/network/source:kafka_response_codec_lib",
    ],
)

envoy_cc_test(
    name = "response_codec_integration_test",
    srcs = ["response_codec_integration_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:kafka_response_codec_lib",
    ],
)

envoy_cc_test(
    name = "response_codec_response_test",
    srcs = ["external/response_codec_response_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        ":serialization_utilities_lib",
        "//contrib/kafka/filters/network/source:kafka_response_codec_lib",
    ],
)

envoy_cc_test(
    name = "responses_test",
    srcs = ["external/responses_test.cc"],
    deps = [
        ":buffer_based_test_lib",
        "//contrib/kafka/filters/network/source:kafka_response_codec_lib",
    ],
)

genrule(
    name = "response_generated_tests",
    srcs = [
        "@kafka_source//:response_protocol_files",
    ],
    outs = [
        "external/responses_test.cc",
        "external/response_codec_response_test.cc",
        "external/response_utilities.cc",
    ],
    cmd = """
        ./$(location :kafka_protocol_test_generator_bin) response \
        $(location external/responses_test.cc) \
        $(location external/response_codec_response_test.cc) \
        $(location external/response_utilities.cc) \
        $(SRCS)
    """,
    tools = [
        ":kafka_protocol_test_generator_bin",
    ],
)

py_binary(
    name = "kafka_protocol_test_generator_bin",
    srcs = ["protocol/launcher.py"],
    data = glob(["protocol/*.j2"]),
    main = "protocol/launcher.py",
    deps = [
        "//contrib/kafka/filters/network/source:kafka_protocol_generator_lib",
        requirement("Jinja2"),
        requirement("MarkupSafe"),
    ],
)

envoy_cc_test(
    name = "metrics_integration_test",
    srcs = ["metrics_integration_test.cc"],
    deps = [
        ":message_utilities",
        "//contrib/kafka/filters/network/source:kafka_metrics_lib",
        "//test/common/stats:stat_test_utility_lib",
    ],
)
#pragma once

#include "source/common/buffer/buffer_impl.h"

#include "absl/container/fixed_array.h"
#include "absl/strings/string_view.h"
#include "contrib/kafka/filters/network/source/serialization.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {

/**
 * Verifies that 'incremented' string view is actually 'original' string view, that has incremented
 * by 'difference' bytes.
 */
void assertStringViewIncrement(absl::string_view incremented, absl::string_view original,
                               size_t difference);

// Helper function converting buffer to raw bytes.
const char* getRawData(const Buffer::Instance& buffer);

// Helper methods for testing serialization and deserialization.
// We have two dimensions to test here: single-pass vs chunks (as we never know how the input is
// going to be delivered), and normal vs compact for some data types (like strings).

// Exactly what is says on the tin:
// 1. serialize expected using Encoder,
// 2. deserialize byte array using testee deserializer,
// 3. verify that testee is ready, and its result is equal to expected,
// 4. verify that data pointer moved correct amount,
// 5. feed testee more data,
// 6. verify that nothing more was consumed (because the testee has been ready since step 3).
template <typename BT>
void serializeThenDeserializeAndCheckEqualityInOneGo(const typename BT::result_type expected) {
  // given
  BT testee{};

  Buffer::OwnedImpl buffer;
  EncodingContext encoder{-1};
  const uint32_t written = encoder.encode(expected, buffer);
  // Insert garbage after serialized payload.
  const uint32_t garbage_size = encoder.encode(Bytes(10000), buffer);

  // Tell parser that there is more data, it should never consume more than written.
  const absl::string_view orig_data = {getRawData(buffer), written + garbage_size};
  absl::string_view data = orig_data;

  // when
  const uint32_t consumed = testee.feed(data);

  // then
  ASSERT_EQ(consumed, written);
  ASSERT_EQ(testee.ready(), true);
  ASSERT_EQ(testee.get(), expected);
  assertStringViewIncrement(data, orig_data, consumed);

  // when - 2
  const uint32_t consumed2 = testee.feed(data);

  // then - 2 (nothing changes)
  ASSERT_EQ(consumed2, 0);
  assertStringViewIncrement(data, orig_data, consumed);
}

// Does the same thing as the above test, but instead of providing whole data at one, it provides
// it in N one-byte chunks.
// This verifies if deserializer keeps state properly (no overwrites etc.).
template <typename BT>
void serializeThenDeserializeAndCheckEqualityWithChunks(const typename BT::result_type expected) {
  // given
  BT testee{};

  Buffer::OwnedImpl buffer;
  EncodingContext encoder{-1};
  const uint32_t written = encoder.encode(expected, buffer);
  // Insert garbage after serialized payload.
  const uint32_t garbage_size = encoder.encode(Bytes(10000), buffer);

  const absl::string_view orig_data = {getRawData(buffer), written + garbage_size};

  // when
  absl::string_view data = orig_data;
  uint32_t consumed = 0;
  for (uint32_t i = 0; i < written; ++i) {
    data = {data.data(), 1}; // Consume data byte-by-byte.
    uint32_t step = testee.feed(data);
    consumed += step;
    ASSERT_EQ(step, 1);
    ASSERT_EQ(data.size(), 0);
  }

  // then
  ASSERT_EQ(consumed, written);
  ASSERT_EQ(testee.ready(), true);
  ASSERT_EQ(testee.get(), expected);

  ASSERT_EQ(data.data(), orig_data.data() + consumed);

  // when - 2
  absl::string_view more_data = {data.data(), garbage_size};
  const uint32_t consumed2 = testee.feed(more_data);

  // then - 2 (nothing changes)
  ASSERT_EQ(consumed2, 0);
  ASSERT_EQ(more_data.data(), data.data());
  ASSERT_EQ(more_data.size(), garbage_size);
}

// Deserialization (only) of compact-encoded data (for data types where we do not need serializer
// code).
template <typename BT>
void deserializeCompactAndCheckEqualityInOneGo(Buffer::Instance& buffer,
                                               const typename BT::result_type expected) {
  // given
  BT testee{};

  EncodingContext encoder{-1};
  const uint32_t written = buffer.length();
  // Insert garbage after serialized payload.
  const uint32_t garbage_size = encoder.encode(Bytes(10000), buffer);
  const char* raw_buffer_ptr =
      reinterpret_cast<const char*>(buffer.linearize(written + garbage_size));
  // Tell parser that there is more data, it should never consume more than written.
  const absl::string_view orig_data = {raw_buffer_ptr, written + garbage_size};
  absl::string_view data = orig_data;

  // when
  const uint32_t consumed = testee.feed(data);

  // then
  ASSERT_EQ(consumed, written);
  ASSERT_EQ(testee.ready(), true);
  ASSERT_EQ(testee.get(), expected);
  assertStringViewIncrement(data, orig_data, consumed);

  // when - 2
  const uint32_t consumed2 = testee.feed(data);

  // then - 2 (nothing changes)
  ASSERT_EQ(consumed2, 0);
  assertStringViewIncrement(data, orig_data, consumed);
}

// Does the same thing as the above test, but instead of providing whole data at one, it provides
// it in N one-byte chunks.
// This verifies if deserializer keeps state properly (no overwrites etc.).
template <typename BT>
void deserializeCompactAndCheckEqualityWithChunks(Buffer::Instance& buffer,
                                                  const typename BT::result_type expected) {
  // given
  BT testee{};

  EncodingContext encoder{-1};
  const uint32_t written = buffer.length();
  // Insert garbage after serialized payload.
  const uint32_t garbage_size = encoder.encode(Bytes(10000), buffer);

  const char* raw_buffer_ptr =
      reinterpret_cast<const char*>(buffer.linearize(written + garbage_size));
  // Tell parser that there is more data, it should never consume more than written.
  const absl::string_view orig_data = {raw_buffer_ptr, written + garbage_size};

  // when
  absl::string_view data = orig_data;
  uint32_t consumed = 0;
  for (uint32_t i = 0; i < written; ++i) {
    data = {data.data(), 1}; // Consume data byte-by-byte.
    uint32_t step = testee.feed(data);
    consumed += step;
    ASSERT_EQ(step, 1);
    ASSERT_EQ(data.size(), 0);
  }

  // then
  ASSERT_EQ(consumed, written);
  ASSERT_EQ(testee.ready(), true);
  ASSERT_EQ(testee.get(), expected);

  ASSERT_EQ(data.data(), orig_data.data() + consumed);

  // when - 2
  absl::string_view more_data = {data.data(), garbage_size};
  const uint32_t consumed2 = testee.feed(more_data);

  // then - 2 (nothing changes)
  ASSERT_EQ(consumed2, 0);
  ASSERT_EQ(more_data.data(), data.data());
  ASSERT_EQ(more_data.size(), garbage_size);
}

// Same thing as 'serializeThenDeserializeAndCheckEqualityInOneGo', just uses compact encoding.
template <typename BT>
void serializeCompactThenDeserializeAndCheckEqualityInOneGo(
    const typename BT::result_type expected) {
  Buffer::OwnedImpl buffer;
  EncodingContext encoder{-1};
  const uint32_t expected_written_size = encoder.computeCompactSize(expected);
  const uint32_t written = encoder.encodeCompact(expected, buffer);
  ASSERT_EQ(written, expected_written_size);
  deserializeCompactAndCheckEqualityInOneGo<BT>(buffer, expected);
}

// Same thing as 'serializeThenDeserializeAndCheckEqualityWithChunks', just uses compact encoding.
template <typename BT>
void serializeCompactThenDeserializeAndCheckEqualityWithChunks(
    const typename BT::result_type expected) {
  // given
  BT testee{};

  Buffer::OwnedImpl buffer;
  EncodingContext encoder{-1};
  const uint32_t expected_written_size = encoder.computeCompactSize(expected);
  const uint32_t written = encoder.encodeCompact(expected, buffer);
  ASSERT_EQ(written, expected_written_size);
  // Insert garbage after serialized payload.
  const uint32_t garbage_size = encoder.encode(Bytes(10000), buffer);

  const char* raw_buffer_ptr =
      reinterpret_cast<const char*>(buffer.linearize(written + garbage_size));
  // Tell parser that there is more data, it should never consume more than written.
  const absl::string_view orig_data = {raw_buffer_ptr, written + garbage_size};

  // when
  absl::string_view data = orig_data;
  uint32_t consumed = 0;
  for (uint32_t i = 0; i < written; ++i) {
    data = {data.data(), 1}; // Consume data byte-by-byte.
    uint32_t step = testee.feed(data);
    consumed += step;
    ASSERT_EQ(step, 1);
    ASSERT_EQ(data.size(), 0);
  }

  // then
  ASSERT_EQ(consumed, written);
  ASSERT_EQ(testee.ready(), true);
  ASSERT_EQ(testee.get(), expected);

  ASSERT_EQ(data.data(), orig_data.data() + consumed);

  // when - 2
  absl::string_view more_data = {data.data(), garbage_size};
  const uint32_t consumed2 = testee.feed(more_data);

  // then - 2 (nothing changes)
  ASSERT_EQ(consumed2, 0);
  ASSERT_EQ(more_data.data(), data.data());
  ASSERT_EQ(more_data.size(), garbage_size);
}

// Wrapper to run both tests for normal serialization.
template <typename BT>
void serializeThenDeserializeAndCheckEquality(const typename BT::result_type expected) {
  serializeThenDeserializeAndCheckEqualityInOneGo<BT>(expected);
  serializeThenDeserializeAndCheckEqualityWithChunks<BT>(expected);
}

// Wrapper to run both tests for compact serialization.
template <typename BT>
void serializeCompactThenDeserializeAndCheckEquality(const typename BT::result_type expected) {
  serializeCompactThenDeserializeAndCheckEqualityInOneGo<BT>(expected);
  serializeCompactThenDeserializeAndCheckEqualityWithChunks<BT>(expected);
}

// Wrapper to run both tests for compact deserialization (for non-serializable types).
template <typename BT>
void deserializeCompactAndCheckEquality(Buffer::Instance& buffer,
                                        const typename BT::result_type expected) {
  Buffer::OwnedImpl
      copy_for_chunking_test; // Tests modify input buffers, so let's just make a copy.
  copy_for_chunking_test.add(getRawData(buffer), buffer.length());
  deserializeCompactAndCheckEqualityInOneGo<BT>(buffer, expected);
  deserializeCompactAndCheckEqualityWithChunks<BT>(copy_for_chunking_test, expected);
}

/**
 * Message callback that captures the messages.
 */
template <typename Base, typename Message, typename Failure> class CapturingCallback : public Base {
public:
  /**
   * Stores the message.
   */
  void onMessage(Message message) override { captured_messages_.push_back(message); }

  /**
   * Returns the stored messages.
   */
  const std::vector<Message>& getCapturedMessages() const { return captured_messages_; }

  void onFailedParse(Failure failure_data) override { parse_failures_.push_back(failure_data); }

  const std::vector<Failure>& getParseFailures() const { return parse_failures_; }

private:
  std::vector<Message> captured_messages_;
  std::vector<Failure> parse_failures_;
};

template <typename Base, typename Message, typename Failure>
using CapturingCallbackSharedPtr = std::shared_ptr<CapturingCallback<Base, Message, Failure>>;

} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "source/common/buffer/buffer_impl.h"

#include "absl/container/fixed_array.h"
#include "absl/strings/string_view.h"
#include "contrib/kafka/filters/network/source/serialization.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {

// Common utilities for various Kafka buffer-related tests.

/**
 * Utility superclass that keeps a buffer that can be played with during the test.
 */
class BufferBasedTest {
protected:
  const char* getBytes() {
    Buffer::RawSliceVector slices = buffer_.getRawSlices(1);
    ASSERT(slices.size() == 1);
    return reinterpret_cast<const char*>((slices[0]).mem_);
  }

  template <typename T> uint32_t putIntoBuffer(const T& arg) {
    EncodingContext encoder_{-1}; // Context's api_version is not used when serializing primitives.
    return encoder_.encode(arg, buffer_);
  }

  absl::string_view putGarbageIntoBuffer(uint32_t size = 1024) {
    putIntoBuffer(Bytes(size));
    return {getBytes(), size};
  }

  Buffer::OwnedImpl buffer_;
};

/**
 * Utility superclass that keeps a buffer and can put messages into buffer.
 * @param Encoder class used for encoding messages into buffer
 */
template <class Encoder> class MessageBasedTest : public BufferBasedTest {
protected:
  template <typename T> void putMessageIntoBuffer(const T& arg) {
    Encoder encoder{buffer_};
    encoder.encode(arg);
  }
};

} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "test/common/stats/stat_test_utility.h"

#include "contrib/kafka/filters/network/source/external/request_metrics.h"
#include "contrib/kafka/filters/network/source/external/response_metrics.h"
#include "contrib/kafka/filters/network/test/message_utilities.h"
#include "gtest/gtest.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace MetricsIntegrationTest {

class MetricsIntegrationTest : public testing::Test {
protected:
  Stats::TestUtil::TestStore store_;
  Stats::Scope& scope_{*store_.rootScope()};
  RichRequestMetricsImpl request_metrics_{scope_, "prefix"};
  RichResponseMetricsImpl response_metrics_{scope_, "prefix"};
};

constexpr static int32_t UPDATE_COUNT = 42;

TEST_F(MetricsIntegrationTest, ShouldUpdateRequestMetrics) {
  for (const int16_t api_key : MessageUtilities::apiKeys()) {
    // given
    // when
    for (int i = 0; i < UPDATE_COUNT; ++i) {
      request_metrics_.onRequest(api_key);
    }

    // then
    Stats::Counter& counter = store_.counter(MessageUtilities::requestMetric(api_key));
    ASSERT_EQ(counter.value(), UPDATE_COUNT);
  };
}

TEST_F(MetricsIntegrationTest, ShouldHandleUnparseableRequest) {
  // given
  // when
  for (int i = 0; i < UPDATE_COUNT; ++i) {
    request_metrics_.onUnknownRequest();
  }

  // then
  ASSERT_EQ(store_.counter("kafka.prefix.request.unknown").value(), UPDATE_COUNT);
}

TEST_F(MetricsIntegrationTest, ShouldUpdateResponseMetrics) {
  for (const int16_t api_key : MessageUtilities::apiKeys()) {
    // given
    // when
    for (int i = 0; i < UPDATE_COUNT; ++i) {
      response_metrics_.onResponse(api_key, 0);
    }

    // then
    Stats::Counter& counter = store_.counter(MessageUtilities::responseMetric(api_key));
    ASSERT_EQ(counter.value(), UPDATE_COUNT);
  };
}

TEST_F(MetricsIntegrationTest, ShouldHandleUnparseableResponse) {
  // given
  // when
  for (int i = 0; i < UPDATE_COUNT; ++i) {
    response_metrics_.onUnknownResponse();
  }

  // then
  ASSERT_EQ(store_.counter("kafka.prefix.response.unknown").value(), UPDATE_COUNT);
}

} // namespace MetricsIntegrationTest
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <string>
#include <utility>
#include <vector>

#include "absl/strings/string_view.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// Kafka header.
using Header = std::pair<absl::string_view, absl::string_view>;

// Binds a single inbound record from Kafka client with its delivery information.
struct OutboundRecord {

  // These fields were received from downstream.
  const std::string topic_;
  const int32_t partition_;
  const absl::string_view key_;
  const absl::string_view value_;
  const std::vector<Header> headers_;

  // These fields will get updated when delivery to upstream Kafka cluster finishes.
  int16_t error_code_{0};
  uint32_t saved_offset_{0};

  OutboundRecord(const std::string& topic, const int32_t partition, const absl::string_view key,
                 const absl::string_view value, const std::vector<Header>& headers)
      : topic_{topic}, partition_{partition}, key_{key}, value_{value}, headers_{headers} {};
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "source/common/common/logger.h"

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"
#include "contrib/kafka/filters/network/source/mesh/shared_consumer_manager.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_config.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_facade.h"
#include "contrib/kafka/filters/network/source/request_codec.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Processes (enriches) incoming requests and passes it back to origin.
 */
class RequestProcessor : public RequestCallback, private Logger::Loggable<Logger::Id::kafka> {
public:
  RequestProcessor(AbstractRequestListener& origin, const UpstreamKafkaConfiguration& configuration,
                   UpstreamKafkaFacade& upstream_kafka_facade,
                   RecordCallbackProcessor& record_callback_processor);

  // RequestCallback
  void onMessage(AbstractRequestSharedPtr arg) override;
  void onFailedParse(RequestParseFailureSharedPtr) override;

private:
  void process(const std::shared_ptr<Request<ProduceRequest>> request) const;
  void process(const std::shared_ptr<Request<FetchRequest>> request) const;
  void process(const std::shared_ptr<Request<ListOffsetsRequest>> request) const;
  void process(const std::shared_ptr<Request<MetadataRequest>> request) const;
  void process(const std::shared_ptr<Request<ApiVersionsRequest>> request) const;

  AbstractRequestListener& origin_;
  const UpstreamKafkaConfiguration& configuration_;
  UpstreamKafkaFacade& upstream_kafka_facade_;
  RecordCallbackProcessor& record_callback_processor_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <memory>
#include <string>

#include "absl/strings/str_cat.h"
#include "contrib/kafka/filters/network/source/kafka_types.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Simple structure representing the record received from upstream Kafka cluster.
 */
struct InboundRecord {

  const std::string topic_;
  const int32_t partition_;
  const int64_t offset_;

  const NullableBytes key_;
  const NullableBytes value_;

  InboundRecord(const std::string& topic, const int32_t partition, const int64_t offset,
                const NullableBytes& key, const NullableBytes& value)
      : topic_{topic}, partition_{partition}, offset_{offset}, key_{key}, value_{value} {};

  // Estimates how many bytes this record would take.
  uint32_t dataLengthEstimate() const {
    uint32_t result = 15; // Max key length, value length, header count.
    result += key_ ? key_->size() : 0;
    result += value_ ? value_->size() : 0;
    return result;
  }

  // Used in logging.
  std::string toString() const {
    return absl::StrCat("[", topic_, "-", partition_, "/", offset_, "]");
  }
};

using InboundRecordSharedPtr = std::shared_ptr<InboundRecord>;

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <vector>

#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

using RdKafkaPartitionPtr = std::unique_ptr<RdKafka::TopicPartition>;
using RdKafkaPartitionVector = std::vector<RdKafka::TopicPartition*>;

/**
 * Real implementation that just performs librdkafka operations.
 */
class LibRdKafkaUtilsImpl : public LibRdKafkaUtils {
public:
  // LibRdKafkaUtils
  RdKafka::Conf::ConfResult setConfProperty(RdKafka::Conf& conf, const std::string& name,
                                            const std::string& value,
                                            std::string& errstr) const override;

  // LibRdKafkaUtils
  RdKafka::Conf::ConfResult setConfDeliveryCallback(RdKafka::Conf& conf,
                                                    RdKafka::DeliveryReportCb* dr_cb,
                                                    std::string& errstr) const override;

  // LibRdKafkaUtils
  std::unique_ptr<RdKafka::Producer> createProducer(RdKafka::Conf* conf,
                                                    std::string& errstr) const override;

  // LibRdKafkaUtils
  std::unique_ptr<RdKafka::KafkaConsumer> createConsumer(RdKafka::Conf* conf,
                                                         std::string& errstr) const override;

  // LibRdKafkaUtils
  RdKafka::Headers* convertHeaders(
      const std::vector<std::pair<absl::string_view, absl::string_view>>& headers) const override;

  // LibRdKafkaUtils
  void deleteHeaders(RdKafka::Headers* librdkafka_headers) const override;

  // LibRdKafkaUtils
  ConsumerAssignmentConstPtr assignConsumerPartitions(RdKafka::KafkaConsumer& consumer,
                                                      const std::string& topic,
                                                      const int32_t partitions) const override;

  // Default singleton accessor.
  static const LibRdKafkaUtils& getDefaultInstance();
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <atomic>

#include "envoy/event/dispatcher.h"

#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_client.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Combines the librdkafka producer and its dedicated monitoring thread.
 * Producer is used to schedule messages to be sent to Kafka.
 * Independently running monitoring thread picks up delivery confirmations from producer and uses
 * Dispatcher to notify itself about delivery in worker thread.
 */
class RichKafkaProducer : public KafkaProducer,
                          public RdKafka::DeliveryReportCb,
                          private Logger::Loggable<Logger::Id::kafka> {
public:
  // Main constructor.
  RichKafkaProducer(Event::Dispatcher& dispatcher, Thread::ThreadFactory& thread_factory,
                    const RawKafkaConfig& configuration);

  // Visible for testing (allows injection of LibRdKafkaUtils).
  RichKafkaProducer(Event::Dispatcher& dispatcher, Thread::ThreadFactory& thread_factory,
                    const RawKafkaConfig& configuration, const LibRdKafkaUtils& utils);

  // More complex than usual.
  // Marks that monitoring thread should finish and waits for it to join.
  ~RichKafkaProducer() override;

  // KafkaProducer
  void markFinished() override;

  // KafkaProducer
  void send(const ProduceFinishCbSharedPtr origin, const OutboundRecord& record) override;

  // This method gets executed by monitoring thread.
  // Does not finish until this object gets 'markFinished' invoked or gets destroyed.
  // Executed in dedicated monitoring thread.
  void checkDeliveryReports();

  // RdKafka::DeliveryReportCb
  void dr_cb(RdKafka::Message& message) override;

  // Processes the delivery confirmation.
  // Executed in Envoy worker thread.
  void processDelivery(const DeliveryMemento& memento);

  std::list<ProduceFinishCbSharedPtr>& getUnfinishedRequestsForTest();

private:
  Event::Dispatcher& dispatcher_;

  std::list<ProduceFinishCbSharedPtr> unfinished_produce_requests_;

  // Real Kafka producer (thread-safe).
  // Invoked by Envoy handler thread (to produce), and internal monitoring thread
  // (to poll for delivery events).
  std::unique_ptr<RdKafka::Producer> producer_;

  // Flag controlling monitoring threads's execution.
  std::atomic<bool> poller_thread_active_;

  // Monitoring thread that's responsible for continuously polling for new Kafka producer events.
  Thread::ThreadPtr poller_thread_;

  // Abstracts out pure Kafka operations.
  const LibRdKafkaUtils& utils_;
};

using RichKafkaProducerPtr = std::unique_ptr<RichKafkaProducer>;

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/command_handlers/list_offsets.h"

#include "contrib/kafka/filters/network/source/external/responses.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

ListOffsetsRequestHolder::ListOffsetsRequestHolder(
    AbstractRequestListener& filter, const std::shared_ptr<Request<ListOffsetsRequest>> request)
    : BaseInFlightRequest{filter}, request_{request} {}

void ListOffsetsRequestHolder::startProcessing() { notifyFilter(); }

bool ListOffsetsRequestHolder::finished() const { return true; }

AbstractResponseSharedPtr ListOffsetsRequestHolder::computeAnswer() const {
  const auto& header = request_->request_header_;
  const ResponseMetadata metadata = {header.api_key_, header.api_version_, header.correlation_id_};

  // The response contains all the requested topics (we do not do any filtering here).
  const auto& topics = request_->data_.topics_;
  std::vector<ListOffsetsTopicResponse> topic_responses;
  topic_responses.reserve(topics.size());
  for (const auto& topic : topics) {
    const auto& partitions = topic.partitions_;
    std::vector<ListOffsetsPartitionResponse> partition_responses;
    partition_responses.reserve(partitions.size());
    for (const auto& partition : partitions) {
      const int16_t error_code = 0;
      const int64_t timestamp = 0;
      /* As we are going to ignore consumer offset requests, we can reply with dummy values. */
      const int64_t offset = 0;
      const ListOffsetsPartitionResponse partition_response = {partition.partition_index_,
                                                               error_code, timestamp, offset};
      partition_responses.push_back(partition_response);
    }
    const ListOffsetsTopicResponse topic_response = {topic.name_, partition_responses};
    topic_responses.push_back(topic_response);
  }

  const ListOffsetsResponse data = {topic_responses};
  return std::make_shared<Response<ListOffsetsResponse>>(metadata, data);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <map>
#include <memory>
#include <vector>

#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/source/kafka_types.h"
#include "contrib/kafka/filters/network/source/mesh/inbound_record.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

using InboundRecordsMap = std::map<KafkaPartition, std::vector<InboundRecordSharedPtr>>;

/**
 * Dependency injection class responsible for converting received records into serializable form
 * that we can put into Fetch responses.
 */
class FetchRecordConverter {
public:
  virtual ~FetchRecordConverter() = default;

  // Converts received records into the serialized form.
  virtual std::vector<FetchableTopicResponse> convert(const InboundRecordsMap& arg) const PURE;
};

/**
 * Proper implementation.
 */
class FetchRecordConverterImpl : public FetchRecordConverter {
public:
  // FetchRecordConverter
  std::vector<FetchableTopicResponse> convert(const InboundRecordsMap& arg) const override;

  // Default singleton accessor.
  static const FetchRecordConverter& getDefaultInstance();

  static uint32_t computeCrc32cForTest(const unsigned char* data, const size_t len);

private:
  // Helper function: transform records from a partition into a record batch.
  // See: https://kafka.apache.org/33/documentation.html#recordbatch
  Bytes renderRecordBatch(const std::vector<InboundRecordSharedPtr>& records) const;

  // Helper function: append record to output array.
  // See: https://kafka.apache.org/33/documentation.html#record
  // https://github.com/apache/kafka/blob/3.3.2/clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java#L164
  void appendRecord(const InboundRecord& record, Bytes& out) const;

  // Helper function: render CRC32C bytes from given input.
  Bytes renderCrc32c(const unsigned char* data, const size_t len) const;

  // Helper function: compute CRC32C.
  static uint32_t computeCrc32c(const unsigned char* data, const size_t len);
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/produce_record_extractor.h"
#include "contrib/kafka/filters/network/source/mesh/outbound_record.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_client.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_facade.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Kafka 'Produce' request, that is aimed at particular cluster.
 * A single Produce request coming from downstream can map into multiple entries,
 * as the topics can be hosted on different clusters.
 *
 * These requests stored in 2 places: this filter (request's origin) and in RichKafkaProducer
 * instances (to match pure-Kafka confirmations to the requests).
 *
 *                               +--------------+
 *                               |<<librdkafka  |
 *                               |notification>>+--------+
 *                               +-+------------+        |
 *                                 |                     |
 *                                 |<notifies>           |
 *                                 |                     |
 *       +---------------+       +-v---------------+     |
 *       |KafkaMeshFilter+--+ +--+RichKafkaProducer|     |
 *       +-^-------------+  | |  +-----------------+     |
 *         |                | |                          |
 *         |     <in-flight>| |<requests-waiting         |
 *         |                | |for-delivery>             |
 *         |                | |                 <matches>|
 *         |       +--------v-v---------+                |
 *         +-------+ProduceRequestHolder|----------+     |
 * <notifies-      +---------+----------+<contains>|     |
 * when-finished>            |                     |     |
 *                 +---------v----------+          |     |
 *                 |PartitionProduceData|          |     |
 *                 +---------^----------+          |     |
 *                           |<absl::string_view>  |     |
 *         +-----------------+----------------+    |     |
 *         |                 |                |    |     |
 *   +-----+--------+ +------+-------+ +------+----v--+  |
 *   |OutboundRecord| |OutboundRecord| |OutboundRecord<--+
 *   +--------------+ +--------------+ +--------------+
 */
class ProduceRequestHolder : public BaseInFlightRequest,
                             public ProduceFinishCb,
                             public std::enable_shared_from_this<ProduceRequestHolder> {
public:
  ProduceRequestHolder(AbstractRequestListener& filter, UpstreamKafkaFacade& kafka_facade,
                       const std::shared_ptr<Request<ProduceRequest>> request);

  // Visible for testing.
  ProduceRequestHolder(AbstractRequestListener& filter, UpstreamKafkaFacade& kafka_facade,
                       const RecordExtractor& record_extractor,
                       const std::shared_ptr<Request<ProduceRequest>> request);

  // AbstractInFlightRequest
  void startProcessing() override;

  // AbstractInFlightRequest
  bool finished() const override;

  // AbstractInFlightRequest
  AbstractResponseSharedPtr computeAnswer() const override;

  // ProduceFinishCb
  bool accept(const DeliveryMemento& memento) override;

private:
  // Access to Kafka producers pointing to upstream Kafka clusters.
  UpstreamKafkaFacade& kafka_facade_;

  // Original request.
  const std::shared_ptr<Request<ProduceRequest>> request_;

  // How many responses from Kafka Producer handling our request we still expect.
  // This value decreases to 0 as we get confirmations from Kafka (successful or not).
  int expected_responses_;

  // Real records extracted out of request.
  std::vector<OutboundRecord> outbound_records_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/outbound_record.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Dependency injection class responsible for extracting records out of produce request's contents.
 */
class RecordExtractor {
public:
  virtual ~RecordExtractor() = default;

  virtual std::vector<OutboundRecord>
  extractRecords(const std::vector<TopicProduceData>& data) const PURE;
};

/**
 * Proper implementation of record extractor, capable of parsing V2 record set.
 * Reference: https://kafka.apache.org/24/documentation/#messageformat
 */
class RecordExtractorImpl : public RecordExtractor {
public:
  // RecordExtractor
  std::vector<OutboundRecord>
  extractRecords(const std::vector<TopicProduceData>& data) const override;

  // Helper function to get the data (such as key, value) out of given input, as most of the
  // interesting fields in records are kept as variable-encoded length and following bytes.
  static absl::string_view extractByteArray(absl::string_view& input);

private:
  std::vector<OutboundRecord> extractPartitionRecords(const std::string& topic,
                                                      const int32_t partition,
                                                      const Bytes& records) const;

  std::vector<OutboundRecord> processRecordBatch(const std::string& topic, const int32_t partition,
                                                 absl::string_view data) const;

  OutboundRecord extractRecord(const std::string& topic, const int32_t partition,
                               absl::string_view& data) const;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Api version requests are the first requests sent by Kafka clients to brokers.
 * We send our customized response to fail clients that might be trying to accomplish something more
 * than this filter supports.
 */
class ApiVersionsRequestHolder : public BaseInFlightRequest {
public:
  ApiVersionsRequestHolder(AbstractRequestListener& filter, const RequestHeader request_header);

  void startProcessing() override;

  bool finished() const override;

  AbstractResponseSharedPtr computeAnswer() const override;

private:
  // Original request header.
  const RequestHeader request_header_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_config.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

class MetadataRequestHolder : public BaseInFlightRequest {
public:
  MetadataRequestHolder(AbstractRequestListener& filter,
                        const UpstreamKafkaConfiguration& configuration,
                        const std::shared_ptr<Request<MetadataRequest>> request);

  void startProcessing() override;

  bool finished() const override;

  AbstractResponseSharedPtr computeAnswer() const override;

private:
  // Configuration used to provide data for response.
  const UpstreamKafkaConfiguration& configuration_;

  // Original request.
  const std::shared_ptr<Request<MetadataRequest>> request_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch_record_converter.h"

#include "contrib/kafka/filters/network/source/serialization.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

const FetchRecordConverter& FetchRecordConverterImpl::getDefaultInstance() {
  CONSTRUCT_ON_FIRST_USE(FetchRecordConverterImpl);
}

std::vector<FetchableTopicResponse>
FetchRecordConverterImpl::convert(const InboundRecordsMap& arg) const {

  // Compute record batches.
  std::map<KafkaPartition, Bytes> record_batches;
  for (const auto& partition_and_records : arg) {
    const KafkaPartition& kp = partition_and_records.first;
    const std::vector<InboundRecordSharedPtr>& partition_records = partition_and_records.second;
    const Bytes batch = renderRecordBatch(partition_records);
    record_batches[kp] = batch;
  }

  // Transform our maps into the Kafka structs.
  std::map<std::string, std::vector<FetchResponseResponsePartitionData>> topic_to_frrpd;
  for (const auto& record_batch : record_batches) {
    const std::string& topic_name = record_batch.first.first;
    const int32_t partition = record_batch.first.second;

    std::vector<FetchResponseResponsePartitionData>& frrpds = topic_to_frrpd[topic_name];
    const int16_t error_code = 0;
    const int64_t high_watermark = 0;
    const auto frrpd = FetchResponseResponsePartitionData{partition, error_code, high_watermark,
                                                          absl::make_optional(record_batch.second)};

    frrpds.push_back(frrpd);
  }

  std::vector<FetchableTopicResponse> result;
  for (const auto& partition_and_records : topic_to_frrpd) {
    const std::string& topic_name = partition_and_records.first;
    const auto ftr = FetchableTopicResponse{topic_name, partition_and_records.second};
    result.push_back(ftr);
  }
  return result;
}

// Magic format introduced around Kafka 1.0.0 and still used with Kafka 3.3.
constexpr int8_t MAGIC = 2;

Bytes FetchRecordConverterImpl::renderRecordBatch(
    const std::vector<InboundRecordSharedPtr>& records) const {

  Bytes result = {};

  // Base offset (bytes 0..7).
  const int64_t base_offset = htobe64(0);
  const unsigned char* base_offset_b = reinterpret_cast<const unsigned char*>(&base_offset);
  result.insert(result.end(), base_offset_b, base_offset_b + sizeof(base_offset));

  // Batch length placeholder (bytes 8..11).
  result.insert(result.end(), {0, 0, 0, 0});

  // All other attributes (spans partitionLeaderEpoch .. baseSequence) (bytes 12..56).
  const std::vector zeros(45, 0);
  result.insert(result.end(), zeros.begin(), zeros.end());

  // Last offset delta.
  // -1 means we always claim that we are at the beginning of partition.
  const int32_t last_offset_delta = htobe32(-1);
  const unsigned char* last_offset_delta_bytes =
      reinterpret_cast<const unsigned char*>(&last_offset_delta);
  const auto last_offset_delta_pos = result.begin() + 8 + 4 + 11;
  std::copy(last_offset_delta_bytes, last_offset_delta_bytes + sizeof(last_offset_delta),
            last_offset_delta_pos);

  // Records (count) (bytes 57..60).
  const int32_t record_count = htobe32(records.size());
  const unsigned char* record_count_b = reinterpret_cast<const unsigned char*>(&record_count);
  result.insert(result.end(), record_count_b, record_count_b + sizeof(record_count));

  // Records (data) (bytes 61+).
  for (const auto& record : records) {
    appendRecord(*record, result);
  }

  // Set batch length.
  const int32_t batch_len = htobe32(result.size() - (sizeof(base_offset) + sizeof(batch_len)));
  const unsigned char* batch_len_bytes = reinterpret_cast<const unsigned char*>(&batch_len);
  std::copy(batch_len_bytes, batch_len_bytes + sizeof(batch_len),
            result.begin() + sizeof(base_offset));

  // Set magic.
  const uint32_t magic_offset = sizeof(base_offset) + sizeof(batch_len) + sizeof(int32_t);
  result[magic_offset] = MAGIC;

  // Compute and set CRC.
  const uint32_t crc_offset = magic_offset + 1;
  const auto crc_data_start = result.data() + crc_offset + sizeof(int32_t);
  const auto crc_data_len = result.size() - (crc_offset + sizeof(int32_t));
  const Bytes crc = renderCrc32c(crc_data_start, crc_data_len);
  std::copy(crc.begin(), crc.end(), result.begin() + crc_offset);

  return result;
}

void FetchRecordConverterImpl::appendRecord(const InboundRecord& record, Bytes& out) const {

  Bytes tmp = {};
  // This is not precise maths, as we could be over-reserving a little due to var-length fields.
  tmp.reserve(sizeof(int8_t) + sizeof(int64_t) + sizeof(int32_t) + record.dataLengthEstimate());

  // attributes: int8
  const int8_t attributes = 0;
  tmp.push_back(static_cast<unsigned char>(attributes));

  // timestampDelta: varlong
  const int64_t timestamp_delta = 0;
  VarlenUtils::writeVarlong(timestamp_delta, tmp);

  // offsetDelta: varint
  const int32_t offset_delta = record.offset_;
  VarlenUtils::writeVarint(offset_delta, tmp);

  // Impl note: compared to requests/responses, records serialize byte arrays as varint length +
  // bytes (and not length + 1, then bytes). So we cannot use EncodingContext from serialization.h.

  // keyLength: varint
  // key: byte[]
  const NullableBytes& key = record.key_;
  if (key.has_value()) {
    VarlenUtils::writeVarint(key->size(), tmp);
    tmp.insert(tmp.end(), key->begin(), key->end());
  } else {
    VarlenUtils::writeVarint(-1, tmp);
  }

  // valueLen: varint
  // value: byte[]
  const NullableBytes& value = record.value_;
  if (value.has_value()) {
    VarlenUtils::writeVarint(value->size(), tmp);
    tmp.insert(tmp.end(), value->begin(), value->end());
  } else {
    VarlenUtils::writeVarint(-1, tmp);
  }

  // TODO (adam.kotwasinski) Headers are not supported yet.
  const int32_t header_count = 0;
  VarlenUtils::writeVarint(header_count, tmp);

  // Put tmp's length into 'out'.
  VarlenUtils::writeVarint(tmp.size(), out);

  // Put tmp's contents into 'out'.
  out.insert(out.end(), tmp.begin(), tmp.end());
}

// XXX (adam.kotwasinski) Instead of computing it naively, either link against librdkafka's
// implementation or generate it.
// https://github.com/confluentinc/librdkafka/blob/v1.8.0/src/crc32c.c#L1
uint32_t FetchRecordConverterImpl::computeCrc32c(const unsigned char* data, const size_t len) {
  uint32_t crc = 0xFFFFFFFF;
  for (size_t i = 0; i < len; i++) {
    char ch = data[i];
    for (size_t j = 0; j < 8; j++) {
      uint32_t b = (ch ^ crc) & 1;
      crc >>= 1;
      if (b) {
        crc = crc ^ 0x82F63B78;
      }
      ch >>= 1;
    }
  }
  return ~crc;
}

uint32_t FetchRecordConverterImpl::computeCrc32cForTest(const unsigned char* data,
                                                        const size_t len) {
  return computeCrc32c(data, len);
}

Bytes FetchRecordConverterImpl::renderCrc32c(const unsigned char* data, const size_t len) const {
  uint32_t crc = htobe32(computeCrc32c(data, len));
  Bytes result;
  unsigned char* raw = reinterpret_cast<unsigned char*>(&crc);
  result.insert(result.end(), raw, raw + sizeof(crc));
  return result;
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/command_handlers/produce_record_extractor.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

std::vector<OutboundRecord>
RecordExtractorImpl::extractRecords(const std::vector<TopicProduceData>& data) const {
  std::vector<OutboundRecord> result;
  for (const auto& topic_data : data) {
    for (const auto& partition_data : topic_data.partition_data_) {
      // Kafka protocol allows nullable data.
      if (partition_data.records_) {
        const auto topic_result = extractPartitionRecords(topic_data.name_, partition_data.index_,
                                                          *(partition_data.records_));
        std::copy(topic_result.begin(), topic_result.end(), std::back_inserter(result));
      }
    }
  }
  return result;
}

// Fields common to any record batch payload.
// See:
// https://github.com/apache/kafka/blob/2.4.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L46
constexpr unsigned int RECORD_BATCH_COMMON_FIELDS_SIZE = /* BaseOffset */ sizeof(int64_t) +
                                                         /* Length */ sizeof(int32_t) +
                                                         /* PartitionLeaderEpoch */ sizeof(int32_t);

// Magic format introduced around Kafka 1.0.0 and still used with Kafka 2.4.
// We can extract records out of record batches that use this magic.
constexpr int8_t SUPPORTED_MAGIC = 2;

// Reference implementation:
// https://github.com/apache/kafka/blob/2.4.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L443
std::vector<OutboundRecord> RecordExtractorImpl::extractPartitionRecords(const std::string& topic,
                                                                         const int32_t partition,
                                                                         const Bytes& bytes) const {

  absl::string_view data = {reinterpret_cast<const char*>(bytes.data()), bytes.size()};

  // Let's skip these common fields, because we are not using them.
  if (data.length() < RECORD_BATCH_COMMON_FIELDS_SIZE) {
    throw EnvoyException(fmt::format("record batch for [{}-{}] is too short (no common fields): {}",
                                     topic, partition, data.length()));
  }
  data = {data.data() + RECORD_BATCH_COMMON_FIELDS_SIZE,
          data.length() - RECORD_BATCH_COMMON_FIELDS_SIZE};

  // Extract magic - it what is the format of records present in the bytes provided.
  Int8Deserializer magic_deserializer;
  magic_deserializer.feed(data);
  if (!magic_deserializer.ready()) {
    throw EnvoyException(
        fmt::format("magic byte is not present in record batch for [{}-{}]", topic, partition));
  }

  // Old client sending old magic, or Apache Kafka introducing new magic.
  const int8_t magic = magic_deserializer.get();
  if (SUPPORTED_MAGIC != magic) {
    throw EnvoyException(fmt::format("unknown magic value in record batch for [{}-{}]: {}", topic,
                                     partition, magic));
  }

  // We have received a record batch with good magic.
  return processRecordBatch(topic, partition, data);
}

// Record batch fields we are going to ignore (because we rip it up and send its contents).
// See:
// https://github.com/apache/kafka/blob/2.4.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L50
// and:
// https://github.com/apache/kafka/blob/2.4.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L471
constexpr unsigned int IGNORED_FIELDS_SIZE =
    /* CRC */ sizeof(int32_t) + /* Attributes */ sizeof(int16_t) +
    /* LastOffsetDelta */ sizeof(int32_t) + /* FirstTimestamp */ sizeof(int64_t) +
    /* MaxTimestamp */ sizeof(int64_t) + /* ProducerId */ sizeof(int64_t) +
    /* ProducerEpoch */ sizeof(int16_t) + /* BaseSequence */ sizeof(int32_t) +
    /* RecordCount */ sizeof(int32_t);

std::vector<OutboundRecord> RecordExtractorImpl::processRecordBatch(const std::string& topic,
                                                                    const int32_t partition,
                                                                    absl::string_view data) const {

  if (data.length() < IGNORED_FIELDS_SIZE) {
    throw EnvoyException(
        fmt::format("record batch for [{}-{}] is too short (no attribute fields): {}", topic,
                    partition, data.length()));
  }
  data = {data.data() + IGNORED_FIELDS_SIZE, data.length() - IGNORED_FIELDS_SIZE};

  // We have managed to consume all the fancy bytes, now it's time to get to records.
  std::vector<OutboundRecord> result;
  while (!data.empty()) {
    const OutboundRecord record = extractRecord(topic, partition, data);
    result.push_back(record);
  }
  return result;
}

// Reference implementation:
// https://github.com/apache/kafka/blob/2.4.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java#L179
OutboundRecord RecordExtractorImpl::extractRecord(const std::string& topic, const int32_t partition,
                                                  absl::string_view& data) const {

  VarInt32Deserializer length;
  length.feed(data);
  if (!length.ready()) {
    throw EnvoyException(
        fmt::format("record for [{}-{}] is too short (no length)", topic, partition));
  }
  const int32_t len = length.get();
  if (len < 0) {
    throw EnvoyException(
        fmt::format("record for [{}-{}] has invalid length: {}", topic, partition, len));
  }
  if (static_cast<uint32_t>(len) > data.length()) {
    throw EnvoyException(fmt::format("record for [{}-{}] is too short (not enough bytes provided)",
                                     topic, partition));
  }

  const absl::string_view expected_end_of_record = {data.data() + len, data.length() - len};

  // We throw away the following batch fields: attributes, timestamp delta, offset delta (cannot do
  // an easy jump, as some are variable-length).
  Int8Deserializer attributes;
  attributes.feed(data);
  VarInt64Deserializer tsDelta;
  tsDelta.feed(data);
  VarUInt32Deserializer offsetDelta;
  offsetDelta.feed(data);
  if (!attributes.ready() || !tsDelta.ready() || !offsetDelta.ready()) {
    throw EnvoyException(
        fmt::format("attributes not present in record for [{}-{}]", topic, partition));
  }

  // Record key and value.
  const absl::string_view key = extractByteArray(data);
  const absl::string_view value = extractByteArray(data);

  // Headers.
  VarInt32Deserializer headers_count_deserializer;
  headers_count_deserializer.feed(data);
  if (!headers_count_deserializer.ready()) {
    throw EnvoyException(
        fmt::format("header count not present in record for [{}-{}]", topic, partition));
  }
  const int32_t headers_count = headers_count_deserializer.get();
  if (headers_count < 0) {
    throw EnvoyException(fmt::format("invalid header count in record for [{}-{}]: {}", topic,
                                     partition, headers_count));
  }
  std::vector<Header> headers;
  headers.reserve(headers_count);
  for (int32_t i = 0; i < headers_count; ++i) {
    const absl::string_view header_key = extractByteArray(data);
    const absl::string_view header_value = extractByteArray(data);
    headers.emplace_back(header_key, header_value);
  }

  if (data == expected_end_of_record) {
    // We have consumed everything nicely.
    return OutboundRecord{topic, partition, key, value, headers};
  } else {
    // Bad data - there are bytes left.
    throw EnvoyException(fmt::format("data left after consuming record for [{}-{}]: {}", topic,
                                     partition, data.length()));
  }
}

absl::string_view RecordExtractorImpl::extractByteArray(absl::string_view& input) {

  // Get the length.
  VarInt32Deserializer length_deserializer;
  length_deserializer.feed(input);
  if (!length_deserializer.ready()) {
    throw EnvoyException("byte array length not present");
  }
  const int32_t length = length_deserializer.get();

  // Length can be -1 (null value was published by client).
  if (-1 == length) {
    return {};
  }

  // Otherwise, length cannot be negative.
  if (length < 0) {
    throw EnvoyException(fmt::format("byte array length less than -1: {}", length));
  }

  // Underflow handling.
  if (static_cast<absl::string_view::size_type>(length) > input.size()) {
    throw EnvoyException(
        fmt::format("byte array length larger than data provided: {} vs {}", length, input.size()));
  }

  // We have enough data to return it.
  const absl::string_view result = {input.data(),
                                    static_cast<absl::string_view::size_type>(length)};
  input = {input.data() + length, input.length() - length};
  return result;
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "envoy/event/dispatcher.h"
#include "envoy/event/timer.h"

#include "absl/synchronization/mutex.h"
#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch_record_converter.h"
#include "contrib/kafka/filters/network/source/mesh/inbound_record.h"
#include "contrib/kafka/filters/network/source/mesh/shared_consumer_manager.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_consumer.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

class FetchRequestHolder : public BaseInFlightRequest,
                           public RecordCb,
                           public std::enable_shared_from_this<FetchRequestHolder> {
public:
  FetchRequestHolder(AbstractRequestListener& filter, RecordCallbackProcessor& consumer_manager,
                     const std::shared_ptr<Request<FetchRequest>> request);

  // Visible for testing.
  FetchRequestHolder(AbstractRequestListener& filter, RecordCallbackProcessor& consumer_manager,
                     const std::shared_ptr<Request<FetchRequest>>,
                     const FetchRecordConverter& converter);

  // AbstractInFlightRequest
  void startProcessing() override;

  // AbstractInFlightRequest
  bool finished() const override;

  // AbstractInFlightRequest
  void abandon() override;

  // AbstractInFlightRequest
  AbstractResponseSharedPtr computeAnswer() const override;

  // Invoked by timer as this requests's time runs out.
  // It is possible that this request has already been finished (there was data to send),
  // then this method does nothing.
  void markFinishedByTimer();

  // RecordCb
  CallbackReply receive(InboundRecordSharedPtr message) override;

  // RecordCb
  TopicToPartitionsMap interest() const override;

  // RecordCb
  std::string toString() const override;

private:
  // Invoked internally when we want to mark this Fetch request as done.
  // This means: we are no longer interested in future messages and might need to unregister
  // ourselves.
  void cleanup(bool unregister);

  // Provides access to upstream-pointing consumers.
  RecordCallbackProcessor& consumer_manager_;
  // Original request.
  const std::shared_ptr<Request<FetchRequest>> request_;

  mutable absl::Mutex state_mutex_;
  // Whether this request has finished processing and is ready for sending upstream.
  bool finished_ ABSL_GUARDED_BY(state_mutex_) = false;
  // The messages to send downstream.
  std::map<KafkaPartition, std::vector<InboundRecordSharedPtr>>
      messages_ ABSL_GUARDED_BY(state_mutex_);

  // Filter's dispatcher.
  Event::Dispatcher& dispatcher_;
  // Timeout timer (invalidated when request is finished).
  Event::TimerPtr timer_;

  // Translates librdkafka objects into bytes to be sent downstream.
  const FetchRecordConverter& converter_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

class ListOffsetsRequestHolder : public BaseInFlightRequest {
public:
  ListOffsetsRequestHolder(AbstractRequestListener& filter,
                           const std::shared_ptr<Request<ListOffsetsRequest>> request);

  void startProcessing() override;

  bool finished() const override;

  AbstractResponseSharedPtr computeAnswer() const override;

private:
  // Original request.
  const std::shared_ptr<Request<ListOffsetsRequest>> request_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/command_handlers/produce.h"

#include "contrib/kafka/filters/network/source/external/responses.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

constexpr static int16_t NO_ERROR = 0;

ProduceRequestHolder::ProduceRequestHolder(AbstractRequestListener& filter,
                                           UpstreamKafkaFacade& kafka_facade,
                                           const std::shared_ptr<Request<ProduceRequest>> request)
    : ProduceRequestHolder{filter, kafka_facade, RecordExtractorImpl{}, request} {};

ProduceRequestHolder::ProduceRequestHolder(AbstractRequestListener& filter,
                                           UpstreamKafkaFacade& kafka_facade,
                                           const RecordExtractor& record_extractor,
                                           const std::shared_ptr<Request<ProduceRequest>> request)
    : BaseInFlightRequest{filter}, kafka_facade_{kafka_facade}, request_{request} {
  outbound_records_ = record_extractor.extractRecords(request_->data_.topic_data_);
  expected_responses_ = outbound_records_.size();
}

void ProduceRequestHolder::startProcessing() {
  // Main part of the proxy: for each outbound record we get the appropriate sink (effectively a
  // facade for upstream Kafka cluster), and send the record to it.
  for (const OutboundRecord& outbound_record : outbound_records_) {
    KafkaProducer& producer = kafka_facade_.getProducerForTopic(outbound_record.topic_);
    // We need to provide our object as first argument, as we will want to be notified when the
    // delivery finishes.
    producer.send(shared_from_this(), outbound_record);
  }
  // Corner case handling:
  // If we ever receive produce request without records, we need to notify the filter we are ready,
  // because otherwise no notification will ever come from the real Kafka producer.
  if (finished()) {
    notifyFilter();
  }
}

bool ProduceRequestHolder::finished() const { return 0 == expected_responses_; }

// Find a record that matches provided delivery confirmation coming from Kafka producer.
// If all the records got their delivery data filled in, we are done, and can notify the origin
// filter.
bool ProduceRequestHolder::accept(const DeliveryMemento& memento) {
  for (auto& outbound_record : outbound_records_) {
    if (outbound_record.value_.data() == memento.data_) {
      // We have matched the downstream request that matches our confirmation from upstream Kafka.
      outbound_record.error_code_ = memento.error_code_;
      outbound_record.saved_offset_ = memento.offset_;
      --expected_responses_;
      if (finished()) {
        // All elements had their responses matched.
        ENVOY_LOG(trace, "All deliveries finished for produce request {}",
                  request_->request_header_.correlation_id_);
        notifyFilter();
      }
      return true;
    }
  }
  return false;
}

AbstractResponseSharedPtr ProduceRequestHolder::computeAnswer() const {

  // Header.
  const RequestHeader& rh = request_->request_header_;
  ResponseMetadata metadata = {rh.api_key_, rh.api_version_, rh.correlation_id_};

  // Real answer.
  using ErrorCodeAndOffset = std::pair<int16_t, uint32_t>;
  std::map<std::string, std::map<int32_t, ErrorCodeAndOffset>> topic_to_partition_responses;
  for (const auto& outbound_record : outbound_records_) {
    auto& partition_map = topic_to_partition_responses[outbound_record.topic_];
    auto it = partition_map.find(outbound_record.partition_);
    if (it == partition_map.end()) {
      partition_map[outbound_record.partition_] = {outbound_record.error_code_,
                                                   outbound_record.saved_offset_};
    } else {
      // Proxy logic - aggregating multiple upstream answers into single downstream answer.
      // Let's fail if anything fails, otherwise use the lowest offset (like Kafka would have done).
      ErrorCodeAndOffset& curr = it->second;
      if (NO_ERROR == curr.first) {
        curr.first = outbound_record.error_code_;
        curr.second = std::min(curr.second, outbound_record.saved_offset_);
      }
    }
  }

  std::vector<TopicProduceResponse> topic_responses;
  for (const auto& topic_entry : topic_to_partition_responses) {
    std::vector<PartitionProduceResponse> partition_responses;
    for (const auto& partition_entry : topic_entry.second) {
      const int32_t& partition = partition_entry.first;
      const int16_t& error_code = partition_entry.second.first;
      const int64_t& offset = partition_entry.second.second;
      partition_responses.emplace_back(partition, error_code, offset);
    }
    const std::string& topic = topic_entry.first;
    topic_responses.emplace_back(topic, partition_responses);
  }

  ProduceResponse data = {topic_responses, 0};
  return std::make_shared<Response<ProduceResponse>>(metadata, data);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_library",
    "envoy_contrib_package",
)

licenses(["notice"])  # Apache 2

envoy_contrib_package()

# Handlers for particular Kafka requests that are used by Kafka-mesh filter.

envoy_cc_library(
    name = "produce_lib",
    srcs = [
        "produce.cc",
    ],
    hdrs = [
        "produce.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":produce_record_extractor_lib",
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
        "//contrib/kafka/filters/network/source/mesh:abstract_command_lib",
        "//contrib/kafka/filters/network/source/mesh:outbound_record_lib",
        "//contrib/kafka/filters/network/source/mesh:upstream_kafka_facade_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "produce_record_extractor_lib",
    srcs = [
        "produce_record_extractor.cc",
    ],
    hdrs = [
        "produce_record_extractor.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source/mesh:outbound_record_lib",
    ],
)

envoy_cc_library(
    name = "fetch_lib",
    srcs = [
        "fetch.cc",
    ],
    hdrs = [
        "fetch.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":fetch_record_converter_lib",
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
        "//contrib/kafka/filters/network/source/mesh:abstract_command_lib",
        "//contrib/kafka/filters/network/source/mesh:shared_consumer_manager_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "fetch_record_converter_lib",
    srcs = [
        "fetch_record_converter.cc",
    ],
    hdrs = [
        "fetch_record_converter.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
        "//contrib/kafka/filters/network/source:serialization_lib",
        "//contrib/kafka/filters/network/source/mesh:inbound_record_lib",
    ],
)

envoy_cc_library(
    name = "list_offsets_lib",
    srcs = [
        "list_offsets.cc",
    ],
    hdrs = [
        "list_offsets.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
        "//contrib/kafka/filters/network/source/mesh:abstract_command_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "metadata_lib",
    srcs = [
        "metadata.cc",
    ],
    hdrs = [
        "metadata.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
        "//contrib/kafka/filters/network/source/mesh:abstract_command_lib",
        "//contrib/kafka/filters/network/source/mesh:upstream_config_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "api_versions_lib",
    srcs = [
        "api_versions.cc",
    ],
    hdrs = [
        "api_versions.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source:kafka_response_parser_lib",
        "//contrib/kafka/filters/network/source:tagged_fields_lib",
        "//contrib/kafka/filters/network/source/mesh:abstract_command_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)
#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch.h"

#include <thread>

#include "source/common/common/fmt.h"

#include "absl/synchronization/mutex.h"
#include "contrib/kafka/filters/network/source/external/responses.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

FetchRequestHolder::FetchRequestHolder(AbstractRequestListener& filter,
                                       RecordCallbackProcessor& consumer_manager,
                                       const std::shared_ptr<Request<FetchRequest>> request)
    : FetchRequestHolder{filter, consumer_manager, request,
                         FetchRecordConverterImpl::getDefaultInstance()} {}

FetchRequestHolder::FetchRequestHolder(AbstractRequestListener& filter,
                                       RecordCallbackProcessor& consumer_manager,
                                       const std::shared_ptr<Request<FetchRequest>> request,
                                       const FetchRecordConverter& converter)
    : BaseInFlightRequest{filter}, consumer_manager_{consumer_manager}, request_{request},
      dispatcher_{filter.dispatcher()}, converter_{converter} {}

// XXX (adam.kotwasinski) This should be made configurable in future.
constexpr uint32_t FETCH_TIMEOUT_MS = 5000;

static Event::TimerPtr registerTimeoutCallback(Event::Dispatcher& dispatcher,
                                               const Event::TimerCb callback,
                                               const int32_t timeout) {
  auto event = dispatcher.createTimer(callback);
  event->enableTimer(std::chrono::milliseconds(timeout));
  return event;
}

void FetchRequestHolder::startProcessing() {
  const TopicToPartitionsMap requested_topics = interest();

  {
    absl::MutexLock lock(&state_mutex_);
    for (const auto& topic_and_partitions : requested_topics) {
      const std::string& topic_name = topic_and_partitions.first;
      for (const int32_t partition : topic_and_partitions.second) {
        // This makes sure that all requested KafkaPartitions are tracked,
        // so then output generation is simpler.
        messages_[{topic_name, partition}] = {};
      }
    }
  }

  const auto self_reference = shared_from_this();
  consumer_manager_.processCallback(self_reference);

  Event::TimerCb callback = [this]() -> void {
    // Fun fact: if the request is degenerate (no partitions requested),
    // this will ensure it gets processed.
    markFinishedByTimer();
  };
  timer_ = registerTimeoutCallback(dispatcher_, callback, FETCH_TIMEOUT_MS);
}

TopicToPartitionsMap FetchRequestHolder::interest() const {
  TopicToPartitionsMap result;
  const std::vector<FetchTopic>& topics = request_->data_.topics_;
  for (const FetchTopic& topic : topics) {
    const std::string topic_name = topic.topic_;
    const std::vector<FetchPartition> partitions = topic.partitions_;
    for (const FetchPartition& partition : partitions) {
      result[topic_name].push_back(partition.partition_);
    }
  }
  return result;
}

// This method is called by a Envoy-worker thread.
void FetchRequestHolder::markFinishedByTimer() {
  ENVOY_LOG(trace, "Request {} timed out", toString());
  bool doCleanup = false;
  {
    absl::MutexLock lock(&state_mutex_);
    timer_ = nullptr;
    if (!finished_) {
      finished_ = true;
      doCleanup = true;
    }
  }
  if (doCleanup) {
    cleanup(true);
  }
}

// XXX (adam.kotwasinski) This should be made configurable in future.
// Right now the Fetch request is going to send up to 3 records.
// In future this should transform into some kind of method that's invoked inside 'receive' calls,
// as Kafka can have limits on records per partition.
constexpr int32_t MINIMAL_MSG_CNT = 3;

// This method is called by:
// - Kafka-consumer thread - when have the records delivered,
// - dispatcher thread  - when we start processing and check whether anything was cached.
CallbackReply FetchRequestHolder::receive(InboundRecordSharedPtr message) {
  absl::MutexLock lock(&state_mutex_);
  if (!finished_) {
    // Store a new record.
    const KafkaPartition kp = {message->topic_, message->partition_};
    messages_[kp].push_back(message);

    // Count all the records currently stored within this request.
    uint32_t current_messages = 0;
    for (const auto& e : messages_) {
      current_messages += e.second.size();
    }

    if (current_messages < MINIMAL_MSG_CNT) {
      // We can consume more in future.
      return CallbackReply::AcceptedAndWantMore;
    } else {
      // We have all we needed, we can finish processing.
      finished_ = true;
      cleanup(false);
      return CallbackReply::AcceptedAndFinished;
    }
  } else {
    // This fetch request has finished processing, so it will not accept a record.
    return CallbackReply::Rejected;
  }
}

std::string FetchRequestHolder::toString() const {
  return fmt::format("[Fetch id={}]", request_->request_header_.correlation_id_);
}

void FetchRequestHolder::cleanup(bool unregister) {
  ENVOY_LOG(trace, "Cleanup starting for {}", toString());
  if (unregister) {
    const auto self_reference = shared_from_this();
    consumer_manager_.removeCallback(self_reference);
  }

  // Our request is ready and can be sent downstream.
  // However, the caller here could be a Kafka-consumer worker thread (not an Envoy worker one),
  // so we need to use dispatcher to notify the filter that we are finished.
  auto notifyCallback = [this]() -> void {
    timer_ = nullptr;
    filter_.onRequestReadyForAnswer();
  };
  // Impl note: usually this will be invoked by non-Envoy thread,
  // so let's not optimize that this might be invoked by dispatcher callback.
  dispatcher_.post(notifyCallback);
  ENVOY_LOG(trace, "Cleanup finished for {}", toString());
}

bool FetchRequestHolder::finished() const {
  absl::MutexLock lock(&state_mutex_);
  return finished_;
}

void FetchRequestHolder::abandon() {
  ENVOY_LOG(trace, "Abandoning {}", toString());
  // We remove the timeout-callback and unregister this request so no deliveries happen to it.
  timer_ = nullptr;
  const auto self_reference = shared_from_this();
  consumer_manager_.removeCallback(self_reference);
  BaseInFlightRequest::abandon();
}

AbstractResponseSharedPtr FetchRequestHolder::computeAnswer() const {
  const auto& header = request_->request_header_;
  const ResponseMetadata metadata = {header.api_key_, header.api_version_, header.correlation_id_};

  std::vector<FetchableTopicResponse> responses;
  {
    absl::MutexLock lock(&state_mutex_);
    responses = converter_.convert(messages_);
  }
  const FetchResponse data = {responses};
  return std::make_shared<Response<FetchResponse>>(metadata, data);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/command_handlers/api_versions.h"

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/external/responses.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// These constants define which versions of requests this "Kafka server" will understand.

// As we can process only record format 2 (which itself is pretty old coming from Kafka 1.0), we are
// going to handle only produce requests with versions higher than 5.
constexpr int16_t MIN_PRODUCE_SUPPORTED = 5;
constexpr int16_t MAX_PRODUCE_SUPPORTED = PRODUCE_REQUEST_MAX_VERSION; /* Generated value. */
// List-offsets version 0 uses old-style offsets.
constexpr int16_t MIN_LIST_OFFSETS_SUPPORTED = 1;
constexpr int16_t MAX_LIST_OFFSETS_SUPPORTED =
    LIST_OFFSETS_REQUEST_MAX_VERSION; /* Generated value. */
// Right now we do not want to handle old version 0 request, as it expects us to enumerate all
// topics if list of requested topics is empty.
// Impl note: if filter gains knowledge of upstream cluster topics (e.g. thru admin clients), we
// could decrease this value.
constexpr int16_t MIN_METADATA_SUPPORTED = 1;
constexpr int16_t MAX_METADATA_SUPPORTED = METADATA_REQUEST_MAX_VERSION; /* Generated value. */

ApiVersionsRequestHolder::ApiVersionsRequestHolder(AbstractRequestListener& filter,
                                                   const RequestHeader request_header)
    : BaseInFlightRequest{filter}, request_header_{request_header} {}

// Api Versions requests are immediately ready for answer (as they do not need to reach upstream).
void ApiVersionsRequestHolder::startProcessing() { notifyFilter(); }

// Because these requests can be trivially handled, the responses are okay to be sent downstream at
// any time.
bool ApiVersionsRequestHolder::finished() const { return true; }

AbstractResponseSharedPtr ApiVersionsRequestHolder::computeAnswer() const {
  const ResponseMetadata metadata = {request_header_.api_key_, request_header_.api_version_,
                                     request_header_.correlation_id_};

  const int16_t error_code = 0;
  const ApiVersion produce_entry = {PRODUCE_REQUEST_API_KEY, MIN_PRODUCE_SUPPORTED,
                                    MAX_PRODUCE_SUPPORTED};
  const ApiVersion fetch_entry = {FETCH_REQUEST_API_KEY, 0, FETCH_REQUEST_MAX_VERSION};
  const ApiVersion list_offsets_entry = {LIST_OFFSETS_REQUEST_API_KEY, MIN_LIST_OFFSETS_SUPPORTED,
                                         MAX_LIST_OFFSETS_SUPPORTED};
  const ApiVersion metadata_entry = {METADATA_REQUEST_API_KEY, MIN_METADATA_SUPPORTED,
                                     MAX_METADATA_SUPPORTED};
  const ApiVersionsResponse real_response = {
      error_code, {produce_entry, fetch_entry, list_offsets_entry, metadata_entry}};

  return std::make_shared<Response<ApiVersionsResponse>>(metadata, real_response);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/command_handlers/metadata.h"

#include "contrib/kafka/filters/network/source/external/responses.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

MetadataRequestHolder::MetadataRequestHolder(
    AbstractRequestListener& filter, const UpstreamKafkaConfiguration& configuration,
    const std::shared_ptr<Request<MetadataRequest>> request)
    : BaseInFlightRequest{filter}, configuration_{configuration}, request_{request} {}

// Metadata requests are immediately ready for answer (as they do not need to reach upstream).
void MetadataRequestHolder::startProcessing() { notifyFilter(); }

bool MetadataRequestHolder::finished() const { return true; }

constexpr int32_t ENVOY_BROKER_ID = 0;
constexpr int32_t NO_ERROR = 0;

// Cornerstone of how the mesh-filter actually works.
// We pretend to be one-node Kafka cluster, with Envoy instance being the only member.
// What means all the Kafka future traffic will go through this instance.
AbstractResponseSharedPtr MetadataRequestHolder::computeAnswer() const {
  const auto& header = request_->request_header_;
  const ResponseMetadata metadata = {header.api_key_, header.api_version_, header.correlation_id_};

  const auto advertised_address = configuration_.getAdvertisedAddress();
  MetadataResponseBroker broker = {ENVOY_BROKER_ID, advertised_address.first,
                                   advertised_address.second};
  std::vector<MetadataResponseTopic> response_topics;
  if (request_->data_.topics_) {
    for (const MetadataRequestTopic& topic : *(request_->data_.topics_)) {
      if (!topic.name_) {
        // The client sent request without topic name (UUID was sent instead).
        // We do not know how to handle it, so do not send any metadata.
        // This will cause failures in clients downstream.
        continue;
      }
      const std::string& topic_name = *(topic.name_);
      std::vector<MetadataResponsePartition> topic_partitions;
      const absl::optional<ClusterConfig> cluster_config =
          configuration_.computeClusterConfigForTopic(topic_name);
      if (!cluster_config) {
        // Someone is requesting topics that are not known to our configuration.
        // So we do not attach any metadata, this will cause clients failures downstream as they
        // will never be able to get metadata for these topics.
        continue;
      }
      for (int32_t partition_id = 0; partition_id < cluster_config->partition_count_;
           ++partition_id) {
        // Every partition is hosted by this proxy-broker.
        MetadataResponsePartition partition = {
            NO_ERROR, partition_id, broker.node_id_, {broker.node_id_}, {broker.node_id_}};
        topic_partitions.push_back(partition);
      }
      MetadataResponseTopic response_topic = {NO_ERROR, topic_name, false, topic_partitions};
      response_topics.push_back(response_topic);
    }
  }
  MetadataResponse data = {{broker}, broker.node_id_, response_topics};
  return std::make_shared<Response<MetadataResponse>>(metadata, data);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_consumer_impl.h"

#include "contrib/kafka/filters/network/source/kafka_types.h"
#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils_impl.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

RichKafkaConsumer::RichKafkaConsumer(InboundRecordProcessor& record_processor,
                                     Thread::ThreadFactory& thread_factory,
                                     const std::string& topic, const int32_t partition_count,
                                     const RawKafkaConfig& configuration)
    : RichKafkaConsumer(record_processor, thread_factory, topic, partition_count, configuration,
                        LibRdKafkaUtilsImpl::getDefaultInstance()){};

RichKafkaConsumer::RichKafkaConsumer(InboundRecordProcessor& record_processor,
                                     Thread::ThreadFactory& thread_factory,
                                     const std::string& topic, const int32_t partition_count,
                                     const RawKafkaConfig& configuration,
                                     const LibRdKafkaUtils& utils)
    : record_processor_{record_processor}, topic_{topic} {

  ENVOY_LOG(debug, "Creating consumer for topic [{}] with {} partitions", topic, partition_count);

  // Create consumer configuration object.
  std::unique_ptr<RdKafka::Conf> conf =
      std::unique_ptr<RdKafka::Conf>(RdKafka::Conf::create(RdKafka::Conf::CONF_GLOBAL));

  std::string errstr;

  // Setup consumer custom properties.
  for (const auto& e : configuration) {
    ENVOY_LOG(debug, "Setting consumer property {}={}", e.first, e.second);
    if (utils.setConfProperty(*conf, e.first, e.second, errstr) != RdKafka::Conf::CONF_OK) {
      throw EnvoyException(absl::StrCat("Could not set consumer property [", e.first, "] to [",
                                        e.second, "]:", errstr));
    }
  }

  // We create the consumer.
  consumer_ = utils.createConsumer(conf.get(), errstr);
  if (!consumer_) {
    throw EnvoyException(absl::StrCat("Could not create consumer:", errstr));
  }

  // Consumer is going to read from all the topic partitions.
  assignment_ = utils.assignConsumerPartitions(*consumer_, topic, partition_count);

  // Start the worker thread.
  worker_thread_active_ = true;
  const std::function<void()> thread_routine = [this]() -> void { runWorkerLoop(); };
  worker_thread_ = thread_factory.createThread(thread_routine);
}

RichKafkaConsumer::~RichKafkaConsumer() {
  ENVOY_LOG(debug, "Closing Kafka consumer [{}]", topic_);

  worker_thread_active_ = false;
  // This should take at most INTEREST_TIMEOUT_MS + POLL_TIMEOUT_MS.
  worker_thread_->join();

  consumer_->unassign();
  consumer_->close();

  ENVOY_LOG(debug, "Kafka consumer [{}] closed succesfully", topic_);
}

// Read timeout constants.
// Large values are okay, but make the Envoy shutdown take longer
// (as there is no good way to interrupt a Kafka 'consume' call).
// XXX (adam.kotwasinski) This could be made configurable.

// How long a thread should wait for interest before checking if it's cancelled.
constexpr int32_t INTEREST_TIMEOUT_MS = 1000;

// How long a consumer should poll Kafka for messages.
constexpr int32_t POLL_TIMEOUT_MS = 1000;

void RichKafkaConsumer::runWorkerLoop() {
  while (worker_thread_active_) {

    // It makes no sense to poll and receive records if there is no interest right now,
    // so we can just block instead.
    bool can_poll = record_processor_.waitUntilInterest(topic_, INTEREST_TIMEOUT_MS);
    if (!can_poll) {
      // There is nothing to do, so we keep checking again.
      // Also we happen to check if we were closed - this makes Envoy shutdown bit faster.
      continue;
    }

    // There is interest in messages present in this topic, so we can start polling.
    std::vector<InboundRecordSharedPtr> records = receiveRecordBatch();
    for (auto& record : records) {
      record_processor_.receive(record);
    }
  }
  ENVOY_LOG(debug, "Worker thread for consumer [{}] finished", topic_);
}

// Helper method, converts byte array.
static NullableBytes toBytes(const void* data, const size_t size) {
  const unsigned char* as_char = static_cast<const unsigned char*>(data);
  if (data) {
    Bytes bytes(as_char, as_char + size);
    return {bytes};
  } else {
    return absl::nullopt;
  }
}

// Helper method, gets rid of librdkafka.
static InboundRecordSharedPtr transform(RdKafkaMessagePtr arg) {
  const auto topic = arg->topic_name();
  const auto partition = arg->partition();
  const auto offset = arg->offset();

  const NullableBytes key = toBytes(arg->key_pointer(), arg->key_len());
  const NullableBytes value = toBytes(arg->payload(), arg->len());

  return std::make_shared<InboundRecord>(topic, partition, offset, key, value);
}

std::vector<InboundRecordSharedPtr> RichKafkaConsumer::receiveRecordBatch() {
  // This message kicks off librdkafka consumer's Fetch requests and delivers a message.
  auto message = std::unique_ptr<RdKafka::Message>(consumer_->consume(POLL_TIMEOUT_MS));
  if (RdKafka::ERR_NO_ERROR == message->err()) {
    // We got a message.
    auto inbound_record = transform(std::move(message));
    ENVOY_LOG(trace, "Received Kafka message (first one): {}", inbound_record->toString());

    // XXX (adam.kotwasinski) There could be something more present in the consumer,
    // and we could drain it (at least a little) in the next commits.
    // See: https://github.com/confluentinc/librdkafka/discussions/3897
    return {inbound_record};
  } else {
    // Nothing extraordinary (timeout because there is nothing upstream),
    // or upstream connectivity failure.
    ENVOY_LOG(trace, "No message received in consumer [{}]: {}/{}", topic_, message->err(),
              RdKafka::err2str(message->err()));
    return {};
  }
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <map>
#include <memory>
#include <utility>
#include <vector>

#include "envoy/common/pure.h"

#include "contrib/kafka/filters/network/source/mesh/inbound_record.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// Topic name to topic partitions.
using TopicToPartitionsMap = std::map<std::string, std::vector<int32_t>>;

/**
 * Callback's response when it is provided with a record.
 */
enum class CallbackReply {
  // Callback is not interested in any record anymore.
  // Impl note: this can be caused by callback timing out in between.
  Rejected,
  // Callback consumed the record and is capable of accepting more.
  AcceptedAndWantMore,
  // Callback consumed the record and will not receive more (allows us to optimize a little).
  AcceptedAndFinished,
};

/**
 * Callback for objects that want to be notified that new Kafka record has been received.
 */
class RecordCb {
public:
  virtual ~RecordCb() = default;

  /**
   * Notify the callback with a record.
   * @return whether the callback could accept the message
   */
  virtual CallbackReply receive(InboundRecordSharedPtr record) PURE;

  /**
   * What partitions this callback is interested in.
   */
  virtual TopicToPartitionsMap interest() const PURE;

  /**
   * Pretty string identifier of given callback (i.e. downstream request). Used in logging.
   */
  virtual std::string toString() const PURE;
};

using RecordCbSharedPtr = std::shared_ptr<RecordCb>;

/**
 * An entity that is interested in inbound records delivered by Kafka consumer.
 */
class InboundRecordProcessor {
public:
  virtual ~InboundRecordProcessor() = default;

  /**
   * Passes the record to the processor.
   */
  virtual void receive(InboundRecordSharedPtr message) PURE;

  /**
   * Blocks until there is interest in records in a given topic, or timeout expires.
   * Conceptually a thick condition variable.
   * @return true if there was interest.
   */
  virtual bool waitUntilInterest(const std::string& topic, const int32_t timeout_ms) const PURE;
};

using InboundRecordProcessorPtr = std::unique_ptr<InboundRecordProcessor>;

/**
 * Kafka consumer pointing to some upstream Kafka cluster.
 */
class KafkaConsumer {
public:
  virtual ~KafkaConsumer() = default;
};

using KafkaConsumerPtr = std::unique_ptr<KafkaConsumer>;

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "source/extensions/filters/network/common/factory_base.h"

#include "contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha/kafka_mesh.pb.h"
#include "contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha/kafka_mesh.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

using KafkaMeshProtoConfig = envoy::extensions::filters::network::kafka_mesh::v3alpha::KafkaMesh;

/**
 * Config registration for the Kafka mesh filter.
 */
class KafkaMeshConfigFactory : public Common::FactoryBase<KafkaMeshProtoConfig> {
public:
  KafkaMeshConfigFactory() : FactoryBase("envoy.filters.network.kafka_mesh", true) {}

private:
  Network::FilterFactoryCb
  createFilterFactoryFromProtoTyped(const KafkaMeshProtoConfig& config,
                                    Server::Configuration::FactoryContext& context) override;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/upstream_config.h"

#include "envoy/common/exception.h"

#include "source/common/common/assert.h"

#include "absl/strings/match.h"
#include "absl/strings/str_cat.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

using KafkaClusterDefinition =
    envoy::extensions::filters::network::kafka_mesh::v3alpha::KafkaClusterDefinition;
using ForwardingRule = envoy::extensions::filters::network::kafka_mesh::v3alpha::ForwardingRule;
using KafkaMesh = envoy::extensions::filters::network::kafka_mesh::v3alpha::KafkaMesh;
using ConsumerProxyMode = KafkaMesh::ConsumerProxyMode;

const std::string DEFAULT_CONSUMER_GROUP_ID = "envoy";

UpstreamKafkaConfigurationImpl::UpstreamKafkaConfigurationImpl(const KafkaMeshProtoConfig& config)
    : advertised_address_{config.advertised_host(), config.advertised_port()} {

  // Processing cluster data.
  const auto& upstream_clusters = config.upstream_clusters();
  if (upstream_clusters.empty()) {
    throw EnvoyException("kafka-mesh filter needs to have at least one upstream Kafka cluster");
  }

  // Processing cluster configuration.
  std::map<std::string, ClusterConfig> cluster_name_to_cluster_config;
  for (const auto& upstream_cluster_definition : upstream_clusters) {
    const std::string& cluster_name = upstream_cluster_definition.cluster_name();

    // No duplicates are allowed.
    if (cluster_name_to_cluster_config.find(cluster_name) != cluster_name_to_cluster_config.end()) {
      throw EnvoyException(
          absl::StrCat("kafka-mesh filter has multiple Kafka clusters referenced by the same name",
                       cluster_name));
    }

    // Upstream producer configuration.
    std::map<std::string, std::string> producer_configs = {
        upstream_cluster_definition.producer_config().begin(),
        upstream_cluster_definition.producer_config().end()};
    producer_configs["bootstrap.servers"] = upstream_cluster_definition.bootstrap_servers();

    // Upstream consumer configuration.
    std::map<std::string, std::string> consumer_configs = {
        upstream_cluster_definition.consumer_config().begin(),
        upstream_cluster_definition.consumer_config().end()};
    if (consumer_configs.end() == consumer_configs.find("group.id")) {
      // librdkafka consumer needs a group id, let's use a default one if nothing was provided.
      consumer_configs["group.id"] = DEFAULT_CONSUMER_GROUP_ID;
    }
    consumer_configs["bootstrap.servers"] = upstream_cluster_definition.bootstrap_servers();

    ClusterConfig cluster_config = {cluster_name, upstream_cluster_definition.partition_count(),
                                    producer_configs, consumer_configs};
    cluster_name_to_cluster_config[cluster_name] = cluster_config;
  }

  // Processing forwarding rules.
  const auto& forwarding_rules = config.forwarding_rules();
  if (forwarding_rules.empty()) {
    throw EnvoyException("kafka-mesh filter needs to have at least one forwarding rule");
  }

  for (const auto& rule : forwarding_rules) {
    const std::string& target_cluster = rule.target_cluster();
    ASSERT(rule.trigger_case() == ForwardingRule::TriggerCase::kTopicPrefix);
    ENVOY_LOG(trace, "Setting up forwarding rule: {} -> {}", rule.topic_prefix(), target_cluster);
    // Each forwarding rule needs to reference a cluster.
    if (cluster_name_to_cluster_config.find(target_cluster) ==
        cluster_name_to_cluster_config.end()) {
      throw EnvoyException(absl::StrCat(
          "kafka-mesh filter forwarding rule is referencing unknown upstream Kafka cluster: ",
          target_cluster));
    }
    topic_prefix_to_cluster_config_[rule.topic_prefix()] =
        cluster_name_to_cluster_config[target_cluster];
  }

  // The only mode we support right now - embedded librdkafka consumers.
  ASSERT(config.consumer_proxy_mode() == KafkaMesh::StatefulConsumerProxy);
}

absl::optional<ClusterConfig>
UpstreamKafkaConfigurationImpl::computeClusterConfigForTopic(const std::string& topic) const {
  // We find the first matching prefix (this is why ordering is important).
  for (const auto& it : topic_prefix_to_cluster_config_) {
    if (absl::StartsWith(topic, it.first)) {
      const ClusterConfig cluster_config = it.second;
      return absl::make_optional(cluster_config);
    }
  }
  return absl::nullopt;
}

std::pair<std::string, int32_t> UpstreamKafkaConfigurationImpl::getAdvertisedAddress() const {
  return advertised_address_;
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <map>
#include <vector>

#include "envoy/thread/thread.h"

#include "source/common/common/logger.h"

#include "contrib/kafka/filters/network/source/kafka_types.h"
#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils.h"
#include "contrib/kafka/filters/network/source/mesh/shared_consumer_manager.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_config.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_consumer.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

template <typename T> using PartitionMap = std::map<KafkaPartition, std::vector<T>>;

/**
 * Processor implementation that stores received records (that had no interest),
 * and callbacks waiting for records (that had no matching records delivered yet).
 * Basically core of Fetch-handling business logic.
 */
class RecordDistributor : public RecordCallbackProcessor,
                          public InboundRecordProcessor,
                          private Logger::Loggable<Logger::Id::kafka> {
public:
  // Main constructor.
  RecordDistributor();

  // Visible for testing.
  RecordDistributor(const PartitionMap<RecordCbSharedPtr>& callbacks,
                    const PartitionMap<InboundRecordSharedPtr>& records);

  // InboundRecordProcessor
  bool waitUntilInterest(const std::string& topic, const int32_t timeout_ms) const override;

  // InboundRecordProcessor
  void receive(InboundRecordSharedPtr message) override;

  // RecordCallbackProcessor
  void processCallback(const RecordCbSharedPtr& callback) override;

  // RecordCallbackProcessor
  void removeCallback(const RecordCbSharedPtr& callback) override;

  int32_t getCallbackCountForTest(const std::string& topic, const int32_t partition) const;

  int32_t getRecordCountForTest(const std::string& topic, const int32_t partition) const;

private:
  // Checks whether any of the callbacks stored right now are interested in the topic.
  bool hasInterest(const std::string& topic) const ABSL_EXCLUSIVE_LOCKS_REQUIRED(callbacks_mutex_);

  // Helper function (passes all stored records to callback).
  bool passRecordsToCallback(const RecordCbSharedPtr& callback);

  // Helper function (passes partition records to callback).
  bool passPartitionRecordsToCallback(const RecordCbSharedPtr& callback,
                                      const KafkaPartition& kafka_partition)
      ABSL_EXCLUSIVE_LOCKS_REQUIRED(stored_records_mutex_);

  // Helper function (real callback removal).
  void doRemoveCallback(const RecordCbSharedPtr& callback)
      ABSL_EXCLUSIVE_LOCKS_REQUIRED(callbacks_mutex_);

  /**
   * Invariant - for every KafkaPartition, there may be callbacks for this partition,
   * or there may be records for this partition, but never both at the same time.
   */

  mutable absl::Mutex callbacks_mutex_;
  PartitionMap<RecordCbSharedPtr> partition_to_callbacks_ ABSL_GUARDED_BY(callbacks_mutex_);

  mutable absl::Mutex stored_records_mutex_;
  PartitionMap<InboundRecordSharedPtr> stored_records_ ABSL_GUARDED_BY(stored_records_mutex_);
};

using RecordDistributorPtr = std::unique_ptr<RecordDistributor>;

/**
 * Injectable for tests.
 */
class KafkaConsumerFactory {
public:
  virtual ~KafkaConsumerFactory() = default;

  // Create a Kafka consumer.
  virtual KafkaConsumerPtr createConsumer(InboundRecordProcessor& record_processor,
                                          Thread::ThreadFactory& thread_factory,
                                          const std::string& topic, const int32_t partition_count,
                                          const RawKafkaConfig& configuration) const PURE;
};

/**
 * Maintains a collection of Kafka consumers (one per topic) and the real distributor instance.
 */
class SharedConsumerManagerImpl : public RecordCallbackProcessor,
                                  public SharedConsumerManager,
                                  private Logger::Loggable<Logger::Id::kafka> {
public:
  // Main constructor.
  SharedConsumerManagerImpl(const UpstreamKafkaConfiguration& configuration,
                            Thread::ThreadFactory& thread_factory);

  // Visible for testing.
  SharedConsumerManagerImpl(const UpstreamKafkaConfiguration& configuration,
                            Thread::ThreadFactory& thread_factory,
                            const KafkaConsumerFactory& consumer_factory);

  // RecordCallbackProcessor
  void processCallback(const RecordCbSharedPtr& callback) override;

  // RecordCallbackProcessor
  void removeCallback(const RecordCbSharedPtr& callback) override;

  // SharedConsumerManager
  void registerConsumerIfAbsent(const std::string& topic) override;

  size_t getConsumerCountForTest() const;

private:
  // Mutates 'topic_to_consumer_'.
  void registerNewConsumer(const std::string& topic)
      ABSL_EXCLUSIVE_LOCKS_REQUIRED(consumers_mutex_);

  RecordDistributorPtr distributor_;

  const UpstreamKafkaConfiguration& configuration_;
  Thread::ThreadFactory& thread_factory_;
  const KafkaConsumerFactory& consumer_factory_;

  mutable absl::Mutex consumers_mutex_;
  std::map<std::string, KafkaConsumerPtr> topic_to_consumer_ ABSL_GUARDED_BY(consumers_mutex_);
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <memory>
#include <string>

#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_consumer.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Processes incoming record callbacks (i.e. Fetch requests).
 */
class RecordCallbackProcessor {
public:
  virtual ~RecordCallbackProcessor() = default;

  // Process an inbound record callback by passing cached records to it
  // and (if needed) registering the callback.
  virtual void processCallback(const RecordCbSharedPtr& callback) PURE;

  // Remove the callback (usually invoked by the callback timing out downstream).
  virtual void removeCallback(const RecordCbSharedPtr& callback) PURE;
};

using RecordCallbackProcessorSharedPtr = std::shared_ptr<RecordCallbackProcessor>;

/**
 * Manages (raw) Kafka consumers pointing to upstream Kafka clusters.
 * It is expected to have only one instance of this object per mesh-filter type.
 */
class SharedConsumerManager {
public:
  virtual ~SharedConsumerManager() = default;

  // Start the consumer (if there is none) to make sure that records can be received from the topic.
  virtual void registerConsumerIfAbsent(const std::string& topic) PURE;
};

using SharedConsumerManagerPtr = std::unique_ptr<SharedConsumerManager>;

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/filter.h"

#include "envoy/network/connection.h"

#include "source/common/buffer/buffer_impl.h"

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/external/responses.h"
#include "contrib/kafka/filters/network/source/response_codec.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

KafkaMeshFilter::KafkaMeshFilter(const UpstreamKafkaConfiguration& configuration,
                                 UpstreamKafkaFacade& upstream_kafka_facade,
                                 RecordCallbackProcessor& record_callback_processor)
    : KafkaMeshFilter{std::make_shared<RequestDecoder>(
          std::vector<RequestCallbackSharedPtr>({std::make_shared<RequestProcessor>(
              *this, configuration, upstream_kafka_facade, record_callback_processor)}))} {}

KafkaMeshFilter::KafkaMeshFilter(RequestDecoderSharedPtr request_decoder)
    : request_decoder_{request_decoder} {}

KafkaMeshFilter::~KafkaMeshFilter() { abandonAllInFlightRequests(); }

Network::FilterStatus KafkaMeshFilter::onNewConnection() { return Network::FilterStatus::Continue; }

void KafkaMeshFilter::initializeReadFilterCallbacks(Network::ReadFilterCallbacks& callbacks) {
  read_filter_callbacks_ = &callbacks;
  read_filter_callbacks_->connection().addConnectionCallbacks(*this);
}

Network::FilterStatus KafkaMeshFilter::onData(Buffer::Instance& data, bool) {
  try {
    request_decoder_->onData(data);
    data.drain(data.length()); // All the bytes have been copied to decoder.
    return Network::FilterStatus::StopIteration;
  } catch (const EnvoyException& e) {
    ENVOY_LOG(trace, "Could not process data from Kafka client: {}", e.what());
    request_decoder_->reset();
    // Something very wrong occurred, let's just close the connection.
    read_filter_callbacks_->connection().close(Network::ConnectionCloseType::FlushWrite);
    return Network::FilterStatus::StopIteration;
  }
}

void KafkaMeshFilter::onEvent(Network::ConnectionEvent event) {
  if (Network::ConnectionEvent::RemoteClose == event ||
      Network::ConnectionEvent::LocalClose == event) {
    // Connection is being closed but there might be some requests in flight, abandon them.
    abandonAllInFlightRequests();
  }
}

void KafkaMeshFilter::onAboveWriteBufferHighWatermark() {}

void KafkaMeshFilter::onBelowWriteBufferLowWatermark() {}

/**
 * We have received a request we can actually process.
 */
void KafkaMeshFilter::onRequest(InFlightRequestSharedPtr request) {
  requests_in_flight_.push_back(request);
  request->startProcessing();
}

/**
 * Our filter has been notified that a request that originated in this filter has an answer ready.
 * Because the Kafka messages have ordering, we need to check all messages and can possibly send
 * multiple answers in one go. This can happen if e.g. message 3 finishes first, then 2, then 1,
 * what allows us to send 1, 2, 3 in one invocation.
 */
void KafkaMeshFilter::onRequestReadyForAnswer() {
  while (!requests_in_flight_.empty()) {
    InFlightRequestSharedPtr rq = requests_in_flight_.front();
    if (rq->finished()) {
      // The request has been finished, so we no longer need to store it.
      requests_in_flight_.erase(requests_in_flight_.begin());

      // And write the response downstream.
      const AbstractResponseSharedPtr response = rq->computeAnswer();
      Buffer::OwnedImpl buffer;
      ResponseEncoder encoder{buffer};
      encoder.encode(*response);
      read_filter_callbacks_->connection().write(buffer, false);
    } else {
      break;
    }
  }
}

Event::Dispatcher& KafkaMeshFilter::dispatcher() {
  return read_filter_callbacks_->connection().dispatcher();
}

void KafkaMeshFilter::abandonAllInFlightRequests() {
  for (const auto& request : requests_in_flight_) {
    request->abandon();
  }
  requests_in_flight_.erase(requests_in_flight_.begin(), requests_in_flight_.end());
}

std::list<InFlightRequestSharedPtr>& KafkaMeshFilter::getRequestsInFlightForTest() {
  return requests_in_flight_;
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/shared_consumer_manager_impl.h"

#include <functional>

#include "source/common/common/fmt.h"

#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_consumer_impl.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// KafkaConsumerFactoryImpl

class KafkaConsumerFactoryImpl : public KafkaConsumerFactory {
public:
  // KafkaConsumerFactory
  KafkaConsumerPtr createConsumer(InboundRecordProcessor& record_processor,
                                  Thread::ThreadFactory& thread_factory, const std::string& topic,
                                  const int32_t partition_count,
                                  const RawKafkaConfig& configuration) const override;

  // Default singleton accessor.
  static const KafkaConsumerFactory& getDefaultInstance();
};

KafkaConsumerPtr
KafkaConsumerFactoryImpl::createConsumer(InboundRecordProcessor& record_processor,
                                         Thread::ThreadFactory& thread_factory,
                                         const std::string& topic, const int32_t partition_count,
                                         const RawKafkaConfig& configuration) const {
  return std::make_unique<RichKafkaConsumer>(record_processor, thread_factory, topic,
                                             partition_count, configuration);
}

const KafkaConsumerFactory& KafkaConsumerFactoryImpl::getDefaultInstance() {
  CONSTRUCT_ON_FIRST_USE(KafkaConsumerFactoryImpl);
}

// SharedConsumerManagerImpl

SharedConsumerManagerImpl::SharedConsumerManagerImpl(
    const UpstreamKafkaConfiguration& configuration, Thread::ThreadFactory& thread_factory)
    : SharedConsumerManagerImpl{configuration, thread_factory,
                                KafkaConsumerFactoryImpl::getDefaultInstance()} {}

SharedConsumerManagerImpl::SharedConsumerManagerImpl(
    const UpstreamKafkaConfiguration& configuration, Thread::ThreadFactory& thread_factory,
    const KafkaConsumerFactory& consumer_factory)
    : distributor_{std::make_unique<RecordDistributor>()}, configuration_{configuration},
      thread_factory_{thread_factory}, consumer_factory_{consumer_factory} {}

void SharedConsumerManagerImpl::processCallback(const RecordCbSharedPtr& callback) {

  // For every fetch topic, figure out the upstream cluster,
  // create consumer if needed ...
  const TopicToPartitionsMap interest = callback->interest();
  for (const auto& fetch : interest) {
    const std::string& topic = fetch.first;
    registerConsumerIfAbsent(topic);
  }

  // ... and start processing.
  distributor_->processCallback(callback);
}

void SharedConsumerManagerImpl::removeCallback(const RecordCbSharedPtr& callback) {
  // Real work - let's remove the callback.
  distributor_->removeCallback(callback);
}

void SharedConsumerManagerImpl::registerConsumerIfAbsent(const std::string& topic) {
  absl::MutexLock lock(&consumers_mutex_);
  const auto it = topic_to_consumer_.find(topic);
  // Return consumer already present or create new one and register it.
  if (topic_to_consumer_.end() == it) {
    registerNewConsumer(topic);
  }
}

void SharedConsumerManagerImpl::registerNewConsumer(const std::string& topic) {
  ENVOY_LOG(debug, "Creating consumer for topic [{}]", topic);

  // Compute which upstream cluster corresponds to the topic.
  const absl::optional<ClusterConfig> cluster_config =
      configuration_.computeClusterConfigForTopic(topic);
  if (!cluster_config) {
    throw EnvoyException(
        fmt::format("Could not compute upstream cluster configuration for topic [{}]", topic));
  }

  // Create the consumer and register it.
  KafkaConsumerPtr new_consumer = consumer_factory_.createConsumer(
      *distributor_, thread_factory_, topic, cluster_config->partition_count_,
      cluster_config->upstream_consumer_properties_);
  ENVOY_LOG(debug, "Registering new Kafka consumer for topic [{}], consuming from cluster [{}]",
            topic, cluster_config->name_);
  topic_to_consumer_.emplace(topic, std::move(new_consumer));
}

size_t SharedConsumerManagerImpl::getConsumerCountForTest() const {
  absl::MutexLock lock(&consumers_mutex_);
  return topic_to_consumer_.size();
}

// RecordDistributor

RecordDistributor::RecordDistributor() : RecordDistributor({}, {}){};

RecordDistributor::RecordDistributor(const PartitionMap<RecordCbSharedPtr>& callbacks,
                                     const PartitionMap<InboundRecordSharedPtr>& records)
    : partition_to_callbacks_{callbacks}, stored_records_{records} {};

bool RecordDistributor::waitUntilInterest(const std::string& topic,
                                          const int32_t timeout_ms) const {

  auto distributor_has_interest = std::bind(&RecordDistributor::hasInterest, this, topic);
  // Effectively this means "has an interest appeared within timeout".
  // If not, we let the user know so they could do something else
  // instead of being infinitely blocked.
  bool can_poll = callbacks_mutex_.LockWhenWithTimeout(absl::Condition(&distributor_has_interest),
                                                       absl::Milliseconds(timeout_ms));
  callbacks_mutex_.Unlock(); // Lock...WithTimeout always locks, so we need to unlock.
  return can_poll;
}

bool RecordDistributor::hasInterest(const std::string& topic) const {
  for (const auto& e : partition_to_callbacks_) {
    if (topic == e.first.first && !e.second.empty()) {
      return true;
    }
  }
  return false;
}

// XXX (adam.kotwasinski) Inefficient: locks acquired per record.
void RecordDistributor::receive(InboundRecordSharedPtr record) {

  const KafkaPartition kafka_partition = {record->topic_, record->partition_};

  // Whether this record has been consumed by any of the callbacks.
  // Because then we can safely throw it away instead of storing.
  bool consumed_by_callback = false;

  {
    absl::MutexLock lock(&callbacks_mutex_);
    auto& callbacks = partition_to_callbacks_[kafka_partition];

    std::vector<RecordCbSharedPtr> satisfied_callbacks = {};

    // Typical case: there is some interest in records for given partition.
    // Notify the callback and remove it.
    for (const auto& callback : callbacks) {
      CallbackReply callback_status = callback->receive(record);
      switch (callback_status) {
      case CallbackReply::AcceptedAndFinished: {
        consumed_by_callback = true;
        // A callback is finally satisfied, it will never want more records.
        satisfied_callbacks.push_back(callback);
        break;
      }
      case CallbackReply::AcceptedAndWantMore: {
        consumed_by_callback = true;
        break;
      }
      case CallbackReply::Rejected: {
        break;
      }
      } /* switch */

      /* Some callback has taken the record - this is good, no more iterating. */
      if (consumed_by_callback) {
        break;
      }
    }

    for (const auto& callback : satisfied_callbacks) {
      doRemoveCallback(callback);
    }
  }

  // No-one is interested in our record, so we are going to store it in a local cache.
  if (!consumed_by_callback) {
    absl::MutexLock lock(&stored_records_mutex_);
    auto& stored_records = stored_records_[kafka_partition];
    // XXX (adam.kotwasinski) Implement some kind of limit.
    stored_records.push_back(record);
    ENVOY_LOG(trace, "Stored record [{}]", record->toString());
  }
}

void RecordDistributor::processCallback(const RecordCbSharedPtr& callback) {
  ENVOY_LOG(trace, "Processing callback {}", callback->toString());

  // Attempt to fulfill callback's requirements using the stored records.
  bool fulfilled_at_startup = passRecordsToCallback(callback);

  if (fulfilled_at_startup) {
    // Early exit: callback was fulfilled with only stored records.
    // What means it will not require anything anymore, and does not need to be registered.
    ENVOY_LOG(trace, "No registration for callback {} due to successful early processing",
              callback->toString());
    return;
  }

  // Usual path: the request was not fulfilled at receive time (there were no stored messages).
  // So we just register the callback.
  TopicToPartitionsMap requested = callback->interest();
  absl::MutexLock lock(&callbacks_mutex_);
  for (const auto& topic_and_partitions : requested) {
    const std::string topic = topic_and_partitions.first;
    for (const int32_t partition : topic_and_partitions.second) {
      const KafkaPartition kp = {topic, partition};
      auto& partition_callbacks = partition_to_callbacks_[kp];
      partition_callbacks.push_back(callback);
    }
  }
}

bool RecordDistributor::passRecordsToCallback(const RecordCbSharedPtr& callback) {
  TopicToPartitionsMap requested = callback->interest();
  absl::MutexLock lock(&stored_records_mutex_);

  for (const auto& topic_and_partitions : requested) {
    for (const int32_t partition : topic_and_partitions.second) {
      const KafkaPartition kp = {topic_and_partitions.first, partition};
      // Processing of given partition's records was enough for given callback.
      const bool processing_finished = passPartitionRecordsToCallback(callback, kp);
      if (processing_finished) {
        return true;
      }
    }
  }

  // All the eligible records have been passed to callback, but it still wants more.
  // So we are going to need to register it.
  return false;
}

bool RecordDistributor::passPartitionRecordsToCallback(const RecordCbSharedPtr& callback,
                                                       const KafkaPartition& kafka_partition) {
  const auto it = stored_records_.find(kafka_partition);
  if (stored_records_.end() == it) {
    // This partition does not have any records buffered.
    return false;
  }

  auto& partition_records = it->second;
  ENVOY_LOG(trace, "Early notification for callback {}, as there are {} messages available",
            callback->toString(), partition_records.size());

  bool processing_finished = false;
  for (auto record_it = partition_records.begin(); record_it != partition_records.end();) {
    const CallbackReply callback_status = callback->receive(*record_it);
    switch (callback_status) {
    case CallbackReply::AcceptedAndWantMore: {
      // Callback consumed the record, and wants more. We keep iterating.
      record_it = partition_records.erase(record_it);
      break;
    }
    case CallbackReply::AcceptedAndFinished: {
      // We had a callback that wanted records, and got all it wanted in the initial
      // processing (== everything it needed was buffered), so we won't need to register it.
      record_it = partition_records.erase(record_it);
      processing_finished = true;
      break;
    }
    case CallbackReply::Rejected: {
      // Our callback entered a terminal state in the meantime. We won't work with it anymore.
      processing_finished = true;
      break;
    }
    } /* switch */

    if (processing_finished) {
      // No more processing needed.
      break;
    }
  } /* for-stored-records */

  if (partition_records.empty()) {
    // The partition's buffer got drained - there is no reason to keep empty vectors.
    stored_records_.erase(it);
  }

  return processing_finished;
}

void RecordDistributor::removeCallback(const RecordCbSharedPtr& callback) {
  absl::MutexLock lock(&callbacks_mutex_);
  doRemoveCallback(callback);
}

void RecordDistributor::doRemoveCallback(const RecordCbSharedPtr& callback) {
  ENVOY_LOG(trace, "Removing callback {}", callback->toString());
  for (auto it = partition_to_callbacks_.begin(); it != partition_to_callbacks_.end();) {
    auto& partition_callbacks = it->second;
    partition_callbacks.erase(
        std::remove(partition_callbacks.begin(), partition_callbacks.end(), callback),
        partition_callbacks.end());
    if (partition_callbacks.empty()) {
      it = partition_to_callbacks_.erase(it);
    } else {
      ++it;
    }
  }
}

// Just a helper function for tests.
template <typename T>
int32_t countForTest(const std::string& topic, const int32_t partition, PartitionMap<T> map) {
  const auto it = map.find({topic, partition});
  if (map.end() != it) {
    return it->second.size();
  } else {
    return -1; // Tests are simpler to type if we do this instead of absl::optional.
  }
}

int32_t RecordDistributor::getCallbackCountForTest(const std::string& topic,
                                                   const int32_t partition) const {
  absl::MutexLock lock(&callbacks_mutex_);
  return countForTest(topic, partition, partition_to_callbacks_);
}

int32_t RecordDistributor::getRecordCountForTest(const std::string& topic,
                                                 const int32_t partition) const {
  absl::MutexLock lock(&stored_records_mutex_);
  return countForTest(topic, partition, stored_records_);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
# Command handlers

These simple diagrams show what are the main classes involved in providing Kafka-mesh filter functionality.

Disclaimer: these are not UML diagrams in any shape or form.

## Basics

Raw data is processed by `RequestDecoder`, which notifies `RequestProcessor` on successful parse.
`RequestProcessor` then creates `InFlightRequest` instances that can be processed.

When an `InFlightRequest` is finally processed, it can generate an answer (`AbstractResponse`)
that is later serialized by `ResponseEncoder`.

```mermaid
graph TD;
    InFlightRequest["<< abstract >> \n InFlightRequest"]
    AbstractResponse["<< abstract >> \n AbstractResponse"]

    KafkaMeshFilter <-.-> |"in-flight-reference\n(finish/abandon)"| InFlightRequest
    KafkaMeshFilter --> |feeds| RequestDecoder
    RequestDecoder --> |"notifies"| RequestProcessor
    RequestProcessor --> |"creates"| InFlightRequest
    InFlightRequest --> |"produces"| AbstractResponse

    RequestHolder["...RequestHolder"]
    RequestHolder --> |"subclass"| InFlightRequest

    KafkaMeshFilter --> ResponseEncoder
    ResponseEncoder -.-> |encodes| AbstractResponse
```

## Produce

Produce request (`ProduceRequestHolder`) uses `UpstreamKafkaFacade` to get `RichKafkaProducer` instances that
correspond to its topics.
When the deliveries have finished (successfully or not - the upstream could have rejected the records because
of its own reasons), `RichKafkaProducer` notifies the `ProduceRequestHolder` that it has finished.
The request can then notify its parent (`KafkaMeshFilter`) that the response can be sent downstream.

```mermaid
graph TD;
    KafkaMeshFilter <-.-> |"in-flight-reference\n(finish/abandon)"| ProduceRequestHolder
    KafkaMeshFilter --> RP["RequestDecoder+RequestProcessor"]
    RP --> |"creates"| ProduceRequestHolder
    UpstreamKafkaFacade --> |"accesses (Envoy thread-local)"| ThreadLocalKafkaFacade
    ThreadLocalKafkaFacade --> |"stores multiple"| RichKafkaProducer
    RdKafkaProducer["<< librdkafka >>\nRdKafkaProducer"]
    RichKafkaProducer --> |"wraps"| RdKafkaProducer
    RichKafkaProducer -.-> |"in-flight-reference\n(delivery callback)"| ProduceRequestHolder
    ProduceRequestHolder --> |uses| UpstreamKafkaFacade
    ProduceRequestHolder -.-> |sends data to| RichKafkaProducer
```

## Fetch

Fetch request (`FechRequestHolder`) registers itself with `SharedConsumerManager` to be notified when records matching
its interests appear.
`SharedConsumerManager` maintains multiple `RichKafkaConsumer` instances (what means keeps the Kafka consumer state)
that are responsible for polling records from upstream Kafka clusters.
Each `RichKafkaConsumer` is effectively a librdkafka `KafkaConsumer` and its poller thread.
When `FechRequestHolder` is finished with its processing (whether through record delivery or timeout), it uses an Envoy
`Dispatcher` to notify the parent filter.

```mermaid
graph TD;
    FRH["FechRequestHolder"]
    KafkaMeshFilter <-.-> |"in-flight-reference\n(finish/abandon)"| FRH
    KafkaMeshFilter --> RP["RequestDecoder+RequestProcessor"]
    RP --> |"creates"| FRH

    RCP["<< interface >> \n RecordCallbackProcessor"]
    SCM["SharedConsumerManager"]
    SCM --> |subclass| RCP

    KC["RichKafkaConsumer"]
    FRH -.-> |registers itself with| SCM
    SCM -.-> |provides records| FRH
    SCM --> |stores mutliple| KC

    LibrdKafkaConsumer["<< librdkafka >> \n KafkaConsumer"]
    ConsumerPoller["<< thread >> \n consumer poller"]
    KC --> |wraps| LibrdKafkaConsumer
    KC --> |holds| ConsumerPoller
    ConsumerPoller --> |polls from| LibrdKafkaConsumer

    DSP["<< Envoy >> \n Dispatcher"]
    KafkaMeshFilter ---  DSP
    FRH -.-> |notifies on finish| DSP
```
#pragma once

#include <atomic>
#include <list>
#include <vector>

#include "envoy/event/dispatcher.h"
#include "envoy/thread/thread.h"

#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_consumer.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Combines the librdkafka consumer and its dedicated worker thread.
 * The thread receives the records, and pushes them to the processor.
 * The consumer is going to receive the records from all the partitions for the given topic.
 */
class RichKafkaConsumer : public KafkaConsumer, private Logger::Loggable<Logger::Id::kafka> {
public:
  // Main constructor.
  RichKafkaConsumer(InboundRecordProcessor& record_processor, Thread::ThreadFactory& thread_factory,
                    const std::string& topic, const int32_t partition_count,
                    const RawKafkaConfig& configuration);

  // Visible for testing (allows injection of LibRdKafkaUtils).
  RichKafkaConsumer(InboundRecordProcessor& record_processor, Thread::ThreadFactory& thread_factory,
                    const std::string& topic, const int32_t partition_count,
                    const RawKafkaConfig& configuration, const LibRdKafkaUtils& utils);

  // More complex than usual - closes the real Kafka consumer and disposes of the assignment object.
  ~RichKafkaConsumer() override;

private:
  // This method continuously fetches new records and passes them to processor.
  // Does not finish until this object gets destroyed.
  // Executed in the dedicated worker thread.
  void runWorkerLoop();

  // Uses internal consumer to receive records from upstream.
  std::vector<InboundRecordSharedPtr> receiveRecordBatch();

  // The record processor (provides info whether it wants records and consumes them).
  InboundRecordProcessor& record_processor_;

  // The topic we are consuming from.
  std::string topic_;

  // Real Kafka consumer (NOT thread-safe).
  // All access to this thing happens in the worker thread.
  std::unique_ptr<RdKafka::KafkaConsumer> consumer_;

  // Consumer's partition assignment.
  ConsumerAssignmentConstPtr assignment_;

  // Flag controlling worker threads's execution.
  std::atomic<bool> worker_thread_active_;

  // Real worker thread.
  // Responsible for getting records from upstream Kafka with a consumer and passing these records
  // to the processor.
  Thread::ThreadPtr worker_thread_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <map>
#include <memory>
#include <utility>

#include "envoy/common/pure.h"

#include "absl/strings/string_view.h"
#include "contrib/kafka/filters/network/source/mesh/outbound_record.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// Trivial memento that keeps the information about how given request was delivered:
// in case of success this means offset (if acks > 0), or error code.
struct DeliveryMemento {

  // Pointer to byte array that was passed to Kafka producer.
  // We use this to tell apart messages.
  // Important: we do not free this memory, it's still part of the 'ProduceRequestHandler' object.
  // Future work: adopt Kafka's opaque-pointer functionality so we use less memory instead of
  // keeping whole payload until we receive a confirmation.
  const void* data_;

  // Kafka producer error code.
  const int32_t error_code_;

  // Offset (only meaningful if error code is equal to 0).
  const int64_t offset_;
};

// Callback for objects that want to be notified that record delivery has been finished.
class ProduceFinishCb {
public:
  virtual ~ProduceFinishCb() = default;

  // Attempt to process this delivery.
  // @returns true if given callback is related to this delivery
  virtual bool accept(const DeliveryMemento& memento) PURE;
};

using ProduceFinishCbSharedPtr = std::shared_ptr<ProduceFinishCb>;

/**
 * Filter facing interface.
 * A thing that takes records and sends them to upstream Kafka.
 */
class KafkaProducer {
public:
  virtual ~KafkaProducer() = default;

  /*
   * Sends given record (key, value) to Kafka (topic, partition).
   * When delivery is finished, it notifies the callback provided with corresponding delivery data
   * (error code, offset).
   *
   * @param origin origin of payload to be notified when delivery finishes.
   * @param record record data to be sent.
   */
  virtual void send(const ProduceFinishCbSharedPtr origin, const OutboundRecord& record) PURE;

  // Impl leakage: real implementations of Kafka Producer need to stop a monitoring thread, then
  // they can close the producer. Because the polling thread should not be interrupted, we just mark
  // it as finished, and it's going to notice that change on the next iteration.
  // Theoretically we do not need to do this and leave it all to destructor, but then closing N
  // producers would require doing that in sequence, while we can optimize it somewhat (so we just
  // wait for the slowest one).
  // See https://github.com/confluentinc/librdkafka/issues/2972
  virtual void markFinished() PURE;
};

using KafkaProducerPtr = std::unique_ptr<KafkaProducer>;

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "envoy/thread/thread.h"
#include "envoy/thread_local/thread_local.h"

#include "source/common/common/logger.h"

#include "contrib/kafka/filters/network/source/mesh/upstream_config.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_client.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Provides access to upstream Kafka clients.
 */
class UpstreamKafkaFacade {
public:
  virtual ~UpstreamKafkaFacade() = default;

  /**
   * Returns a Kafka producer that points an upstream Kafka cluster that is supposed to receive
   * messages for the given topic.
   */
  virtual KafkaProducer& getProducerForTopic(const std::string& topic) PURE;
};

using UpstreamKafkaFacadeSharedPtr = std::shared_ptr<UpstreamKafkaFacade>;

/**
 * Provides access to upstream Kafka clients.
 * This is done by using thread-local maps of cluster to producer.
 * We are going to have one Kafka producer per upstream cluster, per Envoy worker thread.
 */
class UpstreamKafkaFacadeImpl : public UpstreamKafkaFacade,
                                private Logger::Loggable<Logger::Id::kafka> {
public:
  UpstreamKafkaFacadeImpl(const UpstreamKafkaConfiguration& configuration,
                          ThreadLocal::SlotAllocator& slot_allocator,
                          Thread::ThreadFactory& thread_factory);

  // UpstreamKafkaFacade
  KafkaProducer& getProducerForTopic(const std::string& topic) override;

  size_t getProducerCountForTest() const;

private:
  ThreadLocal::SlotPtr tls_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <map>
#include <memory>
#include <string>
#include <utility>

#include "envoy/common/pure.h"

#include "source/common/common/logger.h"

#include "absl/types/optional.h"
#include "contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha/kafka_mesh.pb.h"
#include "contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha/kafka_mesh.pb.validate.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

using KafkaMeshProtoConfig = envoy::extensions::filters::network::kafka_mesh::v3alpha::KafkaMesh;

// Minor helper structure that contains information about upstream Kafka clusters.
struct ClusterConfig {

  // Cluster name, as it appears in configuration input.
  std::string name_;

  // How many partitions do we expect for every one of the topics present in given upstream cluster.
  // Impl note: this could be replaced with creating (shared?) AdminClient and having it reach out
  // upstream to get configuration (or we could just send a correct request via codec). The response
  // would need to be cached (as this data is frequently requested).
  int32_t partition_count_;

  // The configuration that will be passed to upstream client for given cluster.
  // This allows us to reference different clusters with different configs (e.g. linger.ms).
  // This map always contains entry with key 'bootstrap.servers', as this is the only mandatory
  // producer property.
  std::map<std::string, std::string> upstream_producer_properties_;

  // This map always contains entries with keys 'bootstrap.servers' and 'group.id', as these are the
  // only mandatory consumer properties.
  std::map<std::string, std::string> upstream_consumer_properties_;

  bool operator==(const ClusterConfig& rhs) const {
    return name_ == rhs.name_ && partition_count_ == rhs.partition_count_ &&
           upstream_producer_properties_ == rhs.upstream_producer_properties_ &&
           upstream_consumer_properties_ == rhs.upstream_consumer_properties_;
  }
};

/**
 * Keeps the configuration related to upstream Kafka clusters.
 */
class UpstreamKafkaConfiguration {
public:
  virtual ~UpstreamKafkaConfiguration() = default;

  // Return this the host-port pair that's provided to Kafka clients.
  // This value needs to follow same rules as 'advertised.address' property of Kafka broker.
  virtual std::pair<std::string, int32_t> getAdvertisedAddress() const PURE;

  // Provides cluster for given Kafka topic, according to the rules contained within this
  // configuration object.
  virtual absl::optional<ClusterConfig>
  computeClusterConfigForTopic(const std::string& topic) const PURE;
};

using UpstreamKafkaConfigurationSharedPtr = std::shared_ptr<const UpstreamKafkaConfiguration>;

/**
 * Implementation that uses only topic-prefix to figure out which Kafka cluster to use.
 */
class UpstreamKafkaConfigurationImpl : public UpstreamKafkaConfiguration,
                                       private Logger::Loggable<Logger::Id::kafka> {
public:
  UpstreamKafkaConfigurationImpl(const KafkaMeshProtoConfig& config);

  // UpstreamKafkaConfiguration
  absl::optional<ClusterConfig>
  computeClusterConfigForTopic(const std::string& topic) const override;

  // UpstreamKafkaConfiguration
  std::pair<std::string, int32_t> getAdvertisedAddress() const override;

private:
  const std::pair<std::string, int32_t> advertised_address_;
  std::map<std::string, ClusterConfig> topic_prefix_to_cluster_config_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils_impl.h"

#include "source/common/common/macros.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// ConsumerAssignmentImpl

class ConsumerAssignmentImpl : public ConsumerAssignment {
public:
  ConsumerAssignmentImpl(std::vector<RdKafkaPartitionPtr>&& assignment)
      : assignment_{std::move(assignment)} {};

  // The assignment in a form that librdkafka likes.
  RdKafkaPartitionVector raw() const;

private:
  const std::vector<RdKafkaPartitionPtr> assignment_;
};

RdKafkaPartitionVector ConsumerAssignmentImpl::raw() const {
  RdKafkaPartitionVector result;
  for (const auto& tp : assignment_) {
    result.push_back(tp.get()); // Raw pointer.
  }
  return result;
}

// LibRdKafkaUtils

RdKafka::Conf::ConfResult LibRdKafkaUtilsImpl::setConfProperty(RdKafka::Conf& conf,
                                                               const std::string& name,
                                                               const std::string& value,
                                                               std::string& errstr) const {
  return conf.set(name, value, errstr);
}

RdKafka::Conf::ConfResult
LibRdKafkaUtilsImpl::setConfDeliveryCallback(RdKafka::Conf& conf, RdKafka::DeliveryReportCb* dr_cb,
                                             std::string& errstr) const {
  return conf.set("dr_cb", dr_cb, errstr);
}

std::unique_ptr<RdKafka::Producer> LibRdKafkaUtilsImpl::createProducer(RdKafka::Conf* conf,
                                                                       std::string& errstr) const {
  return std::unique_ptr<RdKafka::Producer>(RdKafka::Producer::create(conf, errstr));
}

std::unique_ptr<RdKafka::KafkaConsumer>
LibRdKafkaUtilsImpl::createConsumer(RdKafka::Conf* conf, std::string& errstr) const {
  return std::unique_ptr<RdKafka::KafkaConsumer>(RdKafka::KafkaConsumer::create(conf, errstr));
}

RdKafka::Headers* LibRdKafkaUtilsImpl::convertHeaders(
    const std::vector<std::pair<absl::string_view, absl::string_view>>& headers) const {
  RdKafka::Headers* result = RdKafka::Headers::create();
  for (const auto& header : headers) {
    const RdKafka::Headers::Header librdkafka_header = {
        std::string(header.first), header.second.data(), header.second.length()};
    const auto ec = result->add(librdkafka_header);
    // This should never happen ('add' in 1.7.0 does not return any other error codes).
    if (RdKafka::ERR_NO_ERROR != ec) {
      delete result;
      return nullptr;
    }
  }
  return result;
}

void LibRdKafkaUtilsImpl::deleteHeaders(RdKafka::Headers* librdkafka_headers) const {
  delete librdkafka_headers;
}

ConsumerAssignmentConstPtr LibRdKafkaUtilsImpl::assignConsumerPartitions(
    RdKafka::KafkaConsumer& consumer, const std::string& topic, const int32_t partitions) const {

  // Construct the topic-partition vector that we are going to store.
  std::vector<RdKafkaPartitionPtr> assignment;
  for (int partition = 0; partition < partitions; ++partition) {

    // We consume records from the beginning of each partition.
    const int64_t initial_offset = 0;
    assignment.push_back(std::unique_ptr<RdKafka::TopicPartition>(
        RdKafka::TopicPartition::create(topic, partition, initial_offset)));
  }
  auto result = std::make_unique<ConsumerAssignmentImpl>(std::move(assignment));

  // Do the assignment.
  consumer.assign(result->raw());
  return result;
}

const LibRdKafkaUtils& LibRdKafkaUtilsImpl::getDefaultInstance() {
  CONSTRUCT_ON_FIRST_USE(LibRdKafkaUtilsImpl);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

void BaseInFlightRequest::abandon() {
  ENVOY_LOG(trace, "Abandoning request");
  filter_active_ = false;
}

void BaseInFlightRequest::notifyFilter() {
  if (filter_active_) {
    ENVOY_LOG(trace, "Notifying filter for request");
    filter_.onRequestReadyForAnswer();
  } else {
    ENVOY_LOG(trace, "Request has been finished, but we are not doing anything, because filter has "
                     "been already destroyed");
  }
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_facade.h"

#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_client_impl.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Responsible for keeping a map of upstream-facing Kafka clients.
 */
class ThreadLocalKafkaFacade : public ThreadLocal::ThreadLocalObject,
                               private Logger::Loggable<Logger::Id::kafka> {
public:
  ThreadLocalKafkaFacade(const UpstreamKafkaConfiguration& configuration,
                         Event::Dispatcher& dispatcher, Thread::ThreadFactory& thread_factory);
  ~ThreadLocalKafkaFacade() override;

  KafkaProducer& getProducerForTopic(const std::string& topic);

  size_t getProducerCountForTest() const;

private:
  // Mutates 'cluster_to_kafka_client_'.
  KafkaProducer& registerNewProducer(const ClusterConfig& cluster_config);

  const UpstreamKafkaConfiguration& configuration_;
  Event::Dispatcher& dispatcher_;
  Thread::ThreadFactory& thread_factory_;

  std::map<std::string, KafkaProducerPtr> cluster_to_kafka_client_;
};

ThreadLocalKafkaFacade::ThreadLocalKafkaFacade(const UpstreamKafkaConfiguration& configuration,
                                               Event::Dispatcher& dispatcher,
                                               Thread::ThreadFactory& thread_factory)
    : configuration_{configuration}, dispatcher_{dispatcher}, thread_factory_{thread_factory} {}

ThreadLocalKafkaFacade::~ThreadLocalKafkaFacade() {
  // Because the producers take a moment to shutdown, we mark their monitoring threads as shut down
  // before the destructors get called.
  for (auto& entry : cluster_to_kafka_client_) {
    entry.second->markFinished();
  }
}

KafkaProducer& ThreadLocalKafkaFacade::getProducerForTopic(const std::string& topic) {
  const absl::optional<ClusterConfig> cluster_config =
      configuration_.computeClusterConfigForTopic(topic);
  if (cluster_config) {
    const auto it = cluster_to_kafka_client_.find(cluster_config->name_);
    // Return client already present or create new one and register it.
    return (cluster_to_kafka_client_.end() == it) ? registerNewProducer(*cluster_config)
                                                  : *(it->second);
  } else {
    throw EnvoyException(absl::StrCat("cannot compute target producer for topic: ", topic));
  }
}

KafkaProducer& ThreadLocalKafkaFacade::registerNewProducer(const ClusterConfig& cluster_config) {
  ENVOY_LOG(debug, "Registering new Kafka producer for cluster [{}]", cluster_config.name_);
  KafkaProducerPtr new_producer = std::make_unique<RichKafkaProducer>(
      dispatcher_, thread_factory_, cluster_config.upstream_producer_properties_);
  auto result = cluster_to_kafka_client_.emplace(cluster_config.name_, std::move(new_producer));
  return *(result.first->second);
}

size_t ThreadLocalKafkaFacade::getProducerCountForTest() const {
  return cluster_to_kafka_client_.size();
}

UpstreamKafkaFacadeImpl::UpstreamKafkaFacadeImpl(const UpstreamKafkaConfiguration& configuration,
                                                 ThreadLocal::SlotAllocator& slot_allocator,
                                                 Thread::ThreadFactory& thread_factory)
    : tls_{slot_allocator.allocateSlot()} {

  ThreadLocal::Slot::InitializeCb cb =
      [&configuration,
       &thread_factory](Event::Dispatcher& dispatcher) -> ThreadLocal::ThreadLocalObjectSharedPtr {
    return std::make_shared<ThreadLocalKafkaFacade>(configuration, dispatcher, thread_factory);
  };
  tls_->set(cb);
}

// Return KafkaProducer instance that is local to given thread, via ThreadLocalKafkaFacade.
KafkaProducer& UpstreamKafkaFacadeImpl::getProducerForTopic(const std::string& topic) {
  return tls_->getTyped<ThreadLocalKafkaFacade>().getProducerForTopic(topic);
}

size_t UpstreamKafkaFacadeImpl::getProducerCountForTest() const {
  return tls_->getTyped<ThreadLocalKafkaFacade>().getProducerCountForTest();
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_client_impl.h"

#include "contrib/kafka/filters/network/source/mesh/librdkafka_utils_impl.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

RichKafkaProducer::RichKafkaProducer(Event::Dispatcher& dispatcher,
                                     Thread::ThreadFactory& thread_factory,
                                     const RawKafkaConfig& configuration)
    : RichKafkaProducer(dispatcher, thread_factory, configuration,
                        LibRdKafkaUtilsImpl::getDefaultInstance()){};

RichKafkaProducer::RichKafkaProducer(Event::Dispatcher& dispatcher,
                                     Thread::ThreadFactory& thread_factory,
                                     const RawKafkaConfig& configuration,
                                     const LibRdKafkaUtils& utils)
    : dispatcher_{dispatcher}, utils_{utils} {

  // Create producer configuration object.
  std::unique_ptr<RdKafka::Conf> conf =
      std::unique_ptr<RdKafka::Conf>(RdKafka::Conf::create(RdKafka::Conf::CONF_GLOBAL));
  std::string errstr;

  // Setup producer custom properties.
  for (const auto& e : configuration) {
    if (utils.setConfProperty(*conf, e.first, e.second, errstr) != RdKafka::Conf::CONF_OK) {
      throw EnvoyException(absl::StrCat("Could not set producer property [", e.first, "] to [",
                                        e.second, "]:", errstr));
    }
  }

  // Setup callback (this callback is going to be invoked in dedicated monitoring thread).
  if (utils.setConfDeliveryCallback(*conf, this, errstr) != RdKafka::Conf::CONF_OK) {
    throw EnvoyException(absl::StrCat("Could not set producer callback:", errstr));
  }

  // Finally, we create the producer.
  producer_ = utils.createProducer(conf.get(), errstr);
  if (!producer_) {
    throw EnvoyException(absl::StrCat("Could not create producer:", errstr));
  }

  // Start the monitoring thread.
  poller_thread_active_ = true;
  std::function<void()> thread_routine = [this]() -> void { checkDeliveryReports(); };
  poller_thread_ = thread_factory.createThread(thread_routine);
}

RichKafkaProducer::~RichKafkaProducer() {
  ENVOY_LOG(debug, "Shutting down worker thread");
  poller_thread_active_ = false; // This should never be needed, as we call 'markFinished' earlier.
  poller_thread_->join();
  ENVOY_LOG(debug, "Worker thread shut down successfully");
}

void RichKafkaProducer::markFinished() { poller_thread_active_ = false; }

void RichKafkaProducer::send(const ProduceFinishCbSharedPtr origin, const OutboundRecord& record) {
  {
    void* value_data = const_cast<char*>(record.value_.data()); // Needed for Kafka API.
    // Data is a pointer into request internals, and it is going to be managed by
    // ProduceRequestHolder lifecycle. So we are not going to use any of librdkafka's memory
    // management.
    const int flags = 0;
    const int64_t timestamp = 0;

    RdKafka::ErrorCode ec;
    // librdkafka requires a raw pointer and deletes it on success.
    RdKafka::Headers* librdkafka_headers = utils_.convertHeaders(record.headers_);
    if (nullptr != librdkafka_headers) {
      ec = producer_->produce(record.topic_, record.partition_, flags, value_data,
                              record.value_.size(), record.key_.data(), record.key_.size(),
                              timestamp, librdkafka_headers, nullptr);
    } else {
      // Headers could not be converted (this should never happen).
      ENVOY_LOG(trace, "Header conversion failed while sending to [{}/{}]", record.topic_,
                record.partition_);
      ec = RdKafka::ERR_UNKNOWN;
    }

    if (RdKafka::ERR_NO_ERROR == ec) {
      // We have succeeded with submitting data to producer, so we register a callback.
      unfinished_produce_requests_.push_back(origin);
    } else {
      // We could not submit data to producer.
      // Let's treat that as a normal failure (Envoy is a broker after all) and propagate
      // downstream.
      ENVOY_LOG(trace, "Produce failure: {}, while sending to [{}/{}]", ec, record.topic_,
                record.partition_);
      if (nullptr != librdkafka_headers) {
        // Kafka headers need to be deleted manually if produce call fails.
        utils_.deleteHeaders(librdkafka_headers);
      }
      const DeliveryMemento memento = {value_data, ec, 0};
      origin->accept(memento);
    }
  }
}

void RichKafkaProducer::checkDeliveryReports() {
  while (poller_thread_active_) {
    // We are going to wait for 1000ms, returning when an event (message delivery) happens or
    // producer is closed. Unfortunately we do not have any ability to interrupt this call, so every
    // destructor is going to take up to this much time.
    producer_->poll(1000);
    // This invokes the callback below, if any delivery finished (successful or not).
  }
  ENVOY_LOG(debug, "Poller thread finished");
}

// Kafka callback that contains the delivery information.
void RichKafkaProducer::dr_cb(RdKafka::Message& message) {
  ENVOY_LOG(trace, "Delivery finished: {}, payload has been saved at offset {} in {}/{}",
            message.err(), message.topic_name(), message.partition(), message.offset());
  const DeliveryMemento memento = {message.payload(), message.err(), message.offset()};
  // Because this method gets executed in poller thread, we need to pass the data through
  // dispatcher.
  dispatcher_.post([this, memento]() -> void { processDelivery(memento); });
}

// We got the delivery data.
// Now we just check all unfinished requests, find the one that originated this particular delivery,
// and notify it.
void RichKafkaProducer::processDelivery(const DeliveryMemento& memento) {
  for (auto it = unfinished_produce_requests_.begin(); it != unfinished_produce_requests_.end();) {
    bool accepted = (*it)->accept(memento);
    if (accepted) {
      unfinished_produce_requests_.erase(it);
      break; // This is important - a single request can be mapped into multiple callbacks here.
    } else {
      ++it;
    }
  }
}

std::list<ProduceFinishCbSharedPtr>& RichKafkaProducer::getUnfinishedRequestsForTest() {
  return unfinished_produce_requests_;
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
load(
    "//bazel:envoy_build_system.bzl",
    "envoy_cc_contrib_extension",
    "envoy_cc_library",
    "envoy_contrib_package",
)
load("//bazel:envoy_internal.bzl", "envoy_external_dep_path")

licenses(["notice"])  # Apache 2

envoy_contrib_package()

# Kafka-mesh network filter.
# Mesh filter public docs: https://envoyproxy.io/docs/envoy/latest/configuration/listeners/network_filters/kafka_mesh_filter

envoy_cc_contrib_extension(
    name = "config_lib",
    srcs = ["config.cc"],
    hdrs = ["config.h"],
    deps = [
        "//envoy/registry",
        "//source/extensions/filters/network/common:factory_base_lib",
        "@envoy_api//contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha:pkg_cc_proto",
    ] + select({
        "//bazel:windows": [],
        "//conditions:default": [
            ":filter_lib",
            ":shared_consumer_manager_impl_lib",
            ":upstream_config_lib",
            ":upstream_kafka_facade_lib",
        ],
    }),
)

envoy_cc_library(
    name = "filter_lib",
    srcs = ["filter.cc"],
    hdrs = [
        "filter.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":abstract_command_lib",
        ":request_processor_lib",
        ":shared_consumer_manager_lib",
        ":upstream_config_lib",
        ":upstream_kafka_facade_lib",
        "//contrib/kafka/filters/network/source:kafka_request_codec_lib",
        "//contrib/kafka/filters/network/source:kafka_response_codec_lib",
        "//envoy/buffer:buffer_interface",
        "//envoy/network:connection_interface",
        "//envoy/network:filter_interface",
        "//source/common/common:assert_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "request_processor_lib",
    srcs = [
        "request_processor.cc",
    ],
    hdrs = [
        "request_processor.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":abstract_command_lib",
        ":upstream_config_lib",
        ":upstream_kafka_facade_lib",
        "//contrib/kafka/filters/network/source:kafka_request_codec_lib",
        "//contrib/kafka/filters/network/source:kafka_request_parser_lib",
        "//contrib/kafka/filters/network/source/mesh:shared_consumer_manager_lib",
        "//contrib/kafka/filters/network/source/mesh/command_handlers:api_versions_lib",
        "//contrib/kafka/filters/network/source/mesh/command_handlers:fetch_lib",
        "//contrib/kafka/filters/network/source/mesh/command_handlers:list_offsets_lib",
        "//contrib/kafka/filters/network/source/mesh/command_handlers:metadata_lib",
        "//contrib/kafka/filters/network/source/mesh/command_handlers:produce_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "abstract_command_lib",
    srcs = [
        "abstract_command.cc",
    ],
    hdrs = [
        "abstract_command.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//contrib/kafka/filters/network/source:kafka_response_lib",
        "//contrib/kafka/filters/network/source:tagged_fields_lib",
        "//envoy/event:dispatcher_interface",
    ],
)

envoy_cc_library(
    name = "upstream_kafka_facade_lib",
    srcs = [
        "upstream_kafka_facade.cc",
    ],
    hdrs = [
        "upstream_kafka_facade.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":upstream_config_lib",
        ":upstream_kafka_client_impl_lib",
        ":upstream_kafka_client_lib",
        "//envoy/thread:thread_interface",
        "//envoy/thread_local:thread_local_interface",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "shared_consumer_manager_lib",
    srcs = [],
    hdrs = ["shared_consumer_manager.h"],
    tags = ["skip_on_windows"],
    deps = [
        ":upstream_config_lib",
        ":upstream_kafka_consumer_lib",
    ],
)

envoy_cc_library(
    name = "shared_consumer_manager_impl_lib",
    srcs = ["shared_consumer_manager_impl.cc"],
    hdrs = ["shared_consumer_manager_impl.h"],
    tags = ["skip_on_windows"],
    deps = [
        ":shared_consumer_manager_lib",
        ":upstream_config_lib",
        ":upstream_kafka_consumer_impl_lib",
        ":upstream_kafka_consumer_lib",
        "//contrib/kafka/filters/network/source:kafka_types_lib",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "inbound_record_lib",
    srcs = [
    ],
    hdrs = [
        "inbound_record.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
    ],
)

envoy_cc_library(
    name = "outbound_record_lib",
    srcs = [
    ],
    hdrs = [
        "outbound_record.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
    ],
)

envoy_cc_library(
    name = "upstream_kafka_client_lib",
    srcs = [
    ],
    hdrs = [
        "upstream_kafka_client.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":outbound_record_lib",
    ],
)

envoy_cc_library(
    name = "upstream_kafka_client_impl_lib",
    srcs = [
        "upstream_kafka_client_impl.cc",
    ],
    hdrs = [
        "upstream_kafka_client_impl.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":librdkafka_utils_impl_lib",
        ":outbound_record_lib",
        ":upstream_kafka_client_lib",
        "//envoy/event:dispatcher_interface",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "upstream_kafka_consumer_lib",
    srcs = [
    ],
    hdrs = [
        "upstream_kafka_consumer.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":inbound_record_lib",
    ],
)

envoy_cc_library(
    name = "upstream_kafka_consumer_impl_lib",
    srcs = [
        "upstream_kafka_consumer_impl.cc",
    ],
    hdrs = [
        "upstream_kafka_consumer_impl.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":librdkafka_utils_impl_lib",
        ":upstream_kafka_consumer_lib",
        "//contrib/kafka/filters/network/source:kafka_types_lib",
        "//envoy/event:dispatcher_interface",
        "//source/common/common:minimal_logger_lib",
    ],
)

envoy_cc_library(
    name = "upstream_config_lib",
    srcs = [
        "upstream_config.cc",
    ],
    hdrs = [
        "upstream_config.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//envoy/common:exception_lib",
        "//source/common/common:assert_lib",
        "//source/common/common:minimal_logger_lib",
        "@envoy_api//contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha:pkg_cc_proto",
    ],
)

envoy_cc_library(
    name = "librdkafka_utils_lib",
    srcs = [
    ],
    hdrs = [
        "librdkafka_utils.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        "//envoy/common:pure_lib",
        "@com_google_absl//absl/strings",
        envoy_external_dep_path("librdkafka"),
    ],
)

envoy_cc_library(
    name = "librdkafka_utils_impl_lib",
    srcs = [
        "librdkafka_utils_impl.cc",
    ],
    hdrs = [
        "librdkafka_utils_impl.h",
    ],
    tags = ["skip_on_windows"],
    deps = [
        ":librdkafka_utils_lib",
        envoy_external_dep_path("librdkafka"),
        "//source/common/common:macros",
    ],
)
#pragma once

#include "envoy/event/dispatcher.h"

#include "source/common/common/logger.h"

#include "contrib/kafka/filters/network/source/kafka_response.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Represents single downstream client request.
 * Responsible for performing the work on multiple upstream clusters and aggregating the results.
 */
class InFlightRequest {
public:
  virtual ~InFlightRequest() = default;

  /**
   * Begins processing of given request with context provided.
   */
  virtual void startProcessing() PURE;

  /**
   * Whether the given request has finished processing.
   * E.g. produce requests need to be forwarded upstream and get a response from Kafka cluster for
   * this to be true.
   */
  virtual bool finished() const PURE;

  /**
   * Creates a Kafka answer object that can be sent downstream.
   */
  virtual AbstractResponseSharedPtr computeAnswer() const PURE;

  /**
   * Abandon this request.
   * In-flight requests that have been abandoned are not going to cause any action after they have
   * finished processing.
   */
  virtual void abandon() PURE;
};

using InFlightRequestSharedPtr = std::shared_ptr<InFlightRequest>;

/**
 * Callback to be implemented by entities that are interested when the request has finished and has
 * answer ready.
 */
// Impl note: Filter implements this interface to keep track of requests coming to it.
class AbstractRequestListener {
public:
  virtual ~AbstractRequestListener() = default;

  // Notifies the listener that a new request has been received.
  virtual void onRequest(InFlightRequestSharedPtr request) PURE;

  // Notifies the listener, that the request finally has an answer ready.
  // Usually this means that the request has been sent to upstream Kafka clusters and we got answers
  // (unless it's something that could be responded to locally).
  // IMPL: we do not need to pass request here, as filters need to answer in-order.
  // What means that we always need to check if first answer is ready, even if the latter are
  // already finished.
  virtual void onRequestReadyForAnswer() PURE;

  // Accesses listener's dispatcher.
  // Used by non-Envoy threads that need to communicate with listeners.
  virtual Event::Dispatcher& dispatcher() PURE;
};

/**
 * Helper base class for all in flight requests.
 * Binds request to its origin filter.
 * All the fields can be accessed only by the owning dispatcher thread.
 */
class BaseInFlightRequest : public InFlightRequest, protected Logger::Loggable<Logger::Id::kafka> {
public:
  BaseInFlightRequest(AbstractRequestListener& filter) : filter_{filter} {};
  void abandon() override;

protected:
  /**
   * Notify the originating filter that this request has an answer ready.
   * This method is to be invoked by each request after it has finished processing.
   * Obviously, if the filter is no longer active (connection got closed before we were ready to
   * answer) nothing will happen.
   */
  void notifyFilter();

  // Filter that originated this request.
  AbstractRequestListener& filter_;

  // Whether the filter_ reference is still viable.
  bool filter_active_ = true;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include "envoy/common/time.h"
#include "envoy/event/dispatcher.h"
#include "envoy/network/filter.h"
#include "envoy/stats/scope.h"

#include "source/common/common/logger.h"

#include "contrib/kafka/filters/network/source/external/requests.h"
#include "contrib/kafka/filters/network/source/mesh/abstract_command.h"
#include "contrib/kafka/filters/network/source/mesh/request_processor.h"
#include "contrib/kafka/filters/network/source/mesh/shared_consumer_manager.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_config.h"
#include "contrib/kafka/filters/network/source/mesh/upstream_kafka_facade.h"
#include "contrib/kafka/filters/network/source/request_codec.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

/**
 * Main entry point.
 * Decoded request bytes are passed to processor, that calls us back with enriched request.
 * Request then gets invoked to starts its processing.
 * Filter is going to maintain a list of in-flight-request so it can send responses when they
 * finish.
 *
 * See command_handlers.md for particular request interactions.
 **/
class KafkaMeshFilter : public Network::ReadFilter,
                        public Network::ConnectionCallbacks,
                        public AbstractRequestListener,
                        private Logger::Loggable<Logger::Id::kafka> {
public:
  // Main constructor.
  KafkaMeshFilter(const UpstreamKafkaConfiguration& configuration,
                  UpstreamKafkaFacade& upstream_kafka_facade,
                  RecordCallbackProcessor& record_callback_processor);

  // Visible for testing.
  KafkaMeshFilter(RequestDecoderSharedPtr request_decoder);

  // Non-trivial. See 'abandonAllInFlightRequests'.
  ~KafkaMeshFilter() override;

  // Network::ReadFilter
  Network::FilterStatus onNewConnection() override;
  void initializeReadFilterCallbacks(Network::ReadFilterCallbacks& callbacks) override;
  Network::FilterStatus onData(Buffer::Instance& data, bool end_stream) override;

  // Network::ConnectionCallbacks
  void onEvent(Network::ConnectionEvent event) override;
  void onAboveWriteBufferHighWatermark() override;
  void onBelowWriteBufferLowWatermark() override;

  // AbstractRequestListener
  void onRequest(InFlightRequestSharedPtr request) override;
  void onRequestReadyForAnswer() override;
  Event::Dispatcher& dispatcher() override;

  std::list<InFlightRequestSharedPtr>& getRequestsInFlightForTest();

private:
  // Helper method invoked when connection gets dropped.
  // Because filter can be destroyed before confirmations from Kafka are received, we are just going
  // to mark related requests as abandoned, so they do not attempt to reference this filter anymore.
  // Impl note: this is similar to what Redis filter does.
  void abandonAllInFlightRequests();

  const RequestDecoderSharedPtr request_decoder_;

  Network::ReadFilterCallbacks* read_filter_callbacks_;

  std::list<InFlightRequestSharedPtr> requests_in_flight_;
};

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#include "contrib/kafka/filters/network/source/mesh/request_processor.h"

#include "envoy/common/exception.h"

#include "contrib/kafka/filters/network/source/mesh/command_handlers/api_versions.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/fetch.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/list_offsets.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/metadata.h"
#include "contrib/kafka/filters/network/source/mesh/command_handlers/produce.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

RequestProcessor::RequestProcessor(AbstractRequestListener& origin,
                                   const UpstreamKafkaConfiguration& configuration,
                                   UpstreamKafkaFacade& upstream_kafka_facade,
                                   RecordCallbackProcessor& record_callback_processor)
    : origin_{origin}, configuration_{configuration}, upstream_kafka_facade_{upstream_kafka_facade},
      record_callback_processor_{record_callback_processor} {}

// Helper function. Throws a nice message. Filter will react by closing the connection.
static void throwOnUnsupportedRequest(const std::string& reason, const RequestHeader& header) {
  throw EnvoyException(absl::StrCat(reason, " Kafka request (key=", header.api_key_, ", version=",
                                    header.api_version_, ", cid=", header.correlation_id_));
}

void RequestProcessor::onMessage(AbstractRequestSharedPtr arg) {
  switch (arg->request_header_.api_key_) {
  case PRODUCE_REQUEST_API_KEY:
    process(std::dynamic_pointer_cast<Request<ProduceRequest>>(arg));
    break;
  case FETCH_REQUEST_API_KEY:
    process(std::dynamic_pointer_cast<Request<FetchRequest>>(arg));
    break;
  case LIST_OFFSETS_REQUEST_API_KEY:
    process(std::dynamic_pointer_cast<Request<ListOffsetsRequest>>(arg));
    break;
  case METADATA_REQUEST_API_KEY:
    process(std::dynamic_pointer_cast<Request<MetadataRequest>>(arg));
    break;
  case API_VERSIONS_REQUEST_API_KEY:
    process(std::dynamic_pointer_cast<Request<ApiVersionsRequest>>(arg));
    break;
  default:
    // Client sent a request we cannot handle right now.
    throwOnUnsupportedRequest("unsupported (bad client API invoked?)", arg->request_header_);
    break;
  } // switch
}

void RequestProcessor::process(const std::shared_ptr<Request<ProduceRequest>> request) const {
  auto res = std::make_shared<ProduceRequestHolder>(origin_, upstream_kafka_facade_, request);
  origin_.onRequest(res);
}

void RequestProcessor::process(const std::shared_ptr<Request<FetchRequest>> request) const {
  auto res = std::make_shared<FetchRequestHolder>(origin_, record_callback_processor_, request);
  origin_.onRequest(res);
}

void RequestProcessor::process(const std::shared_ptr<Request<ListOffsetsRequest>> request) const {
  auto res = std::make_shared<ListOffsetsRequestHolder>(origin_, request);
  origin_.onRequest(res);
}

void RequestProcessor::process(const std::shared_ptr<Request<MetadataRequest>> request) const {
  auto res = std::make_shared<MetadataRequestHolder>(origin_, configuration_, request);
  origin_.onRequest(res);
}

void RequestProcessor::process(const std::shared_ptr<Request<ApiVersionsRequest>> request) const {
  auto res = std::make_shared<ApiVersionsRequestHolder>(origin_, request->request_header_);
  origin_.onRequest(res);
}

// We got something that the parser could not handle.
void RequestProcessor::onFailedParse(RequestParseFailureSharedPtr arg) {
  throwOnUnsupportedRequest("unknown", arg->request_header_);
}

} // namespace Mesh
} // namespace Kafka
} // namespace NetworkFilters
} // namespace Extensions
} // namespace Envoy
#pragma once

#include <map>
#include <memory>
#include <string>
#include <utility>

#include "envoy/common/pure.h"

#include "absl/strings/string_view.h"
#include "librdkafka/rdkafkacpp.h"

namespace Envoy {
namespace Extensions {
namespace NetworkFilters {
namespace Kafka {
namespace Mesh {

// Used by librdkafka API.
using RdKafkaMessageRawPtr = RdKafka::Message*;

using RdKafkaMessagePtr = std::unique_ptr<RdKafka::Message>;

/**
 * Helper class to wrap librdkafka consumer partition assignment.
 * This object has to live longer than whatever consumer that uses its "raw" data.
 * On its own it does not expose any public API, as it is not intended to be interacted with.
 */
class ConsumerAssignment {
public:
  virtual ~ConsumerAssignment() = default;
};

using ConsumerAssignmentConstPtr = std::unique_ptr<const ConsumerAssignment>;

/**
 * Helper class responsible for creating librdkafka entities, so we can have mocks in tests.
 */
class LibRdKafkaUtils {
public:
  virtual ~LibRdKafkaUtils() = default;

  virtual RdKafka::Conf::ConfResult setConfProperty(RdKafka::Conf& conf, const std::string& name,
                                                    const std::string& value,
                                                    std::string& errstr) const PURE;

  virtual RdKafka::Conf::ConfResult setConfDeliveryCallback(RdKafka::Conf& conf,
                                                            RdKafka::DeliveryReportCb* dr_cb,
                                                            std::string& errstr) const PURE;

  virtual std::unique_ptr<RdKafka::Producer> createProducer(RdKafka::Conf* conf,
                                                            std::string& errstr) const PURE;

  virtual std::unique_ptr<RdKafka::KafkaConsumer> createConsumer(RdKafka::Conf* conf,
                                                                 std::string& errstr) const PURE;

  // Returned type is a raw pointer, as librdkafka does the deletion on successful produce call.
  virtual RdKafka::Headers* convertHeaders(
      const std::vector<std::pair<absl::string_view, absl::string_view>>& headers) const PURE;

  // In case of produce failures, we need to dispose of headers manually.
  virtual void deleteHeaders(RdKafka::Headers* librdkafka_headers) const PURE;

  // Assigns partitions to a consumer.
  // Impl: this metho