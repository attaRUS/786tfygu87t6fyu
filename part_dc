6 [(validate.rules).uint32 = {lte: 100}];

  // The number of hosts in a cluster that must have enough request volume to
  // detect success rate outliers. If the number of hosts is less than this
  // setting, outlier detection via success rate statistics is not performed
  // for any host in the cluster. Defaults to 5.
  google.protobuf.UInt32Value success_rate_minimum_hosts = 7;

  // The minimum number of total requests that must be collected in one
  // interval (as defined by the interval duration above) to include this host
  // in success rate based outlier detection. If the volume is lower than this
  // setting, outlier detection via success rate statistics is not performed
  // for that host. Defaults to 100.
  google.protobuf.UInt32Value success_rate_request_volume = 8;

  // This factor is used to determine the ejection threshold for success rate
  // outlier ejection. The ejection threshold is the difference between the
  // mean success rate, and the product of this factor and the standard
  // deviation of the mean success rate: mean - (stdev *
  // success_rate_stdev_factor). This factor is divided by a thousand to get a
  // double. That is, if the desired factor is 1.9, the runtime value should
  // be 1900. Defaults to 1900.
  google.protobuf.UInt32Value success_rate_stdev_factor = 9;

  // The number of consecutive gateway failures (502, 503, 504 status codes)
  // before a consecutive gateway failure ejection occurs. Defaults to 5.
  google.protobuf.UInt32Value consecutive_gateway_failure = 10;

  // The % chance that a host will be actually ejected when an outlier status
  // is detected through consecutive gateway failures. This setting can be
  // used to disable ejection or to ramp it up slowly. Defaults to 0.
  google.protobuf.UInt32Value enforcing_consecutive_gateway_failure = 11
      [(validate.rules).uint32 = {lte: 100}];

  // Determines whether to distinguish local origin failures from external errors. If set to true
  // the following configuration parameters are taken into account:
  // :ref:`consecutive_local_origin_failure<envoy_api_field_cluster.OutlierDetection.consecutive_local_origin_failure>`,
  // :ref:`enforcing_consecutive_local_origin_failure<envoy_api_field_cluster.OutlierDetection.enforcing_consecutive_local_origin_failure>`
  // and
  // :ref:`enforcing_local_origin_success_rate<envoy_api_field_cluster.OutlierDetection.enforcing_local_origin_success_rate>`.
  // Defaults to false.
  bool split_external_local_origin_errors = 12;

  // The number of consecutive locally originated failures before ejection
  // occurs. Defaults to 5. Parameter takes effect only when
  // :ref:`split_external_local_origin_errors<envoy_api_field_cluster.OutlierDetection.split_external_local_origin_errors>`
  // is set to true.
  google.protobuf.UInt32Value consecutive_local_origin_failure = 13;

  // The % chance that a host will be actually ejected when an outlier status
  // is detected through consecutive locally originated failures. This setting can be
  // used to disable ejection or to ramp it up slowly. Defaults to 100.
  // Parameter takes effect only when
  // :ref:`split_external_local_origin_errors<envoy_api_field_cluster.OutlierDetection.split_external_local_origin_errors>`
  // is set to true.
  google.protobuf.UInt32Value enforcing_consecutive_local_origin_failure = 14
      [(validate.rules).uint32 = {lte: 100}];

  // The % chance that a host will be actually ejected when an outlier status
  // is detected through success rate statistics for locally originated errors.
  // This setting can be used to disable ejection or to ramp it up slowly. Defaults to 100.
  // Parameter takes effect only when
  // :ref:`split_external_local_origin_errors<envoy_api_field_cluster.OutlierDetection.split_external_local_origin_errors>`
  // is set to true.
  google.protobuf.UInt32Value enforcing_local_origin_success_rate = 15
      [(validate.rules).uint32 = {lte: 100}];

  // The failure percentage to use when determining failure percentage-based outlier detection. If
  // the failure percentage of a given host is greater than or equal to this value, it will be
  // ejected. Defaults to 85.
  google.protobuf.UInt32Value failure_percentage_threshold = 16
      [(validate.rules).uint32 = {lte: 100}];

  // The % chance that a host will be actually ejected when an outlier status is detected through
  // failure percentage statistics. This setting can be used to disable ejection or to ramp it up
  // slowly. Defaults to 0.
  //
  // [#next-major-version: setting this without setting failure_percentage_threshold should be
  // invalid in v4.]
  google.protobuf.UInt32Value enforcing_failure_percentage = 17
      [(validate.rules).uint32 = {lte: 100}];

  // The % chance that a host will be actually ejected when an outlier status is detected through
  // local-origin failure percentage statistics. This setting can be used to disable ejection or to
  // ramp it up slowly. Defaults to 0.
  google.protobuf.UInt32Value enforcing_failure_percentage_local_origin = 18
      [(validate.rules).uint32 = {lte: 100}];

  // The minimum number of hosts in a cluster in order to perform failure percentage-based ejection.
  // If the total number of hosts in the cluster is less than this value, failure percentage-based
  // ejection will not be performed. Defaults to 5.
  google.protobuf.UInt32Value failure_percentage_minimum_hosts = 19;

  // The minimum number of total requests that must be collected in one interval (as defined by the
  // interval duration above) to perform failure percentage-based ejection for this host. If the
  // volume is lower than this setting, failure percentage-based ejection will not be performed for
  // this host. Defaults to 50.
  google.protobuf.UInt32Value failure_percentage_request_volume = 20;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/api/v2/core:pkg",
        "//envoy/type:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.api.v2.cluster;

import "envoy/api/v2/core/base.proto";
import "envoy/type/percent.proto";

import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.cluster";
option java_outer_classname = "CircuitBreakerProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/cluster";
option csharp_namespace = "Envoy.Api.V2.ClusterNS";
option ruby_package = "Envoy::Api::V2::ClusterNS";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.cluster.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Circuit breakers]

// :ref:`Circuit breaking<arch_overview_circuit_break>` settings can be
// specified individually for each defined priority.
message CircuitBreakers {
  // A Thresholds defines CircuitBreaker settings for a
  // :ref:`RoutingPriority<envoy_api_enum_core.RoutingPriority>`.
  // [#next-free-field: 9]
  message Thresholds {
    message RetryBudget {
      // Specifies the limit on concurrent retries as a percentage of the sum of active requests and
      // active pending requests. For example, if there are 100 active requests and the
      // budget_percent is set to 25, there may be 25 active retries.
      //
      // This parameter is optional. Defaults to 20%.
      type.Percent budget_percent = 1;

      // Specifies the minimum retry concurrency allowed for the retry budget. The limit on the
      // number of active retries may never go below this number.
      //
      // This parameter is optional. Defaults to 3.
      google.protobuf.UInt32Value min_retry_concurrency = 2;
    }

    // The :ref:`RoutingPriority<envoy_api_enum_core.RoutingPriority>`
    // the specified CircuitBreaker settings apply to.
    core.RoutingPriority priority = 1 [(validate.rules).enum = {defined_only: true}];

    // The maximum number of connections that Envoy will make to the upstream
    // cluster. If not specified, the default is 1024.
    google.protobuf.UInt32Value max_connections = 2;

    // The maximum number of pending requests that Envoy will allow to the
    // upstream cluster. If not specified, the default is 1024.
    google.protobuf.UInt32Value max_pending_requests = 3;

    // The maximum number of parallel requests that Envoy will make to the
    // upstream cluster. If not specified, the default is 1024.
    google.protobuf.UInt32Value max_requests = 4;

    // The maximum number of parallel retries that Envoy will allow to the
    // upstream cluster. If not specified, the default is 3.
    google.protobuf.UInt32Value max_retries = 5;

    // Specifies a limit on concurrent retries in relation to the number of active requests. This
    // parameter is optional.
    //
    // .. note::
    //
    //    If this field is set, the retry budget will override any configured retry circuit
    //    breaker.
    RetryBudget retry_budget = 8;

    // If track_remaining is true, then stats will be published that expose
    // the number of resources remaining until the circuit breakers open. If
    // not specified, the default is false.
    //
    // .. note::
    //
    //    If a retry budget is used in lieu of the max_retries circuit breaker,
    //    the remaining retry resources remaining will not be tracked.
    bool track_remaining = 6;

    // The maximum number of connection pools per cluster that Envoy will concurrently support at
    // once. If not specified, the default is unlimited. Set this for clusters which create a
    // large number of connection pools. See
    // :ref:`Circuit Breaking <arch_overview_circuit_break_cluster_maximum_connection_pools>` for
    // more details.
    google.protobuf.UInt32Value max_connection_pools = 7;
  }

  // If multiple :ref:`Thresholds<envoy_api_msg_cluster.CircuitBreakers.Thresholds>`
  // are defined with the same :ref:`RoutingPriority<envoy_api_enum_core.RoutingPriority>`,
  // the first one in the list is used. If no Thresholds is defined for a given
  // :ref:`RoutingPriority<envoy_api_enum_core.RoutingPriority>`, the default values
  // are used.
  repeated Thresholds thresholds = 1;
}
syntax = "proto3";

package envoy.api.v2.listener;

import "google/protobuf/duration.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.api.v2.listener";
option java_outer_classname = "QuicConfigProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/listener";
option csharp_namespace = "Envoy.Api.V2.ListenerNS";
option ruby_package = "Envoy::Api::V2::ListenerNS";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.listener.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: QUIC listener Config]

// Configuration specific to the QUIC protocol.
// Next id: 4
message QuicProtocolOptions {
  // Maximum number of streams that the client can negotiate per connection. 100
  // if not specified.
  google.protobuf.UInt32Value max_concurrent_streams = 1;

  // Maximum number of milliseconds that connection will be alive when there is
  // no network activity. 300000ms if not specified.
  google.protobuf.Duration idle_timeout = 2;

  // Connection timeout in milliseconds before the crypto handshake is finished.
  // 20000ms if not specified.
  google.protobuf.Duration crypto_handshake_timeout = 3;
}
syntax = "proto3";

package envoy.api.v2.listener;

import public "envoy/api/v2/listener/listener_components.proto";

option java_package = "io.envoyproxy.envoy.api.v2.listener";
option java_outer_classname = "ListenerProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/listener";
option csharp_namespace = "Envoy.Api.V2.ListenerNS";
option ruby_package = "Envoy::Api::V2::ListenerNS";
syntax = "proto3";

package envoy.api.v2.listener;

import "envoy/api/v2/auth/tls.proto";
import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/base.proto";
import "envoy/type/range.proto";

import "google/protobuf/any.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.listener";
option java_outer_classname = "ListenerComponentsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/listener";
option csharp_namespace = "Envoy.Api.V2.ListenerNS";
option ruby_package = "Envoy::Api::V2::ListenerNS";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.listener.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Listener components]
// Listener :ref:`configuration overview <config_listeners>`

message Filter {
  reserved 3;

  // The name of the filter to instantiate. The name must match a
  // :ref:`supported filter <config_network_filters>`.
  string name = 1 [(validate.rules).string = {min_bytes: 1}];

  // Filter specific configuration which depends on the filter being
  // instantiated. See the supported filters for further documentation.
  oneof config_type {
    google.protobuf.Struct config = 2 [deprecated = true];

    google.protobuf.Any typed_config = 4;
  }
}

// Specifies the match criteria for selecting a specific filter chain for a
// listener.
//
// In order for a filter chain to be selected, *ALL* of its criteria must be
// fulfilled by the incoming connection, properties of which are set by the
// networking stack and/or listener filters.
//
// The following order applies:
//
// 1. Destination port.
// 2. Destination IP address.
// 3. Server name (e.g. SNI for TLS protocol),
// 4. Transport protocol.
// 5. Application protocols (e.g. ALPN for TLS protocol).
// 6. Source type (e.g. any, local or external network).
// 7. Source IP address.
// 8. Source port.
//
// For criteria that allow ranges or wildcards, the most specific value in any
// of the configured filter chains that matches the incoming connection is going
// to be used (e.g. for SNI ``www.example.com`` the most specific match would be
// ``www.example.com``, then ``*.example.com``, then ``*.com``, then any filter
// chain without ``server_names`` requirements).
//
// [#comment: Implemented rules are kept in the preference order, with deprecated fields
// listed at the end, because that's how we want to list them in the docs.
//
// [#comment:TODO(PiotrSikora): Add support for configurable precedence of the rules]
// [#next-free-field: 13]
message FilterChainMatch {
  enum ConnectionSourceType {
    // Any connection source matches.
    ANY = 0;

    // Match a connection originating from the same host.
    LOCAL = 1 [(udpa.annotations.enum_value_migrate).rename = "SAME_IP_OR_LOOPBACK"];

    // Match a connection originating from a different host.
    EXTERNAL = 2;
  }

  reserved 1;

  // Optional destination port to consider when use_original_dst is set on the
  // listener in determining a filter chain match.
  google.protobuf.UInt32Value destination_port = 8 [(validate.rules).uint32 = {lte: 65535 gte: 1}];

  // If non-empty, an IP address and prefix length to match addresses when the
  // listener is bound to 0.0.0.0/:: or when use_original_dst is specified.
  repeated core.CidrRange prefix_ranges = 3;

  // If non-empty, an IP address and suffix length to match addresses when the
  // listener is bound to 0.0.0.0/:: or when use_original_dst is specified.
  // [#not-implemented-hide:]
  string address_suffix = 4;

  // [#not-implemented-hide:]
  google.protobuf.UInt32Value suffix_len = 5;

  // Specifies the connection source IP match type. Can be any, local or external network.
  ConnectionSourceType source_type = 12 [(validate.rules).enum = {defined_only: true}];

  // The criteria is satisfied if the source IP address of the downstream
  // connection is contained in at least one of the specified subnets. If the
  // parameter is not specified or the list is empty, the source IP address is
  // ignored.
  repeated core.CidrRange source_prefix_ranges = 6;

  // The criteria is satisfied if the source port of the downstream connection
  // is contained in at least one of the specified ports. If the parameter is
  // not specified, the source port is ignored.
  repeated uint32 source_ports = 7
      [(validate.rules).repeated = {items {uint32 {lte: 65535 gte: 1}}}];

  // If non-empty, a list of server names (e.g. SNI for TLS protocol) to consider when determining
  // a filter chain match. Those values will be compared against the server names of a new
  // connection, when detected by one of the listener filters.
  //
  // The server name will be matched against all wildcard domains, i.e. ``www.example.com``
  // will be first matched against ``www.example.com``, then ``*.example.com``, then ``*.com``.
  //
  // Note that partial wildcards are not supported, and values like ``*w.example.com`` are invalid.
  //
  // .. attention::
  //
  //   See the :ref:`FAQ entry <faq_how_to_setup_sni>` on how to configure SNI for more
  //   information.
  repeated string server_names = 11;

  // If non-empty, a transport protocol to consider when determining a filter chain match.
  // This value will be compared against the transport protocol of a new connection, when
  // it's detected by one of the listener filters.
  //
  // Suggested values include:
  //
  // * ``raw_buffer`` - default, used when no transport protocol is detected,
  // * ``tls`` - set by :ref:`envoy.filters.listener.tls_inspector <config_listener_filters_tls_inspector>`
  //   when TLS protocol is detected.
  string transport_protocol = 9;

  // If non-empty, a list of application protocols (e.g. ALPN for TLS protocol) to consider when
  // determining a filter chain match. Those values will be compared against the application
  // protocols of a new connection, when detected by one of the listener filters.
  //
  // Suggested values include:
  //
  // * ``http/1.1`` - set by :ref:`envoy.filters.listener.tls_inspector
  //   <config_listener_filters_tls_inspector>`,
  // * ``h2`` - set by :ref:`envoy.filters.listener.tls_inspector <config_listener_filters_tls_inspector>`
  //
  // .. attention::
  //
  //   Currently, only :ref:`TLS Inspector <config_listener_filters_tls_inspector>` provides
  //   application protocol detection based on the requested
  //   `ALPN <https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation>`_ values.
  //
  //   However, the use of ALPN is pretty much limited to the HTTP/2 traffic on the Internet,
  //   and matching on values other than ``h2`` is going to lead to a lot of false negatives,
  //   unless all connecting clients are known to use ALPN.
  repeated string application_protocols = 10;
}

// A filter chain wraps a set of match criteria, an option TLS context, a set of filters, and
// various other parameters.
// [#next-free-field: 8]
message FilterChain {
  // The criteria to use when matching a connection to this filter chain.
  FilterChainMatch filter_chain_match = 1;

  // The TLS context for this filter chain.
  //
  // .. attention::
  //
  //   **This field is deprecated**. Use `transport_socket` with name `tls` instead. If both are
  //   set, `transport_socket` takes priority.
  auth.DownstreamTlsContext tls_context = 2 [deprecated = true];

  // A list of individual network filters that make up the filter chain for
  // connections established with the listener. Order matters as the filters are
  // processed sequentially as connection events happen. Note: If the filter
  // list is empty, the connection will close by default.
  repeated Filter filters = 3;

  // Whether the listener should expect a PROXY protocol V1 header on new
  // connections. If this option is enabled, the listener will assume that that
  // remote address of the connection is the one specified in the header. Some
  // load balancers including the AWS ELB support this option. If the option is
  // absent or set to false, Envoy will use the physical peer address of the
  // connection as the remote address.
  google.protobuf.BoolValue use_proxy_proto = 4;

  // [#not-implemented-hide:] filter chain metadata.
  core.Metadata metadata = 5;

  // Optional custom transport socket implementation to use for downstream connections.
  // To setup TLS, set a transport socket with name `tls` and
  // :ref:`DownstreamTlsContext <envoy_api_msg_auth.DownstreamTlsContext>` in the `typed_config`.
  // If no transport socket configuration is specified, new connections
  // will be set up with plaintext.
  core.TransportSocket transport_socket = 6;

  // [#not-implemented-hide:] The unique name (or empty) by which this filter chain is known. If no
  // name is provided, Envoy will allocate an internal UUID for the filter chain. If the filter
  // chain is to be dynamically updated or removed via FCDS a unique name must be provided.
  string name = 7;
}

// Listener filter chain match configuration. This is a recursive structure which allows complex
// nested match configurations to be built using various logical operators.
//
// Examples:
//
// * Matches if the destination port is 3306.
//
// .. code-block:: yaml
//
//  destination_port_range:
//   start: 3306
//   end: 3307
//
// * Matches if the destination port is 3306 or 15000.
//
// .. code-block:: yaml
//
//  or_match:
//    rules:
//      - destination_port_range:
//          start: 3306
//          end: 3307
//      - destination_port_range:
//          start: 15000
//          end: 15001
//
// [#next-free-field: 6]
message ListenerFilterChainMatchPredicate {
  // A set of match configurations used for logical operations.
  message MatchSet {
    // The list of rules that make up the set.
    repeated ListenerFilterChainMatchPredicate rules = 1
        [(validate.rules).repeated = {min_items: 2}];
  }

  oneof rule {
    option (validate.required) = true;

    // A set that describes a logical OR. If any member of the set matches, the match configuration
    // matches.
    MatchSet or_match = 1;

    // A set that describes a logical AND. If all members of the set match, the match configuration
    // matches.
    MatchSet and_match = 2;

    // A negation match. The match configuration will match if the negated match condition matches.
    ListenerFilterChainMatchPredicate not_match = 3;

    // The match configuration will always match.
    bool any_match = 4 [(validate.rules).bool = {const: true}];

    // Match destination port. Particularly, the match evaluation must use the recovered local port if
    // the owning listener filter is after :ref:`an original_dst listener filter <config_listener_filters_original_dst>`.
    type.Int32Range destination_port_range = 5;
  }
}

message ListenerFilter {
  // The name of the filter to instantiate. The name must match a
  // :ref:`supported filter <config_listener_filters>`.
  string name = 1 [(validate.rules).string = {min_bytes: 1}];

  // Filter specific configuration which depends on the filter being instantiated.
  // See the supported filters for further documentation.
  oneof config_type {
    google.protobuf.Struct config = 2 [deprecated = true];

    google.protobuf.Any typed_config = 3;
  }

  // Optional match predicate used to disable the filter. The filter is enabled when this field is empty.
  // See :ref:`ListenerFilterChainMatchPredicate <envoy_api_msg_listener.ListenerFilterChainMatchPredicate>`
  // for further examples.
  ListenerFilterChainMatchPredicate filter_disabled = 4;
}
syntax = "proto3";

package envoy.api.v2.listener;

import "google/protobuf/any.proto";
import "google/protobuf/struct.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.api.v2.listener";
option java_outer_classname = "UdpListenerConfigProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/listener";
option csharp_namespace = "Envoy.Api.V2.ListenerNS";
option ruby_package = "Envoy::Api::V2::ListenerNS";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.listener.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: UDP Listener Config]
// Listener :ref:`configuration overview <config_listeners>`

message UdpListenerConfig {
  // Used to look up UDP listener factory, matches "raw_udp_listener" or
  // "quic_listener" to create a specific udp listener.
  // If not specified, treat as "raw_udp_listener".
  string udp_listener_name = 1;

  // Used to create a specific listener factory. To some factory, e.g.
  // "raw_udp_listener", config is not needed.
  oneof config_type {
    google.protobuf.Struct config = 2 [deprecated = true];

    google.protobuf.Any typed_config = 3;
  }
}

message ActiveRawUdpListenerConfig {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/api/v2/auth:pkg",
        "//envoy/api/v2/core:pkg",
        "//envoy/type:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.api.v2.core;

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "SocketOptionProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Socket Option ]

// Generic socket option message. This would be used to set socket options that
// might not exist in upstream kernels or precompiled Envoy binaries.
// [#next-free-field: 7]
message SocketOption {
  enum SocketState {
    // Socket options are applied after socket creation but before binding the socket to a port
    STATE_PREBIND = 0;

    // Socket options are applied after binding the socket to a port but before calling listen()
    STATE_BOUND = 1;

    // Socket options are applied after calling listen()
    STATE_LISTENING = 2;
  }

  // An optional name to give this socket option for debugging, etc.
  // Uniqueness is not required and no special meaning is assumed.
  string description = 1;

  // Corresponding to the level value passed to setsockopt, such as IPPROTO_TCP
  int64 level = 2;

  // The numeric name as passed to setsockopt
  int64 name = 3;

  oneof value {
    option (validate.required) = true;

    // Because many sockopts take an int value.
    int64 int_value = 4;

    // Otherwise it's a byte buffer.
    bytes buf_value = 5;
  }

  // The state in which the option will be applied. When used in BindConfig
  // STATE_PREBIND is currently the only valid value.
  SocketState state = 6 [(validate.rules).enum = {defined_only: true}];
}
syntax = "proto3";

package envoy.api.v2.core;

import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/event_service_config.proto";
import "envoy/type/http.proto";
import "envoy/type/matcher/string.proto";
import "envoy/type/range.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/wrappers.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "HealthCheckProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Health check]
// * Health checking :ref:`architecture overview <arch_overview_health_checking>`.
// * If health checking is configured for a cluster, additional statistics are emitted. They are
//   documented :ref:`here <config_cluster_manager_cluster_stats>`.

// Endpoint health status.
enum HealthStatus {
  // The health status is not known. This is interpreted by Envoy as *HEALTHY*.
  UNKNOWN = 0;

  // Healthy.
  HEALTHY = 1;

  // Unhealthy.
  UNHEALTHY = 2;

  // Connection draining in progress. E.g.,
  // `<https://aws.amazon.com/blogs/aws/elb-connection-draining-remove-instances-from-service-with-care/>`_
  // or
  // `<https://cloud.google.com/compute/docs/load-balancing/enabling-connection-draining>`_.
  // This is interpreted by Envoy as *UNHEALTHY*.
  DRAINING = 3;

  // Health check timed out. This is part of HDS and is interpreted by Envoy as
  // *UNHEALTHY*.
  TIMEOUT = 4;

  // Degraded.
  DEGRADED = 5;
}

// [#next-free-field: 23]
message HealthCheck {
  // Describes the encoding of the payload bytes in the payload.
  message Payload {
    oneof payload {
      option (validate.required) = true;

      // Hex encoded payload. E.g., "000000FF".
      string text = 1 [(validate.rules).string = {min_bytes: 1}];

      // [#not-implemented-hide:] Binary payload.
      bytes binary = 2;
    }
  }

  // [#next-free-field: 12]
  message HttpHealthCheck {
    // The value of the host header in the HTTP health check request. If
    // left empty (default value), the name of the cluster this health check is associated
    // with will be used. The host header can be customized for a specific endpoint by setting the
    // :ref:`hostname <envoy_api_field_endpoint.Endpoint.HealthCheckConfig.hostname>` field.
    string host = 1;

    // Specifies the HTTP path that will be requested during health checking. For example
    // */healthcheck*.
    string path = 2 [(validate.rules).string = {min_bytes: 1}];

    // [#not-implemented-hide:] HTTP specific payload.
    Payload send = 3;

    // [#not-implemented-hide:] HTTP specific response.
    Payload receive = 4;

    // An optional service name parameter which is used to validate the identity of
    // the health checked cluster. See the :ref:`architecture overview
    // <arch_overview_health_checking_identity>` for more information.
    //
    // .. attention::
    //
    //   This field has been deprecated in favor of `service_name_matcher` for better flexibility
    //   over matching with service-cluster name.
    string service_name = 5 [deprecated = true];

    // Specifies a list of HTTP headers that should be added to each request that is sent to the
    // health checked cluster. For more information, including details on header value syntax, see
    // the documentation on :ref:`custom request headers
    // <config_http_conn_man_headers_custom_request_headers>`.
    repeated HeaderValueOption request_headers_to_add = 6
        [(validate.rules).repeated = {max_items: 1000}];

    // Specifies a list of HTTP headers that should be removed from each request that is sent to the
    // health checked cluster.
    repeated string request_headers_to_remove = 8;

    // If set, health checks will be made using http/2.
    // Deprecated, use :ref:`codec_client_type
    // <envoy_api_field_core.HealthCheck.HttpHealthCheck.codec_client_type>` instead.
    bool use_http2 = 7 [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

    // Specifies a list of HTTP response statuses considered healthy. If provided, replaces default
    // 200-only policy - 200 must be included explicitly as needed. Ranges follow half-open
    // semantics of :ref:`Int64Range <envoy_api_msg_type.Int64Range>`. The start and end of each
    // range are required. Only statuses in the range [100, 600) are allowed.
    repeated type.Int64Range expected_statuses = 9;

    // Use specified application protocol for health checks.
    type.CodecClientType codec_client_type = 10 [(validate.rules).enum = {defined_only: true}];

    // An optional service name parameter which is used to validate the identity of
    // the health checked cluster using a :ref:`StringMatcher
    // <envoy_api_msg_type.matcher.StringMatcher>`. See the :ref:`architecture overview
    // <arch_overview_health_checking_identity>` for more information.
    type.matcher.StringMatcher service_name_matcher = 11;
  }

  message TcpHealthCheck {
    // Empty payloads imply a connect-only health check.
    Payload send = 1;

    // When checking the response, “fuzzy” matching is performed such that each
    // binary block must be found, and in the order specified, but not
    // necessarily contiguous.
    repeated Payload receive = 2;
  }

  message RedisHealthCheck {
    // If set, optionally perform ``EXISTS <key>`` instead of ``PING``. A return value
    // from Redis of 0 (does not exist) is considered a passing healthcheck. A return value other
    // than 0 is considered a failure. This allows the user to mark a Redis instance for maintenance
    // by setting the specified key to any value and waiting for traffic to drain.
    string key = 1;
  }

  // `grpc.health.v1.Health
  // <https://github.com/grpc/grpc/blob/master/src/proto/grpc/health/v1/health.proto>`_-based
  // healthcheck. See `gRPC doc <https://github.com/grpc/grpc/blob/master/doc/health-checking.md>`_
  // for details.
  message GrpcHealthCheck {
    // An optional service name parameter which will be sent to gRPC service in
    // `grpc.health.v1.HealthCheckRequest
    // <https://github.com/grpc/grpc/blob/master/src/proto/grpc/health/v1/health.proto#L20>`_.
    // message. See `gRPC health-checking overview
    // <https://github.com/grpc/grpc/blob/master/doc/health-checking.md>`_ for more information.
    string service_name = 1;

    // The value of the :authority header in the gRPC health check request. If
    // left empty (default value), the name of the cluster this health check is associated
    // with will be used. The authority header can be customized for a specific endpoint by setting
    // the :ref:`hostname <envoy_api_field_endpoint.Endpoint.HealthCheckConfig.hostname>` field.
    string authority = 2;
  }

  // Custom health check.
  message CustomHealthCheck {
    // The registered name of the custom health checker.
    string name = 1 [(validate.rules).string = {min_bytes: 1}];

    // A custom health checker specific configuration which depends on the custom health checker
    // being instantiated. See :api:`envoy/config/health_checker` for reference.
    oneof config_type {
      google.protobuf.Struct config = 2 [deprecated = true];

      google.protobuf.Any typed_config = 3;
    }
  }

  // Health checks occur over the transport socket specified for the cluster. This implies that if a
  // cluster is using a TLS-enabled transport socket, the health check will also occur over TLS.
  //
  // This allows overriding the cluster TLS settings, just for health check connections.
  message TlsOptions {
    // Specifies the ALPN protocols for health check connections. This is useful if the
    // corresponding upstream is using ALPN-based :ref:`FilterChainMatch
    // <envoy_api_msg_listener.FilterChainMatch>` along with different protocols for health checks
    // versus data connections. If empty, no ALPN protocols will be set on health check connections.
    repeated string alpn_protocols = 1;
  }

  reserved 10;

  // The time to wait for a health check response. If the timeout is reached the
  // health check attempt will be considered a failure.
  google.protobuf.Duration timeout = 1 [(validate.rules).duration = {
    required: true
    gt {}
  }];

  // The interval between health checks.
  google.protobuf.Duration interval = 2 [(validate.rules).duration = {
    required: true
    gt {}
  }];

  // An optional jitter amount in milliseconds. If specified, Envoy will start health
  // checking after for a random time in ms between 0 and initial_jitter. This only
  // applies to the first health check.
  google.protobuf.Duration initial_jitter = 20;

  // An optional jitter amount in milliseconds. If specified, during every
  // interval Envoy will add interval_jitter to the wait time.
  google.protobuf.Duration interval_jitter = 3;

  // An optional jitter amount as a percentage of interval_ms. If specified,
  // during every interval Envoy will add interval_ms *
  // interval_jitter_percent / 100 to the wait time.
  //
  // If interval_jitter_ms and interval_jitter_percent are both set, both of
  // them will be used to increase the wait time.
  uint32 interval_jitter_percent = 18;

  // The number of unhealthy health checks required before a host is marked
  // unhealthy. Note that for *http* health checking if a host responds with 503
  // this threshold is ignored and the host is considered unhealthy immediately.
  google.protobuf.UInt32Value unhealthy_threshold = 4 [(validate.rules).message = {required: true}];

  // The number of healthy health checks required before a host is marked
  // healthy. Note that during startup, only a single successful health check is
  // required to mark a host healthy.
  google.protobuf.UInt32Value healthy_threshold = 5 [(validate.rules).message = {required: true}];

  // [#not-implemented-hide:] Non-serving port for health checking.
  google.protobuf.UInt32Value alt_port = 6;

  // Reuse health check connection between health checks. Default is true.
  google.protobuf.BoolValue reuse_connection = 7;

  oneof health_checker {
    option (validate.required) = true;

    // HTTP health check.
    HttpHealthCheck http_health_check = 8;

    // TCP health check.
    TcpHealthCheck tcp_health_check = 9;

    // gRPC health check.
    GrpcHealthCheck grpc_health_check = 11;

    // Custom health check.
    CustomHealthCheck custom_health_check = 13;
  }

  // The "no traffic interval" is a special health check interval that is used when a cluster has
  // never had traffic routed to it. This lower interval allows cluster information to be kept up to
  // date, without sending a potentially large amount of active health checking traffic for no
  // reason. Once a cluster has been used for traffic routing, Envoy will shift back to using the
  // standard health check interval that is defined. Note that this interval takes precedence over
  // any other.
  //
  // The default value for "no traffic interval" is 60 seconds.
  google.protobuf.Duration no_traffic_interval = 12 [(validate.rules).duration = {gt {}}];

  // The "unhealthy interval" is a health check interval that is used for hosts that are marked as
  // unhealthy. As soon as the host is marked as healthy, Envoy will shift back to using the
  // standard health check interval that is defined.
  //
  // The default value for "unhealthy interval" is the same as "interval".
  google.protobuf.Duration unhealthy_interval = 14 [(validate.rules).duration = {gt {}}];

  // The "unhealthy edge interval" is a special health check interval that is used for the first
  // health check right after a host is marked as unhealthy. For subsequent health checks
  // Envoy will shift back to using either "unhealthy interval" if present or the standard health
  // check interval that is defined.
  //
  // The default value for "unhealthy edge interval" is the same as "unhealthy interval".
  google.protobuf.Duration unhealthy_edge_interval = 15 [(validate.rules).duration = {gt {}}];

  // The "healthy edge interval" is a special health check interval that is used for the first
  // health check right after a host is marked as healthy. For subsequent health checks
  // Envoy will shift back to using the standard health check interval that is defined.
  //
  // The default value for "healthy edge interval" is the same as the default interval.
  google.protobuf.Duration healthy_edge_interval = 16 [(validate.rules).duration = {gt {}}];

  // Specifies the path to the :ref:`health check event log <arch_overview_health_check_logging>`.
  // If empty, no event log will be written.
  string event_log_path = 17;

  // [#not-implemented-hide:]
  // The gRPC service for the health check event service.
  // If empty, health check events won't be sent to a remote endpoint.
  EventServiceConfig event_service = 22;

  // If set to true, health check failure events will always be logged. If set to false, only the
  // initial health check failure event will be logged.
  // The default value is false.
  bool always_log_health_check_failures = 19;

  // This allows overriding the cluster TLS settings, just for health check connections.
  TlsOptions tls_options = 21;
}
syntax = "proto3";

package envoy.api.v2.core;

import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/backoff.proto";
import "envoy/api/v2/core/http_uri.proto";
import "envoy/type/percent.proto";
import "envoy/type/semantic_version.proto";

import "google/protobuf/any.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

import public "envoy/api/v2/core/socket_option.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "BaseProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Common types]

// Envoy supports :ref:`upstream priority routing
// <arch_overview_http_routing_priority>` both at the route and the virtual
// cluster level. The current priority implementation uses different connection
// pool and circuit breaking settings for each priority level. This means that
// even for HTTP/2 requests, two physical connections will be used to an
// upstream host. In the future Envoy will likely support true HTTP/2 priority
// over a single upstream connection.
enum RoutingPriority {
  DEFAULT = 0;
  HIGH = 1;
}

// HTTP request method.
enum RequestMethod {
  METHOD_UNSPECIFIED = 0;
  GET = 1;
  HEAD = 2;
  POST = 3;
  PUT = 4;
  DELETE = 5;
  CONNECT = 6;
  OPTIONS = 7;
  TRACE = 8;
  PATCH = 9;
}

// Identifies the direction of the traffic relative to the local Envoy.
enum TrafficDirection {
  // Default option is unspecified.
  UNSPECIFIED = 0;

  // The transport is used for incoming traffic.
  INBOUND = 1;

  // The transport is used for outgoing traffic.
  OUTBOUND = 2;
}

// Identifies location of where either Envoy runs or where upstream hosts run.
message Locality {
  // Region this :ref:`zone <envoy_api_field_core.Locality.zone>` belongs to.
  string region = 1;

  // Defines the local service zone where Envoy is running. Though optional, it
  // should be set if discovery service routing is used and the discovery
  // service exposes :ref:`zone data <envoy_api_field_endpoint.LocalityLbEndpoints.locality>`,
  // either in this message or via :option:`--service-zone`. The meaning of zone
  // is context dependent, e.g. `Availability Zone (AZ)
  // <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html>`_
  // on AWS, `Zone <https://cloud.google.com/compute/docs/regions-zones/>`_ on
  // GCP, etc.
  string zone = 2;

  // When used for locality of upstream hosts, this field further splits zone
  // into smaller chunks of sub-zones so they can be load balanced
  // independently.
  string sub_zone = 3;
}

// BuildVersion combines SemVer version of extension with free-form build information
// (i.e. 'alpha', 'private-build') as a set of strings.
message BuildVersion {
  // SemVer version of extension.
  type.SemanticVersion version = 1;

  // Free-form build information.
  // Envoy defines several well known keys in the source/common/version/version.h file
  google.protobuf.Struct metadata = 2;
}

// Version and identification for an Envoy extension.
// [#next-free-field: 6]
message Extension {
  // This is the name of the Envoy filter as specified in the Envoy
  // configuration, e.g. envoy.filters.http.router, com.acme.widget.
  string name = 1;

  // Category of the extension.
  // Extension category names use reverse DNS notation. For instance "envoy.filters.listener"
  // for Envoy's built-in listener filters or "com.acme.filters.http" for HTTP filters from
  // acme.com vendor.
  // [#comment:TODO(yanavlasov): Link to the doc with existing envoy category names.]
  string category = 2;

  // [#not-implemented-hide:] Type descriptor of extension configuration proto.
  // [#comment:TODO(yanavlasov): Link to the doc with existing configuration protos.]
  // [#comment:TODO(yanavlasov): Add tests when PR #9391 lands.]
  string type_descriptor = 3;

  // The version is a property of the extension and maintained independently
  // of other extensions and the Envoy API.
  // This field is not set when extension did not provide version information.
  BuildVersion version = 4;

  // Indicates that the extension is present but was disabled via dynamic configuration.
  bool disabled = 5;
}

// Identifies a specific Envoy instance. The node identifier is presented to the
// management server, which may use this identifier to distinguish per Envoy
// configuration for serving.
// [#next-free-field: 12]
message Node {
  // An opaque node identifier for the Envoy node. This also provides the local
  // service node name. It should be set if any of the following features are
  // used: :ref:`statsd <arch_overview_statistics>`, :ref:`CDS
  // <config_cluster_manager_cds>`, and :ref:`HTTP tracing
  // <arch_overview_tracing>`, either in this message or via
  // :option:`--service-node`.
  string id = 1;

  // Defines the local service cluster name where Envoy is running. Though
  // optional, it should be set if any of the following features are used:
  // :ref:`statsd <arch_overview_statistics>`, :ref:`health check cluster
  // verification
  // <envoy_api_field_core.HealthCheck.HttpHealthCheck.service_name_matcher>`,
  // :ref:`runtime override directory <envoy_api_msg_config.bootstrap.v2.Runtime>`,
  // :ref:`user agent addition
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.add_user_agent>`,
  // :ref:`HTTP global rate limiting <config_http_filters_rate_limit>`,
  // :ref:`CDS <config_cluster_manager_cds>`, and :ref:`HTTP tracing
  // <arch_overview_tracing>`, either in this message or via
  // :option:`--service-cluster`.
  string cluster = 2;

  // Opaque metadata extending the node identifier. Envoy will pass this
  // directly to the management server.
  google.protobuf.Struct metadata = 3;

  // Locality specifying where the Envoy instance is running.
  Locality locality = 4;

  // This is motivated by informing a management server during canary which
  // version of Envoy is being tested in a heterogeneous fleet. This will be set
  // by Envoy in management server RPCs.
  // This field is deprecated in favor of the user_agent_name and user_agent_version values.
  string build_version = 5 [deprecated = true];

  // Free-form string that identifies the entity requesting config.
  // E.g. "envoy" or "grpc"
  string user_agent_name = 6;

  oneof user_agent_version_type {
    // Free-form string that identifies the version of the entity requesting config.
    // E.g. "1.12.2" or "abcd1234", or "SpecialEnvoyBuild"
    string user_agent_version = 7;

    // Structured version of the entity requesting config.
    BuildVersion user_agent_build_version = 8;
  }

  // List of extensions and their versions supported by the node.
  repeated Extension extensions = 9;

  // Client feature support list. These are well known features described
  // in the Envoy API repository for a given major version of an API. Client features
  // use reverse DNS naming scheme, for example `com.acme.feature`.
  // See :ref:`the list of features <client_features>` that xDS client may
  // support.
  repeated string client_features = 10;

  // Known listening ports on the node as a generic hint to the management server
  // for filtering :ref:`listeners <config_listeners>` to be returned. For example,
  // if there is a listener bound to port 80, the list can optionally contain the
  // SocketAddress `(0.0.0.0,80)`. The field is optional and just a hint.
  repeated Address listening_addresses = 11;
}

// Metadata provides additional inputs to filters based on matched listeners,
// filter chains, routes and endpoints. It is structured as a map, usually from
// filter name (in reverse DNS format) to metadata specific to the filter. Metadata
// key-values for a filter are merged as connection and request handling occurs,
// with later values for the same key overriding earlier values.
//
// An example use of metadata is providing additional values to
// http_connection_manager in the envoy.http_connection_manager.access_log
// namespace.
//
// Another example use of metadata is to per service config info in cluster metadata, which may get
// consumed by multiple filters.
//
// For load balancing, Metadata provides a means to subset cluster endpoints.
// Endpoints have a Metadata object associated and routes contain a Metadata
// object to match against. There are some well defined metadata used today for
// this purpose:
//
// * ``{"envoy.lb": {"canary": <bool> }}`` This indicates the canary status of an
//   endpoint and is also used during header processing
//   (x-envoy-upstream-canary) and for stats purposes.
// [#next-major-version: move to type/metadata/v2]
message Metadata {
  // Key is the reverse DNS filter name, e.g. com.acme.widget. The envoy.*
  // namespace is reserved for Envoy's built-in filters.
  map<string, google.protobuf.Struct> filter_metadata = 1;
}

// Runtime derived uint32 with a default when not specified.
message RuntimeUInt32 {
  // Default value if runtime value is not available.
  uint32 default_value = 2;

  // Runtime key to get value for comparison. This value is used if defined.
  string runtime_key = 3 [(validate.rules).string = {min_bytes: 1}];
}

// Runtime derived double with a default when not specified.
message RuntimeDouble {
  // Default value if runtime value is not available.
  double default_value = 1;

  // Runtime key to get value for comparison. This value is used if defined.
  string runtime_key = 2 [(validate.rules).string = {min_bytes: 1}];
}

// Runtime derived bool with a default when not specified.
message RuntimeFeatureFlag {
  // Default value if runtime value is not available.
  google.protobuf.BoolValue default_value = 1 [(validate.rules).message = {required: true}];

  // Runtime key to get value for comparison. This value is used if defined. The boolean value must
  // be represented via its
  // `canonical JSON encoding <https://developers.google.com/protocol-buffers/docs/proto3#json>`_.
  string runtime_key = 2 [(validate.rules).string = {min_bytes: 1}];
}

// Header name/value pair.
message HeaderValue {
  // Header name.
  string key = 1
      [(validate.rules).string =
           {min_bytes: 1 max_bytes: 16384 well_known_regex: HTTP_HEADER_NAME strict: false}];

  // Header value.
  //
  // The same :ref:`format specifier <config_access_log_format>` as used for
  // :ref:`HTTP access logging <config_access_log>` applies here, however
  // unknown header values are replaced with the empty string instead of `-`.
  string value = 2 [
    (validate.rules).string = {max_bytes: 16384 well_known_regex: HTTP_HEADER_VALUE strict: false}
  ];
}

// Header name/value pair plus option to control append behavior.
message HeaderValueOption {
  // Header name/value pair that this option applies to.
  HeaderValue header = 1 [(validate.rules).message = {required: true}];

  // Should the value be appended? If true (default), the value is appended to
  // existing values.
  google.protobuf.BoolValue append = 2;
}

// Wrapper for a set of headers.
message HeaderMap {
  repeated HeaderValue headers = 1;
}

// Data source consisting of either a file or an inline value.
message DataSource {
  oneof specifier {
    option (validate.required) = true;

    // Local filesystem data source.
    string filename = 1 [(validate.rules).string = {min_bytes: 1}];

    // Bytes inlined in the configuration.
    bytes inline_bytes = 2 [(validate.rules).bytes = {min_len: 1}];

    // String inlined in the configuration.
    string inline_string = 3 [(validate.rules).string = {min_bytes: 1}];
  }
}

// The message specifies the retry policy of remote data source when fetching fails.
message RetryPolicy {
  // Specifies parameters that control :ref:`retry backoff strategy <envoy_api_msg_core.BackoffStrategy>`.
  // This parameter is optional, in which case the default base interval is 1000 milliseconds. The
  // default maximum interval is 10 times the base interval.
  BackoffStrategy retry_back_off = 1;

  // Specifies the allowed number of retries. This parameter is optional and
  // defaults to 1.
  google.protobuf.UInt32Value num_retries = 2;
}

// The message specifies how to fetch data from remote and how to verify it.
message RemoteDataSource {
  // The HTTP URI to fetch the remote data.
  HttpUri http_uri = 1 [(validate.rules).message = {required: true}];

  // SHA256 string for verifying data.
  string sha256 = 2 [(validate.rules).string = {min_bytes: 1}];

  // Retry policy for fetching remote data.
  RetryPolicy retry_policy = 3;
}

// Async data source which support async data fetch.
message AsyncDataSource {
  oneof specifier {
    option (validate.required) = true;

    // Local async data source.
    DataSource local = 1;

    // Remote async data source.
    RemoteDataSource remote = 2;
  }
}

// Configuration for transport socket in :ref:`listeners <config_listeners>` and
// :ref:`clusters <envoy_api_msg_Cluster>`. If the configuration is
// empty, a default transport socket implementation and configuration will be
// chosen based on the platform and existence of tls_context.
message TransportSocket {
  // The name of the transport socket to instantiate. The name must match a supported transport
  // socket implementation.
  string name = 1 [(validate.rules).string = {min_bytes: 1}];

  // Implementation specific configuration which depends on the implementation being instantiated.
  // See the supported transport socket implementations for further documentation.
  oneof config_type {
    google.protobuf.Struct config = 2 [deprecated = true];

    google.protobuf.Any typed_config = 3;
  }
}

// Runtime derived FractionalPercent with defaults for when the numerator or denominator is not
// specified via a runtime key.
//
// .. note::
//
//   Parsing of the runtime key's data is implemented such that it may be represented as a
//   :ref:`FractionalPercent <envoy_api_msg_type.FractionalPercent>` proto represented as JSON/YAML
//   and may also be represented as an integer with the assumption that the value is an integral
//   percentage out of 100. For instance, a runtime key lookup returning the value "42" would parse
//   as a `FractionalPercent` whose numerator is 42 and denominator is HUNDRED.
message RuntimeFractionalPercent {
  // Default value if the runtime value's for the numerator/denominator keys are not available.
  type.FractionalPercent default_value = 1 [(validate.rules).message = {required: true}];

  // Runtime key for a YAML representation of a FractionalPercent.
  string runtime_key = 2;
}

// Identifies a specific ControlPlane instance that Envoy is connected to.
message ControlPlane {
  // An opaque control plane identifier that uniquely identifies an instance
  // of control plane. This can be used to identify which control plane instance,
  // the Envoy is connected to.
  string identifier = 1;
}
syntax = "proto3";

package envoy.api.v2.core;

import "google/protobuf/duration.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "HttpUriProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: HTTP Service URI ]

// Envoy external URI descriptor
message HttpUri {
  // The HTTP server URI. It should be a full FQDN with protocol, host and path.
  //
  // Example:
  //
  // .. code-block:: yaml
  //
  //    uri: https://www.googleapis.com/oauth2/v1/certs
  //
  string uri = 1 [(validate.rules).string = {min_bytes: 1}];

  // Specify how `uri` is to be fetched. Today, this requires an explicit
  // cluster, but in the future we may support dynamic cluster creation or
  // inline DNS resolution. See `issue
  // <https://github.com/envoyproxy/envoy/issues/1606>`_.
  oneof http_upstream_type {
    option (validate.required) = true;

    // A cluster is created in the Envoy "cluster_manager" config
    // section. This field specifies the cluster name.
    //
    // Example:
    //
    // .. code-block:: yaml
    //
    //    cluster: jwks_cluster
    //
    string cluster = 2 [(validate.rules).string = {min_bytes: 1}];
  }

  // Sets the maximum duration in milliseconds that a response can take to arrive upon request.
  google.protobuf.Duration timeout = 3 [(validate.rules).duration = {
    required: true
    gte {}
  }];
}
syntax = "proto3";

package envoy.api.v2.core;

import "google/protobuf/duration.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "ProtocolProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Protocol options]

// [#not-implemented-hide:]
message TcpProtocolOptions {
}

message UpstreamHttpProtocolOptions {
  // Set transport socket `SNI <https://en.wikipedia.org/wiki/Server_Name_Indication>`_ for new
  // upstream connections based on the downstream HTTP host/authority header, as seen by the
  // :ref:`router filter <config_http_filters_router>`.
  bool auto_sni = 1;

  // Automatic validate upstream presented certificate for new upstream connections based on the
  // downstream HTTP host/authority header, as seen by the
  // :ref:`router filter <config_http_filters_router>`.
  // This field is intended to set with `auto_sni` field.
  bool auto_san_validation = 2;
}

// [#next-free-field: 6]
message HttpProtocolOptions {
  // Action to take when Envoy receives client request with header names containing underscore
  // characters.
  // Underscore character is allowed in header names by the RFC-7230 and this behavior is implemented
  // as a security measure due to systems that treat '_' and '-' as interchangeable. Envoy by default allows client request headers with underscore
  // characters.
  enum HeadersWithUnderscoresAction {
    // Allow headers with underscores. This is the default behavior.
    ALLOW = 0;

    // Reject client request. HTTP/1 requests are rejected with the 400 status. HTTP/2 requests
    // end with the stream reset. The "httpN.requests_rejected_with_underscores_in_headers" counter
    // is incremented for each rejected request.
    REJECT_REQUEST = 1;

    // Drop the header with name containing underscores. The header is dropped before the filter chain is
    // invoked and as such filters will not see dropped headers. The
    // "httpN.dropped_headers_with_underscores" is incremented for each dropped header.
    DROP_HEADER = 2;
  }

  // The idle timeout for connections. The idle timeout is defined as the
  // period in which there are no active requests. When the
  // idle timeout is reached the connection will be closed. If the connection is an HTTP/2
  // downstream connection a drain sequence will occur prior to closing the connection, see
  // :ref:`drain_timeout
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.drain_timeout>`.
  // Note that request based timeouts mean that HTTP/2 PINGs will not keep the connection alive.
  // If not specified, this defaults to 1 hour. To disable idle timeouts explicitly set this to 0.
  //
  // .. warning::
  //   Disabling this timeout has a highly likelihood of yielding connection leaks due to lost TCP
  //   FIN packets, etc.
  google.protobuf.Duration idle_timeout = 1;

  // The maximum duration of a connection. The duration is defined as a period since a connection
  // was established. If not set, there is no max duration. When max_connection_duration is reached
  // the connection will be closed. Drain sequence will occur prior to closing the connection if
  // if's applicable. See :ref:`drain_timeout
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.drain_timeout>`.
  // Note: not implemented for upstream connections.
  google.protobuf.Duration max_connection_duration = 3;

  // The maximum number of headers. If unconfigured, the default
  // maximum number of request headers allowed is 100. Requests that exceed this limit will receive
  // a 431 response for HTTP/1.x and cause a stream reset for HTTP/2.
  google.protobuf.UInt32Value max_headers_count = 2 [(validate.rules).uint32 = {gte: 1}];

  // Total duration to keep alive an HTTP request/response stream. If the time limit is reached the stream will be
  // reset independent of any other timeouts. If not specified, this value is not set.
  google.protobuf.Duration max_stream_duration = 4;

  // Action to take when a client request with a header name containing underscore characters is received.
  // If this setting is not specified, the value defaults to ALLOW.
  // Note: upstream responses are not affected by this setting.
  HeadersWithUnderscoresAction headers_with_underscores_action = 5;
}

// [#next-free-field: 6]
message Http1ProtocolOptions {
  message HeaderKeyFormat {
    message ProperCaseWords {
    }

    oneof header_format {
      option (validate.required) = true;

      // Formats the header by proper casing words: the first character and any character following
      // a special character will be capitalized if it's an alpha character. For example,
      // "content-type" becomes "Content-Type", and "foo$b#$are" becomes "Foo$B#$Are".
      // Note that while this results in most headers following conventional casing, certain headers
      // are not covered. For example, the "TE" header will be formatted as "Te".
      ProperCaseWords proper_case_words = 1;
    }
  }

  // Handle HTTP requests with absolute URLs in the requests. These requests
  // are generally sent by clients to forward/explicit proxies. This allows clients to configure
  // envoy as their HTTP proxy. In Unix, for example, this is typically done by setting the
  // *http_proxy* environment variable.
  google.protobuf.BoolValue allow_absolute_url = 1;

  // Handle incoming HTTP/1.0 and HTTP 0.9 requests.
  // This is off by default, and not fully standards compliant. There is support for pre-HTTP/1.1
  // style connect logic, dechunking, and handling lack of client host iff
  // *default_host_for_http_10* is configured.
  bool accept_http_10 = 2;

  // A default host for HTTP/1.0 requests. This is highly suggested if *accept_http_10* is true as
  // Envoy does not otherwise support HTTP/1.0 without a Host header.
  // This is a no-op if *accept_http_10* is not true.
  string default_host_for_http_10 = 3;

  // Describes how the keys for response headers should be formatted. By default, all header keys
  // are lower cased.
  HeaderKeyFormat header_key_format = 4;

  // Enables trailers for HTTP/1. By default the HTTP/1 codec drops proxied trailers.
  //
  // .. attention::
  //
  //   Note that this only happens when Envoy is chunk encoding which occurs when:
  //   - The request is HTTP/1.1.
  //   - Is neither a HEAD only request nor a HTTP Upgrade.
  //   - Not a response to a HEAD request.
  //   - The content length header is not present.
  bool enable_trailers = 5;
}

// [#next-free-field: 14]
message Http2ProtocolOptions {
  // Defines a parameter to be sent in the SETTINGS frame.
  // See `RFC7540, sec. 6.5.1 <https://tools.ietf.org/html/rfc7540#section-6.5.1>`_ for details.
  message SettingsParameter {
    // The 16 bit parameter identifier.
    google.protobuf.UInt32Value identifier = 1 [
      (validate.rules).uint32 = {lte: 65536 gte: 1},
      (validate.rules).message = {required: true}
    ];

    // The 32 bit parameter value.
    google.protobuf.UInt32Value value = 2 [(validate.rules).message = {required: true}];
  }

  // `Maximum table size <https://httpwg.org/specs/rfc7541.html#rfc.section.4.2>`_
  // (in octets) that the encoder is permitted to use for the dynamic HPACK table. Valid values
  // range from 0 to 4294967295 (2^32 - 1) and defaults to 4096. 0 effectively disables header
  // compression.
  google.protobuf.UInt32Value hpack_table_size = 1;

  // `Maximum concurrent streams <https://httpwg.org/specs/rfc7540.html#rfc.section.5.1.2>`_
  // allowed for peer on one HTTP/2 connection. Valid values range from 1 to 2147483647 (2^31 - 1)
  // and defaults to 2147483647.
  //
  // For upstream connections, this also limits how many streams Envoy will initiate concurrently
  // on a single connection. If the limit is reached, Envoy may queue requests or establish
  // additional connections (as allowed per circuit breaker limits).
  google.protobuf.UInt32Value max_concurrent_streams = 2
      [(validate.rules).uint32 = {lte: 2147483647 gte: 1}];

  // `Initial stream-level flow-control window
  // <https://httpwg.org/specs/rfc7540.html#rfc.section.6.9.2>`_ size. Valid values range from 65535
  // (2^16 - 1, HTTP/2 default) to 2147483647 (2^31 - 1, HTTP/2 maximum) and defaults to 268435456
  // (256 * 1024 * 1024).
  //
  // NOTE: 65535 is the initial window size from HTTP/2 spec. We only support increasing the default
  // window size now, so it's also the minimum.
  //
  // This field also acts as a soft limit on the number of bytes Envoy will buffer per-stream in the
  // HTTP/2 codec buffers. Once the buffer reaches this pointer, watermark callbacks will fire to
  // stop the flow of data to the codec buffers.
  google.protobuf.UInt32Value initial_stream_window_size = 3
      [(validate.rules).uint32 = {lte: 2147483647 gte: 65535}];

  // Similar to *initial_stream_window_size*, but for connection-level flow-control
  // window. Currently, this has the same minimum/maximum/default as *initial_stream_window_size*.
  google.protobuf.UInt32Value initial_connection_window_size = 4
      [(validate.rules).uint32 = {lte: 2147483647 gte: 65535}];

  // Allows proxying Websocket and other upgrades over H2 connect.
  bool allow_connect = 5;

  // [#not-implemented-hide:] Hiding until envoy has full metadata support.
  // Still under implementation. DO NOT USE.
  //
  // Allows metadata. See [metadata
  // docs](https://github.com/envoyproxy/envoy/blob/main/source/docs/h2_metadata.md) for more
  // information.
  bool allow_metadata = 6;

  // Limit the number of pending outbound downstream frames of all types (frames that are waiting to
  // be written into the socket). Exceeding this limit triggers flood mitigation and connection is
  // terminated. The ``http2.outbound_flood`` stat tracks the number of terminated connections due
  // to flood mitigation. The default limit is 10000.
  // [#comment:TODO: implement same limits for upstream outbound frames as well.]
  google.protobuf.UInt32Value max_outbound_frames = 7 [(validate.rules).uint32 = {gte: 1}];

  // Limit the number of pending outbound downstream frames of types PING, SETTINGS and RST_STREAM,
  // preventing high memory utilization when receiving continuous stream of these frames. Exceeding
  // this limit triggers flood mitigation and connection is terminated. The
  // ``http2.outbound_control_flood`` stat tracks the number of terminated connections due to flood
  // mitigation. The default limit is 1000.
  // [#comment:TODO: implement same limits for upstream outbound frames as well.]
  google.protobuf.UInt32Value max_outbound_control_frames = 8 [(validate.rules).uint32 = {gte: 1}];

  // Limit the number of consecutive inbound frames of types HEADERS, CONTINUATION and DATA with an
  // empty payload and no end stream flag. Those frames have no legitimate use and are abusive, but
  // might be a result of a broken HTTP/2 implementation. The `http2.inbound_empty_frames_flood``
  // stat tracks the number of connections terminated due to flood mitigation.
  // Setting this to 0 will terminate connection upon receiving first frame with an empty payload
  // and no end stream flag. The default limit is 1.
  // [#comment:TODO: implement same limits for upstream inbound frames as well.]
  google.protobuf.UInt32Value max_consecutive_inbound_frames_with_empty_payload = 9;

  // Limit the number of inbound PRIORITY frames allowed per each opened stream. If the number
  // of PRIORITY frames received over the lifetime of connection exceeds the value calculated
  // using this formula::
  //
  //     max_inbound_priority_frames_per_stream * (1 + inbound_streams)
  //
  // the connection is terminated. The ``http2.inbound_priority_frames_flood`` stat tracks
  // the number of connections terminated due to flood mitigation. The default limit is 100.
  // [#comment:TODO: implement same limits for upstream inbound frames as well.]
  google.protobuf.UInt32Value max_inbound_priority_frames_per_stream = 10;

  // Limit the number of inbound WINDOW_UPDATE frames allowed per DATA frame sent. If the number
  // of WINDOW_UPDATE frames received over the lifetime of connection exceeds the value calculated
  // using this formula::
  //
  //     1 + 2 * (inbound_streams +
  //              max_inbound_window_update_frames_per_data_frame_sent * outbound_data_frames)
  //
  // the connection is terminated. The ``http2.inbound_priority_frames_flood`` stat tracks
  // the number of connections terminated due to flood mitigation. The default limit is 10.
  // Setting this to 1 should be enough to support HTTP/2 implementations with basic flow control,
  // but more complex implementations that try to estimate available bandwidth require at least 2.
  // [#comment:TODO: implement same limits for upstream inbound frames as well.]
  google.protobuf.UInt32Value max_inbound_window_update_frames_per_data_frame_sent = 11
      [(validate.rules).uint32 = {gte: 1}];

  // Allows invalid HTTP messaging and headers. When this option is disabled (default), then
  // the whole HTTP/2 connection is terminated upon receiving invalid HEADERS frame. However,
  // when this option is enabled, only the offending stream is terminated.
  //
  // See `RFC7540, sec. 8.1 <https://tools.ietf.org/html/rfc7540#section-8.1>`_ for details.
  bool stream_error_on_invalid_http_messaging = 12;

  // [#not-implemented-hide:]
  // Specifies SETTINGS frame parameters to be sent to the peer, with two exceptions:
  //
  // 1. SETTINGS_ENABLE_PUSH (0x2) is not configurable as HTTP/2 server push is not supported by
  // Envoy.
  //
  // 2. SETTINGS_ENABLE_CONNECT_PROTOCOL (0x8) is only configurable through the named field
  // 'allow_connect'.
  //
  // Note that custom parameters specified through this field can not also be set in the
  // corresponding named parameters:
  //
  // .. code-block:: text
  //
  //   ID    Field Name
  //   ----------------
  //   0x1   hpack_table_size
  //   0x3   max_concurrent_streams
  //   0x4   initial_stream_window_size
  //
  // Collisions will trigger config validation failure on load/update. Likewise, inconsistencies
  // between custom parameters with the same identifier will trigger a failure.
  //
  // See `IANA HTTP/2 Settings
  // <https://www.iana.org/assignments/http2-parameters/http2-parameters.xhtml#settings>`_ for
  // standardized identifiers.
  repeated SettingsParameter custom_settings_parameters = 13;
}

// [#not-implemented-hide:]
message GrpcProtocolOptions {
  Http2ProtocolOptions http2_protocol_options = 1;
}
syntax = "proto3";

package envoy.api.v2.core;

import "envoy/api/v2/core/socket_option.proto";

import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "AddressProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Network addresses]

message Pipe {
  // Unix Domain Socket path. On Linux, paths starting with '@' will use the
  // abstract namespace. The starting '@' is replaced by a null byte by Envoy.
  // Paths starting with '@' will result in an error in environments other than
  // Linux.
  string path = 1 [(validate.rules).string = {min_bytes: 1}];

  // The mode for the Pipe. Not applicable for abstract sockets.
  uint32 mode = 2 [(validate.rules).uint32 = {lte: 511}];
}

// [#next-free-field: 7]
message SocketAddress {
  enum Protocol {
    TCP = 0;
    UDP = 1;
  }

  Protocol protocol = 1 [(validate.rules).enum = {defined_only: true}];

  // The address for this socket. :ref:`Listeners <config_listeners>` will bind
  // to the address. An empty address is not allowed. Specify ``0.0.0.0`` or ``::``
  // to bind to any address. [#comment:TODO(zuercher) reinstate when implemented:
  // It is possible to distinguish a Listener address via the prefix/suffix matching
  // in :ref:`FilterChainMatch <envoy_api_msg_listener.FilterChainMatch>`.] When used
  // within an upstream :ref:`BindConfig <envoy_api_msg_core.BindConfig>`, the address
  // controls the source address of outbound connections. For :ref:`clusters
  // <envoy_api_msg_Cluster>`, the cluster type determines whether the
  // address must be an IP (*STATIC* or *EDS* clusters) or a hostname resolved by DNS
  // (*STRICT_DNS* or *LOGICAL_DNS* clusters). Address resolution can be customized
  // via :ref:`resolver_name <envoy_api_field_core.SocketAddress.resolver_name>`.
  string address = 2 [(validate.rules).string = {min_bytes: 1}];

  oneof port_specifier {
    option (validate.required) = true;

    uint32 port_value = 3 [(validate.rules).uint32 = {lte: 65535}];

    // This is only valid if :ref:`resolver_name
    // <envoy_api_field_core.SocketAddress.resolver_name>` is specified below and the
    // named resolver is capable of named port resolution.
    string named_port = 4;
  }

  // The name of the custom resolver. This must have been registered with Envoy. If
  // this is empty, a context dependent default applies. If the address is a concrete
  // IP address, no resolution will occur. If address is a hostname this
  // should be set for resolution other than DNS. Specifying a custom resolver with
  // *STRICT_DNS* or *LOGICAL_DNS* will generate an error at runtime.
  string resolver_name = 5;

  // When binding to an IPv6 address above, this enables `IPv4 compatibility
  // <https://tools.ietf.org/html/rfc3493#page-11>`_. Binding to ``::`` will
  // allow both IPv4 and IPv6 connections, with peer IPv4 addresses mapped into
  // IPv6 space as ``::FFFF:<IPv4-address>``.
  bool ipv4_compat = 6;
}

message TcpKeepalive {
  // Maximum number of keepalive probes to send without response before deciding
  // the connection is dead. Default is to use the OS level configuration (unless
  // overridden, Linux defaults to 9.)
  google.protobuf.UInt32Value keepalive_probes = 1;

  // The number of seconds a connection needs to be idle before keep-alive probes
  // start being sent. Default is to use the OS level configuration (unless
  // overridden, Linux defaults to 7200s (i.e., 2 hours.)
  google.protobuf.UInt32Value keepalive_time = 2;

  // The number of seconds between keep-alive probes. Default is to use the OS
  // level configuration (unless overridden, Linux defaults to 75s.)
  google.protobuf.UInt32Value keepalive_interval = 3;
}

message BindConfig {
  // The address to bind to when creating a socket.
  SocketAddress source_address = 1 [(validate.rules).message = {required: true}];

  // Whether to set the *IP_FREEBIND* option when creating the socket. When this
  // flag is set to true, allows the :ref:`source_address
  // <envoy_api_field_UpstreamBindConfig.source_address>` to be an IP address
  // that is not configured on the system running Envoy. When this flag is set
  // to false, the option *IP_FREEBIND* is disabled on the socket. When this
  // flag is not set (default), the socket is not modified, i.e. the option is
  // neither enabled nor disabled.
  google.protobuf.BoolValue freebind = 2;

  // Additional socket options that may not be present in Envoy source code or
  // precompiled binaries.
  repeated SocketOption socket_options = 3;
}

// Addresses specify either a logical or physical address and port, which are
// used to tell Envoy where to bind/listen, connect to upstream and find
// management servers.
message Address {
  oneof address {
    option (validate.required) = true;

    SocketAddress socket_address = 1;

    Pipe pipe = 2;
  }
}

// CidrRange specifies an IP Address and a prefix length to construct
// the subnet mask for a `CIDR <https://tools.ietf.org/html/rfc4632>`_ range.
message CidrRange {
  // IPv4 or IPv6 address, e.g. ``192.0.0.0`` or ``2001:db8::``.
  string address_prefix = 1 [(validate.rules).string = {min_bytes: 1}];

  // Length of prefix, e.g. 0, 32. Defaults to 0 when unset.
  google.protobuf.UInt32Value prefix_len = 2 [(validate.rules).uint32 = {lte: 128}];
}
syntax = "proto3";

package envoy.api.v2.core;

import "envoy/api/v2/core/grpc_service.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "EventServiceConfigProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#not-implemented-hide:]
// Configuration of the event reporting service endpoint.
message EventServiceConfig {
  oneof config_source_specifier {
    option (validate.required) = true;

    // Specifies the gRPC service that hosts the event reporting service.
    GrpcService grpc_service = 1;
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/type:pkg",
        "//envoy/type/matcher:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.api.v2.core;

import "envoy/api/v2/core/base.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/empty.proto";
import "google/protobuf/struct.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/sensitive.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "GrpcServiceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: gRPC services]

// gRPC service configuration. This is used by :ref:`ApiConfigSource
// <envoy_api_msg_core.ApiConfigSource>` and filter configurations.
// [#next-free-field: 6]
message GrpcService {
  message EnvoyGrpc {
    // The name of the upstream gRPC cluster. SSL credentials will be supplied
    // in the :ref:`Cluster <envoy_api_msg_Cluster>` :ref:`transport_socket
    // <envoy_api_field_Cluster.transport_socket>`.
    string cluster_name = 1 [(validate.rules).string = {min_bytes: 1}];
  }

  // [#next-free-field: 7]
  message GoogleGrpc {
    // See https://grpc.io/grpc/cpp/structgrpc_1_1_ssl_credentials_options.html.
    message SslCredentials {
      // PEM encoded server root certificates.
      DataSource root_certs = 1;

      // PEM encoded client private key.
      DataSource private_key = 2 [(udpa.annotations.sensitive) = true];

      // PEM encoded client certificate chain.
      DataSource cert_chain = 3;
    }

    // Local channel credentials. Only UDS is supported for now.
    // See https://github.com/grpc/grpc/pull/15909.
    message GoogleLocalCredentials {
    }

    // See https://grpc.io/docs/guides/auth.html#credential-types to understand Channel and Call
    // credential types.
    message ChannelCredentials {
      oneof credential_specifier {
        option (validate.required) = true;

        SslCredentials ssl_credentials = 1;

        // https://grpc.io/grpc/cpp/namespacegrpc.html#a6beb3ac70ff94bd2ebbd89b8f21d1f61
        google.protobuf.Empty google_default = 2;

        GoogleLocalCredentials local_credentials = 3;
      }
    }

    // [#next-free-field: 8]
    message CallCredentials {
      message ServiceAccountJWTAccessCredentials {
        string json_key = 1;

        uint64 token_lifetime_seconds = 2;
      }

      message GoogleIAMCredentials {
        string authorization_token = 1;

        string authority_selector = 2;
      }

      message MetadataCredentialsFromPlugin {
        string name = 1;

        oneof config_type {
          google.protobuf.Struct config = 2 [deprecated = true];

          google.protobuf.Any typed_config = 3;
        }
      }

      // Security token service configuration that allows Google gRPC to
      // fetch security token from an OAuth 2.0 authorization server.
      // See https://tools.ietf.org/html/draft-ietf-oauth-token-exchange-16 and
      // https://github.com/grpc/grpc/pull/19587.
      // [#next-free-field: 10]
      message StsService {
        // URI of the token exchange service that handles token exchange requests.
        // [#comment:TODO(asraa): Add URI validation when implemented. Tracked by
        // https://github.com/bufbuild/protoc-gen-validate/issues/303]
        string token_exchange_service_uri = 1;

        // Location of the target service or resource where the client
        // intends to use the requested security token.
        string resource = 2;

        // Logical name of the target service where the client intends to
        // use the requested security token.
        string audience = 3;

        // The desired scope of the requested security token in the
        // context of the service or resource where the token will be used.
        string scope = 4;

        // Type of the requested security token.
        string requested_token_type = 5;

        // The path of subject token, a security token that represents the
        // identity of the party on behalf of whom the request is being made.
        string subject_token_path = 6 [(validate.rules).string = {min_bytes: 1}];

        // Type of the subject token.
        string subject_token_type = 7 [(validate.rules).string = {min_bytes: 1}];

        // The path of actor token, a security token that represents the identity
        // of the acting party. The acting party is authorized to use the
        // requested security token and act on behalf of the subject.
        string actor_token_path = 8;

        // Type of the actor token.
        string actor_token_type = 9;
      }

      oneof credential_specifier {
        option (validate.required) = true;

        // Access token credentials.
        // https://grpc.io/grpc/cpp/namespacegrpc.html#ad3a80da696ffdaea943f0f858d7a360d.
        string access_token = 1;

        // Google Compute Engine credentials.
        // https://grpc.io/grpc/cpp/namespacegrpc.html#a6beb3ac70ff94bd2ebbd89b8f21d1f61
        google.protobuf.Empty google_compute_engine = 2;

        // Google refresh token credentials.
        // https://grpc.io/grpc/cpp/namespacegrpc.html#a96901c997b91bc6513b08491e0dca37c.
        string google_refresh_token = 3;

        // Service Account JWT Access credentials.
        // https://grpc.io/grpc/cpp/namespacegrpc.html#a92a9f959d6102461f66ee973d8e9d3aa.
        ServiceAccountJWTAccessCredentials service_account_jwt_access = 4;

        // Google IAM credentials.
        // https://grpc.io/grpc/cpp/namespacegrpc.html#a9fc1fc101b41e680d47028166e76f9d0.
        GoogleIAMCredentials google_iam = 5;

        // Custom authenticator credentials.
        // https://grpc.io/grpc/cpp/namespacegrpc.html#a823c6a4b19ffc71fb33e90154ee2ad07.
        // https://grpc.io/docs/guides/auth.html#extending-grpc-to-support-other-authentication-mechanisms.
        MetadataCredentialsFromPlugin from_plugin = 6;

        // Custom security token service which implements OAuth 2.0 token exchange.
        // https://tools.ietf.org/html/draft-ietf-oauth-token-exchange-16
        // See https://github.com/grpc/grpc/pull/19587.
        StsService sts_service = 7;
      }
    }

    // The target URI when using the `Google C++ gRPC client
    // <https://github.com/grpc/grpc>`_. SSL credentials will be supplied in
    // :ref:`channel_credentials <envoy_api_field_core.GrpcService.GoogleGrpc.channel_credentials>`.
    string target_uri = 1 [(validate.rules).string = {min_bytes: 1}];

    ChannelCredentials channel_credentials = 2;

    // A set of call credentials that can be composed with `channel credentials
    // <https://grpc.io/docs/guides/auth.html#credential-types>`_.
    repeated CallCredentials call_credentials = 3;

    // The human readable prefix to use when emitting statistics for the gRPC
    // service.
    //
    // .. csv-table::
    //    :header: Name, Type, Description
    //    :widths: 1, 1, 2
    //
    //    streams_total, Counter, Total number of streams opened
    //    streams_closed_<gRPC status code>, Counter, Total streams closed with <gRPC status code>
    string stat_prefix = 4 [(validate.rules).string = {min_bytes: 1}];

    // The name of the Google gRPC credentials factory to use. This must have been registered with
    // Envoy. If this is empty, a default credentials factory will be used that sets up channel
    // credentials based on other configuration parameters.
    string credentials_factory_name = 5;

    // Additional configuration for site-specific customizations of the Google
    // gRPC library.
    google.protobuf.Struct config = 6;
  }

  reserved 4;

  oneof target_specifier {
    option (validate.required) = true;

    // Envoy's in-built gRPC client.
    // See the :ref:`gRPC services overview <arch_overview_grpc_services>`
    // documentation for discussion on gRPC client selection.
    EnvoyGrpc envoy_grpc = 1;

    // `Google C++ gRPC client <https://github.com/grpc/grpc>`_
    // See the :ref:`gRPC services overview <arch_overview_grpc_services>`
    // documentation for discussion on gRPC client selection.
    GoogleGrpc google_grpc = 2;
  }

  // The timeout for the gRPC request. This is the timeout for a specific
  // request.
  google.protobuf.Duration timeout = 3;

  // Additional metadata to include in streams initiated to the GrpcService.
  // This can be used for scenarios in which additional ad hoc authorization
  // headers (e.g. ``x-foo-bar: baz-key``) are to be injected.
  repeated HeaderValue initial_metadata = 5;
}
syntax = "proto3";

package envoy.api.v2.core;

import "envoy/api/v2/core/grpc_service.proto";

import "google/protobuf/duration.proto";
import "google/protobuf/wrappers.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "ConfigSourceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Configuration sources]

// xDS API version. This is used to describe both resource and transport
// protocol versions (in distinct configuration fields).
enum ApiVersion {
  // When not specified, we assume v2, to ease migration to Envoy's stable API
  // versioning. If a client does not support v2 (e.g. due to deprecation), this
  // is an invalid value.
  AUTO = 0 [deprecated = true];

  // Use xDS v2 API.
  V2 = 1 [deprecated = true];

  // Use xDS v3 API.
  V3 = 2;
}

// API configuration source. This identifies the API type and cluster that Envoy
// will use to fetch an xDS API.
// [#next-free-field: 9]
message ApiConfigSource {
  // APIs may be fetched via either REST or gRPC.
  enum ApiType {
    // Ideally this would be 'reserved 0' but one can't reserve the default
    // value. Instead we throw an exception if this is ever used.
    UNSUPPORTED_REST_LEGACY = 0
        [deprecated = true, (envoy.annotations.disallowed_by_default_enum) = true];

    // REST-JSON v2 API. The `canonical JSON encoding
    // <https://developers.google.com/protocol-buffers/docs/proto3#json>`_ for
    // the v2 protos is used.
    REST = 1;

    // gRPC v2 API.
    GRPC = 2;

    // Using the delta xDS gRPC service, i.e. DeltaDiscovery{Request,Response}
    // rather than Discovery{Request,Response}. Rather than sending Envoy the entire state
    // with every update, the xDS server only sends what has changed since the last update.
    DELTA_GRPC = 3;
  }

  // API type (gRPC, REST, delta gRPC)
  ApiType api_type = 1 [(validate.rules).enum = {defined_only: true}];

  // API version for xDS transport protocol. This describes the xDS gRPC/REST
  // endpoint and version of [Delta]DiscoveryRequest/Response used on the wire.
  ApiVersion transport_api_version = 8 [(validate.rules).enum = {defined_only: true}];

  // Cluster names should be used only with REST. If > 1
  // cluster is defined, clusters will be cycled through if any kind of failure
  // occurs.
  //
  // .. note::
  //
  //  The cluster with name ``cluster_name`` must be statically defined and its
  //  type must not be ``EDS``.
  repeated string cluster_names = 2;

  // Multiple gRPC services be provided for GRPC. If > 1 cluster is defined,
  // services will be cycled through if any kind of failure occurs.
  repeated GrpcService grpc_services = 4;

  // For REST APIs, the delay between successive polls.
  google.protobuf.Duration refresh_delay = 3;

  // For REST APIs, the request timeout. If not set, a default value of 1s will be used.
  google.protobuf.Duration request_timeout = 5 [(validate.rules).duration = {gt {}}];

  // For GRPC APIs, the rate limit settings. If present, discovery requests made by Envoy will be
  // rate limited.
  RateLimitSettings rate_limit_settings = 6;

  // Skip the node identifier in subsequent discovery requests for streaming gRPC config types.
  bool set_node_on_first_message_only = 7;
}

// Aggregated Discovery Service (ADS) options. This is currently empty, but when
// set in :ref:`ConfigSource <envoy_api_msg_core.ConfigSource>` can be used to
// specify that ADS is to be used.
message AggregatedConfigSource {
}

// [#not-implemented-hide:]
// Self-referencing config source options. This is currently empty, but when
// set in :ref:`ConfigSource <envoy_api_msg_core.ConfigSource>` can be used to
// specify that other data can be obtained from the same server.
message SelfConfigSource {
  // API version for xDS transport protocol. This describes the xDS gRPC/REST
  // endpoint and version of [Delta]DiscoveryRequest/Response used on the wire.
  ApiVersion transport_api_version = 1 [(validate.rules).enum = {defined_only: true}];
}

// Rate Limit settings to be applied for discovery requests made by Envoy.
message RateLimitSettings {
  // Maximum number of tokens to be used for rate limiting discovery request calls. If not set, a
  // default value of 100 will be used.
  google.protobuf.UInt32Value max_tokens = 1;

  // Rate at which tokens will be filled per second. If not set, a default fill rate of 10 tokens
  // per second will be used.
  google.protobuf.DoubleValue fill_rate = 2 [(validate.rules).double = {gt: 0.0}];
}

// Configuration for :ref:`listeners <config_listeners>`, :ref:`clusters
// <config_cluster_manager>`, :ref:`routes
// <envoy_api_msg_RouteConfiguration>`, :ref:`endpoints
// <arch_overview_service_discovery>` etc. may either be sourced from the
// filesystem or from an xDS API source. Filesystem configs are watched with
// inotify for updates.
// [#next-free-field: 7]
message ConfigSource {
  oneof config_source_specifier {
    option (validate.required) = true;

    // Path on the filesystem to source and watch for configuration updates.
    // When sourcing configuration for :ref:`secret <envoy_api_msg_auth.Secret>`,
    // the certificate and key files are also watched for updates.
    //
    // .. note::
    //
    //  The path to the source must exist at config load time.
    //
    // .. note::
    //
    //   Envoy will only watch the file path for *moves.* This is because in general only moves
    //   are atomic. The same method of swapping files as is demonstrated in the
    //   :ref:`runtime documentation <config_runtime_symbolic_link_swap>` can be used here also.
    string path = 1;

    // API configuration source.
    ApiConfigSource api_config_source = 2;

    // When set, ADS will be used to fetch resources. The ADS API configuration
    // source in the bootstrap configuration is used.
    AggregatedConfigSource ads = 3;

    // [#not-implemented-hide:]
    // When set, the client will access the resources from the same server it got the
    // ConfigSource from, although not necessarily from the same stream. This is similar to the
    // :ref:`ads<envoy_api_field.ConfigSource.ads>` field, except that the client may use a
    // different stream to the same server. As a result, this field can be used for things
    // like LRS that cannot be sent on an ADS stream. It can also be used to link from (e.g.)
    // LDS to RDS on the same server without requiring the management server to know its name
    // or required credentials.
    // [#next-major-version: In xDS v3, consider replacing the ads field with this one, since
    // this field can implicitly mean to use the same stream in the case where the ConfigSource
    // is provided via ADS and the specified data can also be obtained via ADS.]
    SelfConfigSource self = 5;
  }

  // When this timeout is specified, Envoy will wait no longer than the specified time for first
  // config response on this xDS subscription during the :ref:`initialization process
  // <arch_overview_initialization>`. After reaching the timeout, Envoy will move to the next
  // initialization phase, even if the first config is not delivered yet. The timer is activated
  // when the xDS API subscription starts, and is disarmed on first config update or on error. 0
  // means no timeout - Envoy will wait indefinitely for the first xDS config (unless another
  // timeout applies). The default is 15s.
  google.protobuf.Duration initial_fetch_timeout = 4;

  // API version for xDS resources. This implies the type URLs that the client
  // will request for resources and the resource type that the client will in
  // turn expect to be delivered.
  ApiVersion resource_api_version = 6 [(validate.rules).enum = {defined_only: true}];
}
syntax = "proto3";

package envoy.api.v2.core;

import "google/protobuf/duration.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "BackoffProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Backoff Strategy]

// Configuration defining a jittered exponential back off strategy.
message BackoffStrategy {
  // The base interval to be used for the next back off computation. It should
  // be greater than zero and less than or equal to :ref:`max_interval
  // <envoy_api_field_core.BackoffStrategy.max_interval>`.
  google.protobuf.Duration base_interval = 1 [(validate.rules).duration = {
    required: true
    gte {nanos: 1000000}
  }];

  // Specifies the maximum interval between retries. This parameter is optional,
  // but must be greater than or equal to the :ref:`base_interval
  // <envoy_api_field_core.BackoffStrategy.base_interval>` if set. The default
  // is 10 times the :ref:`base_interval
  // <envoy_api_field_core.BackoffStrategy.base_interval>`.
  google.protobuf.Duration max_interval = 2 [(validate.rules).duration = {gt {}}];
}
syntax = "proto3";

package envoy.api.v2.core;

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.core";
option java_outer_classname = "GrpcMethodListProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/core";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.core.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: gRPC method list]

// A list of gRPC methods which can be used as an allowlist, for example.
message GrpcMethodList {
  message Service {
    // The name of the gRPC service.
    string name = 1 [(validate.rules).string = {min_bytes: 1}];

    // The names of the gRPC methods in this service.
    repeated string method_names = 2 [(validate.rules).repeated = {min_items: 1}];
  }

  repeated Service services = 1;
}
syntax = "proto3";

package envoy.api.v2.route;

import "envoy/api/v2/core/base.proto";
import "envoy/type/matcher/regex.proto";
import "envoy/type/matcher/string.proto";
import "envoy/type/percent.proto";
import "envoy/type/range.proto";
import "envoy/type/tracing/v2/custom_tag.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/wrappers.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.route";
option java_outer_classname = "RouteComponentsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/route";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.route.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: HTTP route components]
// * Routing :ref:`architecture overview <arch_overview_http_routing>`
// * HTTP :ref:`router filter <config_http_filters_router>`

// The top level element in the routing configuration is a virtual host. Each virtual host has
// a logical name as well as a set of domains that get routed to it based on the incoming request's
// host header. This allows a single listener to service multiple top level domain path trees. Once
// a virtual host is selected based on the domain, the routes are processed in order to see which
// upstream cluster to route to or whether to perform a redirect.
// [#next-free-field: 21]
message VirtualHost {
  enum TlsRequirementType {
    // No TLS requirement for the virtual host.
    NONE = 0;

    // External requests must use TLS. If a request is external and it is not
    // using TLS, a 301 redirect will be sent telling the client to use HTTPS.
    EXTERNAL_ONLY = 1;

    // All requests must use TLS. If a request is not using TLS, a 301 redirect
    // will be sent telling the client to use HTTPS.
    ALL = 2;
  }

  reserved 9;

  // The logical name of the virtual host. This is used when emitting certain
  // statistics but is not relevant for routing.
  string name = 1 [(validate.rules).string = {min_bytes: 1}];

  // A list of domains (host/authority header) that will be matched to this
  // virtual host. Wildcard hosts are supported in the suffix or prefix form.
  //
  // Domain search order:
  //  1. Exact domain names: ``www.foo.com``.
  //  2. Suffix domain wildcards: ``*.foo.com`` or ``*-bar.foo.com``.
  //  3. Prefix domain wildcards: ``foo.*`` or ``foo-*``.
  //  4. Special wildcard ``*`` matching any domain.
  //
  // .. note::
  //
  //   The wildcard will not match the empty string.
  //   e.g. ``*-bar.foo.com`` will match ``baz-bar.foo.com`` but not ``-bar.foo.com``.
  //   The longest wildcards match first.
  //   Only a single virtual host in the entire route configuration can match on ``*``. A domain
  //   must be unique across all virtual hosts or the config will fail to load.
  //
  // Domains cannot contain control characters. This is validated by the well_known_regex HTTP_HEADER_VALUE.
  repeated string domains = 2 [(validate.rules).repeated = {
    min_items: 1
    items {string {well_known_regex: HTTP_HEADER_VALUE strict: false}}
  }];

  // The list of routes that will be matched, in order, for incoming requests.
  // The first route that matches will be used.
  repeated Route routes = 3;

  // Specifies the type of TLS enforcement the virtual host expects. If this option is not
  // specified, there is no TLS requirement for the virtual host.
  TlsRequirementType require_tls = 4 [(validate.rules).enum = {defined_only: true}];

  // A list of virtual clusters defined for this virtual host. Virtual clusters
  // are used for additional statistics gathering.
  repeated VirtualCluster virtual_clusters = 5;

  // Specifies a set of rate limit configurations that will be applied to the
  // virtual host.
  repeated RateLimit rate_limits = 6;

  // Specifies a list of HTTP headers that should be added to each request
  // handled by this virtual host. Headers specified at this level are applied
  // after headers from enclosed :ref:`envoy_api_msg_route.Route` and before headers from the
  // enclosing :ref:`envoy_api_msg_RouteConfiguration`. For more information, including
  // details on header value syntax, see the documentation on :ref:`custom request headers
  // <config_http_conn_man_headers_custom_request_headers>`.
  repeated core.HeaderValueOption request_headers_to_add = 7
      [(validate.rules).repeated = {max_items: 1000}];

  // Specifies a list of HTTP headers that should be removed from each request
  // handled by this virtual host.
  repeated string request_headers_to_remove = 13;

  // Specifies a list of HTTP headers that should be added to each response
  // handled by this virtual host. Headers specified at this level are applied
  // after headers from enclosed :ref:`envoy_api_msg_route.Route` and before headers from the
  // enclosing :ref:`envoy_api_msg_RouteConfiguration`. For more information, including
  // details on header value syntax, see the documentation on :ref:`custom request headers
  // <config_http_conn_man_headers_custom_request_headers>`.
  repeated core.HeaderValueOption response_headers_to_add = 10
      [(validate.rules).repeated = {max_items: 1000}];

  // Specifies a list of HTTP headers that should be removed from each response
  // handled by this virtual host.
  repeated string response_headers_to_remove = 11;

  // Indicates that the virtual host has a CORS policy.
  CorsPolicy cors = 8;

  // The per_filter_config field can be used to provide virtual host-specific
  // configurations for filters. The key should match the filter name, such as
  // *envoy.filters.http.buffer* for the HTTP buffer filter. Use of this field is filter
  // specific; see the :ref:`HTTP filter documentation <config_http_filters>`
  // for if and how it is utilized.
  map<string, google.protobuf.Struct> per_filter_config = 12 [deprecated = true];

  // The per_filter_config field can be used to provide virtual host-specific
  // configurations for filters. The key should match the filter name, such as
  // *envoy.filters.http.buffer* for the HTTP buffer filter. Use of this field is filter
  // specific; see the :ref:`HTTP filter documentation <config_http_filters>`
  // for if and how it is utilized.
  map<string, google.protobuf.Any> typed_per_filter_config = 15;

  // Decides whether the :ref:`x-envoy-attempt-count
  // <config_http_filters_router_x-envoy-attempt-count>` header should be included
  // in the upstream request. Setting this option will cause it to override any existing header
  // value, so in the case of two Envoys on the request path with this option enabled, the upstream
  // will see the attempt count as perceived by the second Envoy. Defaults to false.
  // This header is unaffected by the
  // :ref:`suppress_envoy_headers
  // <envoy_api_field_config.filter.http.router.v2.Router.suppress_envoy_headers>` flag.
  //
  // [#next-major-version: rename to include_attempt_count_in_request.]
  bool include_request_attempt_count = 14;

  // Decides whether the :ref:`x-envoy-attempt-count
  // <config_http_filters_router_x-envoy-attempt-count>` header should be included
  // in the downstream response. Setting this option will cause the router to override any existing header
  // value, so in the case of two Envoys on the request path with this option enabled, the downstream
  // will see the attempt count as perceived by the Envoy closest upstream from itself. Defaults to false.
  // This header is unaffected by the
  // :ref:`suppress_envoy_headers
  // <envoy_api_field_config.filter.http.router.v2.Router.suppress_envoy_headers>` flag.
  bool include_attempt_count_in_response = 19;

  // Indicates the retry policy for all routes in this virtual host. Note that setting a
  // route level entry will take precedence over this config and it'll be treated
  // independently (e.g.: values are not inherited).
  RetryPolicy retry_policy = 16;

  // [#not-implemented-hide:]
  // Specifies the configuration for retry policy extension. Note that setting a route level entry
  // will take precedence over this config and it'll be treated independently (e.g.: values are not
  // inherited). :ref:`Retry policy <envoy_api_field_route.VirtualHost.retry_policy>` should not be
  // set if this field is used.
  google.protobuf.Any retry_policy_typed_config = 20;

  // Indicates the hedge policy for all routes in this virtual host. Note that setting a
  // route level entry will take precedence over this config and it'll be treated
  // independently (e.g.: values are not inherited).
  HedgePolicy hedge_policy = 17;

  // The maximum bytes which will be buffered for retries and shadowing.
  // If set and a route-specific limit is not set, the bytes actually buffered will be the minimum
  // value of this and the listener per_connection_buffer_limit_bytes.
  google.protobuf.UInt32Value per_request_buffer_limit_bytes = 18;
}

// A filter-defined action type.
message FilterAction {
  google.protobuf.Any action = 1;
}

// A route is both a specification of how to match a request as well as an indication of what to do
// next (e.g., redirect, forward, rewrite, etc.).
//
// .. attention::
//
//   Envoy supports routing on HTTP method via :ref:`header matching
//   <envoy_api_msg_route.HeaderMatcher>`.
// [#next-free-field: 18]
message Route {
  reserved 6;

  // Name for the route.
  string name = 14;

  // Route matching parameters.
  RouteMatch match = 1 [(validate.rules).message = {required: true}];

  oneof action {
    option (validate.required) = true;

    // Route request to some upstream cluster.
    RouteAction route = 2;

    // Return a redirect.
    RedirectAction redirect = 3;

    // Return an arbitrary HTTP response directly, without proxying.
    DirectResponseAction direct_response = 7;

    // [#not-implemented-hide:]
    // If true, a filter will define the action (e.g., it could dynamically generate the
    // RouteAction).
    FilterAction filter_action = 17;
  }

  // The Metadata field can be used to provide additional information
  // about the route. It can be used for configuration, stats, and logging.
  // The metadata should go under the filter namespace that will need it.
  // For instance, if the metadata is intended for the Router filter,
  // the filter name should be specified as *envoy.filters.http.router*.
  core.Metadata metadata = 4;

  // Decorator for the matched route.
  Decorator decorator = 5;

  // The per_filter_config field can be used to provide route-specific
  // configurations for filters. The key should match the filter name, such as
  // *envoy.filters.http.buffer* for the HTTP buffer filter. Use of this field is filter
  // specific; see the :ref:`HTTP filter documentation <config_http_filters>` for
  // if and how it is utilized.
  map<string, google.protobuf.Struct> per_filter_config = 8 [deprecated = true];

  // The typed_per_filter_config field can be used to provide route-specific
  // configurations for filters. The key should match the filter name, such as
  // *envoy.filters.http.buffer* for the HTTP buffer filter. Use of this field is filter
  // specific; see the :ref:`HTTP filter documentation <config_http_filters>` for
  // if and how it is utilized.
  map<string, google.protobuf.Any> typed_per_filter_config = 13;

  // Specifies a set of headers that will be added to requests matching this
  // route. Headers specified at this level are applied before headers from the
  // enclosing :ref:`envoy_api_msg_route.VirtualHost` and
  // :ref:`envoy_api_msg_RouteConfiguration`. For more information, including details on
  // header value syntax, see the documentation on :ref:`custom request headers
  // <config_http_conn_man_headers_custom_request_headers>`.
  repeated core.HeaderValueOption request_headers_to_add = 9
      [(validate.rules).repeated = {max_items: 1000}];

  // Specifies a list of HTTP headers that should be removed from each request
  // matching this route.
  repeated string request_headers_to_remove = 12;

  // Specifies a set of headers that will be added to responses to requests
  // matching this route. Headers specified at this level are applied before
  // headers from the enclosing :ref:`envoy_api_msg_route.VirtualHost` and
  // :ref:`envoy_api_msg_RouteConfiguration`. For more information, including
  // details on header value syntax, see the documentation on
  // :ref:`custom request headers <config_http_conn_man_headers_custom_request_headers>`.
  repeated core.HeaderValueOption response_headers_to_add = 10
      [(validate.rules).repeated = {max_items: 1000}];

  // Specifies a list of HTTP headers that should be removed from each response
  // to requests matching this route.
  repeated string response_headers_to_remove = 11;

  // Presence of the object defines whether the connection manager's tracing configuration
  // is overridden by this route specific instance.
  Tracing tracing = 15;

  // The maximum bytes which will be buffered for retries and shadowing.
  // If set, the bytes actually buffered will be the minimum value of this and the
  // listener per_connection_buffer_limit_bytes.
  google.protobuf.UInt32Value per_request_buffer_limit_bytes = 16;
}

// Compared to the :ref:`cluster <envoy_api_field_route.RouteAction.cluster>` field that specifies a
// single upstream cluster as the target of a request, the :ref:`weighted_clusters
// <envoy_api_field_route.RouteAction.weighted_clusters>` option allows for specification of
// multiple upstream clusters along with weights that indicate the percentage of
// traffic to be forwarded to each cluster. The router selects an upstream cluster based on the
// weights.
message WeightedCluster {
  // [#next-free-field: 11]
  message ClusterWeight {
    reserved 7;

    // Name of the upstream cluster. The cluster must exist in the
    // :ref:`cluster manager configuration <config_cluster_manager>`.
    string name = 1 [(validate.rules).string = {min_bytes: 1}];

    // An integer between 0 and :ref:`total_weight
    // <envoy_api_field_route.WeightedCluster.total_weight>`. When a request matches the route,
    // the choice of an upstream cluster is determined by its weight. The sum of weights across all
    // entries in the clusters array must add up to the total_weight, if total_weight is greater than 0.
    google.protobuf.UInt32Value weight = 2;

    // Optional endpoint metadata match criteria used by the subset load balancer. Only endpoints in
    // the upstream cluster with metadata matching what is set in this field will be considered for
    // load balancing. Note that this will be merged with what's provided in
    // :ref:`RouteAction.metadata_match <envoy_api_field_route.RouteAction.metadata_match>`, with
    // values here taking precedence. The filter name should be specified as *envoy.lb*.
    core.Metadata metadata_match = 3;

    // Specifies a list of headers to be added to requests when this cluster is selected
    // through the enclosing :ref:`envoy_api_msg_route.RouteAction`.
    // Headers specified at this level are applied before headers from the enclosing
    // :ref:`envoy_api_msg_route.Route`, :ref:`envoy_api_msg_route.VirtualHost`, and
    // :ref:`envoy_api_msg_RouteConfiguration`. For more information, including details on
    // header value syntax, see the documentation on :ref:`custom request headers
    // <config_http_conn_man_headers_custom_request_headers>`.
    repeated core.HeaderValueOption request_headers_to_add = 4
        [(validate.rules).repeated = {max_items: 1000}];

    // Specifies a list of HTTP headers that should be removed from each request when
    // this cluster is selected through the enclosing :ref:`envoy_api_msg_route.RouteAction`.
    repeated string request_headers_to_remove = 9;

    // Specifies a list of headers to be added to responses when this cluster is selected
    // through the enclosing :ref:`envoy_api_msg_route.RouteAction`.
    // Headers specified at this level are applied before headers from the enclosing
    // :ref:`envoy_api_msg_route.Route`, :ref:`envoy_api_msg_route.VirtualHost`, and
    // :ref:`envoy_api_msg_RouteConfiguration`. For more information, including details on
    // header value syntax, see the documentation on :ref:`custom request headers
    // <config_http_conn_man_headers_custom_request_headers>`.
    repeated core.HeaderValueOption response_headers_to_add = 5
        [(validate.rules).repeated = {max_items: 1000}];

    // Specifies a list of headers to be removed from responses when this cluster is selected
    // through the enclosing :ref:`envoy_api_msg_route.RouteAction`.
    repeated string response_headers_to_remove = 6;

    // The per_filter_config field can be used to provide weighted cluster-specific
    // configurations for filters. The key should match the filter name, such as
    // *envoy.filters.http.buffer* for the HTTP buffer filter. Use of this field is filter
    // specific; see the :ref:`HTTP filter documentation <config_http_filters>`
    // for if and how it is utilized.
    map<string, google.protobuf.Struct> per_filter_config = 8 [deprecated = true];

    // The per_filter_config field can be used to provide weighted cluster-specific
    // configurations for filters. The key should match the filter name, such as
    // *envoy.filters.http.buffer* for the HTTP buffer filter. Use of this field is filter
    // specific; see the :ref:`HTTP filter documentation <config_http_filters>`
    // for if and how it is utilized.
    map<string, google.protobuf.Any> typed_per_filter_config = 10;
  }

  // Specifies one or more upstream clusters associated with the route.
  repeated ClusterWeight clusters = 1 [(validate.rules).repeated = {min_items: 1}];

  // Specifies the total weight across all clusters. The sum of all cluster weights must equal this
  // value, which must be greater than 0. Defaults to 100.
  google.protobuf.UInt32Value total_weight = 3 [(validate.rules).uint32 = {gte: 1}];

  // Specifies the runtime key prefix that should be used to construct the
  // runtime keys associated with each cluster. When the *runtime_key_prefix* is
  // specified, the router will look for weights associated with each upstream
  // cluster under the key *runtime_key_prefix* + "." + *cluster[i].name* where
  // *cluster[i]* denotes an entry in the clusters array field. If the runtime
  // key for the cluster does not exist, the value specified in the
  // configuration file will be used as the default weight. See the :ref:`runtime documentation
  // <operations_runtime>` for how key names map to the underlying implementation.
  string runtime_key_prefix = 2;
}

// [#next-free-field: 12]
message RouteMatch {
  message GrpcRouteMatchOptions {
  }

  message TlsContextMatchOptions {
    // If specified, the route will match against whether or not a certificate is presented.
    // If not specified, certificate presentation status (true or false) will not be considered when route matching.
    google.protobuf.BoolValue presented = 1;

    // If specified, the route will match against whether or not a certificate is validated.
    // If not specified, certificate validation status (true or false) will not be considered when route matching.
    google.protobuf.BoolValue validated = 2;
  }

  reserved 5;

  oneof path_specifier {
    option (validate.required) = true;

    // If specified, the route is a prefix rule meaning that the prefix must
    // match the beginning of the *:path* header.
    string prefix = 1;

    // If specified, the route is an exact path rule meaning that the path must
    // exactly match the *:path* header once the query string is removed.
    string path = 2;

    // If specified, the route is a regular expression rule meaning that the
    // regex must match the *:path* header once the query string is removed. The entire path
    // (without the query string) must match the regex. The rule will not match if only a
    // subsequence of the *:path* header matches the regex. The regex grammar is defined `here
    // <https://en.cppreference.com/w/cpp/regex/ecmascript>`_.
    //
    // Examples:
    //
    // * The regex ``/b[io]t`` matches the path */bit*
    // * The regex ``/b[io]t`` matches the path */bot*
    // * The regex ``/b[io]t`` does not match the path */bite*
    // * The regex ``/b[io]t`` does not match the path */bit/bot*
    //
    // .. attention::
    //   This field has been deprecated in favor of `safe_regex` as it is not safe for use with
    //   untrusted input in all cases.
    string regex = 3 [
      deprecated = true,
      (validate.rules).string = {max_bytes: 1024},
      (envoy.annotations.disallowed_by_default) = true
    ];

    // If specified, the route is a regular expression rule meaning that the
    // regex must match the *:path* header once the query string is removed. The entire path
    // (without the query string) must match the regex. The rule will not match if only a
    // subsequence of the *:path* header matches the regex.
    //
    // [#next-major-version: In the v3 API we should redo how path specification works such
    // that we utilize StringMatcher, and additionally have consistent options around whether we
    // strip query strings, do a case sensitive match, etc. In the interim it will be too disruptive
    // to deprecate the existing options. We should even consider whether we want to do away with
    // path_specifier entirely and just rely on a set of header matchers which can already match
    // on :path, etc. The issue with that is it is unclear how to generically deal with query string
    // stripping. This needs more thought.]
    type.matcher.RegexMatcher safe_regex = 10 [(validate.rules).message = {required: true}];
  }

  // Indicates that prefix/path matching should be case sensitive. The default
  // is true.
  google.protobuf.BoolValue case_sensitive = 4;

  // Indicates that the route should additionally match on a runtime key. Every time the route
  // is considered for a match, it must also fall under the percentage of matches indicated by
  // this field. For some fraction N/D, a random number in the range [0,D) is selected. If the
  // number is <= the value of the numerator N, or if the key is not present, the default
  // value, the router continues to evaluate the remaining match criteria. A runtime_fraction
  // route configuration can be used to roll out route changes in a gradual manner without full
  // code/config deploys. Refer to the :ref:`traffic shifting
  // <config_http_conn_man_route_table_traffic_splitting_shift>` docs for additional documentation.
  //
  // .. note::
  //
  //    Parsing this field is implemented such that the runtime key's data may be represented
  //    as a FractionalPercent proto represented as JSON/YAML and may also be represented as an
  //    integer with the assumption that the value is an integral percentage out of 100. For
  //    instance, a runtime key lookup returning the value "42" would parse as a FractionalPercent
  //    whose numerator is 42 and denominator is HUNDRED. This preserves legacy semantics.
  core.RuntimeFractionalPercent runtime_fraction = 9;

  // Specifies a set of headers that the route should match on. The router will
  // check the request’s headers against all the specified headers in the route
  // config. A match will happen if all the headers in the route are present in
  // the request with the same values (or based on presence if the value field
  // is not in the config).
  repeated HeaderMatcher headers = 6;

  // Specifies a set of URL query parameters on which the route should
  // match. The router will check the query string from the *path* header
  // against all the specified query parameters. If the number of specified
  // query parameters is nonzero, they all must match the *path* header's
  // query string for a match to occur.
  repeated QueryParameterMatcher query_parameters = 7;

  // If specified, only gRPC requests will be matched. The router will check
  // that the content-type header has a application/grpc or one of the various
  // application/grpc+ values.
  GrpcRouteMatchOptions grpc = 8;

  // If specified, the client tls context will be matched against the defined
  // match options.
  //
  // [#next-major-version: unify with RBAC]
  TlsContextMatchOptions tls_context = 11;
}

// [#next-free-field: 12]
message CorsPolicy {
  // Specifies the origins that will be allowed to do CORS requests.
  //
  // An origin is allowed if either allow_origin or allow_origin_regex match.
  //
  // .. attention::
  //  This field has been deprecated in favor of `allow_origin_string_match`.
  repeated string allow_origin = 1
      [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

  // Specifies regex patterns that match allowed origins.
  //
  // An origin is allowed if either allow_origin or allow_origin_regex match.
  //
  // .. attention::
  //   This field has been deprecated in favor of `allow_origin_string_match` as it is not safe for
  //   use with untrusted input in all cases.
  repeated string allow_origin_regex = 8
      [deprecated = true, (validate.rules).repeated = {items {string {max_bytes: 1024}}}];

  // Specifies string patterns that match allowed origins. An origin is allowed if any of the
  // string matchers match.
  repeated type.matcher.StringMatcher allow_origin_string_match = 11;

  // Specifies the content for the *access-control-allow-methods* header.
  string allow_methods = 2;

  // Specifies the content for the *access-control-allow-headers* header.
  string allow_headers = 3;

  // Specifies the content for the *access-control-expose-headers* header.
  string expose_headers = 4;

  // Specifies the content for the *access-control-max-age* header.
  string max_age = 5;

  // Specifies whether the resource allows credentials.
  google.protobuf.BoolValue allow_credentials = 6;

  oneof enabled_specifier {
    // Specifies if the CORS filter is enabled. Defaults to true. Only effective on route.
    //
    // .. attention::
    //
    //   **This field is deprecated**. Set the
    //   :ref:`filter_enabled<envoy_api_field_route.CorsPolicy.filter_enabled>` field instead.
    google.protobuf.BoolValue enabled = 7
        [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

    // Specifies the % of requests for which the CORS filter is enabled.
    //
    // If neither ``enabled``, ``filter_enabled``, nor ``shadow_enabled`` are specified, the CORS
    // filter will be enabled for 100% of the requests.
    //
    // If :ref:`runtime_key <envoy_api_field_core.RuntimeFractionalPercent.runtime_key>` is
    // specified, Envoy will lookup the runtime key to get the percentage of requests to filter.
    core.RuntimeFractionalPercent filter_enabled = 9;
  }

  // Specifies the % of requests for which the CORS policies will be evaluated and tracked, but not
  // enforced.
  //
  // This field is intended to be used when ``filter_enabled`` and ``enabled`` are off. One of those
  // fields have to explicitly disable the filter in order for this setting to take effect.
  //
  // If :ref:`runtime_key <envoy_api_field_core.RuntimeFractionalPercent.runtime_key>` is specified,
  // Envoy will lookup the runtime key to get the percentage of requests for which it will evaluate
  // and track the request's *Origin* to determine if it's valid but will not enforce any policies.
  core.RuntimeFractionalPercent shadow_enabled = 10;
}

// [#next-free-field: 34]
message RouteAction {
  enum ClusterNotFoundResponseCode {
    // HTTP status code - 503 Service Unavailable.
    SERVICE_UNAVAILABLE = 0;

    // HTTP status code - 404 Not Found.
    NOT_FOUND = 1;
  }

  // Configures :ref:`internal redirect <arch_overview_internal_redirects>` behavior.
  enum InternalRedirectAction {
    PASS_THROUGH_INTERNAL_REDIRECT = 0;
    HANDLE_INTERNAL_REDIRECT = 1;
  }

  // The router is capable of shadowing traffic from one cluster to another. The current
  // implementation is "fire and forget," meaning Envoy will not wait for the shadow cluster to
  // respond before returning the response from the primary cluster. All normal statistics are
  // collected for the shadow cluster making this feature useful for testing.
  //
  // During shadowing, the host/authority header is altered such that *-shadow* is appended. This is
  // useful for logging. For example, *cluster1* becomes *cluster1-shadow*.
  //
  // .. note::
  //
  //   Shadowing will not be triggered if the primary cluster does not exist.
  message RequestMirrorPolicy {
    // Specifies the cluster that requests will be mirrored to. The cluster must
    // exist in the cluster manager configuration.
    string cluster = 1 [(validate.rules).string = {min_bytes: 1}];

    // If not specified, all requests to the target cluster will be mirrored. If
    // specified, Envoy will lookup the runtime key to get the % of requests to
    // mirror. Valid values are from 0 to 10000, allowing for increments of
    // 0.01% of requests to be mirrored. If the runtime key is specified in the
    // configuration but not present in runtime, 0 is the default and thus 0% of
    // requests will be mirrored.
    //
    // .. attention::
    //
    //   **This field is deprecated**. Set the
    //   :ref:`runtime_fraction
    //   <envoy_api_field_route.RouteAction.RequestMirrorPolicy.runtime_fraction>`
    //   field instead. Mirroring occurs if both this and
    //   <envoy_api_field_route.RouteAction.RequestMirrorPolicy.runtime_fraction>`
    //   are not set.
    string runtime_key = 2 [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

    // If not specified, all requests to the target cluster will be mirrored.
    //
    // If specified, this field takes precedence over the `runtime_key` field and requests must also
    // fall under the percentage of matches indicated by this field.
    //
    // For some fraction N/D, a random number in the range [0,D) is selected. If the
    // number is <= the value of the numerator N, or if the key is not present, the default
    // value, the request will be mirrored.
    core.RuntimeFractionalPercent runtime_fraction = 3;

    // Determines if the trace span should be sampled. Defaults to true.
    google.protobuf.BoolValue trace_sampled = 4;
  }

  // Specifies the route's hashing policy if the upstream cluster uses a hashing :ref:`load balancer
  // <arch_overview_load_balancing_types>`.
  // [#next-free-field: 7]
  message HashPolicy {
    message Header {
      // The name of the request header that will be used to obtain the hash
      // key. If the request header is not present, no hash will be produced.
      string header_name = 1 [
        (validate.rules).string = {min_bytes: 1 well_known_regex: HTTP_HEADER_NAME strict: false}
      ];
    }

    // Envoy supports two types of cookie affinity:
    //
    // 1. Passive. Envoy takes a cookie that's present in the cookies header and
    //    hashes on its value.
    //
    // 2. Generated. Envoy generates and sets a cookie with an expiration (TTL)
    //    on the first request from the client in its response to the client,
    //    based on the endpoint the request gets sent to. The client then
    //    presents this on the next and all subsequent requests. The hash of
    //    this is sufficient to ensure these requests get sent to the same
    //    endpoint. The cookie is generated by hashing the source and
    //    destination ports and addresses so that multiple independent HTTP2
    //    streams on the same connection will independently receive the same
    //    cookie, even if they arrive at the Envoy simultaneously.
    message Cookie {
      // The name of the cookie that will be used to obtain the hash key. If the
      // cookie is not present and ttl below is not set, no hash will be
      // produced.
      string name = 1 [(validate.rules).string = {min_bytes: 1}];

      // If specified, a cookie with the TTL will be generated if the cookie is
      // not present. If the TTL is present and zero, the generated cookie will
      // be a session cookie.
      google.protobuf.Duration ttl = 2;

      // The name of the path for the cookie. If no path is specified here, no path
      // will be set for the cookie.
      string path = 3;
    }

    message ConnectionProperties {
      // Hash on source IP address.
      bool source_ip = 1;
    }

    message QueryParameter {
      // The name of the URL query parameter that will be used to obtain the hash
      // key. If the parameter is not present, no hash will be produced. Query
      // parameter names are case-sensitive.
      string name = 1 [(validate.rules).string = {min_bytes: 1}];
    }

    message FilterState {
      // The name of the Object in the per-request filterState, which is an
      // Envoy::Hashable object. If there is no data associated with the key,
      // or the stored object is not Envoy::Hashable, no hash will be produced.
      string key = 1 [(validate.rules).string = {min_bytes: 1}];
    }

    oneof policy_specifier {
      option (validate.required) = true;

      // Header hash policy.
      Header header = 1;

      // Cookie hash policy.
      Cookie cookie = 2;

      // Connection properties hash policy.
      ConnectionProperties connection_properties = 3;

      // Query parameter hash policy.
      QueryParameter query_parameter = 5;

      // Filter state hash policy.
      FilterState filter_state = 6;
    }

    // The flag that short-circuits the hash computing. This field provides a
    // 'fallback' style of configuration: "if a terminal policy doesn't work,
    // fallback to rest of the policy list", it saves time when the terminal
    // policy works.
    //
    // If true, and there is already a hash computed, ignore rest of the
    // list of hash polices.
    // For example, if the following hash methods are configured:
    //
    //  ========= ========
    //  specifier terminal
    //  ========= ========
    //  Header A  true
    //  Header B  false
    //  Header C  false
    //  ========= ========
    //
    // The generateHash process ends if policy "header A" generates a hash, as
    // it's a terminal policy.
    bool terminal = 4;
  }

  // Allows enabling and disabling upgrades on a per-route basis.
  // This overrides any enabled/disabled upgrade filter chain specified in the
  // HttpConnectionManager
  // :ref:`upgrade_configs
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.upgrade_configs>`
  // but does not affect any custom filter chain specified there.
  message UpgradeConfig {
    // The case-insensitive name of this upgrade, e.g. "websocket".
    // For each upgrade type present in upgrade_configs, requests with
    // Upgrade: [upgrade_type] will be proxied upstream.
    string upgrade_type = 1
        [(validate.rules).string = {well_known_regex: HTTP_HEADER_VALUE strict: false}];

    // Determines if upgrades are available on this route. Defaults to true.
    google.protobuf.BoolValue enabled = 2;
  }

  reserved 12, 18, 19, 16, 22, 21;

  oneof cluster_specifier {
    option (validate.required) = true;

    // Indicates the upstream cluster to which the request should be routed
    // to.
    string cluster = 1 [(validate.rules).string = {min_bytes: 1}];

    // Envoy will determine the cluster to route to by reading the value of the
    // HTTP header named by cluster_header from the request headers. If the
    // header is not found or the referenced cluster does not exist, Envoy will
    // return a 404 response.
    //
    // .. attention::
    //
    //   Internally, Envoy always uses the HTTP/2 *:authority* header to represent the HTTP/1
    //   *Host* header. Thus, if attempting to match on *Host*, match on *:authority* instead.
    //
    // .. note::
    //
    //   If the header appears multiple times only the first value is used.
    string cluster_header = 2
        [(validate.rules).string = {min_bytes: 1 well_known_regex: HTTP_HEADER_NAME strict: false}];

    // Multiple upstream clusters can be specified for a given route. The
    // request is routed to one of the upstream clusters based on weights
    // assigned to each cluster. See
    // :ref:`traffic splitting <config_http_conn_man_route_table_traffic_splitting_split>`
    // for additional documentation.
    WeightedCluster weighted_clusters = 3;
  }

  // The HTTP status code to use when configured cluster is not found.
  // The default response code is 503 Service Unavailable.
  ClusterNotFoundResponseCode cluster_not_found_response_code = 20
      [(validate.rules).enum = {defined_only: true}];

  // Optional endpoint metadata match criteria used by the subset load balancer. Only endpoints
  // in the upstream cluster with metadata matching what's set in this field will be considered
  // for load balancing. If using :ref:`weighted_clusters
  // <envoy_api_field_route.RouteAction.weighted_clusters>`, metadata will be merged, with values
  // provided there taking precedence. The filter name should be specified as *envoy.lb*.
  core.Metadata metadata_match = 4;

  // Indicates that during forwarding, the matched prefix (or path) should be
  // swapped with this value. This option allows application URLs to be rooted
  // at a different path from those exposed at the reverse proxy layer. The router filter will
  // place the original path before rewrite into the :ref:`x-envoy-original-path
  // <config_http_filters_router_x-envoy-original-path>` header.
  //
  // Only one of *prefix_rewrite* or
  // :ref:`regex_rewrite <envoy_api_field_route.RouteAction.regex_rewrite>`
  // may be specified.
  //
  // .. attention::
  //
  //   Pay careful attention to the use of trailing slashes in the
  //   :ref:`route's match <envoy_api_field_route.Route.match>` prefix value.
  //   Stripping a prefix from a path requires multiple Routes to handle all cases. For example,
  //   rewriting */prefix* to */* and */prefix/etc* to */etc* cannot be done in a single
  //   :ref:`Route <envoy_api_msg_route.Route>`, as shown by the below config entries:
  //
  //   .. code-block:: yaml
  //
  //     - match:
  //         prefix: "/prefix/"
  //       route:
  //         prefix_rewrite: "/"
  //     - match:
  //         prefix: "/prefix"
  //       route:
  //         prefix_rewrite: "/"
  //
  //   Having above entries in the config, requests to */prefix* will be stripped to */*, while
  //   requests to */prefix/etc* will be stripped to */etc*.
  string prefix_rewrite = 5
      [(validate.rules).string = {well_known_regex: HTTP_HEADER_VALUE strict: false}];

  // Indicates that during forwarding, portions of the path that match the
  // pattern should be rewritten, even allowing the substitution of capture
  // groups from the pattern into the new path as specified by the rewrite
  // substitution string. This is useful to allow application paths to be
  // rewritten in a way that is aware of segments with variable content like
  // identifiers. The router filter will place the original path as it was
  // before the rewrite into the :ref:`x-envoy-original-path
  // <config_http_filters_router_x-envoy-original-path>` header.
  //
  // Only one of :ref:`prefix_rewrite <envoy_api_field_route.RouteAction.prefix_rewrite>`
  // or *regex_rewrite* may be specified.
  //
  // Examples using Google's `RE2 <https://github.com/google/re2>`_ engine:
  //
  // * The path pattern ``^/service/([^/]+)(/.*)$`` paired with a substitution
  //   string of ``\2/instance/\1`` would transform ``/service/foo/v1/api``
  //   into ``/v1/api/instance/foo``.
  //
  // * The pattern ``one`` paired with a substitution string of ``two`` would
  //   transform ``/xxx/one/yyy/one/zzz`` into ``/xxx/two/yyy/two/zzz``.
  //
  // * The pattern ``^(.*?)one(.*)$`` paired with a substitution string of
  //   ``\1two\2`` would replace only the first occurrence of ``one``,
  //   transforming path ``/xxx/one/yyy/one/zzz`` into ``/xxx/two/yyy/one/zzz``.
  //
  // * The pattern ``(?i)/xxx/`` paired with a substitution string of ``/yyy/``
  //   would do a case-insensitive match and transform path ``/aaa/XxX/bbb`` to
  //   ``/aaa/yyy/bbb``.
  type.matcher.RegexMatchAndSubstitute regex_rewrite = 32;

  oneof host_rewrite_specifier {
    // Indicates that during forwarding, the host header will be swapped with
    // this value.
    string host_rewrite = 6 [
      (validate.rules).string = {well_known_regex: HTTP_HEADER_VALUE strict: false},
      (udpa.annotations.field_migrate).rename = "host_rewrite_literal"
    ];

    // Indicates that during forwarding, the host header will be swapped with
    // the hostname of the upstream host chosen by the cluster manager. This
    // option is applicable only when the destination cluster for a route is of
    // type ``STRICT_DNS``,  ``LOGICAL_DNS`` or ``STATIC``. For ``STATIC`` clusters, the
    // hostname attribute of the endpoint must be configured. Setting this to true
    // with other cluster types has no effect.
    google.protobuf.BoolValue auto_host_rewrite = 7;

    // Indicates that during forwarding, the host header will be swapped with the content of given
    // downstream or :ref:`custom <config_http_conn_man_headers_custom_request_headers>` header.
    // If header value is empty, host header is left intact.
    //
    // .. attention::
    //
    //   Pay attention to the potential security implications of using this option. Provided header
    //   must come from trusted source.
    //
    // .. note::
    //
    //   If the header appears multiple times only the first value is used.
    string auto_host_rewrite_header = 29 [
      (validate.rules).string = {well_known_regex: HTTP_HEADER_NAME strict: false},
      (udpa.annotations.field_migrate).rename = "host_rewrite_header"
    ];
  }

  // Specifies the upstream timeout for the route. If not specified, the default is 15s. This
  // spans between the point at which the entire downstream request (i.e. end-of-stream) has been
  // processed and when the upstream response has been completely processed. A value of 0 will
  // disable the route's timeout.
  //
  // .. note::
  //
  //   This timeout includes all retries. See also
  //   :ref:`config_http_filters_router_x-envoy-upstream-rq-timeout-ms`,
  //   :ref:`config_http_filters_router_x-envoy-upstream-rq-per-try-timeout-ms`, and the
  //   :ref:`retry overview <arch_overview_http_routing_retry>`.
  google.protobuf.Duration timeout = 8;

  // Specifies the idle timeout for the route. If not specified, there is no per-route idle timeout,
  // although the connection manager wide :ref:`stream_idle_timeout
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.stream_idle_timeout>`
  // will still apply. A value of 0 will completely disable the route's idle timeout, even if a
  // connection manager stream idle timeout is configured.
  //
  // The idle timeout is distinct to :ref:`timeout
  // <envoy_api_field_route.RouteAction.timeout>`, which provides an upper bound
  // on the upstream response time; :ref:`idle_timeout
  // <envoy_api_field_route.RouteAction.idle_timeout>` instead bounds the amount
  // of time the request's stream may be idle.
  //
  // After header decoding, the idle timeout will apply on downstream and
  // upstream request events. Each time an encode/decode event for headers or
  // data is processed for the stream, the timer will be reset. If the timeout
  // fires, the stream is terminated with a 408 Request Timeout error code if no
  // upstream response header has been received, otherwise a stream reset
  // occurs.
  google.protobuf.Duration idle_timeout = 24;

  // Indicates that the route has a retry policy. Note that if this is set,
  // it'll take precedence over the virtual host level retry policy entirely
  // (e.g.: policies are not merged, most internal one becomes the enforced policy).
  RetryPolicy retry_policy = 9;

  // [#not-implemented-hide:]
  // Specifies the configuration for retry policy extension. Note that if this is set, it'll take
  // precedence over the virtual host level retry policy entirely (e.g.: policies are not merged,
  // most internal one becomes the enforced policy). :ref:`Retry policy <envoy_api_field_route.VirtualHost.retry_policy>`
  // should not be set if this field is used.
  google.protobuf.Any retry_policy_typed_config = 33;

  // Indicates that the route has a request mirroring policy.
  //
  // .. attention::
  //   This field has been deprecated in favor of `request_mirror_policies` which supports one or
  //   more mirroring policies.
  RequestMirrorPolicy request_mirror_policy = 10 [deprecated = true];

  // Indicates that the route has request mirroring policies.
  repeated RequestMirrorPolicy request_mirror_policies = 30;

  // Optionally specifies the :ref:`routing priority <arch_overview_http_routing_priority>`.
  core.RoutingPriority priority = 11 [(validate.rules).enum = {defined_only: true}];

  // Specifies a set of rate limit configurations that could be applied to the
  // route.
  repeated RateLimit rate_limits = 13;

  // Specifies if the rate limit filter should include the virtual host rate
  // limits. By default, if the route configured rate limits, the virtual host
  // :ref:`rate_limits <envoy_api_field_route.VirtualHost.rate_limits>` are not applied to the
  // request.
  google.protobuf.BoolValue include_vh_rate_limits = 14;

  // Specifies a list of hash policies to use for ring hash load balancing. Each
  // hash policy is evaluated individually and the combined result is used to
  // route the request. The method of combination is deterministic such that
  // identical lists of hash policies will produce the same hash. Since a hash
  // policy examines specific parts of a request, it can fail to produce a hash
  // (i.e. if the hashed header is not present). If (and only if) all configured
  // hash policies fail to generate a hash, no hash will be produced for
  // the route. In this case, the behavior is the same as if no hash policies
  // were specified (i.e. the ring hash load balancer will choose a random
  // backend). If a hash policy has the "terminal" attribute set to true, and
  // there is already a hash generated, the hash is returned immediately,
  // ignoring the rest of the hash policy list.
  repeated HashPolicy hash_policy = 15;

  // Indicates that the route has a CORS policy.
  CorsPolicy cors = 17;

  // If present, and the request is a gRPC request, use the
  // `grpc-timeout header <https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md>`_,
  // or its default value (infinity) instead of
  // :ref:`timeout <envoy_api_field_route.RouteAction.timeout>`, but limit the applied timeout
  // to the maximum value specified here. If configured as 0, the maximum allowed timeout for
  // gRPC requests is infinity. If not configured at all, the `grpc-timeout` header is not used
  // and gRPC requests time out like any other requests using
  // :ref:`timeout <envoy_api_field_route.RouteAction.timeout>` or its default.
  // This can be used to prevent unexpected upstream request timeouts due to potentially long
  // time gaps between gRPC request and response in gRPC streaming mode.
  //
  // .. note::
  //
  //    If a timeout is specified using :ref:`config_http_filters_router_x-envoy-upstream-rq-timeout-ms`, it takes
  //    precedence over `grpc-timeout header <https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md>`_, when
  //    both are present. See also
  //    :ref:`config_http_filters_router_x-envoy-upstream-rq-timeout-ms`,
  //    :ref:`config_http_filters_router_x-envoy-upstream-rq-per-try-timeout-ms`, and the
  //    :ref:`retry overview <arch_overview_http_routing_retry>`.
  google.protobuf.Duration max_grpc_timeout = 23;

  // If present, Envoy will adjust the timeout provided by the `grpc-timeout` header by subtracting
  // the provided duration from the header. This is useful in allowing Envoy to set its global
  // timeout to be less than that of the deadline imposed by the calling client, which makes it more
  // likely that Envoy will handle the timeout instead of having the call canceled by the client.
  // The offset will only be applied if the provided grpc_timeout is greater than the offset. This
  // ensures that the offset will only ever decrease the timeout and never set it to 0 (meaning
  // infinity).
  google.protobuf.Duration grpc_timeout_offset = 28;

  repeated UpgradeConfig upgrade_configs = 25;

  InternalRedirectAction internal_redirect_action = 26;

  // An internal redirect is handled, iff the number of previous internal redirects that a
  // downstream request has encountered is lower than this value, and
  // :ref:`internal_redirect_action <envoy_api_field_route.RouteAction.internal_redirect_action>`
  // is set to :ref:`HANDLE_INTERNAL_REDIRECT
  // <envoy_api_enum_value_route.RouteAction.InternalRedirectAction.HANDLE_INTERNAL_REDIRECT>`
  // In the case where a downstream request is bounced among multiple routes by internal redirect,
  // the first route that hits this threshold, or has
  // :ref:`internal_redirect_action <envoy_api_field_route.RouteAction.internal_redirect_action>`
  // set to
  // :ref:`PASS_THROUGH_INTERNAL_REDIRECT
  // <envoy_api_enum_value_route.RouteAction.InternalRedirectAction.PASS_THROUGH_INTERNAL_REDIRECT>`
  // will pass the redirect back to downstream.
  //
  // If not specified, at most one redirect will be followed.
  google.protobuf.UInt32Value max_internal_redirects = 31;

  // Indicates that the route has a hedge policy. Note that if this is set,
  // it'll take precedence over the virtual host level hedge policy entirely
  // (e.g.: policies are not merged, most internal one becomes the enforced policy).
  HedgePolicy hedge_policy = 27;
}

// HTTP retry :ref:`architecture overview <arch_overview_http_routing_retry>`.
// [#next-free-field: 11]
message RetryPolicy {
  message RetryPriority {
    string name = 1 [(validate.rules).string = {min_bytes: 1}];

    oneof config_type {
      google.protobuf.Struct config = 2 [deprecated = true];

      google.protobuf.Any typed_config = 3;
    }
  }

  message RetryHostPredicate {
    string name = 1 [(validate.rules).string = {min_bytes: 1}];

    oneof config_type {
      google.protobuf.Struct config = 2 [deprecated = true];

      google.protobuf.Any typed_config = 3;
    }
  }

  message RetryBackOff {
    // Specifies the base interval between retries. This parameter is required and must be greater
    // than zero. Values less than 1 ms are rounded up to 1 ms.
    // See :ref:`config_http_filters_router_x-envoy-max-retries` for a discussion of Envoy's
    // back-off algorithm.
    google.protobuf.Duration base_interval = 1 [(validate.rules).duration = {
      required: true
      gt {}
    }];

    // Specifies the maximum interval between retries. This parameter is optional, but must be
    // greater than or equal to the `base_interval` if set. The default is 10 times the
    // `base_interval`. See :ref:`config_http_filters_router_x-envoy-max-retries` for a discussion
    // of Envoy's back-off algorithm.
    google.protobuf.Duration max_interval = 2 [(validate.rules).duration = {gt {}}];
  }

  // Specifies the conditions under which retry takes place. These are the same
  // conditions documented for :ref:`config_http_filters_router_x-envoy-retry-on` and
  // :ref:`config_http_filters_router_x-envoy-retry-grpc-on`.
  string retry_on = 1;

  // Specifies the allowed number of retries. This parameter is optional and
  // defaults to 1. These are the same conditions documented for
  // :ref:`config_http_filters_router_x-envoy-max-retries`.
  google.protobuf.UInt32Value num_retries = 2;

  // Specifies a non-zero upstream timeout per retry attempt. This parameter is optional. The
  // same conditions documented for
  // :ref:`config_http_filters_router_x-envoy-upstream-rq-per-try-timeout-ms` apply.
  //
  // .. note::
  //
  //   If left unspecified, Envoy will use the global
  //   :ref:`route timeout <envoy_api_field_route.RouteAction.timeout>` for the request.
  //   Consequently, when using a :ref:`5xx <config_http_filters_router_x-envoy-retry-on>` based
  //   retry policy, a request that times out will not be retried as the total timeout budget
  //   would have been exhausted.
  google.protobuf.Duration per_try_timeout = 3;

  // Specifies an implementation of a RetryPriority which is used to determine the
  // distribution of load across priorities used for retries. Refer to
  // :ref:`retry plugin configuration <arch_overview_http_retry_plugins>` for more details.
  RetryPriority retry_priority = 4;

  // Specifies a collection of RetryHostPredicates that will be consulted when selecting a host
  // for retries. If any of the predicates reject the host, host selection will be reattempted.
  // Refer to :ref:`retry plugin configuration <arch_overview_http_retry_plugins>` for more
  // details.
  repeated RetryHostPredicate retry_host_predicate = 5;

  // The maximum number of times host selection will be reattempted before giving up, at which
  // point the host that was last selected will be routed to. If unspecified, this will default to
  // retrying once.
  int64 host_selection_retry_max_attempts = 6;

  // HTTP status codes that should trigger a retry in addition to those specified by retry_on.
  repeated uint32 retriable_status_codes = 7;

  // Specifies parameters that control retry back off. This parameter is optional, in which case the
  // default base interval is 25 milliseconds or, if set, the current value of the
  // `upstream.base_retry_backoff_ms` runtime parameter. The default maximum interval is 10 times
  // the base interval. The documentation for :ref:`config_http_filters_router_x-envoy-max-retries`
  // describes Envoy's back-off algorithm.
  RetryBackOff retry_back_off = 8;

  // HTTP response headers that trigger a retry if present in the response. A retry will be
  // triggered if any of the header matches match the upstream response headers.
  // The field is only consulted if 'retriable-headers' retry policy is active.
  repeated HeaderMatcher retriable_headers = 9;

  // HTTP headers which must be present in the request for retries to be attempted.
  repeated HeaderMatcher retriable_request_headers = 10;
}

// HTTP request hedging :ref:`architecture overview <arch_overview_http_routing_hedging>`.
message HedgePolicy {
  // Specifies the number of initial requests that should be sent upstream.
  // Must be at least 1.
  // Defaults to 1.
  // [#not-implemented-hide:]
  google.protobuf.UInt32Value initial_requests = 1 [(validate.rules).uint32 = {gte: 1}];

  // Specifies a probability that an additional upstream request should be sent
  // on top of what is specified by initial_requests.
  // Defaults to 0.
  // [#not-implemented-hide:]
  type.FractionalPercent additional_request_chance = 2;

  // Indicates that a hedged request should be sent when the per-try timeout is hit.
  // This means that a retry will be issued without resetting the original request, leaving multiple upstream requests in flight.
  // The first request to complete successfully will be the one returned to the caller.
  //
  // * At any time, a successful response (i.e. not triggering any of the retry-on conditions) would be returned to the client.
  // * Before per-try timeout, an error response (per retry-on conditions) would be retried immediately or returned ot the client
  //   if there are no more retries left.
  // * After per-try timeout, an error response would be discarded, as a retry in the form of a hedged request is already in progress.
  //
  // Note: For this to have effect, you must have a :ref:`RetryPolicy <envoy_api_msg_route.RetryPolicy>` that retries at least
  // one error code and specifies a maximum number of retries.
  //
  // Defaults to false.
  bool hedge_on_per_try_timeout = 3;
}

// [#next-free-field: 9]
message RedirectAction {
  enum RedirectResponseCode {
    // Moved Permanently HTTP Status Code - 301.
    MOVED_PERMANENTLY = 0;

    // Found HTTP Status Code - 302.
    FOUND = 1;

    // See Other HTTP Status Code - 303.
    SEE_OTHER = 2;

    // Temporary Redirect HTTP Status Code - 307.
    TEMPORARY_REDIRECT = 3;

    // Permanent Redirect HTTP Status Code - 308.
    PERMANENT_REDIRECT = 4;
  }

  // When the scheme redirection take place, the following rules apply:
  //  1. If the source URI scheme is `http` and the port is explicitly
  //     set to `:80`, the port will be removed after the redirection
  //  2. If the source URI scheme is `https` and the port is explicitly
  //     set to `:443`, the port will be removed after the redirection
  oneof scheme_rewrite_specifier {
    // The scheme portion of the URL will be swapped with "https".
    bool https_redirect = 4;

    // The scheme portion of the URL will be swapped with this value.
    string scheme_redirect = 7;
  }

  // The host portion of the URL will be swapped with this value.
  string host_redirect = 1
      [(validate.rules).string = {well_known_regex: HTTP_HEADER_VALUE strict: false}];

  // The port value of the URL will be swapped with this value.
  uint32 port_redirect = 8;

  oneof path_rewrite_specifier {
    // The path portion of the URL will be swapped with this value.
    // Please note that query string in path_redirect will override the
    // request's query string and will not be stripped.
    //
    // For example, let's say we have the following routes:
    //
    // - match: { path: "/old-path-1" }
    //   redirect: { path_redirect: "/new-path-1" }
    // - match: { path: "/old-path-2" }
    //   redirect: { path_redirect: "/new-path-2", strip-query: "true" }
    // - match: { path: "/old-path-3" }
    //   redirect: { path_redirect: "/new-path-3?foo=1", strip_query: "true" }
    //
    // 1. if request uri is "/old-path-1?bar=1", users will be redirected to "/new-path-1?bar=1"
    // 2. if request uri is "/old-path-2?bar=1", users will be redirected to "/new-path-2"
    // 3. if request uri is "/old-path-3?bar=1", users will be redirected to "/new-path-3?foo=1"
    string path_redirect = 2
        [(validate.rules).string = {well_known_regex: HTTP_HEADER_VALUE strict: false}];

    // Indicates that during redirection, the matched prefix (or path)
    // should be swapped with this value. This option allows redirect URLs be dynamically created
    // based on the request.
    //
    // .. attention::
    //
    //   Pay attention to the use of trailing slashes as mentioned in
    //   :ref:`RouteAction's prefix_rewrite <envoy_api_field_route.RouteAction.prefix_rewrite>`.
    string prefix_rewrite = 5
        [(validate.rules).string = {well_known_regex: HTTP_HEADER_VALUE strict: false}];
  }

  // The HTTP status code to use in the redirect response. The default response
  // code is MOVED_PERMANENTLY (301).
  RedirectResponseCode response_code = 3 [(validate.rules).enum = {defined_only: true}];

  // Indicates that during redirection, the query portion of the URL will
  // be removed. Default value is false.
  bool strip_query = 6;
}

message DirectResponseAction {
  // Specifies the HTTP response status to be returned.
  uint32 status = 1 [(validate.rules).uint32 = {lt: 600 gte: 100}];

  // Specifies the content of the response body. If this setting is omitted,
  // no body is included in the generated response.
  //
  // .. note::
  //
  //   Headers can be specified using *response_headers_to_add* in the enclosing
  //   :ref:`envoy_api_msg_route.Route`, :ref:`envoy_api_msg_RouteConfiguration` or
  //   :ref:`envoy_api_msg_route.VirtualHost`.
  core.DataSource body = 2;
}

message Decorator {
  // The operation name associated with the request matched to this route. If tracing is
  // enabled, this information will be used as the span name reported for this request.
  //
  // .. note::
  //
  //   For ingress (inbound) requests, or egress (outbound) responses, this value may be overridden
  //   by the :ref:`x-envoy-decorator-operation
  //   <config_http_filters_router_x-envoy-decorator-operation>` header.
  string operation = 1 [(validate.rules).string = {min_bytes: 1}];

  // Whether the decorated details should be propagated to the other party. The default is true.
  google.protobuf.BoolValue propagate = 2;
}

message Tracing {
  // Target percentage of requests managed by this HTTP connection manager that will be force
  // traced if the :ref:`x-client-trace-id <config_http_conn_man_headers_x-client-trace-id>`
  // header is set. This field is a direct analog for the runtime variable
  // 'tracing.client_enabled' in the :ref:`HTTP Connection Manager
  // <config_http_conn_man_runtime>`.
  // Default: 100%
  type.FractionalPercent client_sampling = 1;

  // Target percentage of requests managed by this HTTP connection manager that will be randomly
  // selected for trace generation, if not requested by the client or not forced. This field is
  // a direct analog for the runtime variable 'tracing.random_sampling' in the
  // :ref:`HTTP Connection Manager <config_http_conn_man_runtime>`.
  // Default: 100%
  type.FractionalPercent random_sampling = 2;

  // Target percentage of requests managed by this HTTP connection manager that will be traced
  // after all other sampling checks have been applied (client-directed, force tracing, random
  // sampling). This field functions as an upper limit on the total configured sampling rate. For
  // instance, setting client_sampling to 100% but overall_sampling to 1% will result in only 1%
  // of client requests with the appropriate headers to be force traced. This field is a direct
  // analog for the runtime variable 'tracing.global_enabled' in the
  // :ref:`HTTP Connection Manager <config_http_conn_man_runtime>`.
  // Default: 100%
  type.FractionalPercent overall_sampling = 3;

  // A list of custom tags with unique tag name to create tags for the active span.
  // It will take effect after merging with the :ref:`corresponding configuration
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.Tracing.custom_tags>`
  // configured in the HTTP connection manager. If two tags with the same name are configured
  // each in the HTTP connection manager and the route level, the one configured here takes
  // priority.
  repeated type.tracing.v2.CustomTag custom_tags = 4;
}

// A virtual cluster is a way of specifying a regex matching rule against
// certain important endpoints such that statistics are generated explicitly for
// the matched requests. The reason this is useful is that when doing
// prefix/path matching Envoy does not always know what the application
// considers to be an endpoint. Thus, it’s impossible for Envoy to generically
// emit per endpoint statistics. However, often systems have highly critical
// endpoints that they wish to get “perfect” statistics on. Virtual cluster
// statistics are perfect in the sense that they are emitted on the downstream
// side such that they include network level failures.
//
// Documentation for :ref:`virtual cluster statistics <config_http_filters_router_vcluster_stats>`.
//
// .. note::
//
//    Virtual clusters are a useful tool, but we do not recommend setting up a virtual cluster for
//    every application endpoint. This is both not easily maintainable and as well the matching and
//    statistics output are not free.
message VirtualCluster {
  // Specifies a regex pattern to use for matching requests. The entire path of the request
  // must match the regex. The regex grammar used is defined `here
  // <https://en.cppreference.com/w/cpp/regex/ecmascript>`_.
  //
  // Examples:
  //
  // * The regex ``/rides/\d+`` matches the path */rides/0*
  // * The regex ``/rides/\d+`` matches the path */rides/123*
  // * The regex ``/rides/\d+`` does not match the path */rides/123/456*
  //
  // .. attention::
  //   This field has been deprecated in favor of `headers` as it is not safe for use with
  //   untrusted input in all cases.
  string pattern = 1 [
    deprecated = true,
    (validate.rules).string = {max_bytes: 1024},
    (envoy.annotations.disallowed_by_default) = true
  ];

  // Specifies a list of header matchers to use for matching requests. Each specified header must
  // match. The pseudo-headers `:path` and `:method` can be used to match the request path and
  // method, respectively.
  repeated HeaderMatcher headers = 4;

  // Specifies the name of the virtual cluster. The virtual cluster name as well
  // as the virtual host name are used when emitting statistics. The statistics are emitted by the
  // router filter and are documented :ref:`here <config_http_filters_router_stats>`.
  string name = 2 [(validate.rules).string = {min_bytes: 1}];

  // Optionally specifies the HTTP method to match on. For example GET, PUT,
  // etc.
  //
  // .. attention::
  //   This field has been deprecated in favor of `headers`.
  core.RequestMethod method = 3
      [deprecated = true, (envoy.annotations.disallowed_by_default) = true];
}

// Global rate limiting :ref:`architecture overview <arch_overview_global_rate_limit>`.
message RateLimit {
  // [#next-free-field: 7]
  message Action {
    // The following descriptor entry is appended to the descriptor:
    //
    // .. code-block:: cpp
    //
    //   ("source_cluster", "<local service cluster>")
    //
    // <local service cluster> is derived from the :option:`--service-cluster` option.
    message SourceCluster {
    }

    // The following descriptor entry is appended to the descriptor:
    //
    // .. code-block:: cpp
    //
    //   ("destination_cluster", "<routed target cluster>")
    //
    // Once a request matches against a route table rule, a routed cluster is determined by one of
    // the following :ref:`route table configuration <envoy_api_msg_RouteConfiguration>`
    // settings:
    //
    // * :ref:`cluster <envoy_api_field_route.RouteAction.cluster>` indicates the upstream cluster
    //   to route to.
    // * :ref:`weighted_clusters <envoy_api_field_route.RouteAction.weighted_clusters>`
    //   chooses a cluster randomly from a set of clusters with attributed weight.
    // * :ref:`cluster_header <envoy_api_field_route.RouteAction.cluster_header>` indicates which
    //   header in the request contains the target cluster.
    message DestinationCluster {
    }

    // The following descriptor entry is appended when a header contains a key that matches the
    // *header_name*:
    //
    // .. code-block:: cpp
    //
    //   ("<descriptor_key>", "<header_value_queried_from_header>")
    message RequestHeaders {
      // The header name to be queried from the request headers. The header’s
      // value is used to populate the value of the descriptor entry for the
      // descriptor_key.
      string header_name = 1 [
        (validate.rules).string = {min_bytes: 1 well_known_regex: HTTP_HEADER_NAME strict: false}
      ];

      // The key to use in the descriptor entry.
      string descriptor_key = 2 [(validate.rules).string = {min_bytes: 1}];
    }

    // The following descriptor entry is appended to the descriptor and is populated using the
    // trusted address from :ref:`x-forwarded-for <config_http_conn_man_headers_x-forwarded-for>`:
    //
    // .. code-block:: cpp
    //
    //   ("remote_address", "<trusted address from x-forwarded-for>")
    message RemoteAddress {
    }

    // The following descriptor entry is appended to the descriptor:
    //
    // .. code-block:: cpp
    //
    //   ("generic_key", "<descriptor_value>")
    message GenericKey {
      // The value to use in the descriptor entry.
      string descriptor_value = 1 [(validate.rules).string = {min_bytes: 1}];
    }

    // The following descriptor entry is appended to the descriptor:
    //
    // .. code-block:: cpp
    //
    //   ("header_match", "<descriptor_value>")
    message HeaderValueMatch {
      // The value to use in the descriptor entry.
      string descriptor_value = 1 [(validate.rules).string = {min_bytes: 1}];

      // If set to true, the action will append a descriptor entry when the
      // request matches the headers. If set to false, the action will append a
      // descriptor entry when the request does not match the headers. The
      // default value is true.
      google.protobuf.BoolValue expect_match = 2;

      // Specifies a set of headers that the rate limit action should match
      // on. The action will check the request’s headers against all the
      // specified headers in the config. A match will happen if all the
      // headers in the config are present in the request with the same values
      // (or based on presence if the value field is not in the config).
      repeated HeaderMatcher headers = 3 [(validate.rules).repeated = {min_items: 1}];
    }

    oneof action_specifier {
      option (validate.required) = true;

      // Rate limit on source cluster.
      SourceCluster source_cluster = 1;

      // Rate limit on destination cluster.
      DestinationCluster destination_cluster = 2;

      // Rate limit on request headers.
      RequestHeaders request_headers = 3;

      // Rate limit on remote address.
      RemoteAddress remote_address = 4;

      // Rate limit on a generic key.
      GenericKey generic_key = 5;

      // Rate limit on the existence of request headers.
      HeaderValueMatch header_value_match = 6;
    }
  }

  // Refers to the stage set in the filter. The rate limit configuration only
  // applies to filters with the same stage number. The default stage number is
  // 0.
  //
  // .. note::
  //
  //   The filter supports a range of 0 - 10 inclusively for stage numbers.
  google.protobuf.UInt32Value stage = 1 [(validate.rules).uint32 = {lte: 10}];

  // The key to be set in runtime to disable this rate limit configuration.
  string disable_key = 2;

  // A list of actions that are to be applied for this rate limit configuration.
  // Order matters as the actions are processed sequentially and the descriptor
  // is composed by appending descriptor entries in that sequence. If an action
  // cannot append a descriptor entry, no descriptor is generated for the
  // configuration. See :ref:`composing actions
  // <config_http_filters_rate_limit_composing_actions>` for additional documentation.
  repeated Action actions = 3 [(validate.rules).repeated = {min_items: 1}];
}

// .. attention::
//
//   Internally, Envoy always uses the HTTP/2 *:authority* header to represent the HTTP/1 *Host*
//   header. Thus, if attempting to match on *Host*, match on *:authority* instead.
//
// .. attention::
//
//   To route on HTTP method, use the special HTTP/2 *:method* header. This works for both
//   HTTP/1 and HTTP/2 as Envoy normalizes headers. E.g.,
//
//   .. code-block:: json
//
//     {
//       "name": ":method",
//       "exact_match": "POST"
//     }
//
// .. attention::
//   In the absence of any header match specifier, match will default to :ref:`present_match
//   <envoy_api_field_route.HeaderMatcher.present_match>`. i.e, a request that has the :ref:`name
//   <envoy_api_field_route.HeaderMatcher.name>` header will match, regardless of the header's
//   value.
//
//  [#next-major-version: HeaderMatcher should be refactored to use StringMatcher.]
// [#next-free-field: 12]
message HeaderMatcher {
  reserved 2, 3;

  // Specifies the name of the header in the request.
  string name = 1
      [(validate.rules).string = {min_bytes: 1 well_known_regex: HTTP_HEADER_NAME strict: false}];

  // Specifies how the header match will be performed to route the request.
  oneof header_match_specifier {
    // If specified, header match will be performed based on the value of the header.
    string exact_match = 4;

    // If specified, this regex string is a regular expression rule which implies the entire request
    // header value must match the regex. The rule will not match if only a subsequence of the
    // request header value matches the regex. The regex grammar used in the value field is defined
    // `here <https://en.cppreference.com/w/cpp/regex/ecmascript>`_.
    //
    // Examples:
    //
    // * The regex ``\d{3}`` matches the value *123*
    // * The regex ``\d{3}`` does not match the value *1234*
    // * The regex ``\d{3}`` does not match the value *123.456*
    //
    // .. attention::
    //   This field has been deprecated in favor of `safe_regex_match` as it is not safe for use
    //   with untrusted input in all cases.
    string regex_match = 5 [
      deprecated = true,
      (validate.rules).string = {max_bytes: 1024},
      (envoy.annotations.disallowed_by_default) = true
    ];

    // If specified, this regex string is a regular expression rule which implies the entire request
    // header value must match the regex. The rule will not match if only a subsequence of the
    // request header value matches the regex.
    type.matcher.RegexMatcher safe_regex_match = 11;

    // If specified, header match will be performed based on range.
    // The rule will match if the request header value is within this range.
    // The entire request header value must represent an integer in base 10 notation: consisting of
    // an optional plus or minus sign followed by a sequence of digits. The rule will not match if
    // the header value does not represent an integer. Match will fail for empty values, floating
    // point numbers or if only a subsequence of the header value is an integer.
    //
    // Examples:
    //
    // * For range [-10,0), route will match for header value -1, but not for 0, "somestring", 10.9,
    //   "-1somestring"
    type.Int64Range range_match = 6;

    // If specified, header match will be performed based on whether the header is in the
    // request.
    bool present_match = 7;

    // If specified, header match will be performed based on the prefix of the header value.
    // Note: empty prefix is not allowed, please use present_match instead.
    //
    // Examples:
    //
    // * The prefix *abcd* matches the value *abcdxyz*, but not for *abcxyz*.
    string prefix_match = 9 [(validate.rules).string = {min_bytes: 1}];

    // If specified, header match will be performed based on the suffix of the header value.
    // Note: empty suffix is not allowed, please use present_match instead.
    //
    // Examples:
    //
    // * The suffix *abcd* matches the value *xyzabcd*, but not for *xyzbcd*.
    string suffix_match = 10 [(validate.rules).string = {min_bytes: 1}];
  }

  // If specified, the match result will be inverted before checking. Defaults to false.
  //
  // Examples:
  //
  // * The regex ``\d{3}`` does not match the value *1234*, so it will match when inverted.
  // * The range [-10,0) will match the value -1, so it will not match when inverted.
  bool invert_match = 8;
}

// Query parameter matching treats the query string of a request's :path header
// as an ampersand-separated list of keys and/or key=value elements.
// [#next-free-field: 7]
message QueryParameterMatcher {
  // Specifies the name of a key that must be present in the requested
  // *path*'s query string.
  string name = 1 [(validate.rules).string = {min_bytes: 1 max_bytes: 1024}];

  // Specifies the value of the key. If the value is absent, a request
  // that contains the key in its query string will match, whether the
  // key appears with a value (e.g., "?debug=true") or not (e.g., "?debug")
  //
  // ..attention::
  //   This field is deprecated. Use an `exact` match inside the `string_match` field.
  string value = 3 [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

  // Specifies whether the query parameter value is a regular expression.
  // Defaults to false. The entire query parameter value (i.e., the part to
  // the right of the equals sign in "key=value") must match the regex.
  // E.g., the regex ``\d+$`` will match *123* but not *a123* or *123a*.
  //
  // ..attention::
  //   This field is deprecated. Use a `safe_regex` match inside the `string_match` field.
  google.protobuf.BoolValue regex = 4
      [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

  oneof query_parameter_match_specifier {
    // Specifies whether a query parameter value should match against a string.
    type.matcher.StringMatcher string_match = 5 [(validate.rules).message = {required: true}];

    // Specifies whether a query parameter should be present.
    bool present_match = 6;
  }
}
syntax = "proto3";

package envoy.api.v2.route;

import public "envoy/api/v2/route/route_components.proto";

option java_package = "io.envoyproxy.envoy.api.v2.route";
option java_outer_classname = "RouteProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/route";
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/api/v2/core:pkg",
        "//envoy/type:pkg",
        "//envoy/type/matcher:pkg",
        "//envoy/type/tracing/v2:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.api.v2.auth;

import "envoy/api/v2/auth/common.proto";
import "envoy/api/v2/auth/secret.proto";

import "google/protobuf/duration.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.auth";
option java_outer_classname = "TlsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/auth";
option (udpa.annotations.file_migrate).move_to_package =
    "envoy.extensions.transport_sockets.tls.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: TLS transport socket]
// [#extension: envoy.transport_sockets.tls]
// The TLS contexts below provide the transport socket configuration for upstream/downstream TLS.

message UpstreamTlsContext {
  // Common TLS context settings.
  //
  // .. attention::
  //
  //   Server certificate verification is not enabled by default. Configure
  //   :ref:`trusted_ca<envoy_api_field_auth.CertificateValidationContext.trusted_ca>` to enable
  //   verification.
  CommonTlsContext common_tls_context = 1;

  // SNI string to use when creating TLS backend connections.
  string sni = 2 [(validate.rules).string = {max_bytes: 255}];

  // If true, server-initiated TLS renegotiation will be allowed.
  //
  // .. attention::
  //
  //   TLS renegotiation is considered insecure and shouldn't be used unless absolutely necessary.
  bool allow_renegotiation = 3;

  // Maximum number of session keys (Pre-Shared Keys for TLSv1.3+, Session IDs and Session Tickets
  // for TLSv1.2 and older) to store for the purpose of session resumption.
  //
  // Defaults to 1, setting this to 0 disables session resumption.
  google.protobuf.UInt32Value max_session_keys = 4;
}

// [#next-free-field: 8]
message DownstreamTlsContext {
  // Common TLS context settings.
  CommonTlsContext common_tls_context = 1;

  // If specified, Envoy will reject connections without a valid client
  // certificate.
  google.protobuf.BoolValue require_client_certificate = 2;

  // If specified, Envoy will reject connections without a valid and matching SNI.
  // [#not-implemented-hide:]
  google.protobuf.BoolValue require_sni = 3;

  oneof session_ticket_keys_type {
    // TLS session ticket key settings.
    TlsSessionTicketKeys session_ticket_keys = 4;

    // Config for fetching TLS session ticket keys via SDS API.
    SdsSecretConfig session_ticket_keys_sds_secret_config = 5;

    // Config for controlling stateless TLS session resumption: setting this to true will cause the TLS
    // server to not issue TLS session tickets for the purposes of stateless TLS session resumption.
    // If set to false, the TLS server will issue TLS session tickets and encrypt/decrypt them using
    // the keys specified through either :ref:`session_ticket_keys <envoy_api_field_auth.DownstreamTlsContext.session_ticket_keys>`
    // or :ref:`session_ticket_keys_sds_secret_config <envoy_api_field_auth.DownstreamTlsContext.session_ticket_keys_sds_secret_config>`.
    // If this config is set to false and no keys are explicitly configured, the TLS server will issue
    // TLS session tickets and encrypt/decrypt them using an internally-generated and managed key, with the
    // implication that sessions cannot be resumed across hot restarts or on different hosts.
    bool disable_stateless_session_resumption = 7;
  }

  // If specified, ``session_timeout`` will change the maximum lifetime (in seconds) of the TLS session.
  // Currently this value is used as a hint for the `TLS session ticket lifetime (for TLSv1.2) <https://tools.ietf.org/html/rfc5077#section-5.6>`_.
  // Only seconds can be specified (fractional seconds are ignored).
  google.protobuf.Duration session_timeout = 6 [(validate.rules).duration = {
    lt {seconds: 4294967296}
    gte {}
  }];
}

// TLS context shared by both client and server TLS contexts.
// [#next-free-field: 9]
message CommonTlsContext {
  message CombinedCertificateValidationContext {
    // How to validate peer certificates.
    CertificateValidationContext default_validation_context = 1
        [(validate.rules).message = {required: true}];

    // Config for fetching validation context via SDS API.
    SdsSecretConfig validation_context_sds_secret_config = 2
        [(validate.rules).message = {required: true}];
  }

  reserved 5;

  // TLS protocol versions, cipher suites etc.
  TlsParameters tls_params = 1;

  // :ref:`Multiple TLS certificates <arch_overview_ssl_cert_select>` can be associated with the
  // same context to allow both RSA and ECDSA certificates.
  //
  // Only a single TLS certificate is supported in client contexts. In server contexts, the first
  // RSA certificate is used for clients that only support RSA and the first ECDSA certificate is
  // used for clients that support ECDSA.
  repeated TlsCertificate tls_certificates = 2;

  // Configs for fetching TLS certificates via SDS API.
  repeated SdsSecretConfig tls_certificate_sds_secret_configs = 6
      [(validate.rules).repeated = {max_items: 1}];

  oneof validation_context_type {
    // How to validate peer certificates.
    CertificateValidationContext validation_context = 3;

    // Config for fetching validation context via SDS API.
    SdsSecretConfig validation_context_sds_secret_config = 7;

    // Combined certificate validation context holds a default CertificateValidationContext
    // and SDS config. When SDS server returns dynamic CertificateValidationContext, both dynamic
    // and default CertificateValidationContext are merged into a new CertificateValidationContext
    // for validation. This merge is done by Message::MergeFrom(), so dynamic
    // CertificateValidationContext overwrites singular fields in default
    // CertificateValidationContext, and concatenates repeated fields to default
    // CertificateValidationContext, and logical OR is applied to boolean fields.
    CombinedCertificateValidationContext combined_validation_context = 8;
  }

  // Supplies the list of ALPN protocols that the listener should expose. In
  // practice this is likely to be set to one of two values (see the
  // :ref:`codec_type
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.codec_type>`
  // parameter in the HTTP connection manager for more information):
  //
  // * "h2,http/1.1" If the listener is going to support both HTTP/2 and HTTP/1.1.
  // * "http/1.1" If the listener is only going to support HTTP/1.1.
  //
  // There is no default for this parameter. If empty, Envoy will not expose ALPN.
  repeated string alpn_protocols = 4;
}
syntax = "proto3";

package envoy.api.v2.auth;

import "udpa/annotations/migrate.proto";

import public "envoy/api/v2/auth/common.proto";
import public "envoy/api/v2/auth/secret.proto";
import public "envoy/api/v2/auth/tls.proto";

option java_package = "io.envoyproxy.envoy.api.v2.auth";
option java_outer_classname = "CertProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/auth";
option (udpa.annotations.file_migrate).move_to_package =
    "envoy.extensions.transport_sockets.tls.v3";
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/api/v2/core:pkg",
        "//envoy/type/matcher:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.api.v2.auth;

import "envoy/api/v2/auth/common.proto";
import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/config_source.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/sensitive.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.api.v2.auth";
option java_outer_classname = "SecretProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/auth";
option (udpa.annotations.file_migrate).move_to_package =
    "envoy.extensions.transport_sockets.tls.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Secrets configuration]

message GenericSecret {
  // Secret of generic type and is available to filters.
  core.DataSource secret = 1 [(udpa.annotations.sensitive) = true];
}

message SdsSecretConfig {
  // Name (FQDN, UUID, SPKI, SHA256, etc.) by which the secret can be uniquely referred to.
  // When both name and config are specified, then secret can be fetched and/or reloaded via
  // SDS. When only name is specified, then secret will be loaded from static resources.
  string name = 1;

  core.ConfigSource sds_config = 2;
}

// [#next-free-field: 6]
message Secret {
  // Name (FQDN, UUID, SPKI, SHA256, etc.) by which the secret can be uniquely referred to.
  string name = 1;

  oneof type {
    TlsCertificate tls_certificate = 2;

    TlsSessionTicketKeys session_ticket_keys = 3;

    CertificateValidationContext validation_context = 4;

    GenericSecret generic_secret = 5;
  }
}
syntax = "proto3";

package envoy.api.v2.auth;

import "envoy/api/v2/core/base.proto";
import "envoy/type/matcher/string.proto";

import "google/protobuf/any.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/sensitive.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.auth";
option java_outer_classname = "CommonProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/auth";
option (udpa.annotations.file_migrate).move_to_package =
    "envoy.extensions.transport_sockets.tls.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Common TLS configuration]

message TlsParameters {
  enum TlsProtocol {
    // Envoy will choose the optimal TLS version.
    TLS_AUTO = 0;

    // TLS 1.0
    TLSv1_0 = 1;

    // TLS 1.1
    TLSv1_1 = 2;

    // TLS 1.2
    TLSv1_2 = 3;

    // TLS 1.3
    TLSv1_3 = 4;
  }

  // Minimum TLS protocol version. By default, it's ``TLSv1_2`` for both clients and servers.
  TlsProtocol tls_minimum_protocol_version = 1 [(validate.rules).enum = {defined_only: true}];

  // Maximum TLS protocol version. By default, it's ``TLSv1_2`` for clients and ``TLSv1_3`` for
  // servers.
  TlsProtocol tls_maximum_protocol_version = 2 [(validate.rules).enum = {defined_only: true}];

  // If specified, the TLS listener will only support the specified `cipher list
  // <https://commondatastorage.googleapis.com/chromium-boringssl-docs/ssl.h.html#Cipher-suite-configuration>`_
  // when negotiating TLS 1.0-1.2 (this setting has no effect when negotiating TLS 1.3). If not
  // specified, the default list will be used.
  //
  // In non-FIPS builds, the default cipher list is:
  //
  // .. code-block:: none
  //
  //   [ECDHE-ECDSA-AES128-GCM-SHA256|ECDHE-ECDSA-CHACHA20-POLY1305]
  //   [ECDHE-RSA-AES128-GCM-SHA256|ECDHE-RSA-CHACHA20-POLY1305]
  //   ECDHE-ECDSA-AES128-SHA
  //   ECDHE-RSA-AES128-SHA
  //   AES128-GCM-SHA256
  //   AES128-SHA
  //   ECDHE-ECDSA-AES256-GCM-SHA384
  //   ECDHE-RSA-AES256-GCM-SHA384
  //   ECDHE-ECDSA-AES256-SHA
  //   ECDHE-RSA-AES256-SHA
  //   AES256-GCM-SHA384
  //   AES256-SHA
  //
  // In builds using :ref:`BoringSSL FIPS <arch_overview_ssl_fips>`, the default cipher list is:
  //
  // .. code-block:: none
  //
  //   ECDHE-ECDSA-AES128-GCM-SHA256
  //   ECDHE-RSA-AES128-GCM-SHA256
  //   ECDHE-ECDSA-AES128-SHA
  //   ECDHE-RSA-AES128-SHA
  //   AES128-GCM-SHA256
  //   AES128-SHA
  //   ECDHE-ECDSA-AES256-GCM-SHA384
  //   ECDHE-RSA-AES256-GCM-SHA384
  //   ECDHE-ECDSA-AES256-SHA
  //   ECDHE-RSA-AES256-SHA
  //   AES256-GCM-SHA384
  //   AES256-SHA
  repeated string cipher_suites = 3;

  // If specified, the TLS connection will only support the specified ECDH
  // curves. If not specified, the default curves will be used.
  //
  // In non-FIPS builds, the default curves are:
  //
  // .. code-block:: none
  //
  //   X25519
  //   P-256
  //
  // In builds using :ref:`BoringSSL FIPS <arch_overview_ssl_fips>`, the default curve is:
  //
  // .. code-block:: none
  //
  //   P-256
  repeated string ecdh_curves = 4;
}

// BoringSSL private key method configuration. The private key methods are used for external
// (potentially asynchronous) signing and decryption operations. Some use cases for private key
// methods would be TPM support and TLS acceleration.
message PrivateKeyProvider {
  // Private key method provider name. The name must match a
  // supported private key method provider type.
  string provider_name = 1 [(validate.rules).string = {min_bytes: 1}];

  // Private key method provider specific configuration.
  oneof config_type {
    google.protobuf.Struct config = 2 [deprecated = true, (udpa.annotations.sensitive) = true];

    google.protobuf.Any typed_config = 3 [(udpa.annotations.sensitive) = true];
  }
}

// [#next-free-field: 7]
message TlsCertificate {
  // The TLS certificate chain.
  core.DataSource certificate_chain = 1;

  // The TLS private key.
  core.DataSource private_key = 2 [(udpa.annotations.sensitive) = true];

  // BoringSSL private key method provider. This is an alternative to :ref:`private_key
  // <envoy_api_field_auth.TlsCertificate.private_key>` field. This can't be
  // marked as ``oneof`` due to API compatibility reasons. Setting both :ref:`private_key
  // <envoy_api_field_auth.TlsCertificate.private_key>` and
  // :ref:`private_key_provider
  // <envoy_api_field_auth.TlsCertificate.private_key_provider>` fields will result in an
  // error.
  PrivateKeyProvider private_key_provider = 6;

  // The password to decrypt the TLS private key. If this field is not set, it is assumed that the
  // TLS private key is not password encrypted.
  core.DataSource password = 3 [(udpa.annotations.sensitive) = true];

  // [#not-implemented-hide:]
  core.DataSource ocsp_staple = 4;

  // [#not-implemented-hide:]
  repeated core.DataSource signed_certificate_timestamp = 5;
}

message TlsSessionTicketKeys {
  // Keys for encrypting and decrypting TLS session tickets. The
  // first key in the array contains the key to encrypt all new sessions created by this context.
  // All keys are candidates for decrypting received tickets. This allows for easy rotation of keys
  // by, for example, putting the new key first, and the previous key second.
  //
  // If :ref:`session_ticket_keys <envoy_api_field_auth.DownstreamTlsContext.session_ticket_keys>`
  // is not specified, the TLS library will still support resuming sessions via tickets, but it will
  // use an internally-generated and managed key, so sessions cannot be resumed across hot restarts
  // or on different hosts.
  //
  // Each key must contain exactly 80 bytes of cryptographically-secure random data. For
  // example, the output of ``openssl rand 80``.
  //
  // .. attention::
  //
  //   Using this feature has serious security considerations and risks. Improper handling of keys
  //   may result in loss of secrecy in connections, even if ciphers supporting perfect forward
  //   secrecy are used. See https://www.imperialviolet.org/2013/06/27/botchingpfs.html for some
  //   discussion. To minimize the risk, you must:
  //
  //   * Keep the session ticket keys at least as secure as your TLS certificate private keys
  //   * Rotate session ticket keys at least daily, and preferably hourly
  //   * Always generate keys using a cryptographically-secure random data source
  repeated core.DataSource keys = 1
      [(validate.rules).repeated = {min_items: 1}, (udpa.annotations.sensitive) = true];
}

// [#next-free-field: 11]
message CertificateValidationContext {
  // Peer certificate verification mode.
  enum TrustChainVerification {
    // Perform default certificate verification (e.g., against CA / verification lists)
    VERIFY_TRUST_CHAIN = 0;

    // Connections where the certificate fails verification will be permitted.
    // For HTTP connections, the result of certificate verification can be used in route matching. (
    // see :ref:`validated <envoy_api_field_route.RouteMatch.TlsContextMatchOptions.validated>` ).
    ACCEPT_UNTRUSTED = 1;
  }

  // TLS certificate data containing certificate authority certificates to use in verifying
  // a presented peer certificate (e.g. server certificate for clusters or client certificate
  // for listeners). If not specified and a peer certificate is presented it will not be
  // verified. By default, a client certificate is optional, unless one of the additional
  // options (:ref:`require_client_certificate
  // <envoy_api_field_auth.DownstreamTlsContext.require_client_certificate>`,
  // :ref:`verify_certificate_spki
  // <envoy_api_field_auth.CertificateValidationContext.verify_certificate_spki>`,
  // :ref:`verify_certificate_hash
  // <envoy_api_field_auth.CertificateValidationContext.verify_certificate_hash>`, or
  // :ref:`match_subject_alt_names
  // <envoy_api_field_auth.CertificateValidationContext.match_subject_alt_names>`) is also
  // specified.
  //
  // It can optionally contain certificate revocation lists, in which case Envoy will verify
  // that the presented peer certificate has not been revoked by one of the included CRLs.
  //
  // See :ref:`the TLS overview <arch_overview_ssl_enabling_verification>` for a list of common
  // system CA locations.
  core.DataSource trusted_ca = 1;

  // An optional list of base64-encoded SHA-256 hashes. If specified, Envoy will verify that the
  // SHA-256 of the DER-encoded Subject Public Key Information (SPKI) of the presented certificate
  // matches one of the specified values.
  //
  // A base64-encoded SHA-256 of the Subject Public Key Information (SPKI) of the certificate
  // can be generated with the following command:
  //
  // .. code-block:: bash
  //
  //   $ openssl x509 -in path/to/client.crt -noout -pubkey
  //     | openssl pkey -pubin -outform DER
  //     | openssl dgst -sha256 -binary
  //     | openssl enc -base64
  //   NvqYIYSbgK2vCJpQhObf77vv+bQWtc5ek5RIOwPiC9A=
  //
  // This is the format used in HTTP Public Key Pinning.
  //
  // When both:
  // :ref:`verify_certificate_hash
  // <envoy_api_field_auth.CertificateValidationContext.verify_certificate_hash>` and
  // :ref:`verify_certificate_spki
  // <envoy_api_field_auth.CertificateValidationContext.verify_certificate_spki>` are specified,
  // a hash matching value from either of the lists will result in the certificate being accepted.
  //
  // .. attention::
  //
  //   This option is preferred over :ref:`verify_certificate_hash
  //   <envoy_api_field_auth.CertificateValidationContext.verify_certificate_hash>`,
  //   because SPKI is tied to a private key, so it doesn't change when the certificate
  //   is renewed using the same private key.
  repeated string verify_certificate_spki = 3
      [(validate.rules).repeated = {items {string {min_bytes: 44 max_bytes: 44}}}];

  // An optional list of hex-encoded SHA-256 hashes. If specified, Envoy will verify that
  // the SHA-256 of the DER-encoded presented certificate matches one of the specified values.
  //
  // A hex-encoded SHA-256 of the certificate can be generated with the following command:
  //
  // .. code-block:: bash
  //
  //   $ openssl x509 -in path/to/client.crt -outform DER | openssl dgst -sha256 | cut -d" " -f2
  //   df6ff72fe9116521268f6f2dd4966f51df479883fe7037b39f75916ac3049d1a
  //
  // A long hex-encoded and colon-separated SHA-256 (a.k.a. "fingerprint") of the certificate
  // can be generated with the following command:
  //
  // .. code-block:: bash
  //
  //   $ openssl x509 -in path/to/client.crt -noout -fingerprint -sha256 | cut -d"=" -f2
  //   DF:6F:F7:2F:E9:11:65:21:26:8F:6F:2D:D4:96:6F:51:DF:47:98:83:FE:70:37:B3:9F:75:91:6A:C3:04:9D:1A
  //
  // Both of those formats are acceptable.
  //
  // When both:
  // :ref:`verify_certificate_hash
  // <envoy_api_field_auth.CertificateValidationContext.verify_certificate_hash>` and
  // :ref:`verify_certificate_spki
  // <envoy_api_field_auth.CertificateValidationContext.verify_certificate_spki>` are specified,
  // a hash matching value from either of the lists will result in the certificate being accepted.
  repeated string verify_certificate_hash = 2
      [(validate.rules).repeated = {items {string {min_bytes: 64 max_bytes: 95}}}];

  // An optional list of Subject Alternative Names. If specified, Envoy will verify that the
  // Subject Alternative Name of the presented certificate matches one of the specified values.
  //
  // .. attention::
  //
  //   Subject Alternative Names are easily spoofable and verifying only them is insecure,
  //   therefore this option must be used together with :ref:`trusted_ca
  //   <envoy_api_field_auth.CertificateValidationContext.trusted_ca>`.
  repeated string verify_subject_alt_name = 4 [deprecated = true];

  // An optional list of Subject Alternative name matchers. Envoy will verify that the
  // Subject Alternative Name of the presented certificate matches one of the specified matches.
  //
  // When a certificate has wildcard DNS SAN entries, to match a specific client, it should be
  // configured with exact match type in the :ref:`string matcher <envoy_api_msg_type.matcher.StringMatcher>`.
  // For example if the certificate has "\*.example.com" as DNS SAN entry, to allow only "api.example.com",
  // it should be configured as shown below.
  //
  // .. code-block:: yaml
  //
  //  match_subject_alt_names:
  //    exact: "api.example.com"
  //
  // .. attention::
  //
  //   Subject Alternative Names are easily spoofable and verifying only them is insecure,
  //   therefore this option must be used together with :ref:`trusted_ca
  //   <envoy_api_field_auth.CertificateValidationContext.trusted_ca>`.
  repeated type.matcher.StringMatcher match_subject_alt_names = 9;

  // [#not-implemented-hide:] Must present a signed time-stamped OCSP response.
  google.protobuf.BoolValue require_ocsp_staple = 5;

  // [#not-implemented-hide:] Must present signed certificate time-stamp.
  google.protobuf.BoolValue require_signed_certificate_timestamp = 6;

  // An optional `certificate revocation list
  // <https://en.wikipedia.org/wiki/Certificate_revocation_list>`_
  // (in PEM format). If specified, Envoy will verify that the presented peer
  // certificate has not been revoked by this CRL. If this DataSource contains
  // multiple CRLs, all of them will be used.
  core.DataSource crl = 7;

  // If specified, Envoy will not reject expired certificates.
  bool allow_expired_certificate = 8;

  // Certificate trust chain verification mode.
  TrustChainVerification trust_chain_verification = 10
      [(validate.rules).enum = {defined_only: true}];
}
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/auth/tls.proto";
import "envoy/api/v2/cluster/circuit_breaker.proto";
import "envoy/api/v2/cluster/filter.proto";
import "envoy/api/v2/cluster/outlier_detection.proto";
import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/config_source.proto";
import "envoy/api/v2/core/health_check.proto";
import "envoy/api/v2/core/protocol.proto";
import "envoy/api/v2/endpoint.proto";
import "envoy/type/percent.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/wrappers.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "ClusterProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.cluster.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Cluster configuration]

// Configuration for a single upstream cluster.
// [#next-free-field: 48]
message Cluster {
  // Refer to :ref:`service discovery type <arch_overview_service_discovery_types>`
  // for an explanation on each type.
  enum DiscoveryType {
    // Refer to the :ref:`static discovery type<arch_overview_service_discovery_types_static>`
    // for an explanation.
    STATIC = 0;

    // Refer to the :ref:`strict DNS discovery
    // type<arch_overview_service_discovery_types_strict_dns>`
    // for an explanation.
    STRICT_DNS = 1;

    // Refer to the :ref:`logical DNS discovery
    // type<arch_overview_service_discovery_types_logical_dns>`
    // for an explanation.
    LOGICAL_DNS = 2;

    // Refer to the :ref:`service discovery type<arch_overview_service_discovery_types_eds>`
    // for an explanation.
    EDS = 3;

    // Refer to the :ref:`original destination discovery
    // type<arch_overview_service_discovery_types_original_destination>`
    // for an explanation.
    ORIGINAL_DST = 4;
  }

  // Refer to :ref:`load balancer type <arch_overview_load_balancing_types>` architecture
  // overview section for information on each type.
  enum LbPolicy {
    // Refer to the :ref:`round robin load balancing
    // policy<arch_overview_load_balancing_types_round_robin>`
    // for an explanation.
    ROUND_ROBIN = 0;

    // Refer to the :ref:`least request load balancing
    // policy<arch_overview_load_balancing_types_least_request>`
    // for an explanation.
    LEAST_REQUEST = 1;

    // Refer to the :ref:`ring hash load balancing
    // policy<arch_overview_load_balancing_types_ring_hash>`
    // for an explanation.
    RING_HASH = 2;

    // Refer to the :ref:`random load balancing
    // policy<arch_overview_load_balancing_types_random>`
    // for an explanation.
    RANDOM = 3;

    // Refer to the :ref:`original destination load balancing
    // policy<arch_overview_load_balancing_types_original_destination>`
    // for an explanation.
    //
    // .. attention::
    //
    //   **This load balancing policy is deprecated**. Use CLUSTER_PROVIDED instead.
    //
    ORIGINAL_DST_LB = 4 [deprecated = true, (envoy.annotations.disallowed_by_default_enum) = true];

    // Refer to the :ref:`Maglev load balancing policy<arch_overview_load_balancing_types_maglev>`
    // for an explanation.
    MAGLEV = 5;

    // This load balancer type must be specified if the configured cluster provides a cluster
    // specific load balancer. Consult the configured cluster's documentation for whether to set
    // this option or not.
    CLUSTER_PROVIDED = 6;

    // [#not-implemented-hide:] Use the new :ref:`load_balancing_policy
    // <envoy_api_field_Cluster.load_balancing_policy>` field to determine the LB policy.
    // [#next-major-version: In the v3 API, we should consider deprecating the lb_policy field
    // and instead using the new load_balancing_policy field as the one and only mechanism for
    // configuring this.]
    LOAD_BALANCING_POLICY_CONFIG = 7;
  }

  // When V4_ONLY is selected, the DNS resolver will only perform a lookup for
  // addresses in the IPv4 family. If V6_ONLY is selected, the DNS resolver will
  // only perform a lookup for addresses in the IPv6 family. If AUTO is
  // specified, the DNS resolver will first perform a lookup for addresses in
  // the IPv6 family and fallback to a lookup for addresses in the IPv4 family.
  // For cluster types other than
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>` and
  // :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>`,
  // this setting is
  // ignored.
  enum DnsLookupFamily {
    AUTO = 0;
    V4_ONLY = 1;
    V6_ONLY = 2;
  }

  enum ClusterProtocolSelection {
    // Cluster can only operate on one of the possible upstream protocols (HTTP1.1, HTTP2).
    // If :ref:`http2_protocol_options <envoy_api_field_Cluster.http2_protocol_options>` are
    // present, HTTP2 will be used, otherwise HTTP1.1 will be used.
    USE_CONFIGURED_PROTOCOL = 0;

    // Use HTTP1.1 or HTTP2, depending on which one is used on the downstream connection.
    USE_DOWNSTREAM_PROTOCOL = 1;
  }

  // TransportSocketMatch specifies what transport socket config will be used
  // when the match conditions are satisfied.
  message TransportSocketMatch {
    // The name of the match, used in stats generation.
    string name = 1 [(validate.rules).string = {min_len: 1}];

    // Optional endpoint metadata match criteria.
    // The connection to the endpoint with metadata matching what is set in this field
    // will use the transport socket configuration specified here.
    // The endpoint's metadata entry in *envoy.transport_socket_match* is used to match
    // against the values specified in this field.
    google.protobuf.Struct match = 2;

    // The configuration of the transport socket.
    core.TransportSocket transport_socket = 3;
  }

  // Extended cluster type.
  message CustomClusterType {
    // The type of the cluster to instantiate. The name must match a supported cluster type.
    string name = 1 [(validate.rules).string = {min_bytes: 1}];

    // Cluster specific configuration which depends on the cluster being instantiated.
    // See the supported cluster for further documentation.
    google.protobuf.Any typed_config = 2;
  }

  // Only valid when discovery type is EDS.
  message EdsClusterConfig {
    // Configuration for the source of EDS updates for this Cluster.
    core.ConfigSource eds_config = 1;

    // Optional alternative to cluster name to present to EDS. This does not
    // have the same restrictions as cluster name, i.e. it may be arbitrary
    // length.
    string service_name = 2;
  }

  // Optionally divide the endpoints in this cluster into subsets defined by
  // endpoint metadata and selected by route and weighted cluster metadata.
  // [#next-free-field: 8]
  message LbSubsetConfig {
    // If NO_FALLBACK is selected, a result
    // equivalent to no healthy hosts is reported. If ANY_ENDPOINT is selected,
    // any cluster endpoint may be returned (subject to policy, health checks,
    // etc). If DEFAULT_SUBSET is selected, load balancing is performed over the
    // endpoints matching the values from the default_subset field.
    enum LbSubsetFallbackPolicy {
      NO_FALLBACK = 0;
      ANY_ENDPOINT = 1;
      DEFAULT_SUBSET = 2;
    }

    // Specifications for subsets.
    message LbSubsetSelector {
      // Allows to override top level fallback policy per selector.
      enum LbSubsetSelectorFallbackPolicy {
        // If NOT_DEFINED top level config fallback policy is used instead.
        NOT_DEFINED = 0;

        // If NO_FALLBACK is selected, a result equivalent to no healthy hosts is reported.
        NO_FALLBACK = 1;

        // If ANY_ENDPOINT is selected, any cluster endpoint may be returned
        // (subject to policy, health checks, etc).
        ANY_ENDPOINT = 2;

        // If DEFAULT_SUBSET is selected, load balancing is performed over the
        // endpoints matching the values from the default_subset field.
        DEFAULT_SUBSET = 3;

        // If KEYS_SUBSET is selected, subset selector matching is performed again with metadata
        // keys reduced to
        // :ref:`fallback_keys_subset<envoy_api_field_Cluster.LbSubsetConfig.LbSubsetSelector.fallback_keys_subset>`.
        // It allows for a fallback to a different, less specific selector if some of the keys of
        // the selector are considered optional.
        KEYS_SUBSET = 4;
      }

      // List of keys to match with the weighted cluster metadata.
      repeated string keys = 1;

      // The behavior used when no endpoint subset matches the selected route's
      // metadata.
      LbSubsetSelectorFallbackPolicy fallback_policy = 2
          [(validate.rules).enum = {defined_only: true}];

      // Subset of
      // :ref:`keys<envoy_api_field_Cluster.LbSubsetConfig.LbSubsetSelector.keys>` used by
      // :ref:`KEYS_SUBSET<envoy_api_enum_value_Cluster.LbSubsetConfig.LbSubsetSelector.LbSubsetSelectorFallbackPolicy.KEYS_SUBSET>`
      // fallback policy.
      // It has to be a non empty list if KEYS_SUBSET fallback policy is selected.
      // For any other fallback policy the parameter is not used and should not be set.
      // Only values also present in
      // :ref:`keys<envoy_api_field_Cluster.LbSubsetConfig.LbSubsetSelector.keys>` are allowed, but
      // `fallback_keys_subset` cannot be equal to `keys`.
      repeated string fallback_keys_subset = 3;
    }

    // The behavior used when no endpoint subset matches the selected route's
    // metadata. The value defaults to
    // :ref:`NO_FALLBACK<envoy_api_enum_value_Cluster.LbSubsetConfig.LbSubsetFallbackPolicy.NO_FALLBACK>`.
    LbSubsetFallbackPolicy fallback_policy = 1 [(validate.rules).enum = {defined_only: true}];

    // Specifies the default subset of endpoints used during fallback if
    // fallback_policy is
    // :ref:`DEFAULT_SUBSET<envoy_api_enum_value_Cluster.LbSubsetConfig.LbSubsetFallbackPolicy.DEFAULT_SUBSET>`.
    // Each field in default_subset is
    // compared to the matching LbEndpoint.Metadata under the *envoy.lb*
    // namespace. It is valid for no hosts to match, in which case the behavior
    // is the same as a fallback_policy of
    // :ref:`NO_FALLBACK<envoy_api_enum_value_Cluster.LbSubsetConfig.LbSubsetFallbackPolicy.NO_FALLBACK>`.
    google.protobuf.Struct default_subset = 2;

    // For each entry, LbEndpoint.Metadata's
    // *envoy.lb* namespace is traversed and a subset is created for each unique
    // combination of key and value. For example:
    //
    // .. code-block:: json
    //
    //   { "subset_selectors": [
    //       { "keys": [ "version" ] },
    //       { "keys": [ "stage", "hardware_type" ] }
    //   ]}
    //
    // A subset is matched when the metadata from the selected route and
    // weighted cluster contains the same keys and values as the subset's
    // metadata. The same host may appear in multiple subsets.
    repeated LbSubsetSelector subset_selectors = 3;

    // If true, routing to subsets will take into account the localities and locality weights of the
    // endpoints when making the routing decision.
    //
    // There are some potential pitfalls associated with enabling this feature, as the resulting
    // traffic split after applying both a subset match and locality weights might be undesirable.
    //
    // Consider for example a situation in which you have 50/50 split across two localities X/Y
    // which have 100 hosts each without subsetting. If the subset LB results in X having only 1
    // host selected but Y having 100, then a lot more load is being dumped on the single host in X
    // than originally anticipated in the load balancing assignment delivered via EDS.
    bool locality_weight_aware = 4;

    // When used with locality_weight_aware, scales the weight of each locality by the ratio
    // of hosts in the subset vs hosts in the original subset. This aims to even out the load
    // going to an individual locality if said locality is disproportionately affected by the
    // subset predicate.
    bool scale_locality_weight = 5;

    // If true, when a fallback policy is configured and its corresponding subset fails to find
    // a host this will cause any host to be selected instead.
    //
    // This is useful when using the default subset as the fallback policy, given the default
    // subset might become empty. With this option enabled, if that happens the LB will attempt
    // to select a host from the entire cluster.
    bool panic_mode_any = 6;

    // If true, metadata specified for a metadata key will be matched against the corresponding
    // endpoint metadata if the endpoint metadata matches the value exactly OR it is a list value
    // and any of the elements in the list matches the criteria.
    bool list_as_any = 7;
  }

  // Specific configuration for the LeastRequest load balancing policy.
  message LeastRequestLbConfig {
    // The number of random healthy hosts from which the host with the fewest active requests will
    // be chosen. Defaults to 2 so that we perform two-choice selection if the field is not set.
    google.protobuf.UInt32Value choice_count = 1 [(validate.rules).uint32 = {gte: 2}];
  }

  // Specific configuration for the :ref:`RingHash<arch_overview_load_balancing_types_ring_hash>`
  // load balancing policy.
  message RingHashLbConfig {
    // The hash function used to hash hosts onto the ketama ring.
    enum HashFunction {
      // Use `xxHash <https://github.com/Cyan4973/xxHash>`_, this is the default hash function.
      XX_HASH = 0;

      // Use `MurmurHash2 <https://sites.google.com/site/murmurhash/>`_, this is compatible with
      // std:hash<string> in GNU libstdc++ 3.4.20 or above. This is typically the case when compiled
      // on Linux and not macOS.
      MURMUR_HASH_2 = 1;
    }

    reserved 2;

    // Minimum hash ring size. The larger the ring is (that is, the more hashes there are for each
    // provided host) the better the request distribution will reflect the desired weights. Defaults
    // to 1024 entries, and limited to 8M entries. See also
    // :ref:`maximum_ring_size<envoy_api_field_Cluster.RingHashLbConfig.maximum_ring_size>`.
    google.protobuf.UInt64Value minimum_ring_size = 1 [(validate.rules).uint64 = {lte: 8388608}];

    // The hash function used to hash hosts onto the ketama ring. The value defaults to
    // :ref:`XX_HASH<envoy_api_enum_value_Cluster.RingHashLbConfig.HashFunction.XX_HASH>`.
    HashFunction hash_function = 3 [(validate.rules).enum = {defined_only: true}];

    // Maximum hash ring size. Defaults to 8M entries, and limited to 8M entries, but can be lowered
    // to further constrain resource use. See also
    // :ref:`minimum_ring_size<envoy_api_field_Cluster.RingHashLbConfig.minimum_ring_size>`.
    google.protobuf.UInt64Value maximum_ring_size = 4 [(validate.rules).uint64 = {lte: 8388608}];
  }

  // Specific configuration for the
  // :ref:`Original Destination <arch_overview_load_balancing_types_original_destination>`
  // load balancing policy.
  message OriginalDstLbConfig {
    // When true, :ref:`x-envoy-original-dst-host
    // <config_http_conn_man_headers_x-envoy-original-dst-host>` can be used to override destination
    // address.
    //
    // .. attention::
    //
    //   This header isn't sanitized by default, so enabling this feature allows HTTP clients to
    //   route traffic to arbitrary hosts and/or ports, which may have serious security
    //   consequences.
    //
    // .. note::
    //
    //   If the header appears multiple times only the first value is used.
    bool use_http_header = 1;
  }

  // Common configuration for all load balancer implementations.
  // [#next-free-field: 8]
  message CommonLbConfig {
    // Configuration for :ref:`zone aware routing
    // <arch_overview_load_balancing_zone_aware_routing>`.
    message ZoneAwareLbConfig {
      // Configures percentage of requests that will be considered for zone aware routing
      // if zone aware routing is configured. If not specified, the default is 100%.
      // * :ref:`runtime values <config_cluster_manager_cluster_runtime_zone_routing>`.
      // * :ref:`Zone aware routing support <arch_overview_load_balancing_zone_aware_routing>`.
      type.Percent routing_enabled = 1;

      // Configures minimum upstream cluster size required for zone aware routing
      // If upstream cluster size is less than specified, zone aware routing is not performed
      // even if zone aware routing is configured. If not specified, the default is 6.
      // * :ref:`runtime values <config_cluster_manager_cluster_runtime_zone_routing>`.
      // * :ref:`Zone aware routing support <arch_overview_load_balancing_zone_aware_routing>`.
      google.protobuf.UInt64Value min_cluster_size = 2;

      // If set to true, Envoy will not consider any hosts when the cluster is in :ref:`panic
      // mode<arch_overview_load_balancing_panic_threshold>`. Instead, the cluster will fail all
      // requests as if all hosts are unhealthy. This can help avoid potentially overwhelming a
      // failing service.
      bool fail_traffic_on_panic = 3;
    }

    // Configuration for :ref:`locality weighted load balancing
    // <arch_overview_load_balancing_locality_weighted_lb>`
    message LocalityWeightedLbConfig {
    }

    // Common Configuration for all consistent hashing load balancers (MaglevLb, RingHashLb, etc.)
    message ConsistentHashingLbConfig {
      // If set to `true`, the cluster will use hostname instead of the resolved
      // address as the key to consistently hash to an upstream host. Only valid for StrictDNS clusters with hostnames which resolve to a single IP address.
      bool use_hostname_for_hashing = 1;
    }

    // Configures the :ref:`healthy panic threshold <arch_overview_load_balancing_panic_threshold>`.
    // If not specified, the default is 50%.
    // To disable panic mode, set to 0%.
    //
    // .. note::
    //   The specified percent will be truncated to the nearest 1%.
    type.Percent healthy_panic_threshold = 1;

    oneof locality_config_specifier {
      ZoneAwareLbConfig zone_aware_lb_config = 2;

      LocalityWeightedLbConfig locality_weighted_lb_config = 3;
    }

    // If set, all health check/weight/metadata updates that happen within this duration will be
    // merged and delivered in one shot when the duration expires. The start of the duration is when
    // the first update happens. This is useful for big clusters, with potentially noisy deploys
    // that might trigger excessive CPU usage due to a constant stream of healthcheck state changes
    // or metadata updates. The first set of updates to be seen apply immediately (e.g.: a new
    // cluster). Please always keep in mind that the use of sandbox technologies may change this
    // behavior.
    //
    // If this is not set, we default to a merge window of 1000ms. To disable it, set the merge
    // window to 0.
    //
    // Note: merging does not apply to cluster membership changes (e.g.: adds/removes); this is
    // because merging those updates isn't currently safe. See
    // https://github.com/envoyproxy/envoy/pull/3941.
    google.protobuf.Duration update_merge_window = 4;

    // If set to true, Envoy will not consider new hosts when computing load balancing weights until
    // they have been health checked for the first time. This will have no effect unless
    // active health checking is also configured.
    //
    // Ignoring a host means that for any load balancing calculations that adjust weights based
    // on the ratio of eligible hosts and total hosts (priority spillover, locality weighting and
    // panic mode) Envoy will exclude these hosts in the denominator.
    //
    // For example, with hosts in two priorities P0 and P1, where P0 looks like
    // {healthy, unhealthy (new), unhealthy (new)}
    // and where P1 looks like
    // {healthy, healthy}
    // all traffic will still hit P0, as 1 / (3 - 2) = 1.
    //
    // Enabling this will allow scaling up the number of hosts for a given cluster without entering
    // panic mode or triggering priority spillover, assuming the hosts pass the first health check.
    //
    // If panic mode is triggered, new hosts are still eligible for traffic; they simply do not
    // contribute to the calculation when deciding whether panic mode is enabled or not.
    bool ignore_new_hosts_until_first_hc = 5;

    // If set to `true`, the cluster manager will drain all existing
    // connections to upstream hosts whenever hosts are added or removed from the cluster.
    bool close_connections_on_host_set_change = 6;

    // Common Configuration for all consistent hashing load balancers (MaglevLb, RingHashLb, etc.)
    ConsistentHashingLbConfig consistent_hashing_lb_config = 7;
  }

  message RefreshRate {
    // Specifies the base interval between refreshes. This parameter is required and must be greater
    // than zero and less than
    // :ref:`max_interval <envoy_api_field_Cluster.RefreshRate.max_interval>`.
    google.protobuf.Duration base_interval = 1 [(validate.rules).duration = {
      required: true
      gt {nanos: 1000000}
    }];

    // Specifies the maximum interval between refreshes. This parameter is optional, but must be
    // greater than or equal to the
    // :ref:`base_interval <envoy_api_field_Cluster.RefreshRate.base_interval>`  if set. The default
    // is 10 times the :ref:`base_interval <envoy_api_field_Cluster.RefreshRate.base_interval>`.
    google.protobuf.Duration max_interval = 2 [(validate.rules).duration = {gt {nanos: 1000000}}];
  }

  reserved 12, 15;

  // Configuration to use different transport sockets for different endpoints.
  // The entry of *envoy.transport_socket_match* in the
  // :ref:`LbEndpoint.Metadata <envoy_api_field_endpoint.LbEndpoint.metadata>`
  // is used to match against the transport sockets as they appear in the list. The first
  // :ref:`match <envoy_api_msg_Cluster.TransportSocketMatch>` is used.
  // For example, with the following match
  //
  // .. code-block:: yaml
  //
  //  transport_socket_matches:
  //  - name: "enableMTLS"
  //    match:
  //      acceptMTLS: true
  //    transport_socket:
  //      name: envoy.transport_sockets.tls
  //      config: { ... } # tls socket configuration
  //  - name: "defaultToPlaintext"
  //    match: {}
  //    transport_socket:
  //      name: envoy.transport_sockets.raw_buffer
  //
  // Connections to the endpoints whose metadata value under *envoy.transport_socket_match*
  // having "acceptMTLS"/"true" key/value pair use the "enableMTLS" socket configuration.
  //
  // If a :ref:`socket match <envoy_api_msg_Cluster.TransportSocketMatch>` with empty match
  // criteria is provided, that always match any endpoint. For example, the "defaultToPlaintext"
  // socket match in case above.
  //
  // If an endpoint metadata's value under *envoy.transport_socket_match* does not match any
  // *TransportSocketMatch*, socket configuration fallbacks to use the *tls_context* or
  // *transport_socket* specified in this cluster.
  //
  // This field allows gradual and flexible transport socket configuration changes.
  //
  // The metadata of endpoints in EDS can indicate transport socket capabilities. For example,
  // an endpoint's metadata can have two key value pairs as "acceptMTLS": "true",
  // "acceptPlaintext": "true". While some other endpoints, only accepting plaintext traffic
  // has "acceptPlaintext": "true" metadata information.
  //
  // Then the xDS server can configure the CDS to a client, Envoy A, to send mutual TLS
  // traffic for endpoints with "acceptMTLS": "true", by adding a corresponding
  // *TransportSocketMatch* in this field. Other client Envoys receive CDS without
  // *transport_socket_match* set, and still send plain text traffic to the same cluster.
  //
  // [#comment:TODO(incfly): add a detailed architecture doc on intended usage.]
  repeated TransportSocketMatch transport_socket_matches = 43;

  // Supplies the name of the cluster which must be unique across all clusters.
  // The cluster name is used when emitting
  // :ref:`statistics <config_cluster_manager_cluster_stats>` if :ref:`alt_stat_name
  // <envoy_api_field_Cluster.alt_stat_name>` is not provided.
  // Any ``:`` in the cluster name will be converted to ``_`` when emitting statistics.
  string name = 1 [(validate.rules).string = {min_bytes: 1}];

  // An optional alternative to the cluster name to be used while emitting stats.
  // Any ``:`` in the name will be converted to ``_`` when emitting statistics. This should not be
  // confused with :ref:`Router Filter Header
  // <config_http_filters_router_x-envoy-upstream-alt-stat-name>`.
  string alt_stat_name = 28;

  oneof cluster_discovery_type {
    // The :ref:`service discovery type <arch_overview_service_discovery_types>`
    // to use for resolving the cluster.
    DiscoveryType type = 2 [(validate.rules).enum = {defined_only: true}];

    // The custom cluster type.
    CustomClusterType cluster_type = 38;
  }

  // Configuration to use for EDS updates for the Cluster.
  EdsClusterConfig eds_cluster_config = 3;

  // The timeout for new network connections to hosts in the cluster.
  google.protobuf.Duration connect_timeout = 4 [(validate.rules).duration = {gt {}}];

  // Soft limit on size of the cluster’s connections read and write buffers. If
  // unspecified, an implementation defined default is applied (1MiB).
  google.protobuf.UInt32Value per_connection_buffer_limit_bytes = 5;

  // The :ref:`load balancer type <arch_overview_load_balancing_types>` to use
  // when picking a host in the cluster.
  LbPolicy lb_policy = 6 [(validate.rules).enum = {defined_only: true}];

  // If the service discovery type is
  // :ref:`STATIC<envoy_api_enum_value_Cluster.DiscoveryType.STATIC>`,
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>`
  // or :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>`,
  // then hosts is required.
  //
  // .. attention::
  //
  //   **This field is deprecated**. Set the
  //   :ref:`load_assignment<envoy_api_field_Cluster.load_assignment>` field instead.
  //
  repeated core.Address hosts = 7 [deprecated = true];

  // Setting this is required for specifying members of
  // :ref:`STATIC<envoy_api_enum_value_Cluster.DiscoveryType.STATIC>`,
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>`
  // or :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>` clusters.
  // This field supersedes the *hosts* field in the v2 API.
  //
  // .. attention::
  //
  //   Setting this allows non-EDS cluster types to contain embedded EDS equivalent
  //   :ref:`endpoint assignments<envoy_api_msg_ClusterLoadAssignment>`.
  //
  ClusterLoadAssignment load_assignment = 33;

  // Optional :ref:`active health checking <arch_overview_health_checking>`
  // configuration for the cluster. If no
  // configuration is specified no health checking will be done and all cluster
  // members will be considered healthy at all times.
  repeated core.HealthCheck health_checks = 8;

  // Optional maximum requests for a single upstream connection. This parameter
  // is respected by both the HTTP/1.1 and HTTP/2 connection pool
  // implementations. If not specified, there is no limit. Setting this
  // parameter to 1 will effectively disable keep alive.
  google.protobuf.UInt32Value max_requests_per_connection = 9;

  // Optional :ref:`circuit breaking <arch_overview_circuit_break>` for the cluster.
  cluster.CircuitBreakers circuit_breakers = 10;

  // The TLS configuration for connections to the upstream cluster.
  //
  // .. attention::
  //
  //   **This field is deprecated**. Use `transport_socket` with name `tls` instead. If both are
  //   set, `transport_socket` takes priority.
  auth.UpstreamTlsContext tls_context = 11
      [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

  // HTTP protocol options that are applied only to upstream HTTP connections.
  // These options apply to all HTTP versions.
  core.UpstreamHttpProtocolOptions upstream_http_protocol_options = 46;

  // Additional options when handling HTTP requests upstream. These options will be applicable to
  // both HTTP1 and HTTP2 requests.
  core.HttpProtocolOptions common_http_protocol_options = 29;

  // Additional options when handling HTTP1 requests.
  core.Http1ProtocolOptions http_protocol_options = 13;

  // Even if default HTTP2 protocol options are desired, this field must be
  // set so that Envoy will assume that the upstream supports HTTP/2 when
  // making new HTTP connection pool connections. Currently, Envoy only
  // supports prior knowledge for upstream connections. Even if TLS is used
  // with ALPN, `http2_protocol_options` must be specified. As an aside this allows HTTP/2
  // connections to happen over plain text.
  core.Http2ProtocolOptions http2_protocol_options = 14;

  // The extension_protocol_options field is used to provide extension-specific protocol options
  // for upstream connections. The key should match the extension filter name, such as
  // "envoy.filters.network.thrift_proxy". See the extension's documentation for details on
  // specific options.
  map<string, google.protobuf.Struct> extension_protocol_options = 35
      [deprecated = true, (envoy.annotations.disallowed_by_default) = true];

  // The extension_protocol_options field is used to provide extension-specific protocol options
  // for upstream connections. The key should match the extension filter name, such as
  // "envoy.filters.network.thrift_proxy". See the extension's documentation for details on
  // specific options.
  map<string, google.protobuf.Any> typed_extension_protocol_options = 36;

  // If the DNS refresh rate is specified and the cluster type is either
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>`,
  // or :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>`,
  // this value is used as the cluster’s DNS refresh
  // rate. The value configured must be at least 1ms. If this setting is not specified, the
  // value defaults to 5000ms. For cluster types other than
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>`
  // and :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>`
  // this setting is ignored.
  google.protobuf.Duration dns_refresh_rate = 16
      [(validate.rules).duration = {gt {nanos: 1000000}}];

  // If the DNS failure refresh rate is specified and the cluster type is either
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>`,
  // or :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>`,
  // this is used as the cluster’s DNS refresh rate when requests are failing. If this setting is
  // not specified, the failure refresh rate defaults to the DNS refresh rate. For cluster types
  // other than :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>` and
  // :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>` this setting is
  // ignored.
  RefreshRate dns_failure_refresh_rate = 44;

  // Optional configuration for setting cluster's DNS refresh rate. If the value is set to true,
  // cluster's DNS refresh rate will be set to resource record's TTL which comes from DNS
  // resolution.
  bool respect_dns_ttl = 39;

  // The DNS IP address resolution policy. If this setting is not specified, the
  // value defaults to
  // :ref:`AUTO<envoy_api_enum_value_Cluster.DnsLookupFamily.AUTO>`.
  DnsLookupFamily dns_lookup_family = 17 [(validate.rules).enum = {defined_only: true}];

  // If DNS resolvers are specified and the cluster type is either
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>`,
  // or :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>`,
  // this value is used to specify the cluster’s dns resolvers.
  // If this setting is not specified, the value defaults to the default
  // resolver, which uses /etc/resolv.conf for configuration. For cluster types
  // other than
  // :ref:`STRICT_DNS<envoy_api_enum_value_Cluster.DiscoveryType.STRICT_DNS>`
  // and :ref:`LOGICAL_DNS<envoy_api_enum_value_Cluster.DiscoveryType.LOGICAL_DNS>`
  // this setting is ignored.
  // Setting this value causes failure if the
  // ``envoy.restart_features.use_apple_api_for_dns_lookups`` runtime value is true during
  // server startup. Apple's API only allows overriding DNS resolvers via system settings.
  repeated core.Address dns_resolvers = 18;

  // [#next-major-version: Reconcile DNS options in a single message.]
  // Always use TCP queries instead of UDP queries for DNS lookups.
  // Setting this value causes failure if the
  // ``envoy.restart_features.use_apple_api_for_dns_lookups`` runtime value is true during
  // server startup. Apple' API only uses UDP for DNS resolution.
  bool use_tcp_for_dns_lookups = 45;

  // If specified, outlier detection will be enabled for this upstream cluster.
  // Each of the configuration values can be overridden via
  // :ref:`runtime values <config_cluster_manager_cluster_runtime_outlier_detection>`.
  cluster.OutlierDetection outlier_detection = 19;

  // The interval for removing stale hosts from a cluster type
  // :ref:`ORIGINAL_DST<envoy_api_enum_value_Cluster.DiscoveryType.ORIGINAL_DST>`.
  // Hosts are considered stale if they have not been used
  // as upstream destinations during this interval. New hosts are added
  // to original destination clusters on demand as new connections are
  // redirected to Envoy, causing the number of hosts in the cluster to
  // grow over time. Hosts that are not stale (they are actively used as
  // destinations) are kept in the cluster, which allows connections to
  // them remain open, saving the latency that would otherwise be spent
  // on opening new connections. If this setting is not specified, the
  // value defaults to 5000ms. For cluster types other than
  // :ref:`ORIGINAL_DST<envoy_api_enum_value_Cluster.DiscoveryType.ORIGINAL_DST>`
  // this setting is ignored.
  google.protobuf.Duration cleanup_interval = 20 [(validate.rules).duration = {gt {}}];

  // Optional configuration used to bind newly established upstream connections.
  // This overrides any bind_config specified in the bootstrap proto.
  // If the address and port are empty, no bind will be performed.
  core.BindConfig upstream_bind_config = 21;

  // Configuration for load balancing subsetting.
  LbSubsetConfig lb_subset_config = 22;

  // Optional configuration for the load balancing algorithm selected by
  // LbPolicy. Currently only
  // :ref:`RING_HASH<envoy_api_enum_value_Cluster.LbPolicy.RING_HASH>` and
  // :ref:`LEAST_REQUEST<envoy_api_enum_value_Cluster.LbPolicy.LEAST_REQUEST>`
  // has additional configuration options.
  // Specifying ring_hash_lb_config or least_request_lb_config without setting the corresponding
  // LbPolicy will generate an error at runtime.
  oneof lb_config {
    // Optional configuration for the Ring Hash load balancing policy.
    RingHashLbConfig ring_hash_lb_config = 23;

    // Optional configuration for the Original Destination load balancing policy.
    OriginalDstLbConfig original_dst_lb_config = 34;

    // Optional configuration for the LeastRequest load balancing policy.
    LeastRequestLbConfig least_request_lb_config = 37;
  }

  // Common configuration for all load balancer implementations.
  CommonLbConfig common_lb_config = 27;

  // Optional custom transport socket implementation to use for upstream connections.
  // To setup TLS, set a transport socket with name `tls` and
  // :ref:`UpstreamTlsContexts <envoy_api_msg_auth.UpstreamTlsContext>` in the `typed_config`.
  // If no transport socket configuration is specified, new connections
  // will be set up with plaintext.
  core.TransportSocket transport_socket = 24;

  // The Metadata field can be used to provide additional information about the
  // cluster. It can be used for stats, logging, and varying filter behavior.
  // Fields should use reverse DNS notation to denote which entity within Envoy
  // will need the information. For instance, if the metadata is intended for
  // the Router filter, the filter name should be specified as *envoy.filters.http.router*.
  core.Metadata metadata = 25;

  // Determines how Envoy selects the protocol used to speak to upstream hosts.
  ClusterProtocolSelection protocol_selection = 26;

  // Optional options for upstream connections.
  UpstreamConnectionOptions upstream_connection_options = 30;

  // If an upstream host becomes unhealthy (as determined by the configured health checks
  // or outlier detection), immediately close all connections to the failed host.
  //
  // .. note::
  //
  //   This is currently only supported for connections created by tcp_proxy.
  //
  // .. note::
  //
  //   The current implementation of this feature closes all connections immediately when
  //   the unhealthy status is detected. If there are a large number of connections open
  //   to an upstream host that becomes unhealthy, Envoy may spend a substantial amount of
  //   time exclusively closing these connections, and not processing any other traffic.
  bool close_connections_on_host_health_failure = 31;

  // If set to true, Envoy will ignore the health value of a host when processing its removal
  // from service discovery. This means that if active health checking is used, Envoy will *not*
  // wait for the endpoint to go unhealthy before removing it.
  bool drain_connections_on_host_removal = 32
      [(udpa.annotations.field_migrate).rename = "ignore_health_on_host_removal"];

  // An (optional) network filter chain, listed in the order the filters should be applied.
  // The chain will be applied to all outgoing connections that Envoy makes to the upstream
  // servers of this cluster.
  repeated cluster.Filter filters = 40;

  // [#not-implemented-hide:] New mechanism for LB policy configuration. Used only if the
  // :ref:`lb_policy<envoy_api_field_Cluster.lb_policy>` field has the value
  // :ref:`LOAD_BALANCING_POLICY_CONFIG<envoy_api_enum_value_Cluster.LbPolicy.LOAD_BALANCING_POLICY_CONFIG>`.
  LoadBalancingPolicy load_balancing_policy = 41;

  // [#not-implemented-hide:]
  // If present, tells the client where to send load reports via LRS. If not present, the
  // client will fall back to a client-side default, which may be either (a) don't send any
  // load reports or (b) send load reports for all clusters to a single default server
  // (which may be configured in the bootstrap file).
  //
  // Note that if multiple clusters point to the same LRS server, the client may choose to
  // create a separate stream for each cluster or it may choose to coalesce the data for
  // multiple clusters onto a single stream. Either way, the client must make sure to send
  // the data for any given cluster on no more than one stream.
  //
  // [#next-major-version: In the v3 API, we should consider restructuring this somehow,
  // maybe by allowing LRS to go on the ADS stream, or maybe by moving some of the negotiation
  // from the LRS stream here.]
  core.ConfigSource lrs_server = 42;

  // If track_timeout_budgets is true, the :ref:`timeout budget histograms
  // <config_cluster_manager_cluster_stats_timeout_budgets>` will be published for each
  // request. These show what percentage of a request's per try and global timeout was used. A value
  // of 0 would indicate that none of the timeout was used or that the timeout was infinite. A value
  // of 100 would indicate that the request took the entirety of the timeout given to it.
  bool track_timeout_budgets = 47;
}

// [#not-implemented-hide:] Extensible load balancing policy configuration.
//
// Every LB policy defined via this mechanism will be identified via a unique name using reverse
// DNS notation. If the policy needs configuration parameters, it must define a message for its
// own configuration, which will be stored in the config field. The name of the policy will tell
// clients which type of message they should expect to see in the config field.
//
// Note that there are cases where it is useful to be able to independently select LB policies
// for choosing a locality and for choosing an endpoint within that locality. For example, a
// given deployment may always use the same policy to choose the locality, but for choosing the
// endpoint within the locality, some clusters may use weighted-round-robin, while others may
// use some sort of session-based balancing.
//
// This can be accomplished via hierarchical LB policies, where the parent LB policy creates a
// child LB policy for each locality. For each request, the parent chooses the locality and then
// delegates to the child policy for that locality to choose the endpoint within the locality.
//
// To facilitate this, the config message for the top-level LB policy may include a field of
// type LoadBalancingPolicy that specifies the child policy.
message LoadBalancingPolicy {
  message Policy {
    // Required. The name of the LB policy.
    string name = 1;

    // Optional config for the LB policy.
    // No more than one of these two fields may be populated.
    google.protobuf.Struct config = 2 [deprecated = true];

    google.protobuf.Any typed_config = 3;
  }

  // Each client will iterate over the list in order and stop at the first policy that it
  // supports. This provides a mechanism for starting to use new LB policies that are not yet
  // supported by all clients.
  repeated Policy policies = 1;
}

// An extensible structure containing the address Envoy should bind to when
// establishing upstream connections.
message UpstreamBindConfig {
  // The address Envoy should bind to when establishing upstream connections.
  core.Address source_address = 1;
}

message UpstreamConnectionOptions {
  // If set then set SO_KEEPALIVE on the socket to enable TCP Keepalives.
  core.TcpKeepalive tcp_keepalive = 1;
}
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

import public "envoy/api/v2/scoped_route.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "SrdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.route.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: SRDS]
// * Routing :ref:`architecture overview <arch_overview_http_routing>`

// The Scoped Routes Discovery Service (SRDS) API distributes
// :ref:`ScopedRouteConfiguration<envoy_api_msg.ScopedRouteConfiguration>`
// resources. Each ScopedRouteConfiguration resource represents a "routing
// scope" containing a mapping that allows the HTTP connection manager to
// dynamically assign a routing table (specified via a
// :ref:`RouteConfiguration<envoy_api_msg_RouteConfiguration>` message) to each
// HTTP request.
service ScopedRoutesDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.api.v2.ScopedRouteConfiguration";

  rpc StreamScopedRoutes(stream DiscoveryRequest) returns (stream DiscoveryResponse) {
  }

  rpc DeltaScopedRoutes(stream DeltaDiscoveryRequest) returns (stream DeltaDiscoveryResponse) {
  }

  rpc FetchScopedRoutes(DiscoveryRequest) returns (DiscoveryResponse) {
    option (google.api.http).post = "/v2/discovery:scoped-routes";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message SrdsDummy {
}
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/config_source.proto";
import "envoy/api/v2/route/route_components.proto";

import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "RouteProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.route.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: HTTP route configuration]
// * Routing :ref:`architecture overview <arch_overview_http_routing>`
// * HTTP :ref:`router filter <config_http_filters_router>`

// [#next-free-field: 11]
message RouteConfiguration {
  // The name of the route configuration. For example, it might match
  // :ref:`route_config_name
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.Rds.route_config_name>` in
  // :ref:`envoy_api_msg_config.filter.network.http_connection_manager.v2.Rds`.
  string name = 1;

  // An array of virtual hosts that make up the route table.
  repeated route.VirtualHost virtual_hosts = 2;

  // An array of virtual hosts will be dynamically loaded via the VHDS API.
  // Both *virtual_hosts* and *vhds* fields will be used when present. *virtual_hosts* can be used
  // for a base routing table or for infrequently changing virtual hosts. *vhds* is used for
  // on-demand discovery of virtual hosts. The contents of these two fields will be merged to
  // generate a routing table for a given RouteConfiguration, with *vhds* derived configuration
  // taking precedence.
  Vhds vhds = 9;

  // Optionally specifies a list of HTTP headers that the connection manager
  // will consider to be internal only. If they are found on external requests they will be cleaned
  // prior to filter invocation. See :ref:`config_http_conn_man_headers_x-envoy-internal` for more
  // information.
  repeated string internal_only_headers = 3 [
    (validate.rules).repeated = {items {string {well_known_regex: HTTP_HEADER_NAME strict: false}}}
  ];

  // Specifies a list of HTTP headers that should be added to each response that
  // the connection manager encodes. Headers specified at this level are applied
  // after headers from any enclosed :ref:`envoy_api_msg_route.VirtualHost` or
  // :ref:`envoy_api_msg_route.RouteAction`. For more information, including details on
  // header value syntax, see the documentation on :ref:`custom request headers
  // <config_http_conn_man_headers_custom_request_headers>`.
  repeated core.HeaderValueOption response_headers_to_add = 4
      [(validate.rules).repeated = {max_items: 1000}];

  // Specifies a list of HTTP headers that should be removed from each response
  // that the connection manager encodes.
  repeated string response_headers_to_remove = 5 [
    (validate.rules).repeated = {items {string {well_known_regex: HTTP_HEADER_NAME strict: false}}}
  ];

  // Specifies a list of HTTP headers that should be added to each request
  // routed by the HTTP connection manager. Headers specified at this level are
  // applied after headers from any enclosed :ref:`envoy_api_msg_route.VirtualHost` or
  // :ref:`envoy_api_msg_route.RouteAction`. For more information, including details on
  // header value syntax, see the documentation on :ref:`custom request headers
  // <config_http_conn_man_headers_custom_request_headers>`.
  repeated core.HeaderValueOption request_headers_to_add = 6
      [(validate.rules).repeated = {max_items: 1000}];

  // Specifies a list of HTTP headers that should be removed from each request
  // routed by the HTTP connection manager.
  repeated string request_headers_to_remove = 8 [
    (validate.rules).repeated = {items {string {well_known_regex: HTTP_HEADER_NAME strict: false}}}
  ];

  // By default, headers that should be added/removed are evaluated from most to least specific:
  //
  // * route level
  // * virtual host level
  // * connection manager level
  //
  // To allow setting overrides at the route or virtual host level, this order can be reversed
  // by setting this option to true. Defaults to false.
  //
  // [#next-major-version: In the v3 API, this will default to true.]
  bool most_specific_header_mutations_wins = 10;

  // An optional boolean that specifies whether the clusters that the route
  // table refers to will be validated by the cluster manager. If set to true
  // and a route refers to a non-existent cluster, the route table will not
  // load. If set to false and a route refers to a non-existent cluster, the
  // route table will load and the router filter will return a 404 if the route
  // is selected at runtime. This setting defaults to true if the route table
  // is statically defined via the :ref:`route_config
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.route_config>`
  // option. This setting default to false if the route table is loaded dynamically via the
  // :ref:`rds
  // <envoy_api_field_config.filter.network.http_connection_manager.v2.HttpConnectionManager.rds>`
  // option. Users may wish to override the default behavior in certain cases (for example when
  // using CDS with a static route table).
  google.protobuf.BoolValue validate_clusters = 7;
}

message Vhds {
  // Configuration source specifier for VHDS.
  core.ConfigSource config_source = 1 [(validate.rules).message = {required: true}];
}
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/endpoint/endpoint_components.proto";
import "envoy/type/percent.proto";

import "google/protobuf/duration.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "EndpointProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.endpoint.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Endpoint configuration]
// Endpoint discovery :ref:`architecture overview <arch_overview_service_discovery_types_eds>`

// Each route from RDS will map to a single cluster or traffic split across
// clusters using weights expressed in the RDS WeightedCluster.
//
// With EDS, each cluster is treated independently from a LB perspective, with
// LB taking place between the Localities within a cluster and at a finer
// granularity between the hosts within a locality. The percentage of traffic
// for each endpoint is determined by both its load_balancing_weight, and the
// load_balancing_weight of its locality. First, a locality will be selected,
// then an endpoint within that locality will be chose based on its weight.
// [#next-free-field: 6]
message ClusterLoadAssignment {
  // Load balancing policy settings.
  // [#next-free-field: 6]
  message Policy {
    // [#not-implemented-hide:]
    message DropOverload {
      // Identifier for the policy specifying the drop.
      string category = 1 [(validate.rules).string = {min_bytes: 1}];

      // Percentage of traffic that should be dropped for the category.
      type.FractionalPercent drop_percentage = 2;
    }

    reserved 1;

    // Action to trim the overall incoming traffic to protect the upstream
    // hosts. This action allows protection in case the hosts are unable to
    // recover from an outage, or unable to autoscale or unable to handle
    // incoming traffic volume for any reason.
    //
    // At the client each category is applied one after the other to generate
    // the 'actual' drop percentage on all outgoing traffic. For example:
    //
    // .. code-block:: json
    //
    //  { "drop_overloads": [
    //      { "category": "throttle", "drop_percentage": 60 }
    //      { "category": "lb", "drop_percentage": 50 }
    //  ]}
    //
    // The actual drop percentages applied to the traffic at the clients will be
    //    "throttle"_drop = 60%
    //    "lb"_drop = 20%  // 50% of the remaining 'actual' load, which is 40%.
    //    actual_outgoing_load = 20% // remaining after applying all categories.
    // [#not-implemented-hide:]
    repeated DropOverload drop_overloads = 2;

    // Priority levels and localities are considered overprovisioned with this
    // factor (in percentage). This means that we don't consider a priority
    // level or locality unhealthy until the percentage of healthy hosts
    // multiplied by the overprovisioning factor drops below 100.
    // With the default value 140(1.4), Envoy doesn't consider a priority level
    // or a locality unhealthy until their percentage of healthy hosts drops
    // below 72%. For example:
    //
    // .. code-block:: json
    //
    //  { "overprovisioning_factor": 100 }
    //
    // Read more at :ref:`priority levels <arch_overview_load_balancing_priority_levels>` and
    // :ref:`localities <arch_overview_load_balancing_locality_weighted_lb>`.
    google.protobuf.UInt32Value overprovisioning_factor = 3 [(validate.rules).uint32 = {gt: 0}];

    // The max time until which the endpoints from this assignment can be used.
    // If no new assignments are received before this time expires the endpoints
    // are considered stale and should be marked unhealthy.
    // Defaults to 0 which means endpoints never go stale.
    google.protobuf.Duration endpoint_stale_after = 4 [(validate.rules).duration = {gt {}}];

    // The flag to disable overprovisioning. If it is set to true,
    // :ref:`overprovisioning factor
    // <arch_overview_load_balancing_overprovisioning_factor>` will be ignored
    // and Envoy will not perform graceful failover between priority levels or
    // localities as endpoints become unhealthy. Otherwise Envoy will perform
    // graceful failover as :ref:`overprovisioning factor
    // <arch_overview_load_balancing_overprovisioning_factor>` suggests.
    // [#not-implemented-hide:]
    bool disable_overprovisioning = 5 [deprecated = true];
  }

  // Name of the cluster. This will be the :ref:`service_name
  // <envoy_api_field_Cluster.EdsClusterConfig.service_name>` value if specified
  // in the cluster :ref:`EdsClusterConfig
  // <envoy_api_msg_Cluster.EdsClusterConfig>`.
  string cluster_name = 1 [(validate.rules).string = {min_bytes: 1}];

  // List of endpoints to load balance to.
  repeated endpoint.LocalityLbEndpoints endpoints = 2;

  // Map of named endpoints that can be referenced in LocalityLbEndpoints.
  // [#not-implemented-hide:]
  map<string, endpoint.Endpoint> named_endpoints = 5;

  // Load balancing policy settings.
  Policy policy = 4;
}
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

import public "envoy/api/v2/cluster.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "CdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.cluster.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: CDS]

// Return list of all clusters this proxy will load balance to.
service ClusterDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.api.v2.Cluster";

  rpc StreamClusters(stream DiscoveryRequest) returns (stream DiscoveryResponse) {
  }

  rpc DeltaClusters(stream DeltaDiscoveryRequest) returns (stream DeltaDiscoveryResponse) {
  }

  rpc FetchClusters(DiscoveryRequest) returns (DiscoveryResponse) {
    option (google.api.http).post = "/v2/discovery:clusters";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message CdsDummy {
}
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/socket_option.proto";
import "envoy/api/v2/listener/listener_components.proto";
import "envoy/api/v2/listener/udp_listener_config.proto";
import "envoy/config/filter/accesslog/v2/accesslog.proto";
import "envoy/config/listener/v2/api_listener.proto";

import "google/protobuf/duration.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "ListenerProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.listener.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Listener configuration]
// Listener :ref:`configuration overview <config_listeners>`

// [#next-free-field: 23]
message Listener {
  enum DrainType {
    // Drain in response to calling /healthcheck/fail admin endpoint (along with the health check
    // filter), listener removal/modification, and hot restart.
    DEFAULT = 0;

    // Drain in response to listener removal/modification and hot restart. This setting does not
    // include /healthcheck/fail. This setting may be desirable if Envoy is hosting both ingress
    // and egress listeners.
    MODIFY_ONLY = 1;
  }

  // [#not-implemented-hide:]
  message DeprecatedV1 {
    // Whether the listener should bind to the port. A listener that doesn't
    // bind can only receive connections redirected from other listeners that
    // set use_original_dst parameter to true. Default is true.
    //
    // This is deprecated in v2, all Listeners will bind to their port. An
    // additional filter chain must be created for every original destination
    // port this listener may redirect to in v2, with the original port
    // specified in the FilterChainMatch destination_port field.
    //
    // [#comment:TODO(PiotrSikora): Remove this once verified that we no longer need it.]
    google.protobuf.BoolValue bind_to_port = 1;
  }

  // Configuration for listener connection balancing.
  message ConnectionBalanceConfig {
    // A connection balancer implementation that does exact balancing. This means that a lock is
    // held during balancing so that connection counts are nearly exactly balanced between worker
    // threads. This is "nearly" exact in the sense that a connection might close in parallel thus
    // making the counts incorrect, but this should be rectified on the next accept. This balancer
    // sacrifices accept throughput for accuracy and should be used when there are a small number of
    // connections that rarely cycle (e.g., service mesh gRPC egress).
    message ExactBalance {
    }

    oneof balance_type {
      option (validate.required) = true;

      // If specified, the listener will use the exact connection balancer.
      ExactBalance exact_balance = 1;
    }
  }

  reserved 14;

  // The unique name by which this listener is known. If no name is provided,
  // Envoy will allocate an internal UUID for the listener. If the listener is to be dynamically
  // updated or removed via :ref:`LDS <config_listeners_lds>` a unique name must be provided.
  string name = 1;

  // The address that the listener should listen on. In general, the address must be unique, though
  // that is governed by the bind rules of the OS. E.g., multiple listeners can listen on port 0 on
  // Linux as the actual port will be allocated by the OS.
  core.Address address = 2 [(validate.rules).message = {required: true}];

  // A list of filter chains to consider for this listener. The
  // :ref:`FilterChain <envoy_api_msg_listener.FilterChain>` with the most specific
  // :ref:`FilterChainMatch <envoy_api_msg_listener.FilterChainMatch>` criteria is used on a
  // connection.
  //
  // Example using SNI for filter chain selection can be found in the
  // :ref:`FAQ entry <faq_how_to_setup_sni>`.
  repeated listener.FilterChain filter_chains = 3;

  // If a connection is redirected using *iptables*, the port on which the proxy
  // receives it might be different from the original destination address. When this flag is set to
  // true, the listener hands off redirected connections to the listener associated with the
  // original destination address. If there is no listener associated with the original destination
  // address, the connection is handled by the listener that receives it. Defaults to false.
  //
  // .. attention::
  //
  //   This field is deprecated. Use :ref:`an original_dst <config_listener_filters_original_dst>`
  //   :ref:`listener filter <envoy_api_field_Listener.listener_filters>` instead.
  //
  //   Note that hand off to another listener is *NOT* performed without this flag. Once
  //   :ref:`FilterChainMatch <envoy_api_msg_listener.FilterChainMatch>` is implemented this flag
  //   will be removed, as filter chain matching can be used to select a filter chain based on the
  //   restored destination address.
  google.protobuf.BoolValue use_original_dst = 4 [deprecated = true];

  // Soft limit on size of the listener’s new connection read and write buffers.
  // If unspecified, an implementation defined default is applied (1MiB).
  google.protobuf.UInt32Value per_connection_buffer_limit_bytes = 5;

  // Listener metadata.
  core.Metadata metadata = 6;

  // [#not-implemented-hide:]
  DeprecatedV1 deprecated_v1 = 7;

  // The type of draining to perform at a listener-wide level.
  DrainType drain_type = 8;

  // Listener filters have the opportunity to manipulate and augment the connection metadata that
  // is used in connection filter chain matching, for example. These filters are run before any in
  // :ref:`filter_chains <envoy_api_field_Listener.filter_chains>`. Order matters as the
  // filters are processed sequentially right after a socket has been accepted by the listener, and
  // before a connection is created.
  // UDP Listener filters can be specified when the protocol in the listener socket address in
  // :ref:`protocol <envoy_api_field_core.SocketAddress.protocol>` is :ref:`UDP
  // <envoy_api_enum_value_core.SocketAddress.Protocol.UDP>`.
  // UDP listeners currently support a single filter.
  repeated listener.ListenerFilter listener_filters = 9;

  // The timeout to wait for all listener filters to complete operation. If the timeout is reached,
  // the accepted socket is closed without a connection being created unless
  // `continue_on_listener_filters_timeout` is set to true. Specify 0 to disable the
  // timeout. If not specified, a default timeout of 15s is used.
  google.protobuf.Duration listener_filters_timeout = 15;

  // Whether a connection should be created when listener filters timeout. Default is false.
  //
  // .. attention::
  //
  //   Some listener filters, such as :ref:`Proxy Protocol filter
  //   <config_listener_filters_proxy_protocol>`, should not be used with this option. It will cause
  //   unexpected behavior when a connection is created.
  bool continue_on_listener_filters_timeout = 17;

  // Whether the listener should be set as a transparent socket.
  // When this flag is set to true, connections can be redirected to the listener using an
  // *iptables* *TPROXY* target, in which case the original source and destination addresses and
  // ports are preserved on accepted connections. This flag should be used in combination with
  // :ref:`an original_dst <config_listener_filters_original_dst>` :ref:`listener filter
  // <envoy_api_field_Listener.listener_filters>` to mark the connections' local addresses as
  // "restored." This can be used to hand off each redirected connection to another listener
  // associated with the connection's destination address. Direct connections to the socket without
  // using *TPROXY* cannot be distinguished from connections redirected using *TPROXY* and are
  // therefore treated as if they were redirected.
  // When this flag is set to false, the listener's socket is explicitly reset as non-transparent.
  // Setting this flag requires Envoy to run with the *CAP_NET_ADMIN* capability.
  // When this flag is not set (default), the socket is not modified, i.e. the transparent option
  // is neither set nor reset.
  google.protobuf.BoolValue transparent = 10;

  // Whether the listener should set the *IP_FREEBIND* socket option. When this
  // flag is set to true, listeners can be bound to an IP address that is not
  // configured on the system running Envoy. When this flag is set to false, the
  // option *IP_FREEBIND* is disabled on the socket. When this flag is not set
  // (default), the socket is not modified, i.e. the option is neither enabled
  // nor disabled.
  google.protobuf.BoolValue freebind = 11;

  // Additional socket options that may not be present in Envoy source code or
  // precompiled binaries.
  repeated core.SocketOption socket_options = 13;

  // Whether the listener should accept TCP Fast Open (TFO) connections.
  // When this flag is set to a value greater than 0, the option TCP_FASTOPEN is enabled on
  // the socket, with a queue length of the specified size
  // (see `details in RFC7413 <https://tools.ietf.org/html/rfc7413#section-5.1>`_).
  // When this flag is set to 0, the option TCP_FASTOPEN is disabled on the socket.
  // When this flag is not set (default), the socket is not modified,
  // i.e. the option is neither enabled nor disabled.
  //
  // On Linux, the net.ipv4.tcp_fastopen kernel parameter must include flag 0x2 to enable
  // TCP_FASTOPEN.
  // See `ip-sysctl.txt <https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt>`_.
  //
  // On macOS, only values of 0, 1, and unset are valid; other values may result in an error.
  // To set the queue length on macOS, set the net.inet.tcp.fastopen_backlog kernel parameter.
  google.protobuf.UInt32Value tcp_fast_open_queue_length = 12;

  // Specifies the intended direction of the traffic relative to the local Envoy.
  // This property is required on Windows for listeners using the original destination filter,
  // see :ref:`Original Destination <config_listener_filters_original_dst>`.
  core.TrafficDirection traffic_direction = 16;

  // If the protocol in the listener socket address in :ref:`protocol
  // <envoy_api_field_core.SocketAddress.protocol>` is :ref:`UDP
  // <envoy_api_enum_value_core.SocketAddress.Protocol.UDP>`, this field specifies the actual udp
  // listener to create, i.e. :ref:`udp_listener_name
  // <envoy_api_field_listener.UdpListenerConfig.udp_listener_name>` = "raw_udp_listener" for
  // creating a packet-oriented UDP listener. If not present, treat it as "raw_udp_listener".
  listener.UdpListenerConfig udp_listener_config = 18;

  // Used to represent an API listener, which is used in non-proxy clients. The type of API
  // exposed to the non-proxy application depends on the type of API listener.
  // When this field is set, no other field except for :ref:`name<envoy_api_field_Listener.name>`
  // should be set.
  //
  // .. note::
  //
  //  Currently only one ApiListener can be installed; and it can only be done via bootstrap config,
  //  not LDS.
  //
  // [#next-major-version: In the v3 API, instead of this messy approach where the socket
  // listener fields are directly in the top-level Listener message and the API listener types
  // are in the ApiListener message, the socket listener messages should be in their own message,
  // and the top-level Listener should essentially be a oneof that selects between the
  // socket listener and the various types of API listener. That way, a given Listener message
  // can structurally only contain the fields of the relevant type.]
  config.listener.v2.ApiListener api_listener = 19;

  // The listener's connection balancer configuration, currently only applicable to TCP listeners.
  // If no configuration is specified, Envoy will not attempt to balance active connections between
  // worker threads.
  ConnectionBalanceConfig connection_balance_config = 20;

  // When this flag is set to true, listeners set the *SO_REUSEPORT* socket option and
  // create one socket for each worker thread. This makes inbound connections
  // distribute among worker threads roughly evenly in cases where there are a high number
  // of connections. When this flag is set to false, all worker threads share one socket.
  //
  // Before Linux v4.19-rc1, new TCP connections may be rejected during hot restart
  // (see `3rd paragraph in 'soreuseport' commit message
  // <https://github.com/torvalds/linux/commit/c617f398edd4db2b8567a28e89>`_).
  // This issue was fixed by `tcp: Avoid TCP syncookie rejected by SO_REUSEPORT socket
  // <https://github.com/torvalds/linux/commit/40a1227ea845a37ab197dd1caffb60b047fa36b1>`_.
  bool reuse_port = 21;

  // Configuration for :ref:`access logs <arch_overview_access_logs>`
  // emitted by this listener.
  repeated config.filter.accesslog.v2.AccessLog access_log = 22;
}
Protocol buffer definitions for xDS and top-level resource API messages.

Package group `//envoy/api/v2:friends` enumerates all consumers of the shared
API messages. That includes package envoy.api.v2 itself, which contains several
xDS definitions. Default visibility for all shared definitions should be set to
`//envoy/api/v2:friends`.

Additionally, packages envoy.api.v2.core and envoy.api.v2.auth are also
consumed throughout the subpackages of `//envoy/api/v2`.
syntax = "proto3";

package envoy.api.v2.ratelimit;

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.ratelimit";
option java_outer_classname = "RatelimitProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/ratelimit";
option (udpa.annotations.file_migrate).move_to_package = "envoy.extensions.common.ratelimit.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Common rate limit components]

// A RateLimitDescriptor is a list of hierarchical entries that are used by the service to
// determine the final rate limit key and overall allowed limit. Here are some examples of how
// they might be used for the domain "envoy".
//
// .. code-block:: cpp
//
//   ["authenticated": "false"], ["remote_address": "10.0.0.1"]
//
// What it does: Limits all unauthenticated traffic for the IP address 10.0.0.1. The
// configuration supplies a default limit for the *remote_address* key. If there is a desire to
// raise the limit for 10.0.0.1 or block it entirely it can be specified directly in the
// configuration.
//
// .. code-block:: cpp
//
//   ["authenticated": "false"], ["path": "/foo/bar"]
//
// What it does: Limits all unauthenticated traffic globally for a specific path (or prefix if
// configured that way in the service).
//
// .. code-block:: cpp
//
//   ["authenticated": "false"], ["path": "/foo/bar"], ["remote_address": "10.0.0.1"]
//
// What it does: Limits unauthenticated traffic to a specific path for a specific IP address.
// Like (1) we can raise/block specific IP addresses if we want with an override configuration.
//
// .. code-block:: cpp
//
//   ["authenticated": "true"], ["client_id": "foo"]
//
// What it does: Limits all traffic for an authenticated client "foo"
//
// .. code-block:: cpp
//
//   ["authenticated": "true"], ["client_id": "foo"], ["path": "/foo/bar"]
//
// What it does: Limits traffic to a specific path for an authenticated client "foo"
//
// The idea behind the API is that (1)/(2)/(3) and (4)/(5) can be sent in 1 request if desired.
// This enables building complex application scenarios with a generic backend.
message RateLimitDescriptor {
  message Entry {
    // Descriptor key.
    string key = 1 [(validate.rules).string = {min_bytes: 1}];

    // Descriptor value.
    string value = 2 [(validate.rules).string = {min_bytes: 1}];
  }

  // Descriptor entries.
  repeated Entry entries = 1 [(validate.rules).repeated = {min_items: 1}];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.api.v2.endpoint;

import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/base.proto";

import "google/protobuf/duration.proto";
import "google/protobuf/struct.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.endpoint";
option java_outer_classname = "LoadReportProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/endpoint";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.endpoint.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// These are stats Envoy reports to GLB every so often. Report frequency is
// defined by
// :ref:`LoadStatsResponse.load_reporting_interval<envoy_api_field_service.load_stats.v2.LoadStatsResponse.load_reporting_interval>`.
// Stats per upstream region/zone and optionally per subzone.
// [#not-implemented-hide:] Not configuration. TBD how to doc proto APIs.
// [#next-free-field: 9]
message UpstreamLocalityStats {
  // Name of zone, region and optionally endpoint group these metrics were
  // collected from. Zone and region names could be empty if unknown.
  core.Locality locality = 1;

  // The total number of requests successfully completed by the endpoints in the
  // locality.
  uint64 total_successful_requests = 2;

  // The total number of unfinished requests
  uint64 total_requests_in_progress = 3;

  // The total number of requests that failed due to errors at the endpoint,
  // aggregated over all endpoints in the locality.
  uint64 total_error_requests = 4;

  // The total number of requests that were issued by this Envoy since
  // the last report. This information is aggregated over all the
  // upstream endpoints in the locality.
  uint64 total_issued_requests = 8;

  // Stats for multi-dimensional load balancing.
  repeated EndpointLoadMetricStats load_metric_stats = 5;

  // Endpoint granularity stats information for this locality. This information
  // is populated if the Server requests it by setting
  // :ref:`LoadStatsResponse.report_endpoint_granularity<envoy_api_field_service.load_stats.v2.LoadStatsResponse.report_endpoint_granularity>`.
  repeated UpstreamEndpointStats upstream_endpoint_stats = 7;

  // [#not-implemented-hide:] The priority of the endpoint group these metrics
  // were collected from.
  uint32 priority = 6;
}

// [#not-implemented-hide:] Not configuration. TBD how to doc proto APIs.
// [#next-free-field: 8]
message UpstreamEndpointStats {
  // Upstream host address.
  core.Address address = 1;

  // Opaque and implementation dependent metadata of the
  // endpoint. Envoy will pass this directly to the management server.
  google.protobuf.Struct metadata = 6;

  // The total number of requests successfully completed by the endpoints in the
  // locality. These include non-5xx responses for HTTP, where errors
  // originate at the client and the endpoint responded successfully. For gRPC,
  // the grpc-status values are those not covered by total_error_requests below.
  uint64 total_successful_requests = 2;

  // The total number of unfinished requests for this endpoint.
  uint64 total_requests_in_progress = 3;

  // The total number of requests that failed due to errors at the endpoint.
  // For HTTP these are responses with 5xx status codes and for gRPC the
  // grpc-status values:
  //
  //   - DeadlineExceeded
  //   - Unimplemented
  //   - Internal
  //   - Unavailable
  //   - Unknown
  //   - DataLoss
  uint64 total_error_requests = 4;

  // The total number of requests that were issued to this endpoint
  // since the last report. A single TCP connection, HTTP or gRPC
  // request or stream is counted as one request.
  uint64 total_issued_requests = 7;

  // Stats for multi-dimensional load balancing.
  repeated EndpointLoadMetricStats load_metric_stats = 5;
}

// [#not-implemented-hide:] Not configuration. TBD how to doc proto APIs.
message EndpointLoadMetricStats {
  // Name of the metric; may be empty.
  string metric_name = 1;

  // Number of calls that finished and included this metric.
  uint64 num_requests_finished_with_metric = 2;

  // Sum of metric values across all calls that finished with this metric for
  // load_reporting_interval.
  double total_metric_value = 3;
}

// Per cluster load stats. Envoy reports these stats a management server in a
// :ref:`LoadStatsRequest<envoy_api_msg_service.load_stats.v2.LoadStatsRequest>`
// [#not-implemented-hide:] Not configuration. TBD how to doc proto APIs.
// Next ID: 7
// [#next-free-field: 7]
message ClusterStats {
  message DroppedRequests {
    // Identifier for the policy specifying the drop.
    string category = 1 [(validate.rules).string = {min_bytes: 1}];

    // Total number of deliberately dropped requests for the category.
    uint64 dropped_count = 2;
  }

  // The name of the cluster.
  string cluster_name = 1 [(validate.rules).string = {min_bytes: 1}];

  // The eds_cluster_config service_name of the cluster.
  // It's possible that two clusters send the same service_name to EDS,
  // in that case, the management server is supposed to do aggregation on the load reports.
  string cluster_service_name = 6;

  // Need at least one.
  repeated UpstreamLocalityStats upstream_locality_stats = 2
      [(validate.rules).repeated = {min_items: 1}];

  // Cluster-level stats such as total_successful_requests may be computed by
  // summing upstream_locality_stats. In addition, below there are additional
  // cluster-wide stats.
  //
  // The total number of dropped requests. This covers requests
  // deliberately dropped by the drop_overload policy and circuit breaking.
  uint64 total_dropped_requests = 3;

  // Information about deliberately dropped requests for each category specified
  // in the DropOverload policy.
  repeated DroppedRequests dropped_requests = 5;

  // Period over which the actual load report occurred. This will be guaranteed to include every
  // request reported. Due to system load and delays between the *LoadStatsRequest* sent from Envoy
  // and the *LoadStatsResponse* message sent from the management server, this may be longer than
  // the requested load reporting interval in the *LoadStatsResponse*.
  google.protobuf.Duration load_report_interval = 4;
}
syntax = "proto3";

package envoy.api.v2.endpoint;

import public "envoy/api/v2/endpoint/endpoint_components.proto";

option java_package = "io.envoyproxy.envoy.api.v2.endpoint";
option java_outer_classname = "EndpointProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/endpoint";
syntax = "proto3";

package envoy.api.v2.endpoint;

import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/health_check.proto";

import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2.endpoint";
option java_outer_classname = "EndpointComponentsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2/endpoint";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.endpoint.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Endpoints]

// Upstream host identifier.
message Endpoint {
  // The optional health check configuration.
  message HealthCheckConfig {
    // Optional alternative health check port value.
    //
    // By default the health check address port of an upstream host is the same
    // as the host's serving address port. This provides an alternative health
    // check port. Setting this with a non-zero value allows an upstream host
    // to have different health check address port.
    uint32 port_value = 1 [(validate.rules).uint32 = {lte: 65535}];

    // By default, the host header for L7 health checks is controlled by cluster level configuration
    // (see: :ref:`host <envoy_api_field_core.HealthCheck.HttpHealthCheck.host>` and
    // :ref:`authority <envoy_api_field_core.HealthCheck.GrpcHealthCheck.authority>`). Setting this
    // to a non-empty value allows overriding the cluster level configuration for a specific
    // endpoint.
    string hostname = 2;
  }

  // The upstream host address.
  //
  // .. attention::
  //
  //   The form of host address depends on the given cluster type. For STATIC or EDS,
  //   it is expected to be a direct IP address (or something resolvable by the
  //   specified :ref:`resolver <envoy_api_field_core.SocketAddress.resolver_name>`
  //   in the Address). For LOGICAL or STRICT DNS, it is expected to be hostname,
  //   and will be resolved via DNS.
  core.Address address = 1;

  // The optional health check configuration is used as configuration for the
  // health checker to contact the health checked host.
  //
  // .. attention::
  //
  //   This takes into effect only for upstream clusters with
  //   :ref:`active health checking <arch_overview_health_checking>` enabled.
  HealthCheckConfig health_check_config = 2;

  // The hostname associated with this endpoint. This hostname is not used for routing or address
  // resolution. If provided, it will be associated with the endpoint, and can be used for features
  // that require a hostname, like
  // :ref:`auto_host_rewrite <envoy_api_field_route.RouteAction.auto_host_rewrite>`.
  string hostname = 3;
}

// An Endpoint that Envoy can route traffic to.
// [#next-free-field: 6]
message LbEndpoint {
  // Upstream host identifier or a named reference.
  oneof host_identifier {
    Endpoint endpoint = 1;

    // [#not-implemented-hide:]
    string endpoint_name = 5;
  }

  // Optional health status when known and supplied by EDS server.
  core.HealthStatus health_status = 2;

  // The endpoint metadata specifies values that may be used by the load
  // balancer to select endpoints in a cluster for a given request. The filter
  // name should be specified as *envoy.lb*. An example boolean key-value pair
  // is *canary*, providing the optional canary status of the upstream host.
  // This may be matched against in a route's
  // :ref:`RouteAction <envoy_api_msg_route.RouteAction>` metadata_match field
  // to subset the endpoints considered in cluster load balancing.
  core.Metadata metadata = 3;

  // The optional load balancing weight of the upstream host; at least 1.
  // Envoy uses the load balancing weight in some of the built in load
  // balancers. The load balancing weight for an endpoint is divided by the sum
  // of the weights of all endpoints in the endpoint's locality to produce a
  // percentage of traffic for the endpoint. This percentage is then further
  // weighted by the endpoint's locality's load balancing weight from
  // LocalityLbEndpoints. If unspecified, each host is presumed to have equal
  // weight in a locality. The sum of the weights of all endpoints in the
  // endpoint's locality must not exceed uint32_t maximal value (4294967295).
  google.protobuf.UInt32Value load_balancing_weight = 4 [(validate.rules).uint32 = {gte: 1}];
}

// A group of endpoints belonging to a Locality.
// One can have multiple LocalityLbEndpoints for a locality, but this is
// generally only done if the different groups need to have different load
// balancing weights or different priorities.
// [#next-free-field: 7]
message LocalityLbEndpoints {
  // Identifies location of where the upstream hosts run.
  core.Locality locality = 1;

  // The group of endpoints belonging to the locality specified.
  repeated LbEndpoint lb_endpoints = 2;

  // Optional: Per priority/region/zone/sub_zone weight; at least 1. The load
  // balancing weight for a locality is divided by the sum of the weights of all
  // localities  at the same priority level to produce the effective percentage
  // of traffic for the locality. The sum of the weights of all localities at
  // the same priority level must not exceed uint32_t maximal value (4294967295).
  //
  // Locality weights are only considered when :ref:`locality weighted load
  // balancing <arch_overview_load_balancing_locality_weighted_lb>` is
  // configured. These weights are ignored otherwise. If no weights are
  // specified when locality weighted load balancing is enabled, the locality is
  // assigned no load.
  google.protobuf.UInt32Value load_balancing_weight = 3 [(validate.rules).uint32 = {gte: 1}];

  // Optional: the priority for this LocalityLbEndpoints. If unspecified this will
  // default to the highest priority (0).
  //
  // Under usual circumstances, Envoy will only select endpoints for the highest
  // priority (0). In the event all endpoints for a particular priority are
  // unavailable/unhealthy, Envoy will fail over to selecting endpoints for the
  // next highest priority group.
  //
  // Priorities should range from 0 (highest) to N (lowest) without skipping.
  uint32 priority = 5 [(validate.rules).uint32 = {lte: 128}];

  // Optional: Per locality proximity value which indicates how close this
  // locality is from the source locality. This value only provides ordering
  // information (lower the value, closer it is to the source locality).
  // This will be consumed by load balancing schemes that need proximity order
  // to determine where to route the requests.
  // [#not-implemented-hide:]
  google.protobuf.UInt32Value proximity = 6;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/api/v2/core:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

import public "envoy/api/v2/route.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "RdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.route.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: RDS]

// The resource_names field in DiscoveryRequest specifies a route configuration.
// This allows an Envoy configuration with multiple HTTP listeners (and
// associated HTTP connection manager filters) to use different route
// configurations. Each listener will bind its HTTP connection manager filter to
// a route table via this identifier.
service RouteDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.api.v2.RouteConfiguration";

  rpc StreamRoutes(stream DiscoveryRequest) returns (stream DiscoveryResponse) {
  }

  rpc DeltaRoutes(stream DeltaDiscoveryRequest) returns (stream DeltaDiscoveryResponse) {
  }

  rpc FetchRoutes(DiscoveryRequest) returns (DiscoveryResponse) {
    option (google.api.http).post = "/v2/discovery:routes";
    option (google.api.http).body = "*";
  }
}

// Virtual Host Discovery Service (VHDS) is used to dynamically update the list of virtual hosts for
// a given RouteConfiguration. If VHDS is configured a virtual host list update will be triggered
// during the processing of an HTTP request if a route for the request cannot be resolved. The
// :ref:`resource_names_subscribe <envoy_api_field_DeltaDiscoveryRequest.resource_names_subscribe>`
// field contains a list of virtual host names or aliases to track. The contents of an alias would
// be the contents of a *host* or *authority* header used to make an http request. An xDS server
// will match an alias to a virtual host based on the content of :ref:`domains'
// <envoy_api_field_route.VirtualHost.domains>` field. The *resource_names_unsubscribe* field
// contains a list of virtual host names that have been :ref:`unsubscribed
// <xds_protocol_unsubscribe>` from the routing table associated with the RouteConfiguration.
service VirtualHostDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.api.v2.route.VirtualHost";

  rpc DeltaVirtualHosts(stream DeltaDiscoveryRequest) returns (stream DeltaDiscoveryResponse) {
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message RdsDummy {
}
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

import public "envoy/api/v2/endpoint.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "EdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.endpoint.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: EDS]
// Endpoint discovery :ref:`architecture overview <arch_overview_service_discovery_types_eds>`

service EndpointDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.api.v2.ClusterLoadAssignment";

  // The resource_names field in DiscoveryRequest specifies a list of clusters
  // to subscribe to updates for.
  rpc StreamEndpoints(stream DiscoveryRequest) returns (stream DiscoveryResponse) {
  }

  rpc DeltaEndpoints(stream DeltaDiscoveryRequest) returns (stream DeltaDiscoveryResponse) {
  }

  rpc FetchEndpoints(DiscoveryRequest) returns (DiscoveryResponse) {
    option (google.api.http).post = "/v2/discovery:endpoints";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message EdsDummy {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/api/v2/auth:pkg",
        "//envoy/api/v2/cluster:pkg",
        "//envoy/api/v2/core:pkg",
        "//envoy/api/v2/endpoint:pkg",
        "//envoy/api/v2/listener:pkg",
        "//envoy/api/v2/route:pkg",
        "//envoy/config/filter/accesslog/v2:pkg",
        "//envoy/config/listener/v2:pkg",
        "//envoy/type:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.api.v2;

import "envoy/api/v2/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

import public "envoy/api/v2/listener.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "LdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.listener.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Listener]
// Listener :ref:`configuration overview <config_listeners>`

// The Envoy instance initiates an RPC at startup to discover a list of
// listeners. Updates are delivered via streaming from the LDS server and
// consist of a complete update of all listeners. Existing connections will be
// allowed to drain from listeners that are no longer present.
service ListenerDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.api.v2.Listener";

  rpc DeltaListeners(stream DeltaDiscoveryRequest) returns (stream DeltaDiscoveryResponse) {
  }

  rpc StreamListeners(stream DiscoveryRequest) returns (stream DiscoveryResponse) {
  }

  rpc FetchListeners(DiscoveryRequest) returns (DiscoveryResponse) {
    option (google.api.http).post = "/v2/discovery:listeners";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message LdsDummy {
}
syntax = "proto3";

package envoy.api.v2;

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.api.v2";
option java_outer_classname = "ScopedRouteProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/api/v2;apiv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.route.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: HTTP scoped routing configuration]
// * Routing :ref:`architecture overview <arch_overview_http_routing>`

// Specifies a routing scope, which associates a
// :ref:`Key<envoy_api_msg_ScopedRouteConfiguration.Key>` to a
// :ref:`envoy_api_msg_RouteConfiguration` (identified by its resource name).
//
// The HTTP connection manager builds up a table consisting of these Key to
// RouteConfiguration mappings, and looks up the RouteConfiguration to use per
// request according to the algorithm specified in the
// :ref:`scope_key_builder<envoy_api_field_config.filter.network.http_connection_manager.v2.ScopedRoutes.scope_key_builder>`
// assigned to the HttpConnectionManager.
//
// For example, with the following configurations (in YAML):
//
// HttpConnectionManager config:
//
// .. code::
//
//   ...
//   scoped_routes:
//     name: foo-scoped-routes
//     scope_key_builder:
//       fragments:
//         - header_value_extractor:
//             name: X-Route-Selector
//             element_separator: ","
//             element:
//               separator: =
//               key: vip
//
// ScopedRouteConfiguration resources (specified statically via
// :ref:`scoped_route_configurations_list<envoy_api_field_config.filter.network.http_connection_manager.v2.ScopedRoutes.scoped_route_configurations_list>`
// or obtained dynamically via SRDS):
//
// .. code::
//
//  (1)
//   name: route-scope1
//   route_configuration_name: route-config1
//   key:
//      fragments:
//        - string_key: 172.10.10.20
//
//  (2)
//   name: route-scope2
//   route_configuration_name: route-config2
//   key:
//     fragments:
//       - string_key: 172.20.20.30
//
// A request from a client such as:
//
// .. code::
//
//     GET / HTTP/1.1
//     Host: foo.com
//     X-Route-Selector: vip=172.10.10.20
//
// would result in the routing table defined by the `route-config1`
// RouteConfiguration being assigned to the HTTP request/stream.
//
message ScopedRouteConfiguration {
  // Specifies a key which is matched against the output of the
  // :ref:`scope_key_builder<envoy_api_field_config.filter.network.http_connection_manager.v2.ScopedRoutes.scope_key_builder>`
  // specified in the HttpConnectionManager. The matching is done per HTTP
  // request and is dependent on the order of the fragments contained in the
  // Key.
  message Key {
    message Fragment {
      oneof type {
        option (validate.required) = true;

        // A string to match against.
        string string_key = 1;
      }
    }

    // The ordered set of fragments to match against. The order must match the
    // fragments in the corresponding
    // :ref:`scope_key_builder<envoy_api_field_config.filter.network.http_connection_manager.v2.ScopedRoutes.scope_key_builder>`.
    repeated Fragment fragments = 1 [(validate.rules).repeated = {min_items: 1}];
  }

  // The name assigned to the routing scope.
  string name = 1 [(validate.rules).string = {min_bytes: 1}];

  // The resource name to use for a :ref:`envoy_api_msg_DiscoveryRequest` to an
  // RDS server to fetch the :ref:`envoy_api_msg_RouteConfiguration` associated
  // with this scope.
  string route_configuration_name = 2 [(validate.rules).string = {min_bytes: 1}];

  // The key to match against.
  Key key = 3 [(validate.rules).message = {required: true}];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@prometheus_metrics_model//:client_model",
    ],
)
syntax = "proto3";

package envoy.service.metrics.v2;

import "envoy/api/v2/core/base.proto";

import "io/prometheus/client/metrics.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.metrics.v2";
option java_outer_classname = "MetricsServiceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/metrics/v2;metricsv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Metrics service]

// Service for streaming metrics to server that consumes the metrics data. It uses Prometheus metric
// data model as a standard to represent metrics information.
service MetricsService {
  // Envoy will connect and send StreamMetricsMessage messages forever. It does not expect any
  // response to be sent as nothing would be done in the case of failure.
  rpc StreamMetrics(stream StreamMetricsMessage) returns (StreamMetricsResponse) {
  }
}

message StreamMetricsResponse {
}

message StreamMetricsMessage {
  message Identifier {
    // The node sending metrics over the stream.
    api.v2.core.Node node = 1 [(validate.rules).message = {required: true}];
  }

  // Identifier data effectively is a structured metadata. As a performance optimization this will
  // only be sent in the first message on the stream.
  Identifier identifier = 1;

  // A list of metric entries
  repeated io.prometheus.client.MetricFamily envoy_metrics = 2;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@prometheus_metrics_model//:client_model",
    ],
)
syntax = "proto3";

package envoy.service.metrics.v3;

import "envoy/config/core/v3/base.proto";

import "io/prometheus/client/metrics.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.metrics.v3";
option java_outer_classname = "MetricsServiceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/metrics/v3;metricsv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Metrics service]

// Service for streaming metrics to server that consumes the metrics data. It uses Prometheus metric
// data model as a standard to represent metrics information.
service MetricsService {
  // Envoy will connect and send StreamMetricsMessage messages forever. It does not expect any
  // response to be sent as nothing would be done in the case of failure.
  rpc StreamMetrics(stream StreamMetricsMessage) returns (StreamMetricsResponse) {
  }
}

message StreamMetricsResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.metrics.v2.StreamMetricsResponse";
}

message StreamMetricsMessage {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.metrics.v2.StreamMetricsMessage";

  message Identifier {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.metrics.v2.StreamMetricsMessage.Identifier";

    // The node sending metrics over the stream.
    config.core.v3.Node node = 1 [(validate.rules).message = {required: true}];
  }

  // Identifier data effectively is a structured metadata. As a performance optimization this will
  // only be sent in the first message on the stream.
  Identifier identifier = 1;

  // A list of metric entries
  repeated io.prometheus.client.MetricFamily envoy_metrics = 2;
}
syntax = "proto3";

package envoy.service.cluster.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.cluster.v3";
option java_outer_classname = "CdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/cluster/v3;clusterv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: CDS]

// Return list of all clusters this proxy will load balance to.
service ClusterDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.cluster.v3.Cluster";

  rpc StreamClusters(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc DeltaClusters(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc FetchClusters(discovery.v3.DiscoveryRequest) returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:clusters";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message CdsDummy {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.CdsDummy";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/service/discovery/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "//envoy/api/v2/endpoint:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.load_stats.v2;

import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/endpoint/load_report.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.load_stats.v2";
option java_outer_classname = "LrsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/load_stats/v2;load_statsv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Load reporting service]

service LoadReportingService {
  // Advanced API to allow for multi-dimensional load balancing by remote
  // server. For receiving LB assignments, the steps are:
  // 1, The management server is configured with per cluster/zone/load metric
  //    capacity configuration. The capacity configuration definition is
  //    outside of the scope of this document.
  // 2. Envoy issues a standard {Stream,Fetch}Endpoints request for the clusters
  //    to balance.
  //
  // Independently, Envoy will initiate a StreamLoadStats bidi stream with a
  // management server:
  // 1. Once a connection establishes, the management server publishes a
  //    LoadStatsResponse for all clusters it is interested in learning load
  //    stats about.
  // 2. For each cluster, Envoy load balances incoming traffic to upstream hosts
  //    based on per-zone weights and/or per-instance weights (if specified)
  //    based on intra-zone LbPolicy. This information comes from the above
  //    {Stream,Fetch}Endpoints.
  // 3. When upstream hosts reply, they optionally add header <define header
  //    name> with ASCII representation of EndpointLoadMetricStats.
  // 4. Envoy aggregates load reports over the period of time given to it in
  //    LoadStatsResponse.load_reporting_interval. This includes aggregation
  //    stats Envoy maintains by itself (total_requests, rpc_errors etc.) as
  //    well as load metrics from upstream hosts.
  // 5. When the timer of load_reporting_interval expires, Envoy sends new
  //    LoadStatsRequest filled with load reports for each cluster.
  // 6. The management server uses the load reports from all reported Envoys
  //    from around the world, computes global assignment and prepares traffic
  //    assignment destined for each zone Envoys are located in. Goto 2.
  rpc StreamLoadStats(stream LoadStatsRequest) returns (stream LoadStatsResponse) {
  }
}

// A load report Envoy sends to the management server.
// [#not-implemented-hide:] Not configuration. TBD how to doc proto APIs.
message LoadStatsRequest {
  // Node identifier for Envoy instance.
  api.v2.core.Node node = 1;

  // A list of load stats to report.
  repeated api.v2.endpoint.ClusterStats cluster_stats = 2;
}

// The management server sends envoy a LoadStatsResponse with all clusters it
// is interested in learning load stats about.
// [#not-implemented-hide:] Not configuration. TBD how to doc proto APIs.
message LoadStatsResponse {
  // Clusters to report stats for.
  // Not populated if *send_all_clusters* is true.
  repeated string clusters = 1;

  // If true, the client should send all clusters it knows about.
  // Only clients that advertise the "envoy.lrs.supports_send_all_clusters" capability in their
  // :ref:`client_features<envoy_api_field_core.Node.client_features>` field will honor this field.
  bool send_all_clusters = 4;

  // The minimum interval of time to collect stats over. This is only a minimum for two reasons:
  // 1. There may be some delay from when the timer fires until stats sampling occurs.
  // 2. For clusters that were already feature in the previous *LoadStatsResponse*, any traffic
  //    that is observed in between the corresponding previous *LoadStatsRequest* and this
  //    *LoadStatsResponse* will also be accumulated and billed to the cluster. This avoids a period
  //    of inobservability that might otherwise exists between the messages. New clusters are not
  //    subject to this consideration.
  google.protobuf.Duration load_reporting_interval = 2;

  // Set to *true* if the management server supports endpoint granularity
  // report.
  bool report_endpoint_granularity = 3;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "//envoy/config/endpoint/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.load_stats.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/config/endpoint/v3/load_report.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.load_stats.v3";
option java_outer_classname = "LrsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/load_stats/v3;load_statsv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Load reporting service (LRS)]

// Load Reporting Service is an Envoy API to emit load reports. Envoy will initiate a bi-directional
// stream with a management server. Upon connecting, the management server can send a
// :ref:`LoadStatsResponse <envoy_v3_api_msg_service.load_stats.v3.LoadStatsResponse>` to a node it is
// interested in getting the load reports for. Envoy in this node will start sending
// :ref:`LoadStatsRequest <envoy_v3_api_msg_service.load_stats.v3.LoadStatsRequest>`. This is done periodically
// based on the :ref:`load reporting interval <envoy_v3_api_field_service.load_stats.v3.LoadStatsResponse.load_reporting_interval>`
// For details, take a look at the :ref:`Load Reporting Service sandbox example <install_sandboxes_load_reporting_service>`.

service LoadReportingService {
  // Advanced API to allow for multi-dimensional load balancing by remote
  // server. For receiving LB assignments, the steps are:
  // 1, The management server is configured with per cluster/zone/load metric
  //    capacity configuration. The capacity configuration definition is
  //    outside of the scope of this document.
  // 2. Envoy issues a standard {Stream,Fetch}Endpoints request for the clusters
  //    to balance.
  //
  // Independently, Envoy will initiate a StreamLoadStats bidi stream with a
  // management server:
  // 1. Once a connection establishes, the management server publishes a
  //    LoadStatsResponse for all clusters it is interested in learning load
  //    stats about.
  // 2. For each cluster, Envoy load balances incoming traffic to upstream hosts
  //    based on per-zone weights and/or per-instance weights (if specified)
  //    based on intra-zone LbPolicy. This information comes from the above
  //    {Stream,Fetch}Endpoints.
  // 3. When upstream hosts reply, they optionally add header <define header
  //    name> with ASCII representation of EndpointLoadMetricStats.
  // 4. Envoy aggregates load reports over the period of time given to it in
  //    LoadStatsResponse.load_reporting_interval. This includes aggregation
  //    stats Envoy maintains by itself (total_requests, rpc_errors etc.) as
  //    well as load metrics from upstream hosts.
  // 5. When the timer of load_reporting_interval expires, Envoy sends new
  //    LoadStatsRequest filled with load reports for each cluster.
  // 6. The management server uses the load reports from all reported Envoys
  //    from around the world, computes global assignment and prepares traffic
  //    assignment destined for each zone Envoys are located in. Goto 2.
  rpc StreamLoadStats(stream LoadStatsRequest) returns (stream LoadStatsResponse) {
  }
}

// A load report Envoy sends to the management server.
message LoadStatsRequest {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.load_stats.v2.LoadStatsRequest";

  // Node identifier for Envoy instance.
  config.core.v3.Node node = 1;

  // A list of load stats to report.
  repeated config.endpoint.v3.ClusterStats cluster_stats = 2;
}

// The management server sends envoy a LoadStatsResponse with all clusters it
// is interested in learning load stats about.
message LoadStatsResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.load_stats.v2.LoadStatsResponse";

  // Clusters to report stats for.
  // Not populated if ``send_all_clusters`` is true.
  repeated string clusters = 1;

  // If true, the client should send all clusters it knows about.
  // Only clients that advertise the "envoy.lrs.supports_send_all_clusters" capability in their
  // :ref:`client_features<envoy_v3_api_field_config.core.v3.Node.client_features>` field will honor this field.
  bool send_all_clusters = 4;

  // The minimum interval of time to collect stats over. This is only a minimum for two reasons:
  //
  // 1. There may be some delay from when the timer fires until stats sampling occurs.
  // 2. For clusters that were already feature in the previous ``LoadStatsResponse``, any traffic
  //    that is observed in between the corresponding previous ``LoadStatsRequest`` and this
  //    ``LoadStatsResponse`` will also be accumulated and billed to the cluster. This avoids a period
  //    of inobservability that might otherwise exists between the messages. New clusters are not
  //    subject to this consideration.
  google.protobuf.Duration load_reporting_interval = 2;

  // Set to ``true`` if the management server supports endpoint granularity
  // report.
  bool report_endpoint_granularity = 3;
}
syntax = "proto3";

package envoy.service.rate_limit_quota.v3;

import "envoy/type/v3/ratelimit_strategy.proto";

import "google/protobuf/duration.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.rate_limit_quota.v3";
option java_outer_classname = "RlqsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/rate_limit_quota/v3;rate_limit_quotav3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Rate Limit Quota Service (RLQS)]

// The Rate Limit Quota Service (RLQS) is a Envoy global rate limiting service that allows to
// delegate rate limit decisions to a remote service. The service will aggregate the usage reports
// from multiple data plane instances, and distribute Rate Limit Assignments to each instance
// based on its business logic. The logic is outside of the scope of the protocol API.
//
// The protocol is designed as a streaming-first API. It utilizes watch-like subscription model.
// The data plane groups requests into Quota Buckets as directed by the filter config,
// and periodically reports them to the RLQS server along with the Bucket identifier, :ref:`BucketId
// <envoy_v3_api_msg_service.rate_limit_quota.v3.BucketId>`. Once RLQS server has collected enough
// reports to make a decision, it'll send back the assignment with the rate limiting instructions.
//
// The first report sent by the data plane is interpreted by the RLQS server as a "watch" request,
// indicating that the data plane instance is interested in receiving further updates for the
// ``BucketId``. From then on, RLQS server may push assignments to this instance at will, even if
// the instance is not sending usage reports. It's the responsibility of the RLQS server
// to determine when the data plane instance didn't send ``BucketId`` reports for too long,
// and to respond with the :ref:`AbandonAction
// <envoy_v3_api_msg_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.AbandonAction>`,
// indicating that the server has now stopped sending quota assignments for the ``BucketId`` bucket,
// and the data plane instance should :ref:`abandon
// <envoy_v3_api_field_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.abandon_action>`
// it.
//
// If for any reason the RLQS client doesn't receive the initial assignment for the reported bucket,
// in order to prevent memory exhaustion, the data plane will limit the time such bucket
// is retained. The exact time to wait for the initial assignment is chosen by the filter,
// and may vary based on the implementation.
// Once the duration ends, the data plane will stop reporting bucket usage, reject any enqueued
// requests, and purge the bucket from the memory. Subsequent requests matched into the bucket
// will re-initialize the bucket in the "no assignment" state, restarting the reports.
//
// Refer to Rate Limit Quota :ref:`configuration overview <config_http_filters_rate_limit_quota>`
// for further details.

// Defines the Rate Limit Quota Service (RLQS).
service RateLimitQuotaService {
  // Main communication channel: the data plane sends usage reports to the RLQS server,
  // and the server asynchronously responding with the assignments.
  rpc StreamRateLimitQuotas(stream RateLimitQuotaUsageReports)
      returns (stream RateLimitQuotaResponse) {
  }
}

message RateLimitQuotaUsageReports {
  // The usage report for a bucket.
  //
  // .. note::
  //   Note that the first report sent for a ``BucketId`` indicates to the RLQS server that
  //   the RLQS client is subscribing for the future assignments for this ``BucketId``.
  message BucketQuotaUsage {
    // ``BucketId`` for which request quota usage is reported.
    BucketId bucket_id = 1 [(validate.rules).message = {required: true}];

    // Time elapsed since the last report.
    google.protobuf.Duration time_elapsed = 2 [(validate.rules).duration = {
      required: true
      gt {}
    }];

    // Requests the data plane has allowed through.
    uint64 num_requests_allowed = 3;

    // Requests throttled.
    uint64 num_requests_denied = 4;
  }

  // All quota requests must specify the domain. This enables sharing the quota
  // server between different applications without fear of overlap.
  // E.g., "envoy".
  //
  // Should only be provided in the first report, all subsequent messages on the same
  // stream are considered to be in the same domain. In case the domain needs to be
  // changes, close the stream, and reopen a new one with the different domain.
  string domain = 1 [(validate.rules).string = {min_len: 1}];

  // A list of quota usage reports. The list is processed by the RLQS server in the same order
  // it's provided by the client.
  repeated BucketQuotaUsage bucket_quota_usages = 2 [(validate.rules).repeated = {min_items: 1}];
}

message RateLimitQuotaResponse {
  // Commands the data plane to apply one of the actions to the bucket with the
  // :ref:`bucket_id <envoy_v3_api_field_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.bucket_id>`.
  message BucketAction {
    // Quota assignment for the bucket. Configures the rate limiting strategy and the duration
    // for the given :ref:`bucket_id
    // <envoy_v3_api_field_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.bucket_id>`.
    //
    // **Applying the first assignment to the bucket**
    //
    // Once the data plane receives the ``QuotaAssignmentAction``, it must send the current usage
    // report for the bucket, and start rate limiting requests matched into the bucket
    // using the strategy configured in the :ref:`rate_limit_strategy
    // <envoy_v3_api_field_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.QuotaAssignmentAction.rate_limit_strategy>`
    // field. The assignment becomes bucket's ``active`` assignment.
    //
    // **Expiring the assignment**
    //
    // The duration of the assignment defined in the :ref:`assignment_time_to_live
    // <envoy_v3_api_field_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.QuotaAssignmentAction.assignment_time_to_live>`
    // field. When the duration runs off, the assignment is ``expired``, and no longer ``active``.
    // The data plane should stop applying the rate limiting strategy to the bucket, and transition
    // the bucket to the "expired assignment" state. This activates the behavior configured in the
    // :ref:`expired_assignment_behavior <envoy_v3_api_field_extensions.filters.http.rate_limit_quota.v3.RateLimitQuotaBucketSettings.expired_assignment_behavior>`
    // field.
    //
    // **Replacing the assignment**
    //
    // * If the rate limiting strategy is different from bucket's ``active`` assignment, or
    //   the current bucket assignment is ``expired``, the data plane must immediately
    //   end the current assignment, report the bucket usage, and apply the new assignment.
    //   The new assignment becomes bucket's ``active`` assignment.
    // * If the rate limiting strategy is the same as the bucket's ``active`` (not ``expired``)
    //   assignment, the data plane should extend the duration of the ``active`` assignment
    //   for the duration of the new assignment provided in the :ref:`assignment_time_to_live
    //   <envoy_v3_api_field_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.QuotaAssignmentAction.assignment_time_to_live>`
    //   field. The ``active`` assignment is considered unchanged.
    message QuotaAssignmentAction {
      // A duration after which the assignment is be considered ``expired``. The process of the
      // expiration is described :ref:`above
      // <envoy_v3_api_msg_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.QuotaAssignmentAction>`.
      //
      // * If unset, the assignment has no expiration date.
      // * If set to ``0``, the assignment expires immediately, forcing the client into the
      //   :ref:`"expired assignment"
      //   <envoy_v3_api_field_extensions.filters.http.rate_limit_quota.v3.RateLimitQuotaBucketSettings.ExpiredAssignmentBehavior.expired_assignment_behavior_timeout>`
      //   state. This may be used by the RLQS server in cases when it needs clients to proactively
      //   fall back to the pre-configured :ref:`ExpiredAssignmentBehavior
      //   <envoy_v3_api_msg_extensions.filters.http.rate_limit_quota.v3.RateLimitQuotaBucketSettings.ExpiredAssignmentBehavior>`,
      //   f.e. before the server going into restart.
      //
      // .. attention::
      //   Note that :ref:`expiring
      //   <envoy_v3_api_msg_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.QuotaAssignmentAction>`
      //   the assignment is not the same as :ref:`abandoning
      //   <envoy_v3_api_msg_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.AbandonAction>`
      //   the assignment. While expiring the assignment just transitions the bucket to
      //   the "expired assignment" state; abandoning the assignment completely erases
      //   the bucket from the data plane memory, and stops the usage reports.
      google.protobuf.Duration assignment_time_to_live = 2 [(validate.rules).duration = {gte {}}];

      // Configures the local rate limiter for the request matched to the bucket.
      // If not set, allow all requests.
      type.v3.RateLimitStrategy rate_limit_strategy = 3;
    }

    // Abandon action for the bucket. Indicates that the RLQS server will no longer be
    // sending updates for the given :ref:`bucket_id
    // <envoy_v3_api_field_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.bucket_id>`.
    //
    // If no requests are reported for a bucket, after some time the server considers the bucket
    // inactive. The server stops tracking the bucket, and instructs the the data plane to abandon
    // the bucket via this message.
    //
    // **Abandoning the assignment**
    //
    // The data plane is to erase the bucket (including its usage data) from the memory.
    // It should stop tracking the bucket, and stop reporting its usage. This effectively resets
    // the data plane to the state prior to matching the first request into the bucket.
    //
    // **Restarting the subscription**
    //
    // If a new request is matched into a bucket previously abandoned, the data plane must behave
    // as if it has never tracked the bucket, and it's the first request matched into it:
    //
    // 1. The process of :ref:`subscription and reporting
    //    <envoy_v3_api_field_extensions.filters.http.rate_limit_quota.v3.RateLimitQuotaBucketSettings.reporting_interval>`
    //    starts from the beginning.
    //
    // 2. The bucket transitions to the :ref:`"no assignment"
    //    <envoy_v3_api_field_extensions.filters.http.rate_limit_quota.v3.RateLimitQuotaBucketSettings.no_assignment_behavior>`
    //    state.
    //
    // 3. Once the new assignment is received, it's applied per
    //    "Applying the first assignment to the bucket" section of the :ref:`QuotaAssignmentAction
    //    <envoy_v3_api_msg_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.QuotaAssignmentAction>`.
    message AbandonAction {
    }

    // ``BucketId`` for which request the action is applied.
    BucketId bucket_id = 1 [(validate.rules).message = {required: true}];

    oneof bucket_action {
      option (validate.required) = true;

      // Apply the quota assignment to the bucket.
      //
      // Commands the data plane to apply a rate limiting strategy to the bucket.
      // The process of applying and expiring the rate limiting strategy is detailed in the
      // :ref:`QuotaAssignmentAction
      // <envoy_v3_api_msg_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.QuotaAssignmentAction>`
      // message.
      QuotaAssignmentAction quota_assignment_action = 2;

      // Abandon the bucket.
      //
      // Commands the data plane to abandon the bucket.
      // The process of abandoning the bucket is described in the :ref:`AbandonAction
      // <envoy_v3_api_msg_service.rate_limit_quota.v3.RateLimitQuotaResponse.BucketAction.AbandonAction>`
      // message.
      AbandonAction abandon_action = 3;
    }
  }

  // An ordered list of actions to be applied to the buckets. The actions are applied in the
  // given order, from top to bottom.
  repeated BucketAction bucket_action = 1 [(validate.rules).repeated = {min_items: 1}];
}

// The identifier for the bucket. Used to match the bucket between the control plane (RLQS server),
// and the data plane (RLQS client), f.e.:
//
// * the data plane sends a usage report for requests matched into the bucket with ``BucketId``
//   to the control plane
// * the control plane sends an assignment for the bucket with ``BucketId`` to the data plane
//   Bucket ID.
//
// Example:
//
// .. validated-code-block:: yaml
//   :type-name: envoy.service.rate_limit_quota.v3.BucketId
//
//   bucket:
//     name: my_bucket
//     env: staging
//
// .. note::
//   The order of ``BucketId`` keys do not matter. Buckets ``{ a: 'A', b: 'B' }`` and
//   ``{ b: 'B', a: 'A' }`` are identical.
message BucketId {
  map<string, string> bucket = 1 [(validate.rules).map = {
    min_pairs: 1
    keys {string {min_len: 1}}
    values {string {min_len: 1}}
  }];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/type/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/service/discovery/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.listener.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.listener.v3";
option java_outer_classname = "LdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/listener/v3;listenerv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Listener]
// Listener :ref:`configuration overview <config_listeners>`

// The Envoy instance initiates an RPC at startup to discover a list of
// listeners. Updates are delivered via streaming from the LDS server and
// consist of a complete update of all listeners. Existing connections will be
// allowed to drain from listeners that are no longer present.
service ListenerDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.listener.v3.Listener";

  rpc DeltaListeners(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc StreamListeners(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc FetchListeners(discovery.v3.DiscoveryRequest) returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:listeners";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message LdsDummy {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.LdsDummy";
}
syntax = "proto3";

package envoy.service.trace.v2;

import "envoy/api/v2/core/base.proto";

import "opencensus/proto/trace/v1/trace.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.trace.v2";
option java_outer_classname = "TraceServiceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/trace/v2;tracev2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Trace service]

// Service for streaming traces to server that consumes the trace data. It
// uses OpenCensus data model as a standard to represent trace information.
service TraceService {
  // Envoy will connect and send StreamTracesMessage messages forever. It does
  // not expect any response to be sent as nothing would be done in the case
  // of failure.
  rpc StreamTraces(stream StreamTracesMessage) returns (StreamTracesResponse) {
  }
}

message StreamTracesResponse {
}

message StreamTracesMessage {
  message Identifier {
    // The node sending the access log messages over the stream.
    api.v2.core.Node node = 1 [(validate.rules).message = {required: true}];
  }

  // Identifier data effectively is a structured metadata.
  // As a performance optimization this will only be sent in the first message
  // on the stream.
  Identifier identifier = 1;

  // A list of Span entries
  repeated opencensus.proto.trace.v1.Span spans = 2;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@opencensus_proto//opencensus/proto/trace/v1:trace_proto",
    ],
)
syntax = "proto3";

package envoy.service.trace.v3;

import "envoy/config/core/v3/base.proto";

import "opencensus/proto/trace/v1/trace.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.trace.v3";
option java_outer_classname = "TraceServiceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/trace/v3;tracev3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Trace service]

// Service for streaming traces to server that consumes the trace data. It
// uses OpenCensus data model as a standard to represent trace information.
service TraceService {
  // Envoy will connect and send StreamTracesMessage messages forever. It does
  // not expect any response to be sent as nothing would be done in the case
  // of failure.
  rpc StreamTraces(stream StreamTracesMessage) returns (StreamTracesResponse) {
  }
}

message StreamTracesResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.trace.v2.StreamTracesResponse";
}

message StreamTracesMessage {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.trace.v2.StreamTracesMessage";

  message Identifier {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.trace.v2.StreamTracesMessage.Identifier";

    // The node sending the access log messages over the stream.
    config.core.v3.Node node = 1 [(validate.rules).message = {required: true}];
  }

  // Identifier data effectively is a structured metadata.
  // As a performance optimization this will only be sent in the first message
  // on the stream.
  Identifier identifier = 1;

  // A list of Span entries
  repeated opencensus.proto.trace.v1.Span spans = 2;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@opencensus_proto//opencensus/proto/trace/v1:trace_proto",
    ],
)
syntax = "proto3";

package envoy.service.extension.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.extension.v3";
option java_outer_classname = "ConfigDiscoveryProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/extension/v3;extensionv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Extension config discovery service (ECDS)]

// A service that supports dynamic configuration updates for a specific filter.
// Currently, ECDS is supported for network filters, HTTP filters and Listener filters.
// Please check :ref:`Extension Config Discovery Service (ECDS) API <config_overview_extension_discovery>`.
// The overall extension config discovery service works as follows:
//
// 1. A filter (:ref:`Downstream Network <envoy_v3_api_field_config.listener.v3.Filter.config_discovery>`,
//    :ref:`Upstream Network <envoy_v3_api_field_config.cluster.v3.Filter.config_discovery>`,
//    :ref:`Listener <envoy_v3_api_field_config.listener.v3.ListenerFilter.config_discovery>`
//    or :ref:`HTTP <envoy_v3_api_field_extensions.filters.network.http_connection_manager.v3.HttpFilter.config_discovery>`)
//    contains a :ref:`config_discovery <envoy_v3_api_msg_config.core.v3.ExtensionConfigSource>` configuration. This configuration
//    includes a :ref:`config_source <envoy_v3_api_field_config.core.v3.ExtensionConfigSource.config_source>`,
//    from which the filter configuration will be fetched.
// 2. The client then registers for a resource using the filter name as the resource_name.
// 3. The xDS server sends back the filter's configuration.
// 4. The client stores the configuration that will be used in the next instantiation of the filter chain,
//    i.e., for the next requests. Whenever an updated filter configuration arrives, it will be taken into
//    account in the following instantiation of the filter chain.
//
// Note: Filters that are configured using ECDS are warmed. For more details see
// :ref:`ExtensionConfigSource <envoy_v3_api_msg_config.core.v3.ExtensionConfigSource>`.

// Return extension configurations.
service ExtensionConfigDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.core.v3.TypedExtensionConfig";

  rpc StreamExtensionConfigs(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc DeltaExtensionConfigs(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc FetchExtensionConfigs(discovery.v3.DiscoveryRequest)
      returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:extension_configs";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue
// with importing services: https://github.com/google/protobuf/issues/4221 and
// protoxform to upgrade the file.
message EcdsDummy {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/service/discovery/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.route.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.route.v3";
option java_outer_classname = "SrdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/route/v3;routev3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: SRDS]
// * Routing :ref:`architecture overview <arch_overview_http_routing>`

// The Scoped Routes Discovery Service (SRDS) API distributes
// :ref:`ScopedRouteConfiguration<envoy_v3_api_msg.ScopedRouteConfiguration>`
// resources. Each ScopedRouteConfiguration resource represents a "routing
// scope" containing a mapping that allows the HTTP connection manager to
// dynamically assign a routing table (specified via a
// :ref:`RouteConfiguration<envoy_v3_api_msg_config.route.v3.RouteConfiguration>` message) to each
// HTTP request.
service ScopedRoutesDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.route.v3.ScopedRouteConfiguration";

  rpc StreamScopedRoutes(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc DeltaScopedRoutes(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc FetchScopedRoutes(discovery.v3.DiscoveryRequest) returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:scoped-routes";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message SrdsDummy {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.SrdsDummy";
}
syntax = "proto3";

package envoy.service.route.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.route.v3";
option java_outer_classname = "RdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/route/v3;routev3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: RDS]

// The resource_names field in DiscoveryRequest specifies a route configuration.
// This allows an Envoy configuration with multiple HTTP listeners (and
// associated HTTP connection manager filters) to use different route
// configurations. Each listener will bind its HTTP connection manager filter to
// a route table via this identifier.
service RouteDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.route.v3.RouteConfiguration";

  rpc StreamRoutes(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc DeltaRoutes(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc FetchRoutes(discovery.v3.DiscoveryRequest) returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:routes";
    option (google.api.http).body = "*";
  }
}

// Virtual Host Discovery Service (VHDS) is used to dynamically update the list of virtual hosts for
// a given RouteConfiguration. If VHDS is configured a virtual host list update will be triggered
// during the processing of an HTTP request if a route for the request cannot be resolved. The
// :ref:`resource_names_subscribe <envoy_v3_api_field_service.discovery.v3.DeltaDiscoveryRequest.resource_names_subscribe>`
// field contains a list of virtual host names or aliases to track. The contents of an alias would
// be the contents of a ``host`` or ``authority`` header used to make an http request. An xDS server
// will match an alias to a virtual host based on the content of :ref:`domains'
// <envoy_v3_api_field_config.route.v3.VirtualHost.domains>` field. The ``resource_names_unsubscribe`` field
// contains a list of virtual host names that have been :ref:`unsubscribed
// <xds_protocol_unsubscribe>` from the routing table associated with the RouteConfiguration.
service VirtualHostDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.route.v3.VirtualHost";

  rpc DeltaVirtualHosts(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message RdsDummy {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.RdsDummy";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/service/discovery/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.auth.v2;

import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/base.proto";

import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.auth.v2";
option java_outer_classname = "AttributeContextProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/auth/v2;authv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Attribute Context ]

// See :ref:`network filter configuration overview <config_network_filters_ext_authz>`
// and :ref:`HTTP filter configuration overview <config_http_filters_ext_authz>`.

// An attribute is a piece of metadata that describes an activity on a network.
// For example, the size of an HTTP request, or the status code of an HTTP response.
//
// Each attribute has a type and a name, which is logically defined as a proto message field
// of the `AttributeContext`. The `AttributeContext` is a collection of individual attributes
// supported by Envoy authorization system.
// [#comment: The following items are left out of this proto
// Request.Auth field for jwt tokens
// Request.Api for api management
// Origin peer that originated the request
// Caching Protocol
// request_context return values to inject back into the filter chain
// peer.claims -- from X.509 extensions
// Configuration
// - field mask to send
// - which return values from request_context are copied back
// - which return values are copied into request_headers]
// [#next-free-field: 12]
message AttributeContext {
  // This message defines attributes for a node that handles a network request.
  // The node can be either a service or an application that sends, forwards,
  // or receives the request. Service peers should fill in the `service`,
  // `principal`, and `labels` as appropriate.
  // [#next-free-field: 6]
  message Peer {
    // The address of the peer, this is typically the IP address.
    // It can also be UDS path, or others.
    api.v2.core.Address address = 1;

    // The canonical service name of the peer.
    // It should be set to :ref:`the HTTP x-envoy-downstream-service-cluster
    // <config_http_conn_man_headers_downstream-service-cluster>`
    // If a more trusted source of the service name is available through mTLS/secure naming, it
    // should be used.
    string service = 2;

    // The labels associated with the peer.
    // These could be pod labels for Kubernetes or tags for VMs.
    // The source of the labels could be an X.509 certificate or other configuration.
    map<string, string> labels = 3;

    // The authenticated identity of this peer.
    // For example, the identity associated with the workload such as a service account.
    // If an X.509 certificate is used to assert the identity this field should be sourced from
    // `URI Subject Alternative Names`, `DNS Subject Alternate Names` or `Subject` in that order.
    // The primary identity should be the principal. The principal format is issuer specific.
    //
    // Example:
    // *    SPIFFE format is `spiffe://trust-domain/path`
    // *    Google account format is `https://accounts.google.com/{userid}`
    string principal = 4;

    // The X.509 certificate used to authenticate the identify of this peer.
    // When present, the certificate contents are encoded in URL and PEM format.
    string certificate = 5;
  }

  // Represents a network request, such as an HTTP request.
  message Request {
    // The timestamp when the proxy receives the first byte of the request.
    google.protobuf.Timestamp time = 1;

    // Represents an HTTP request or an HTTP-like request.
    HttpRequest http = 2;
  }

  // This message defines attributes for an HTTP request.
  // HTTP/1.x, HTTP/2, gRPC are all considered as HTTP requests.
  // [#next-free-field: 12]
  message HttpRequest {
    // The unique ID for a request, which can be propagated to downstream
    // systems. The ID should have low probability of collision
    // within a single day for a specific service.
    // For HTTP requests, it should be X-Request-ID or equivalent.
    string id = 1;

    // The HTTP request method, such as `GET`, `POST`.
    string method = 2;

    // The HTTP request headers. If multiple headers share the same key, they
    // must be merged according to the HTTP spec. All header keys must be
    // lower-cased, because HTTP header keys are case-insensitive.
    map<string, string> headers = 3;

    // The request target, as it appears in the first line of the HTTP request. This includes
    // the URL path and query-string. No decoding is performed.
    string path = 4;

    // The HTTP request `Host` or 'Authority` header value.
    string host = 5;

    // The HTTP URL scheme, such as `http` and `https`. This is set for HTTP/2
    // requests only. For HTTP/1.1, use "x-forwarded-for" header value to lookup
    // the scheme of the request.
    string scheme = 6;

    // This field is always empty, and exists for compatibility reasons. The HTTP URL query is
    // included in `path` field.
    string query = 7;

    // This field is always empty, and exists for compatibility reasons. The URL fragment is
    // not submitted as part of HTTP requests; it is unknowable.
    string fragment = 8;

    // The HTTP request size in bytes. If unknown, it must be -1.
    int64 size = 9;

    // The network protocol used with the request, such as "HTTP/1.0", "HTTP/1.1", or "HTTP/2".
    //
    // See :repo:`headers.h:ProtocolStrings <source/common/http/headers.h>` for a list of all
    // possible values.
    string protocol = 10;

    // The HTTP request body.
    string body = 11;
  }

  // The source of a network activity, such as starting a TCP connection.
  // In a multi hop network activity, the source represents the sender of the
  // last hop.
  Peer source = 1;

  // The destination of a network activity, such as accepting a TCP connection.
  // In a multi hop network activity, the destination represents the receiver of
  // the last hop.
  Peer destination = 2;

  // Represents a network request, such as an HTTP request.
  Request request = 4;

  // This is analogous to http_request.headers, however these contents will not be sent to the
  // upstream server. Context_extensions provide an extension mechanism for sending additional
  // information to the auth server without modifying the proto definition. It maps to the
  // internal opaque context in the filter chain.
  map<string, string> context_extensions = 10;

  // Dynamic metadata associated with the request.
  api.v2.core.Metadata metadata_context = 11;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "//envoy/type:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.auth.v2;

import "envoy/api/v2/core/base.proto";
import "envoy/service/auth/v2/attribute_context.proto";
import "envoy/type/http_status.proto";

import "google/rpc/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.auth.v2";
option java_outer_classname = "ExternalAuthProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/auth/v2;authv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Authorization Service ]

// The authorization service request messages used by external authorization :ref:`network filter
// <config_network_filters_ext_authz>` and :ref:`HTTP filter <config_http_filters_ext_authz>`.

// A generic interface for performing authorization check on incoming
// requests to a networked service.
service Authorization {
  // Performs authorization check based on the attributes associated with the
  // incoming request, and returns status `OK` or not `OK`.
  rpc Check(CheckRequest) returns (CheckResponse) {
  }
}

message CheckRequest {
  // The request attributes.
  AttributeContext attributes = 1;
}

// HTTP attributes for a denied response.
message DeniedHttpResponse {
  // This field allows the authorization service to send a HTTP response status
  // code to the downstream client other than 403 (Forbidden).
  type.HttpStatus status = 1 [(validate.rules).message = {required: true}];

  // This field allows the authorization service to send HTTP response headers
  // to the downstream client. Note that the `append` field in `HeaderValueOption` defaults to
  // false when used in this message.
  repeated api.v2.core.HeaderValueOption headers = 2;

  // This field allows the authorization service to send a response body data
  // to the downstream client.
  string body = 3;
}

// HTTP attributes for an ok response.
message OkHttpResponse {
  // HTTP entity headers in addition to the original request headers. This allows the authorization
  // service to append, to add or to override headers from the original request before
  // dispatching it to the upstream. Note that the `append` field in `HeaderValueOption` defaults to
  // false when used in this message. By setting the `append` field to `true`,
  // the filter will append the correspondent header value to the matched request header.
  // By leaving `append` as false, the filter will either add a new header, or override an existing
  // one if there is a match.
  repeated api.v2.core.HeaderValueOption headers = 2;
}

// Intended for gRPC and Network Authorization servers `only`.
message CheckResponse {
  // Status `OK` allows the request. Any other status indicates the request should be denied.
  google.rpc.Status status = 1;

  // An message that contains HTTP response attributes. This message is
  // used when the authorization service needs to send custom responses to the
  // downstream client or, to modify/add request headers being dispatched to the upstream.
  oneof http_response {
    // Supplies http attributes for a denied response.
    DeniedHttpResponse denied_response = 2;

    // Supplies http attributes for an ok response.
    OkHttpResponse ok_response = 3;
  }
}
syntax = "proto3";

package envoy.service.auth.v3;

import "envoy/config/core/v3/address.proto";
import "envoy/config/core/v3/base.proto";

import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.auth.v3";
option java_outer_classname = "AttributeContextProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/auth/v3;authv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Attribute context]

// See :ref:`network filter configuration overview <config_network_filters_ext_authz>`
// and :ref:`HTTP filter configuration overview <config_http_filters_ext_authz>`.

// An attribute is a piece of metadata that describes an activity on a network.
// For example, the size of an HTTP request, or the status code of an HTTP response.
//
// Each attribute has a type and a name, which is logically defined as a proto message field
// of the ``AttributeContext``. The ``AttributeContext`` is a collection of individual attributes
// supported by Envoy authorization system.
// [#comment: The following items are left out of this proto
// Request.Auth field for jwt tokens
// Request.Api for api management
// Origin peer that originated the request
// Caching Protocol
// request_context return values to inject back into the filter chain
// peer.claims -- from X.509 extensions
// Configuration
// - field mask to send
// - which return values from request_context are copied back
// - which return values are copied into request_headers]
// [#next-free-field: 14]
message AttributeContext {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.auth.v2.AttributeContext";

  // This message defines attributes for a node that handles a network request.
  // The node can be either a service or an application that sends, forwards,
  // or receives the request. Service peers should fill in the ``service``,
  // ``principal``, and ``labels`` as appropriate.
  // [#next-free-field: 6]
  message Peer {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.auth.v2.AttributeContext.Peer";

    // The address of the peer, this is typically the IP address.
    // It can also be UDS path, or others.
    config.core.v3.Address address = 1;

    // The canonical service name of the peer.
    // It should be set to :ref:`the HTTP x-envoy-downstream-service-cluster
    // <config_http_conn_man_headers_downstream-service-cluster>`
    // If a more trusted source of the service name is available through mTLS/secure naming, it
    // should be used.
    string service = 2;

    // The labels associated with the peer.
    // These could be pod labels for Kubernetes or tags for VMs.
    // The source of the labels could be an X.509 certificate or other configuration.
    map<string, string> labels = 3;

    // The authenticated identity of this peer.
    // For example, the identity associated with the workload such as a service account.
    // If an X.509 certificate is used to assert the identity this field should be sourced from
    // ``URI Subject Alternative Names``, ``DNS Subject Alternate Names`` or ``Subject`` in that order.
    // The primary identity should be the principal. The principal format is issuer specific.
    //
    // Examples:
    //
    // - SPIFFE format is ``spiffe://trust-domain/path``.
    // - Google account format is ``https://accounts.google.com/{userid}``.
    string principal = 4;

    // The X.509 certificate used to authenticate the identify of this peer.
    // When present, the certificate contents are encoded in URL and PEM format.
    string certificate = 5;
  }

  // Represents a network request, such as an HTTP request.
  message Request {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.auth.v2.AttributeContext.Request";

    // The timestamp when the proxy receives the first byte of the request.
    google.protobuf.Timestamp time = 1;

    // Represents an HTTP request or an HTTP-like request.
    HttpRequest http = 2;
  }

  // This message defines attributes for an HTTP request.
  // HTTP/1.x, HTTP/2, gRPC are all considered as HTTP requests.
  // [#next-free-field: 13]
  message HttpRequest {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.auth.v2.AttributeContext.HttpRequest";

    // The unique ID for a request, which can be propagated to downstream
    // systems. The ID should have low probability of collision
    // within a single day for a specific service.
    // For HTTP requests, it should be X-Request-ID or equivalent.
    string id = 1;

    // The HTTP request method, such as ``GET``, ``POST``.
    string method = 2;

    // The HTTP request headers. If multiple headers share the same key, they
    // must be merged according to the HTTP spec. All header keys must be
    // lower-cased, because HTTP header keys are case-insensitive.
    map<string, string> headers = 3;

    // The request target, as it appears in the first line of the HTTP request. This includes
    // the URL path and query-string. No decoding is performed.
    string path = 4;

    // The HTTP request ``Host`` or ``:authority`` header value.
    string host = 5;

    // The HTTP URL scheme, such as ``http`` and ``https``.
    string scheme = 6;

    // This field is always empty, and exists for compatibility reasons. The HTTP URL query is
    // included in ``path`` field.
    string query = 7;

    // This field is always empty, and exists for compatibility reasons. The URL fragment is
    // not submitted as part of HTTP requests; it is unknowable.
    string fragment = 8;

    // The HTTP request size in bytes. If unknown, it must be -1.
    int64 size = 9;

    // The network protocol used with the request, such as "HTTP/1.0", "HTTP/1.1", or "HTTP/2".
    //
    // See :repo:`headers.h:ProtocolStrings <source/common/http/headers.h>` for a list of all
    // possible values.
    string protocol = 10;

    // The HTTP request body.
    string body = 11;

    // The HTTP request body in bytes. This is used instead of
    // :ref:`body <envoy_v3_api_field_service.auth.v3.AttributeContext.HttpRequest.body>` when
    // :ref:`pack_as_bytes <envoy_v3_api_field_extensions.filters.http.ext_authz.v3.BufferSettings.pack_as_bytes>`
    // is set to true.
    bytes raw_body = 12;
  }

  // This message defines attributes for the underlying TLS session.
  message TLSSession {
    // SNI used for TLS session.
    string sni = 1;
  }

  // The source of a network activity, such as starting a TCP connection.
  // In a multi hop network activity, the source represents the sender of the
  // last hop.
  Peer source = 1;

  // The destination of a network activity, such as accepting a TCP connection.
  // In a multi hop network activity, the destination represents the receiver of
  // the last hop.
  Peer destination = 2;

  // Represents a network request, such as an HTTP request.
  Request request = 4;

  // This is analogous to http_request.headers, however these contents will not be sent to the
  // upstream server. Context_extensions provide an extension mechanism for sending additional
  // information to the auth server without modifying the proto definition. It maps to the
  // internal opaque context in the filter chain.
  map<string, string> context_extensions = 10;

  // Dynamic metadata associated with the request.
  config.core.v3.Metadata metadata_context = 11;

  // Metadata associated with the selected route.
  config.core.v3.Metadata route_metadata_context = 13;

  // TLS session details of the underlying connection.
  // This is not populated by default and will be populated if ext_authz filter's
  // :ref:`include_tls_session <config_http_filters_ext_authz>` is set to true.
  TLSSession tls_session = 12;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/config/core/v3:pkg",
        "//envoy/type/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.auth.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/service/auth/v3/attribute_context.proto";
import "envoy/type/v3/http_status.proto";

import "google/protobuf/struct.proto";
import "google/rpc/status.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.auth.v3";
option java_outer_classname = "ExternalAuthProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/auth/v3;authv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Authorization service]

// The authorization service request messages used by external authorization :ref:`network filter
// <config_network_filters_ext_authz>` and :ref:`HTTP filter <config_http_filters_ext_authz>`.

// A generic interface for performing authorization check on incoming
// requests to a networked service.
service Authorization {
  // Performs authorization check based on the attributes associated with the
  // incoming request, and returns status `OK` or not `OK`.
  rpc Check(CheckRequest) returns (CheckResponse) {
  }
}

message CheckRequest {
  option (udpa.annotations.versioning).previous_message_type = "envoy.service.auth.v2.CheckRequest";

  // The request attributes.
  AttributeContext attributes = 1;
}

// HTTP attributes for a denied response.
message DeniedHttpResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.auth.v2.DeniedHttpResponse";

  // This field allows the authorization service to send an HTTP response status code to the
  // downstream client. If not set, Envoy sends ``403 Forbidden`` HTTP status code by default.
  type.v3.HttpStatus status = 1;

  // This field allows the authorization service to send HTTP response headers
  // to the downstream client. Note that the :ref:`append field in HeaderValueOption <envoy_v3_api_field_config.core.v3.HeaderValueOption.append>` defaults to
  // false when used in this message.
  repeated config.core.v3.HeaderValueOption headers = 2;

  // This field allows the authorization service to send a response body data
  // to the downstream client.
  string body = 3;
}

// HTTP attributes for an OK response.
// [#next-free-field: 9]
message OkHttpResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.auth.v2.OkHttpResponse";

  // HTTP entity headers in addition to the original request headers. This allows the authorization
  // service to append, to add or to override headers from the original request before
  // dispatching it to the upstream. Note that the :ref:`append field in HeaderValueOption <envoy_v3_api_field_config.core.v3.HeaderValueOption.append>` defaults to
  // false when used in this message. By setting the ``append`` field to ``true``,
  // the filter will append the correspondent header value to the matched request header.
  // By leaving ``append`` as false, the filter will either add a new header, or override an existing
  // one if there is a match.
  repeated config.core.v3.HeaderValueOption headers = 2;

  // HTTP entity headers to remove from the original request before dispatching
  // it to the upstream. This allows the authorization service to act on auth
  // related headers (like ``Authorization``), process them, and consume them.
  // Under this model, the upstream will either receive the request (if it's
  // authorized) or not receive it (if it's not), but will not see headers
  // containing authorization credentials.
  //
  // Pseudo headers (such as ``:authority``, ``:method``, ``:path`` etc), as well as
  // the header ``Host``, may not be removed as that would make the request
  // malformed. If mentioned in ``headers_to_remove`` these special headers will
  // be ignored.
  //
  // When using the HTTP service this must instead be set by the HTTP
  // authorization service as a comma separated list like so:
  // ``x-envoy-auth-headers-to-remove: one-auth-header, another-auth-header``.
  repeated string headers_to_remove = 5;

  // This field has been deprecated in favor of :ref:`CheckResponse.dynamic_metadata
  // <envoy_v3_api_field_service.auth.v3.CheckResponse.dynamic_metadata>`. Until it is removed,
  // setting this field overrides :ref:`CheckResponse.dynamic_metadata
  // <envoy_v3_api_field_service.auth.v3.CheckResponse.dynamic_metadata>`.
  google.protobuf.Struct dynamic_metadata = 3
      [deprecated = true, (envoy.annotations.deprecated_at_minor_version) = "3.0"];

  // This field allows the authorization service to send HTTP response headers
  // to the downstream client on success. Note that the :ref:`append field in HeaderValueOption <envoy_v3_api_field_config.core.v3.HeaderValueOption.append>`
  // defaults to false when used in this message.
  repeated config.core.v3.HeaderValueOption response_headers_to_add = 6;

  // This field allows the authorization service to set (and overwrite) query
  // string parameters on the original request before it is sent upstream.
  repeated config.core.v3.QueryParameter query_parameters_to_set = 7;

  // This field allows the authorization service to specify which query parameters
  // should be removed from the original request before it is sent upstream. Each
  // element in this list is a case-sensitive query parameter name to be removed.
  repeated string query_parameters_to_remove = 8;
}

// Intended for gRPC and Network Authorization servers ``only``.
message CheckResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.auth.v2.CheckResponse";

  // Status ``OK`` allows the request. Any other status indicates the request should be denied, and
  // for HTTP filter, if not overridden by :ref:`denied HTTP response status <envoy_v3_api_field_service.auth.v3.DeniedHttpResponse.status>`
  // Envoy sends ``403 Forbidden`` HTTP status code by default.
  google.rpc.Status status = 1;

  // An message that contains HTTP response attributes. This message is
  // used when the authorization service needs to send custom responses to the
  // downstream client or, to modify/add request headers being dispatched to the upstream.
  oneof http_response {
    // Supplies http attributes for a denied response.
    DeniedHttpResponse denied_response = 2;

    // Supplies http attributes for an ok response.
    OkHttpResponse ok_response = 3;
  }

  // Optional response metadata that will be emitted as dynamic metadata to be consumed by the next
  // filter. This metadata lives in a namespace specified by the canonical name of extension filter
  // that requires it:
  //
  // - :ref:`envoy.filters.http.ext_authz <config_http_filters_ext_authz_dynamic_metadata>` for HTTP filter.
  // - :ref:`envoy.filters.network.ext_authz <config_network_filters_ext_authz_dynamic_metadata>` for network filter.
  google.protobuf.Struct dynamic_metadata = 4;
}
load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

api_proto_package(
    has_services = True,
    deps = ["//envoy/service/auth/v2:pkg"],
)
syntax = "proto3";

package envoy.service.auth.v2alpha;

option java_multiple_files = true;
option java_outer_classname = "CertsProto";
option java_package = "io.envoyproxy.envoy.service.auth.v2alpha";
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/auth/v2alpha";

import "envoy/service/auth/v2/external_auth.proto";

// [#protodoc-title: Authorization Service ]

// The authorization service request messages used by external authorization :ref:`network filter
// <config_network_filters_ext_authz>` and :ref:`HTTP filter <config_http_filters_ext_authz>`.

// A generic interface for performing authorization check on incoming
// requests to a networked service.
service Authorization {
  // Performs authorization check based on the attributes associated with the
  // incoming request, and returns status `OK` or not `OK`.
  rpc Check(v2.CheckRequest) returns (v2.CheckResponse);
}
syntax = "proto3";

package envoy.service.health.v3;

import "envoy/config/cluster/v3/cluster.proto";
import "envoy/config/core/v3/address.proto";
import "envoy/config/core/v3/base.proto";
import "envoy/config/core/v3/health_check.proto";
import "envoy/config/endpoint/v3/endpoint_components.proto";

import "google/api/annotations.proto";
import "google/protobuf/duration.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.health.v3";
option java_outer_classname = "HdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/health/v3;healthv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Health discovery service (HDS)]

// HDS is Health Discovery Service. It compliments Envoy’s health checking
// service by designating this Envoy to be a healthchecker for a subset of hosts
// in the cluster. The status of these health checks will be reported to the
// management server, where it can be aggregated etc and redistributed back to
// Envoy through EDS.
service HealthDiscoveryService {
  // 1. Envoy starts up and if its can_healthcheck option in the static
  //    bootstrap config is enabled, sends HealthCheckRequest to the management
  //    server. It supplies its capabilities (which protocol it can health check
  //    with, what zone it resides in, etc.).
  // 2. In response to (1), the management server designates this Envoy as a
  //    healthchecker to health check a subset of all upstream hosts for a given
  //    cluster (for example upstream Host 1 and Host 2). It streams
  //    HealthCheckSpecifier messages with cluster related configuration for all
  //    clusters this Envoy is designated to health check. Subsequent
  //    HealthCheckSpecifier message will be sent on changes to:
  //    a. Endpoints to health checks
  //    b. Per cluster configuration change
  // 3. Envoy creates a health probe based on the HealthCheck config and sends
  //    it to endpoint(ip:port) of Host 1 and 2. Based on the HealthCheck
  //    configuration Envoy waits upon the arrival of the probe response and
  //    looks at the content of the response to decide whether the endpoint is
  //    healthy or not. If a response hasn't been received within the timeout
  //    interval, the endpoint health status is considered TIMEOUT.
  // 4. Envoy reports results back in an EndpointHealthResponse message.
  //    Envoy streams responses as often as the interval configured by the
  //    management server in HealthCheckSpecifier.
  // 5. The management Server collects health statuses for all endpoints in the
  //    cluster (for all clusters) and uses this information to construct
  //    EndpointDiscoveryResponse messages.
  // 6. Once Envoy has a list of upstream endpoints to send traffic to, it load
  //    balances traffic to them without additional health checking. It may
  //    use inline healthcheck (i.e. consider endpoint UNHEALTHY if connection
  //    failed to a particular endpoint to account for health status propagation
  //    delay between HDS and EDS).
  // By default, can_healthcheck is true. If can_healthcheck is false, Cluster
  // configuration may not contain HealthCheck message.
  // TODO(htuch): How is can_healthcheck communicated to CDS to ensure the above
  // invariant?
  // TODO(htuch): Add @amb67's diagram.
  rpc StreamHealthCheck(stream HealthCheckRequestOrEndpointHealthResponse)
      returns (stream HealthCheckSpecifier) {
  }

  // TODO(htuch): Unlike the gRPC version, there is no stream-based binding of
  // request/response. Should we add an identifier to the HealthCheckSpecifier
  // to bind with the response?
  rpc FetchHealthCheck(HealthCheckRequestOrEndpointHealthResponse) returns (HealthCheckSpecifier) {
    option (google.api.http).post = "/v3/discovery:health_check";
    option (google.api.http).body = "*";
  }
}

// Defines supported protocols etc, so the management server can assign proper
// endpoints to healthcheck.
message Capability {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.Capability";

  // Different Envoy instances may have different capabilities (e.g. Redis)
  // and/or have ports enabled for different protocols.
  enum Protocol {
    HTTP = 0;
    TCP = 1;
    REDIS = 2;
  }

  repeated Protocol health_check_protocols = 1;
}

message HealthCheckRequest {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.HealthCheckRequest";

  config.core.v3.Node node = 1;

  Capability capability = 2;
}

message EndpointHealth {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.EndpointHealth";

  config.endpoint.v3.Endpoint endpoint = 1;

  config.core.v3.HealthStatus health_status = 2;
}

// Group endpoint health by locality under each cluster.
message LocalityEndpointsHealth {
  config.core.v3.Locality locality = 1;

  repeated EndpointHealth endpoints_health = 2;
}

// The health status of endpoints in a cluster. The cluster name and locality
// should match the corresponding fields in ClusterHealthCheck message.
message ClusterEndpointsHealth {
  string cluster_name = 1;

  repeated LocalityEndpointsHealth locality_endpoints_health = 2;
}

message EndpointHealthResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.EndpointHealthResponse";

  // Deprecated - Flat list of endpoint health information.
  repeated EndpointHealth endpoints_health = 1
      [deprecated = true, (envoy.annotations.deprecated_at_minor_version) = "3.0"];

  // Organize Endpoint health information by cluster.
  repeated ClusterEndpointsHealth cluster_endpoints_health = 2;
}

message HealthCheckRequestOrEndpointHealthResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.HealthCheckRequestOrEndpointHealthResponse";

  oneof request_type {
    HealthCheckRequest health_check_request = 1;

    EndpointHealthResponse endpoint_health_response = 2;
  }
}

message LocalityEndpoints {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.LocalityEndpoints";

  config.core.v3.Locality locality = 1;

  repeated config.endpoint.v3.Endpoint endpoints = 2;
}

// The cluster name and locality is provided to Envoy for the endpoints that it
// health checks to support statistics reporting, logging and debugging by the
// Envoy instance (outside of HDS). For maximum usefulness, it should match the
// same cluster structure as that provided by EDS.
// [#next-free-field: 6]
message ClusterHealthCheck {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.ClusterHealthCheck";

  string cluster_name = 1;

  repeated config.core.v3.HealthCheck health_checks = 2;

  repeated LocalityEndpoints locality_endpoints = 3;

  // Optional map that gets filtered by :ref:`health_checks.transport_socket_match_criteria <envoy_v3_api_field_config.core.v3.HealthCheck.transport_socket_match_criteria>`
  // on connection when health checking. For more details, see
  // :ref:`config.cluster.v3.Cluster.transport_socket_matches <envoy_v3_api_field_config.cluster.v3.Cluster.transport_socket_matches>`.
  repeated config.cluster.v3.Cluster.TransportSocketMatch transport_socket_matches = 4;

  // Optional configuration used to bind newly established upstream connections.
  // If the address and port are empty, no bind will be performed.
  config.core.v3.BindConfig upstream_bind_config = 5;
}

message HealthCheckSpecifier {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.HealthCheckSpecifier";

  repeated ClusterHealthCheck cluster_health_checks = 1;

  // The default is 1 second.
  google.protobuf.Duration interval = 2;
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message HdsDummy {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/config/cluster/v3:pkg",
        "//envoy/config/core/v3:pkg",
        "//envoy/config/endpoint/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/service/discovery/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.runtime.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";
import "google/protobuf/struct.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.runtime.v3";
option java_outer_classname = "RtdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/runtime/v3;runtimev3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Runtime discovery service (RTDS)]
// RTDS :ref:`configuration overview <config_runtime_rtds>`

// Discovery service for Runtime resources.
service RuntimeDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.service.runtime.v3.Runtime";

  rpc StreamRuntime(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc DeltaRuntime(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc FetchRuntime(discovery.v3.DiscoveryRequest) returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:runtime";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221
message RtdsDummy {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.RtdsDummy";
}

// RTDS resource type. This describes a layer in the runtime virtual filesystem.
message Runtime {
  option (udpa.annotations.versioning).previous_message_type = "envoy.service.discovery.v2.Runtime";

  // Runtime resource name. This makes the Runtime a self-describing xDS
  // resource.
  string name = 1 [(validate.rules).string = {min_len: 1}];

  google.protobuf.Struct layer = 2;
}
syntax = "proto3";

package envoy.service.discovery.v2;

import "envoy/api/v2/discovery.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.discovery.v2";
option java_outer_classname = "AdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/discovery/v2;discoveryv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Aggregated Discovery Service (ADS)]

// Discovery services for endpoints, clusters, routes,
// and listeners are retained in the package `envoy.api.v2` for backwards
// compatibility with existing management servers. New development in discovery
// services should proceed in the package `envoy.service.discovery.v2`.

// See https://github.com/envoyproxy/envoy-api#apis for a description of the role of
// ADS and how it is intended to be used by a management server. ADS requests
// have the same structure as their singleton xDS counterparts, but can
// multiplex many resource types on a single stream. The type_url in the
// DiscoveryRequest/DiscoveryResponse provides sufficient information to recover
// the multiplexed singleton APIs at the Envoy instance and management server.
service AggregatedDiscoveryService {
  // This is a gRPC-only API.
  rpc StreamAggregatedResources(stream api.v2.DiscoveryRequest)
      returns (stream api.v2.DiscoveryResponse) {
  }

  rpc DeltaAggregatedResources(stream api.v2.DeltaDiscoveryRequest)
      returns (stream api.v2.DeltaDiscoveryResponse) {
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221
message AdsDummy {
}
syntax = "proto3";

package envoy.service.discovery.v2;

import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/health_check.proto";
import "envoy/api/v2/endpoint/endpoint_components.proto";

import "google/api/annotations.proto";
import "google/protobuf/duration.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.discovery.v2";
option java_outer_classname = "HdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/discovery/v2;discoveryv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.health.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Health Discovery Service (HDS)]

// HDS is Health Discovery Service. It compliments Envoy’s health checking
// service by designating this Envoy to be a healthchecker for a subset of hosts
// in the cluster. The status of these health checks will be reported to the
// management server, where it can be aggregated etc and redistributed back to
// Envoy through EDS.
service HealthDiscoveryService {
  // 1. Envoy starts up and if its can_healthcheck option in the static
  //    bootstrap config is enabled, sends HealthCheckRequest to the management
  //    server. It supplies its capabilities (which protocol it can health check
  //    with, what zone it resides in, etc.).
  // 2. In response to (1), the management server designates this Envoy as a
  //    healthchecker to health check a subset of all upstream hosts for a given
  //    cluster (for example upstream Host 1 and Host 2). It streams
  //    HealthCheckSpecifier messages with cluster related configuration for all
  //    clusters this Envoy is designated to health check. Subsequent
  //    HealthCheckSpecifier message will be sent on changes to:
  //    a. Endpoints to health checks
  //    b. Per cluster configuration change
  // 3. Envoy creates a health probe based on the HealthCheck config and sends
  //    it to endpoint(ip:port) of Host 1 and 2. Based on the HealthCheck
  //    configuration Envoy waits upon the arrival of the probe response and
  //    looks at the content of the response to decide whether the endpoint is
  //    healthy or not. If a response hasn't been received within the timeout
  //    interval, the endpoint health status is considered TIMEOUT.
  // 4. Envoy reports results back in an EndpointHealthResponse message.
  //    Envoy streams responses as often as the interval configured by the
  //    management server in HealthCheckSpecifier.
  // 5. The management Server collects health statuses for all endpoints in the
  //    cluster (for all clusters) and uses this information to construct
  //    EndpointDiscoveryResponse messages.
  // 6. Once Envoy has a list of upstream endpoints to send traffic to, it load
  //    balances traffic to them without additional health checking. It may
  //    use inline healthcheck (i.e. consider endpoint UNHEALTHY if connection
  //    failed to a particular endpoint to account for health status propagation
  //    delay between HDS and EDS).
  // By default, can_healthcheck is true. If can_healthcheck is false, Cluster
  // configuration may not contain HealthCheck message.
  // TODO(htuch): How is can_healthcheck communicated to CDS to ensure the above
  // invariant?
  // TODO(htuch): Add @amb67's diagram.
  rpc StreamHealthCheck(stream HealthCheckRequestOrEndpointHealthResponse)
      returns (stream HealthCheckSpecifier) {
  }

  // TODO(htuch): Unlike the gRPC version, there is no stream-based binding of
  // request/response. Should we add an identifier to the HealthCheckSpecifier
  // to bind with the response?
  rpc FetchHealthCheck(HealthCheckRequestOrEndpointHealthResponse) returns (HealthCheckSpecifier) {
    option (google.api.http).post = "/v2/discovery:health_check";
    option (google.api.http).body = "*";
  }
}

// Defines supported protocols etc, so the management server can assign proper
// endpoints to healthcheck.
message Capability {
  // Different Envoy instances may have different capabilities (e.g. Redis)
  // and/or have ports enabled for different protocols.
  enum Protocol {
    HTTP = 0;
    TCP = 1;
    REDIS = 2;
  }

  repeated Protocol health_check_protocols = 1;
}

message HealthCheckRequest {
  api.v2.core.Node node = 1;

  Capability capability = 2;
}

message EndpointHealth {
  api.v2.endpoint.Endpoint endpoint = 1;

  api.v2.core.HealthStatus health_status = 2;
}

message EndpointHealthResponse {
  repeated EndpointHealth endpoints_health = 1;
}

message HealthCheckRequestOrEndpointHealthResponse {
  oneof request_type {
    HealthCheckRequest health_check_request = 1;

    EndpointHealthResponse endpoint_health_response = 2;
  }
}

message LocalityEndpoints {
  api.v2.core.Locality locality = 1;

  repeated api.v2.endpoint.Endpoint endpoints = 2;
}

// The cluster name and locality is provided to Envoy for the endpoints that it
// health checks to support statistics reporting, logging and debugging by the
// Envoy instance (outside of HDS). For maximum usefulness, it should match the
// same cluster structure as that provided by EDS.
message ClusterHealthCheck {
  string cluster_name = 1;

  repeated api.v2.core.HealthCheck health_checks = 2;

  repeated LocalityEndpoints locality_endpoints = 3;
}

message HealthCheckSpecifier {
  repeated ClusterHealthCheck cluster_health_checks = 1;

  // The default is 1 second.
  google.protobuf.Duration interval = 2;
}
syntax = "proto3";

package envoy.service.discovery.v2;

import "envoy/api/v2/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.discovery.v2";
option java_outer_classname = "SdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/discovery/v2;discoveryv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.secret.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Secret Discovery Service (SDS)]

service SecretDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.api.v2.auth.Secret";

  rpc DeltaSecrets(stream api.v2.DeltaDiscoveryRequest)
      returns (stream api.v2.DeltaDiscoveryResponse) {
  }

  rpc StreamSecrets(stream api.v2.DiscoveryRequest) returns (stream api.v2.DiscoveryResponse) {
  }

  rpc FetchSecrets(api.v2.DiscoveryRequest) returns (api.v2.DiscoveryResponse) {
    option (google.api.http).post = "/v2/discovery:secrets";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221
message SdsDummy {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/api/v2:pkg",
        "//envoy/api/v2/core:pkg",
        "//envoy/api/v2/endpoint:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.discovery.v2;

import "envoy/api/v2/discovery.proto";

import "google/api/annotations.proto";
import "google/protobuf/struct.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.discovery.v2";
option java_outer_classname = "RtdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/discovery/v2;discoveryv2";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.runtime.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Runtime Discovery Service (RTDS)]
// RTDS :ref:`configuration overview <config_runtime_rtds>`

// Discovery service for Runtime resources.
service RuntimeDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.service.discovery.v2.Runtime";

  rpc StreamRuntime(stream api.v2.DiscoveryRequest) returns (stream api.v2.DiscoveryResponse) {
  }

  rpc DeltaRuntime(stream api.v2.DeltaDiscoveryRequest)
      returns (stream api.v2.DeltaDiscoveryResponse) {
  }

  rpc FetchRuntime(api.v2.DiscoveryRequest) returns (api.v2.DiscoveryResponse) {
    option (google.api.http).post = "/v2/discovery:runtime";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221
message RtdsDummy {
}

// RTDS resource type. This describes a layer in the runtime virtual filesystem.
message Runtime {
  // Runtime resource name. This makes the Runtime a self-describing xDS
  // resource.
  string name = 1 [(validate.rules).string = {min_bytes: 1}];

  google.protobuf.Struct layer = 2;
}
syntax = "proto3";

package envoy.service.discovery.v3;

import "envoy/config/core/v3/base.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/rpc/status.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.discovery.v3";
option java_outer_classname = "DiscoveryProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/discovery/v3;discoveryv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Common discovery API components]

// Specifies a resource to be subscribed to.
message ResourceLocator {
  // The resource name to subscribe to.
  string name = 1;

  // A set of dynamic parameters used to match against the dynamic parameter
  // constraints on the resource. This allows clients to select between
  // multiple variants of the same resource.
  map<string, string> dynamic_parameters = 2;
}

// Specifies a concrete resource name.
message ResourceName {
  // The name of the resource.
  string name = 1;

  // Dynamic parameter constraints associated with this resource. To be used by client-side caches
  // (including xDS proxies) when matching subscribed resource locators.
  DynamicParameterConstraints dynamic_parameter_constraints = 2;
}

// A DiscoveryRequest requests a set of versioned resources of the same type for
// a given Envoy node on some API.
// [#next-free-field: 8]
message DiscoveryRequest {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.DiscoveryRequest";

  // The version_info provided in the request messages will be the version_info
  // received with the most recent successfully processed response or empty on
  // the first request. It is expected that no new request is sent after a
  // response is received until the Envoy instance is ready to ACK/NACK the new
  // configuration. ACK/NACK takes place by returning the new API config version
  // as applied or the previous API config version respectively. Each type_url
  // (see below) has an independent version associated with it.
  string version_info = 1;

  // The node making the request.
  config.core.v3.Node node = 2;

  // List of resources to subscribe to, e.g. list of cluster names or a route
  // configuration name. If this is empty, all resources for the API are
  // returned. LDS/CDS may have empty resource_names, which will cause all
  // resources for the Envoy instance to be returned. The LDS and CDS responses
  // will then imply a number of resources that need to be fetched via EDS/RDS,
  // which will be explicitly enumerated in resource_names.
  repeated string resource_names = 3;

  // [#not-implemented-hide:]
  // Alternative to ``resource_names`` field that allows specifying dynamic
  // parameters along with each resource name. Clients that populate this
  // field must be able to handle responses from the server where resources
  // are wrapped in a Resource message.
  // Note that it is legal for a request to have some resources listed
  // in ``resource_names`` and others in ``resource_locators``.
  repeated ResourceLocator resource_locators = 7;

  // Type of the resource that is being requested, e.g.
  // "type.googleapis.com/envoy.api.v2.ClusterLoadAssignment". This is implicit
  // in requests made via singleton xDS APIs such as CDS, LDS, etc. but is
  // required for ADS.
  string type_url = 4;

  // nonce corresponding to DiscoveryResponse being ACK/NACKed. See above
  // discussion on version_info and the DiscoveryResponse nonce comment. This
  // may be empty only if 1) this is a non-persistent-stream xDS such as HTTP,
  // or 2) the client has not yet accepted an update in this xDS stream (unlike
  // delta, where it is populated only for new explicit ACKs).
  string response_nonce = 5;

  // This is populated when the previous :ref:`DiscoveryResponse <envoy_v3_api_msg_service.discovery.v3.DiscoveryResponse>`
  // failed to update configuration. The ``message`` field in ``error_details`` provides the Envoy
  // internal exception related to the failure. It is only intended for consumption during manual
  // debugging, the string provided is not guaranteed to be stable across Envoy versions.
  google.rpc.Status error_detail = 6;
}

// [#next-free-field: 7]
message DiscoveryResponse {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.DiscoveryResponse";

  // The version of the response data.
  string version_info = 1;

  // The response resources. These resources are typed and depend on the API being called.
  repeated google.protobuf.Any resources = 2;

  // [#not-implemented-hide:]
  // Canary is used to support two Envoy command line flags:
  //
  // * --terminate-on-canary-transition-failure. When set, Envoy is able to
  //   terminate if it detects that configuration is stuck at canary. Consider
  //   this example sequence of updates:
  //   - Management server applies a canary config successfully.
  //   - Management server rolls back to a production config.
  //   - Envoy rejects the new production config.
  //   Since there is no sensible way to continue receiving configuration
  //   updates, Envoy will then terminate and apply production config from a
  //   clean slate.
  // * --dry-run-canary. When set, a canary response will never be applied, only
  //   validated via a dry run.
  bool canary = 3;

  // Type URL for resources. Identifies the xDS API when muxing over ADS.
  // Must be consistent with the type_url in the 'resources' repeated Any (if non-empty).
  string type_url = 4;

  // For gRPC based subscriptions, the nonce provides a way to explicitly ack a
  // specific DiscoveryResponse in a following DiscoveryRequest. Additional
  // messages may have been sent by Envoy to the management server for the
  // previous version on the stream prior to this DiscoveryResponse, that were
  // unprocessed at response send time. The nonce allows the management server
  // to ignore any further DiscoveryRequests for the previous version until a
  // DiscoveryRequest bearing the nonce. The nonce is optional and is not
  // required for non-stream based xDS implementations.
  string nonce = 5;

  // The control plane instance that sent the response.
  config.core.v3.ControlPlane control_plane = 6;
}

// DeltaDiscoveryRequest and DeltaDiscoveryResponse are used in a new gRPC
// endpoint for Delta xDS.
//
// With Delta xDS, the DeltaDiscoveryResponses do not need to include a full
// snapshot of the tracked resources. Instead, DeltaDiscoveryResponses are a
// diff to the state of a xDS client.
// In Delta XDS there are per-resource versions, which allow tracking state at
// the resource granularity.
// An xDS Delta session is always in the context of a gRPC bidirectional
// stream. This allows the xDS server to keep track of the state of xDS clients
// connected to it.
//
// In Delta xDS the nonce field is required and used to pair
// DeltaDiscoveryResponse to a DeltaDiscoveryRequest ACK or NACK.
// Optionally, a response message level system_version_info is present for
// debugging purposes only.
//
// DeltaDiscoveryRequest plays two independent roles. Any DeltaDiscoveryRequest
// can be either or both of: [1] informing the server of what resources the
// client has gained/lost interest in (using resource_names_subscribe and
// resource_names_unsubscribe), or [2] (N)ACKing an earlier resource update from
// the server (using response_nonce, with presence of error_detail making it a NACK).
// Additionally, the first message (for a given type_url) of a reconnected gRPC stream
// has a third role: informing the server of the resources (and their versions)
// that the client already possesses, using the initial_resource_versions field.
//
// As with state-of-the-world, when multiple resource types are multiplexed (ADS),
// all requests/acknowledgments/updates are logically walled off by type_url:
// a Cluster ACK exists in a completely separate world from a prior Route NACK.
// In particular, initial_resource_versions being sent at the "start" of every
// gRPC stream actually entails a message for each type_url, each with its own
// initial_resource_versions.
// [#next-free-field: 10]
message DeltaDiscoveryRequest {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.DeltaDiscoveryRequest";

  // The node making the request.
  config.core.v3.Node node = 1;

  // Type of the resource that is being requested, e.g.
  // ``type.googleapis.com/envoy.api.v2.ClusterLoadAssignment``. This does not need to be set if
  // resources are only referenced via ``xds_resource_subscribe`` and
  // ``xds_resources_unsubscribe``.
  string type_url = 2;

  // DeltaDiscoveryRequests allow the client to add or remove individual
  // resources to the set of tracked resources in the context of a stream.
  // All resource names in the resource_names_subscribe list are added to the
  // set of tracked resources and all resource names in the resource_names_unsubscribe
  // list are removed from the set of tracked resources.
  //
  // *Unlike* state-of-the-world xDS, an empty resource_names_subscribe or
  // resource_names_unsubscribe list simply means that no resources are to be
  // added or removed to the resource list.
  // *Like* state-of-the-world xDS, the server must send updates for all tracked
  // resources, but can also send updates for resources the client has not subscribed to.
  //
  // NOTE: the server must respond with all resources listed in resource_names_subscribe,
  // even if it believes the client has the most recent version of them. The reason:
  // the client may have dropped them, but then regained interest before it had a chance
  // to send the unsubscribe message. See DeltaSubscriptionStateTest.RemoveThenAdd.
  //
  // These two fields can be set in any DeltaDiscoveryRequest, including ACKs
  // and initial_resource_versions.
  //
  // A list of Resource names to add to the list of tracked resources.
  repeated string resource_names_subscribe = 3;

  // A list of Resource names to remove from the list of tracked resources.
  repeated string resource_names_unsubscribe = 4;

  // [#not-implemented-hide:]
  // Alternative to ``resource_names_subscribe`` field that allows specifying dynamic parameters
  // along with each resource name.
  // Note that it is legal for a request to have some resources listed
  // in ``resource_names_subscribe`` and others in ``resource_locators_subscribe``.
  repeated ResourceLocator resource_locators_subscribe = 8;

  // [#not-implemented-hide:]
  // Alternative to ``resource_names_unsubscribe`` field that allows specifying dynamic parameters
  // along with each resource name.
  // Note that it is legal for a request to have some resources listed
  // in ``resource_names_unsubscribe`` and others in ``resource_locators_unsubscribe``.
  repeated ResourceLocator resource_locators_unsubscribe = 9;

  // Informs the server of the versions of the resources the xDS client knows of, to enable the
  // client to continue the same logical xDS session even in the face of gRPC stream reconnection.
  // It will not be populated: [1] in the very first stream of a session, since the client will
  // not yet have any resources,  [2] in any message after the first in a stream (for a given
  // type_url), since the server will already be correctly tracking the client's state.
  // (In ADS, the first message *of each type_url* of a reconnected stream populates this map.)
  // The map's keys are names of xDS resources known to the xDS client.
  // The map's values are opaque resource versions.
  map<string, string> initial_resource_versions = 5;

  // When the DeltaDiscoveryRequest is a ACK or NACK message in response
  // to a previous DeltaDiscoveryResponse, the response_nonce must be the
  // nonce in the DeltaDiscoveryResponse.
  // Otherwise (unlike in DiscoveryRequest) response_nonce must be omitted.
  string response_nonce = 6;

  // This is populated when the previous :ref:`DiscoveryResponse <envoy_v3_api_msg_service.discovery.v3.DiscoveryResponse>`
  // failed to update configuration. The ``message`` field in ``error_details``
  // provides the Envoy internal exception related to the failure.
  google.rpc.Status error_detail = 7;
}

// [#next-free-field: 9]
message DeltaDiscoveryResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.api.v2.DeltaDiscoveryResponse";

  // The version of the response data (used for debugging).
  string system_version_info = 1;

  // The response resources. These are typed resources, whose types must match
  // the type_url field.
  repeated Resource resources = 2;

  // field id 3 IS available!

  // Type URL for resources. Identifies the xDS API when muxing over ADS.
  // Must be consistent with the type_url in the Any within 'resources' if 'resources' is non-empty.
  string type_url = 4;

  // Resources names of resources that have be deleted and to be removed from the xDS Client.
  // Removed resources for missing resources can be ignored.
  repeated string removed_resources = 6;

  // Alternative to removed_resources that allows specifying which variant of
  // a resource is being removed. This variant must be used for any resource
  // for which dynamic parameter constraints were sent to the client.
  repeated ResourceName removed_resource_names = 8;

  // The nonce provides a way for DeltaDiscoveryRequests to uniquely
  // reference a DeltaDiscoveryResponse when (N)ACKing. The nonce is required.
  string nonce = 5;

  // [#not-implemented-hide:]
  // The control plane instance that sent the response.
  config.core.v3.ControlPlane control_plane = 7;
}

// A set of dynamic parameter constraints associated with a variant of an individual xDS resource.
// These constraints determine whether the resource matches a subscription based on the set of
// dynamic parameters in the subscription, as specified in the
// :ref:`ResourceLocator.dynamic_parameters<envoy_v3_api_field_service.discovery.v3.ResourceLocator.dynamic_parameters>`
// field. This allows xDS implementations (clients, servers, and caching proxies) to determine
// which variant of a resource is appropriate for a given client.
message DynamicParameterConstraints {
  // A single constraint for a given key.
  message SingleConstraint {
    message Exists {
    }

    // The key to match against.
    string key = 1;

    oneof constraint_type {
      option (validate.required) = true;

      // Matches this exact value.
      string value = 2;

      // Key is present (matches any value except for the key being absent).
      // This allows setting a default constraint for clients that do
      // not send a key at all, while there may be other clients that need
      // special configuration based on that key.
      Exists exists = 3;
    }
  }

  message ConstraintList {
    repeated DynamicParameterConstraints constraints = 1;
  }

  oneof type {
    // A single constraint to evaluate.
    SingleConstraint constraint = 1;

    // A list of constraints that match if any one constraint in the list
    // matches.
    ConstraintList or_constraints = 2;

    // A list of constraints that must all match.
    ConstraintList and_constraints = 3;

    // The inverse (NOT) of a set of constraints.
    DynamicParameterConstraints not_constraints = 4;
  }
}

// [#next-free-field: 10]
message Resource {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.Resource";

  // Cache control properties for the resource.
  // [#not-implemented-hide:]
  message CacheControl {
    // If true, xDS proxies may not cache this resource.
    // Note that this does not apply to clients other than xDS proxies, which must cache resources
    // for their own use, regardless of the value of this field.
    bool do_not_cache = 1;
  }

  // The resource's name, to distinguish it from others of the same type of resource.
  // Only one of ``name`` or ``resource_name`` may be set.
  string name = 3;

  // Alternative to the ``name`` field, to be used when the server supports
  // multiple variants of the named resource that are differentiated by
  // dynamic parameter constraints.
  // Only one of ``name`` or ``resource_name`` may be set.
  ResourceName resource_name = 8;

  // The aliases are a list of other names that this resource can go by.
  repeated string aliases = 4;

  // The resource level version. It allows xDS to track the state of individual
  // resources.
  string version = 1;

  // The resource being tracked.
  google.protobuf.Any resource = 2;

  // Time-to-live value for the resource. For each resource, a timer is started. The timer is
  // reset each time the resource is received with a new TTL. If the resource is received with
  // no TTL set, the timer is removed for the resource. Upon expiration of the timer, the
  // configuration for the resource will be removed.
  //
  // The TTL can be refreshed or changed by sending a response that doesn't change the resource
  // version. In this case the resource field does not need to be populated, which allows for
  // light-weight "heartbeat" updates to keep a resource with a TTL alive.
  //
  // The TTL feature is meant to support configurations that should be removed in the event of
  // a management server failure. For example, the feature may be used for fault injection
  // testing where the fault injection should be terminated in the event that Envoy loses contact
  // with the management server.
  google.protobuf.Duration ttl = 6;

  // Cache control properties for the resource.
  // [#not-implemented-hide:]
  CacheControl cache_control = 7;

  // The Metadata field can be used to provide additional information for the resource.
  // E.g. the trace data for debugging.
  config.core.v3.Metadata metadata = 9;
}
syntax = "proto3";

package envoy.service.discovery.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.discovery.v3";
option java_outer_classname = "AdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/discovery/v3;discoveryv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Aggregated Discovery Service (ADS)]

// Discovery services for endpoints, clusters, routes,
// and listeners are retained in the package `envoy.api.v2` for backwards
// compatibility with existing management servers. New development in discovery
// services should proceed in the package `envoy.service.discovery.v2`.

// See https://github.com/envoyproxy/envoy-api#apis for a description of the role of
// ADS and how it is intended to be used by a management server. ADS requests
// have the same structure as their singleton xDS counterparts, but can
// multiplex many resource types on a single stream. The type_url in the
// DiscoveryRequest/DiscoveryResponse provides sufficient information to recover
// the multiplexed singleton APIs at the Envoy instance and management server.
service AggregatedDiscoveryService {
  // This is a gRPC-only API.
  rpc StreamAggregatedResources(stream DiscoveryRequest) returns (stream DiscoveryResponse) {
  }

  rpc DeltaAggregatedResources(stream DeltaDiscoveryRequest)
      returns (stream DeltaDiscoveryResponse) {
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221
message AdsDummy {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.AdsDummy";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.status.v2;

import "envoy/admin/v2alpha/config_dump.proto";
import "envoy/api/v2/core/base.proto";
import "envoy/type/matcher/node.proto";

import "google/api/annotations.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.status.v2";
option java_outer_classname = "CsdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/status/v2;statusv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Client Status Discovery Service (CSDS)]

// CSDS is Client Status Discovery Service. It can be used to get the status of
// an xDS-compliant client from the management server's point of view. In the
// future, it can potentially be used as an interface to get the current
// state directly from the client.
service ClientStatusDiscoveryService {
  rpc StreamClientStatus(stream ClientStatusRequest) returns (stream ClientStatusResponse) {
  }

  rpc FetchClientStatus(ClientStatusRequest) returns (ClientStatusResponse) {
    option (google.api.http).post = "/v2/discovery:client_status";
    option (google.api.http).body = "*";
  }
}

// Status of a config.
enum ConfigStatus {
  // Status info is not available/unknown.
  UNKNOWN = 0;

  // Management server has sent the config to client and received ACK.
  SYNCED = 1;

  // Config is not sent.
  NOT_SENT = 2;

  // Management server has sent the config to client but hasn’t received
  // ACK/NACK.
  STALE = 3;

  // Management server has sent the config to client but received NACK.
  ERROR = 4;
}

// Request for client status of clients identified by a list of NodeMatchers.
message ClientStatusRequest {
  // Management server can use these match criteria to identify clients.
  // The match follows OR semantics.
  repeated type.matcher.NodeMatcher node_matchers = 1;
}

// Detailed config (per xDS) with status.
// [#next-free-field: 6]
message PerXdsConfig {
  ConfigStatus status = 1;

  oneof per_xds_config {
    admin.v2alpha.ListenersConfigDump listener_config = 2;

    admin.v2alpha.ClustersConfigDump cluster_config = 3;

    admin.v2alpha.RoutesConfigDump route_config = 4;

    admin.v2alpha.ScopedRoutesConfigDump scoped_route_config = 5;
  }
}

// All xds configs for a particular client.
message ClientConfig {
  // Node for a particular client.
  api.v2.core.Node node = 1;

  repeated PerXdsConfig xds_config = 2;
}

message ClientStatusResponse {
  // Client configs for the clients specified in the ClientStatusRequest.
  repeated ClientConfig config = 1;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/admin/v2alpha:pkg",
        "//envoy/api/v2/core:pkg",
        "//envoy/type/matcher:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.status.v3;

import "envoy/admin/v3/config_dump_shared.proto";
import "envoy/config/core/v3/base.proto";
import "envoy/type/matcher/v3/node.proto";

import "google/api/annotations.proto";
import "google/protobuf/any.proto";
import "google/protobuf/timestamp.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.status.v3";
option java_outer_classname = "CsdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/status/v3;statusv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Client status discovery service (CSDS)]

// CSDS is Client Status Discovery Service. It can be used to get the status of
// an xDS-compliant client from the management server's point of view. It can
// also be used to get the current xDS states directly from the client.
service ClientStatusDiscoveryService {
  rpc StreamClientStatus(stream ClientStatusRequest) returns (stream ClientStatusResponse) {
  }

  rpc FetchClientStatus(ClientStatusRequest) returns (ClientStatusResponse) {
    option (google.api.http).post = "/v3/discovery:client_status";
    option (google.api.http).body = "*";
  }
}

// Status of a config from a management server view.
enum ConfigStatus {
  // Status info is not available/unknown.
  UNKNOWN = 0;

  // Management server has sent the config to client and received ACK.
  SYNCED = 1;

  // Config is not sent.
  NOT_SENT = 2;

  // Management server has sent the config to client but hasn’t received
  // ACK/NACK.
  STALE = 3;

  // Management server has sent the config to client but received NACK. The
  // attached config dump will be the latest config (the rejected one), since
  // it is the persisted version in the management server.
  ERROR = 4;
}

// Config status from a client-side view.
enum ClientConfigStatus {
  // Config status is not available/unknown.
  CLIENT_UNKNOWN = 0;

  // Client requested the config but hasn't received any config from management
  // server yet.
  CLIENT_REQUESTED = 1;

  // Client received the config and replied with ACK.
  CLIENT_ACKED = 2;

  // Client received the config and replied with NACK. Notably, the attached
  // config dump is not the NACKed version, but the most recent accepted one. If
  // no config is accepted yet, the attached config dump will be empty.
  CLIENT_NACKED = 3;
}

// Request for client status of clients identified by a list of NodeMatchers.
message ClientStatusRequest {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.status.v2.ClientStatusRequest";

  // Management server can use these match criteria to identify clients.
  // The match follows OR semantics.
  repeated type.matcher.v3.NodeMatcher node_matchers = 1;

  // The node making the csds request.
  config.core.v3.Node node = 2;

  // If true, the server will not include the resource contents in the response
  // (i.e., the generic_xds_configs.xds_config field will not be populated).
  // [#not-implemented-hide:]
  bool exclude_resource_contents = 3;
}

// Detailed config (per xDS) with status.
// [#next-free-field: 8]
message PerXdsConfig {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.status.v2.PerXdsConfig";

  // Config status generated by management servers. Will not be present if the
  // CSDS server is an xDS client.
  ConfigStatus status = 1;

  // Client config status is populated by xDS clients. Will not be present if
  // the CSDS server is an xDS server. No matter what the client config status
  // is, xDS clients should always dump the most recent accepted xDS config.
  //
  // .. attention::
  //   This field is deprecated. Use :ref:`ClientResourceStatus
  //   <envoy_v3_api_enum_admin.v3.ClientResourceStatus>` for per-resource
  //   config status instead.
  ClientConfigStatus client_status = 7
      [deprecated = true, (envoy.annotations.deprecated_at_minor_version) = "3.0"];

  oneof per_xds_config {
    admin.v3.ListenersConfigDump listener_config = 2;

    admin.v3.ClustersConfigDump cluster_config = 3;

    admin.v3.RoutesConfigDump route_config = 4;

    admin.v3.ScopedRoutesConfigDump scoped_route_config = 5;

    admin.v3.EndpointsConfigDump endpoint_config = 6;
  }
}

// All xds configs for a particular client.
message ClientConfig {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.status.v2.ClientConfig";

  // GenericXdsConfig is used to specify the config status and the dump
  // of any xDS resource identified by their type URL. It is the generalized
  // version of the now deprecated ListenersConfigDump, ClustersConfigDump etc
  // [#next-free-field: 10]
  message GenericXdsConfig {
    // Type_url represents the fully qualified name of xDS resource type
    // like envoy.v3.Cluster, envoy.v3.ClusterLoadAssignment etc.
    string type_url = 1;

    // Name of the xDS resource
    string name = 2;

    // This is the :ref:`version_info <envoy_v3_api_field_service.discovery.v3.DiscoveryResponse.version_info>`
    // in the last processed xDS discovery response. If there are only
    // static bootstrap listeners, this field will be ""
    string version_info = 3;

    // The xDS resource config. Actual content depends on the type
    google.protobuf.Any xds_config = 4;

    // Timestamp when the xDS resource was last updated
    google.protobuf.Timestamp last_updated = 5;

    // Per xDS resource config status. It is generated by management servers.
    // It will not be present if the CSDS server is an xDS client.
    ConfigStatus config_status = 6;

    // Per xDS resource status from the view of a xDS client
    admin.v3.ClientResourceStatus client_status = 7;

    // Set if the last update failed, cleared after the next successful
    // update. The *error_state* field contains the rejected version of
    // this particular resource along with the reason and timestamp. For
    // successfully updated or acknowledged resource, this field should
    // be empty.
    // [#not-implemented-hide:]
    admin.v3.UpdateFailureState error_state = 8;

    // Is static resource is true if it is specified in the config supplied
    // through the file at the startup.
    bool is_static_resource = 9;
  }

  // Node for a particular client.
  config.core.v3.Node node = 1;

  // This field is deprecated in favor of generic_xds_configs which is
  // much simpler and uniform in structure.
  repeated PerXdsConfig xds_config = 2
      [deprecated = true, (envoy.annotations.deprecated_at_minor_version) = "3.0"];

  // Represents generic xDS config and the exact config structure depends on
  // the type URL (like Cluster if it is CDS)
  repeated GenericXdsConfig generic_xds_configs = 3;

  // For xDS clients, the scope in which the data is used.
  // For example, gRPC indicates the data plane target or that the data is
  // associated with gRPC server(s).
  string client_scope = 4;
}

message ClientStatusResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.status.v2.ClientStatusResponse";

  // Client configs for the clients specified in the ClientStatusRequest.
  repeated ClientConfig config = 1;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/admin/v3:pkg",
        "//envoy/annotations:pkg",
        "//envoy/config/core/v3:pkg",
        "//envoy/type/matcher/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.tap.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/data/tap/v3/wrapper.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.tap.v3";
option java_outer_classname = "TapProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/tap/v3;tapv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Tap sink service]

// [#not-implemented-hide:] A tap service to receive incoming taps. Envoy will call
// StreamTaps to deliver captured taps to the server
service TapSinkService {
  // Envoy will connect and send StreamTapsRequest messages forever. It does not expect any
  // response to be sent as nothing would be done in the case of failure. The server should
  // disconnect if it expects Envoy to reconnect.
  rpc StreamTaps(stream StreamTapsRequest) returns (StreamTapsResponse) {
  }
}

// [#not-implemented-hide:] Stream message for the Tap API. Envoy will open a stream to the server
// and stream taps without ever expecting a response.
message StreamTapsRequest {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.tap.v2alpha.StreamTapsRequest";

  message Identifier {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.tap.v2alpha.StreamTapsRequest.Identifier";

    // The node sending taps over the stream.
    config.core.v3.Node node = 1 [(validate.rules).message = {required: true}];

    // The opaque identifier that was set in the :ref:`output config
    // <envoy_v3_api_field_config.tap.v3.StreamingGrpcSink.tap_id>`.
    string tap_id = 2;
  }

  // Identifier data effectively is a structured metadata. As a performance optimization this will
  // only be sent in the first message on the stream.
  Identifier identifier = 1;

  // The trace id. this can be used to merge together a streaming trace. Note that the trace_id
  // is not guaranteed to be spatially or temporally unique.
  uint64 trace_id = 2;

  // The trace data.
  data.tap.v3.TraceWrapper trace = 3;
}

// [#not-implemented-hide:]
message StreamTapsResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.tap.v2alpha.StreamTapsResponse";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "//envoy/data/tap/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.tap.v2alpha;

import "envoy/api/v2/core/base.proto";
import "envoy/data/tap/v2alpha/wrapper.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.tap.v2alpha";
option java_outer_classname = "TapProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/tap/v2alpha";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Tap Sink Service]

// [#not-implemented-hide:] A tap service to receive incoming taps. Envoy will call
// StreamTaps to deliver captured taps to the server
service TapSinkService {
  // Envoy will connect and send StreamTapsRequest messages forever. It does not expect any
  // response to be sent as nothing would be done in the case of failure. The server should
  // disconnect if it expects Envoy to reconnect.
  rpc StreamTaps(stream StreamTapsRequest) returns (StreamTapsResponse) {
  }
}

// [#not-implemented-hide:] Stream message for the Tap API. Envoy will open a stream to the server
// and stream taps without ever expecting a response.
message StreamTapsRequest {
  message Identifier {
    // The node sending taps over the stream.
    api.v2.core.Node node = 1 [(validate.rules).message = {required: true}];

    // The opaque identifier that was set in the :ref:`output config
    // <envoy_api_field_service.tap.v2alpha.StreamingGrpcSink.tap_id>`.
    string tap_id = 2;
  }

  // Identifier data effectively is a structured metadata. As a performance optimization this will
  // only be sent in the first message on the stream.
  Identifier identifier = 1;

  // The trace id. this can be used to merge together a streaming trace. Note that the trace_id
  // is not guaranteed to be spatially or temporally unique.
  uint64 trace_id = 2;

  // The trace data.
  data.tap.v2alpha.TraceWrapper trace = 3;
}

// [#not-implemented-hide:]
message StreamTapsResponse {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "//envoy/api/v2/route:pkg",
        "//envoy/data/tap/v2alpha:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.tap.v2alpha;

import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/core/grpc_service.proto";
import "envoy/api/v2/route/route_components.proto";

import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.tap.v2alpha";
option java_outer_classname = "CommonProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/tap/v2alpha";
option (udpa.annotations.file_migrate).move_to_package = "envoy.config.tap.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Common tap configuration]

// Tap configuration.
message TapConfig {
  // [#comment:TODO(mattklein123): Rate limiting]

  // The match configuration. If the configuration matches the data source being tapped, a tap will
  // occur, with the result written to the configured output.
  MatchPredicate match_config = 1 [(validate.rules).message = {required: true}];

  // The tap output configuration. If a match configuration matches a data source being tapped,
  // a tap will occur and the data will be written to the configured output.
  OutputConfig output_config = 2 [(validate.rules).message = {required: true}];

  // [#not-implemented-hide:] Specify if Tap matching is enabled. The % of requests\connections for
  // which the tap matching is enabled. When not enabled, the request\connection will not be
  // recorded.
  //
  // .. note::
  //
  //   This field defaults to 100/:ref:`HUNDRED
  //   <envoy_api_enum_type.FractionalPercent.DenominatorType>`.
  api.v2.core.RuntimeFractionalPercent tap_enabled = 3;
}

// Tap match configuration. This is a recursive structure which allows complex nested match
// configurations to be built using various logical operators.
// [#next-free-field: 9]
message MatchPredicate {
  // A set of match configurations used for logical operations.
  message MatchSet {
    // The list of rules that make up the set.
    repeated MatchPredicate rules = 1 [(validate.rules).repeated = {min_items: 2}];
  }

  oneof rule {
    option (validate.required) = true;

    // A set that describes a logical OR. If any member of the set matches, the match configuration
    // matches.
    MatchSet or_match = 1;

    // A set that describes a logical AND. If all members of the set match, the match configuration
    // matches.
    MatchSet and_match = 2;

    // A negation match. The match configuration will match if the negated match condition matches.
    MatchPredicate not_match = 3;

    // The match configuration will always match.
    bool any_match = 4 [(validate.rules).bool = {const: true}];

    // HTTP request headers match configuration.
    HttpHeadersMatch http_request_headers_match = 5;

    // HTTP request trailers match configuration.
    HttpHeadersMatch http_request_trailers_match = 6;

    // HTTP response headers match configuration.
    HttpHeadersMatch http_response_headers_match = 7;

    // HTTP response trailers match configuration.
    HttpHeadersMatch http_response_trailers_match = 8;
  }
}

// HTTP headers match configuration.
message HttpHeadersMatch {
  // HTTP headers to match.
  repeated api.v2.route.HeaderMatcher headers = 1;
}

// Tap output configuration.
message OutputConfig {
  // Output sinks for tap data. Currently a single sink is allowed in the list. Once multiple
  // sink types are supported this constraint will be relaxed.
  repeated OutputSink sinks = 1 [(validate.rules).repeated = {min_items: 1 max_items: 1}];

  // For buffered tapping, the maximum amount of received body that will be buffered prior to
  // truncation. If truncation occurs, the :ref:`truncated
  // <envoy_api_field_data.tap.v2alpha.Body.truncated>` field will be set. If not specified, the
  // default is 1KiB.
  google.protobuf.UInt32Value max_buffered_rx_bytes = 2;

  // For buffered tapping, the maximum amount of transmitted body that will be buffered prior to
  // truncation. If truncation occurs, the :ref:`truncated
  // <envoy_api_field_data.tap.v2alpha.Body.truncated>` field will be set. If not specified, the
  // default is 1KiB.
  google.protobuf.UInt32Value max_buffered_tx_bytes = 3;

  // Indicates whether taps produce a single buffered message per tap, or multiple streamed
  // messages per tap in the emitted :ref:`TraceWrapper
  // <envoy_api_msg_data.tap.v2alpha.TraceWrapper>` messages. Note that streamed tapping does not
  // mean that no buffering takes place. Buffering may be required if data is processed before a
  // match can be determined. See the HTTP tap filter :ref:`streaming
  // <config_http_filters_tap_streaming>` documentation for more information.
  bool streaming = 4;
}

// Tap output sink configuration.
message OutputSink {
  // Output format. All output is in the form of one or more :ref:`TraceWrapper
  // <envoy_api_msg_data.tap.v2alpha.TraceWrapper>` messages. This enumeration indicates
  // how those messages are written. Note that not all sinks support all output formats. See
  // individual sink documentation for more information.
  enum Format {
    // Each message will be written as JSON. Any :ref:`body <envoy_api_msg_data.tap.v2alpha.Body>`
    // data will be present in the :ref:`as_bytes
    // <envoy_api_field_data.tap.v2alpha.Body.as_bytes>` field. This means that body data will be
    // base64 encoded as per the `proto3 JSON mappings
    // <https://developers.google.com/protocol-buffers/docs/proto3#json>`_.
    JSON_BODY_AS_BYTES = 0;

    // Each message will be written as JSON. Any :ref:`body <envoy_api_msg_data.tap.v2alpha.Body>`
    // data will be present in the :ref:`as_string
    // <envoy_api_field_data.tap.v2alpha.Body.as_string>` field. This means that body data will be
    // string encoded as per the `proto3 JSON mappings
    // <https://developers.google.com/protocol-buffers/docs/proto3#json>`_. This format type is
    // useful when it is known that that body is human readable (e.g., JSON over HTTP) and the
    // user wishes to view it directly without being forced to base64 decode the body.
    JSON_BODY_AS_STRING = 1;

    // Binary proto format. Note that binary proto is not self-delimiting. If a sink writes
    // multiple binary messages without any length information the data stream will not be
    // useful. However, for certain sinks that are self-delimiting (e.g., one message per file)
    // this output format makes consumption simpler.
    PROTO_BINARY = 2;

    // Messages are written as a sequence tuples, where each tuple is the message length encoded
    // as a `protobuf 32-bit varint
    // <https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.io.coded_stream>`_
    // followed by the binary message. The messages can be read back using the language specific
    // protobuf coded stream implementation to obtain the message length and the message.
    PROTO_BINARY_LENGTH_DELIMITED = 3;

    // Text proto format.
    PROTO_TEXT = 4;
  }

  // Sink output format.
  Format format = 1 [(validate.rules).enum = {defined_only: true}];

  oneof output_sink_type {
    option (validate.required) = true;

    // Tap output will be streamed out the :http:post:`/tap` admin endpoint.
    //
    // .. attention::
    //
    //   It is only allowed to specify the streaming admin output sink if the tap is being
    //   configured from the :http:post:`/tap` admin endpoint. Thus, if an extension has
    //   been configured to receive tap configuration from some other source (e.g., static
    //   file, XDS, etc.) configuring the streaming admin output type will fail.
    StreamingAdminSink streaming_admin = 2;

    // Tap output will be written to a file per tap sink.
    FilePerTapSink file_per_tap = 3;

    // [#not-implemented-hide:]
    // GrpcService to stream data to. The format argument must be PROTO_BINARY.
    StreamingGrpcSink streaming_grpc = 4;
  }
}

// Streaming admin sink configuration.
message StreamingAdminSink {
}

// The file per tap sink outputs a discrete file for every tapped stream.
message FilePerTapSink {
  // Path prefix. The output file will be of the form <path_prefix>_<id>.pb, where <id> is an
  // identifier distinguishing the recorded trace for stream instances (the Envoy
  // connection ID, HTTP stream ID, etc.).
  string path_prefix = 1 [(validate.rules).string = {min_bytes: 1}];
}

// [#not-implemented-hide:] Streaming gRPC sink configuration sends the taps to an external gRPC
// server.
message StreamingGrpcSink {
  // Opaque identifier, that will be sent back to the streaming grpc server.
  string tap_id = 1;

  // The gRPC server that hosts the Tap Sink Service.
  api.v2.core.GrpcService grpc_service = 2 [(validate.rules).message = {required: true}];
}
Protocol buffer definitions for gRPC and REST services.

Visibility should be constrained to none (default).
syntax = "proto3";

package envoy.service.ratelimit.v2;

import "envoy/api/v2/core/base.proto";
import "envoy/api/v2/ratelimit/ratelimit.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.ratelimit.v2";
option java_outer_classname = "RlsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/ratelimit/v2;ratelimitv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Rate Limit Service (RLS)]

service RateLimitService {
  // Determine whether rate limiting should take place.
  rpc ShouldRateLimit(RateLimitRequest) returns (RateLimitResponse) {
  }
}

// Main message for a rate limit request. The rate limit service is designed to be fully generic
// in the sense that it can operate on arbitrary hierarchical key/value pairs. The loaded
// configuration will parse the request and find the most specific limit to apply. In addition,
// a RateLimitRequest can contain multiple "descriptors" to limit on. When multiple descriptors
// are provided, the server will limit on *ALL* of them and return an OVER_LIMIT response if any
// of them are over limit. This enables more complex application level rate limiting scenarios
// if desired.
message RateLimitRequest {
  // All rate limit requests must specify a domain. This enables the configuration to be per
  // application without fear of overlap. E.g., "envoy".
  string domain = 1;

  // All rate limit requests must specify at least one RateLimitDescriptor. Each descriptor is
  // processed by the service (see below). If any of the descriptors are over limit, the entire
  // request is considered to be over limit.
  repeated api.v2.ratelimit.RateLimitDescriptor descriptors = 2;

  // Rate limit requests can optionally specify the number of hits a request adds to the matched
  // limit. If the value is not set in the message, a request increases the matched limit by 1.
  uint32 hits_addend = 3;
}

// A response from a ShouldRateLimit call.
message RateLimitResponse {
  enum Code {
    // The response code is not known.
    UNKNOWN = 0;

    // The response code to notify that the number of requests are under limit.
    OK = 1;

    // The response code to notify that the number of requests are over limit.
    OVER_LIMIT = 2;
  }

  // Defines an actual rate limit in terms of requests per unit of time and the unit itself.
  message RateLimit {
    enum Unit {
      // The time unit is not known.
      UNKNOWN = 0;

      // The time unit representing a second.
      SECOND = 1;

      // The time unit representing a minute.
      MINUTE = 2;

      // The time unit representing an hour.
      HOUR = 3;

      // The time unit representing a day.
      DAY = 4;
    }

    // A name or description of this limit.
    string name = 3;

    // The number of requests per unit of time.
    uint32 requests_per_unit = 1;

    // The unit of time.
    Unit unit = 2;
  }

  message DescriptorStatus {
    // The response code for an individual descriptor.
    Code code = 1;

    // The current limit as configured by the server. Useful for debugging, etc.
    RateLimit current_limit = 2;

    // The limit remaining in the current time unit.
    uint32 limit_remaining = 3;
  }

  // The overall response code which takes into account all of the descriptors that were passed
  // in the RateLimitRequest message.
  Code overall_code = 1;

  // A list of DescriptorStatus messages which matches the length of the descriptor list passed
  // in the RateLimitRequest. This can be used by the caller to determine which individual
  // descriptors failed and/or what the currently configured limits are for all of them.
  repeated DescriptorStatus statuses = 2;

  // A list of headers to add to the response
  repeated api.v2.core.HeaderValue headers = 3
      [(udpa.annotations.field_migrate).rename = "response_headers_to_add"];

  // A list of headers to add to the request when forwarded
  repeated api.v2.core.HeaderValue request_headers_to_add = 4;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "//envoy/api/v2/ratelimit:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.ratelimit.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/extensions/common/ratelimit/v3/ratelimit.proto";

import "google/protobuf/duration.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.ratelimit.v3";
option java_outer_classname = "RlsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/ratelimit/v3;ratelimitv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Rate limit service (RLS)]

service RateLimitService {
  // Determine whether rate limiting should take place.
  rpc ShouldRateLimit(RateLimitRequest) returns (RateLimitResponse) {
  }
}

// Main message for a rate limit request. The rate limit service is designed to be fully generic
// in the sense that it can operate on arbitrary hierarchical key/value pairs. The loaded
// configuration will parse the request and find the most specific limit to apply. In addition,
// a RateLimitRequest can contain multiple "descriptors" to limit on. When multiple descriptors
// are provided, the server will limit on *ALL* of them and return an OVER_LIMIT response if any
// of them are over limit. This enables more complex application level rate limiting scenarios
// if desired.
message RateLimitRequest {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.ratelimit.v2.RateLimitRequest";

  // All rate limit requests must specify a domain. This enables the configuration to be per
  // application without fear of overlap. E.g., "envoy".
  string domain = 1;

  // All rate limit requests must specify at least one RateLimitDescriptor. Each descriptor is
  // processed by the service (see below). If any of the descriptors are over limit, the entire
  // request is considered to be over limit.
  repeated envoy.extensions.common.ratelimit.v3.RateLimitDescriptor descriptors = 2;

  // Rate limit requests can optionally specify the number of hits a request adds to the matched
  // limit. If the value is not set in the message, a request increases the matched limit by 1.
  uint32 hits_addend = 3;
}

// A response from a ShouldRateLimit call.
// [#next-free-field: 8]
message RateLimitResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.ratelimit.v2.RateLimitResponse";

  enum Code {
    // The response code is not known.
    UNKNOWN = 0;

    // The response code to notify that the number of requests are under limit.
    OK = 1;

    // The response code to notify that the number of requests are over limit.
    OVER_LIMIT = 2;
  }

  // Defines an actual rate limit in terms of requests per unit of time and the unit itself.
  message RateLimit {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.ratelimit.v2.RateLimitResponse.RateLimit";

    // Identifies the unit of of time for rate limit.
    // [#comment: replace by envoy/type/v3/ratelimit_unit.proto in v4]
    enum Unit {
      // The time unit is not known.
      UNKNOWN = 0;

      // The time unit representing a second.
      SECOND = 1;

      // The time unit representing a minute.
      MINUTE = 2;

      // The time unit representing an hour.
      HOUR = 3;

      // The time unit representing a day.
      DAY = 4;

      // The time unit representing a month.
      MONTH = 5;

      // The time unit representing a year.
      YEAR = 6;
    }

    // A name or description of this limit.
    string name = 3;

    // The number of requests per unit of time.
    uint32 requests_per_unit = 1;

    // The unit of time.
    Unit unit = 2;
  }

  // Cacheable quota for responses.
  // Quota can be granted at different levels: either for each individual descriptor or for the whole descriptor set.
  // This is a certain number of requests over a period of time.
  // The client may cache this result and apply the effective RateLimitResponse to future matching
  // requests without querying rate limit service.
  //
  // When quota expires due to timeout, a new RLS request will also be made.
  // The implementation may choose to preemptively query the rate limit server for more quota on or
  // before expiration or before the available quota runs out.
  // [#not-implemented-hide:]
  message Quota {
    // Number of matching requests granted in quota. Must be 1 or more.
    uint32 requests = 1 [(validate.rules).uint32 = {gt: 0}];

    oneof expiration_specifier {
      // Point in time at which the quota expires.
      google.protobuf.Timestamp valid_until = 2;
    }

    // The unique id that is associated with each Quota either at individual descriptor level or whole descriptor set level.
    //
    // For a matching policy with boolean logic, for example, match: "request.headers['environment'] == 'staging' || request.headers['environment'] == 'dev'"),
    // the request_headers action produces a distinct list of descriptors for each possible value of the ‘environment’ header even though the granted quota is same.
    // Thus, the client will use this id information (returned from RLS server) to correctly correlate the multiple descriptors/descriptor sets that have been granted with same quota (i.e., share the same quota among multiple descriptors or descriptor sets.)
    //
    // If id is empty, this id field will be ignored. If quota for the same id changes (e.g. due to configuration update), the old quota will be overridden by the new one. Shared quotas referenced by ID will still adhere to expiration after `valid_until`.
    string id = 3;
  }

  // [#next-free-field: 6]
  message DescriptorStatus {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.ratelimit.v2.RateLimitResponse.DescriptorStatus";

    // The response code for an individual descriptor.
    Code code = 1;

    // The current limit as configured by the server. Useful for debugging, etc.
    RateLimit current_limit = 2;

    // The limit remaining in the current time unit.
    uint32 limit_remaining = 3;

    // Duration until reset of the current limit window.
    google.protobuf.Duration duration_until_reset = 4;

    // Quota is available for a request if its descriptor set has cached quota available for all
    // descriptors.
    // This is for each individual descriptor in the descriptor set. The client will perform matches for each individual descriptor against available per-descriptor quota.
    //
    // If quota is available, a RLS request will not be made and the quota will be reduced by 1 for
    // all matching descriptors.
    //
    // If there is not sufficient quota, there are three cases:
    // 1. A cached entry exists for a RLS descriptor that is out-of-quota, but not expired.
    //    In this case, the request will be treated as OVER_LIMIT.
    // 2. Some RLS descriptors have a cached entry that has valid quota but some RLS descriptors
    //    have no cached entry. This will trigger a new RLS request.
    //    When the result is returned, a single unit will be consumed from the quota for all
    //    matching descriptors.
    //    If the server did not provide a quota, such as the quota message is empty for some of
    //    the descriptors, then the request admission is determined by the
    //    :ref:`overall_code <envoy_v3_api_field_service.ratelimit.v3.RateLimitResponse.overall_code>`.
    // 3. All RLS descriptors lack a cached entry, this will trigger a new RLS request,
    //    When the result is returned, a single unit will be consumed from the quota for all
    //    matching descriptors.
    //    If the server did not provide a quota, such as the quota message is empty for some of
    //    the descriptors, then the request admission is determined by the
    //    :ref:`overall_code <envoy_v3_api_field_service.ratelimit.v3.RateLimitResponse.overall_code>`.
    // [#not-implemented-hide:]
    Quota quota = 5;
  }

  // The overall response code which takes into account all of the descriptors that were passed
  // in the RateLimitRequest message.
  Code overall_code = 1;

  // A list of DescriptorStatus messages which matches the length of the descriptor list passed
  // in the RateLimitRequest. This can be used by the caller to determine which individual
  // descriptors failed and/or what the currently configured limits are for all of them.
  repeated DescriptorStatus statuses = 2;

  // A list of headers to add to the response
  repeated config.core.v3.HeaderValue response_headers_to_add = 3;

  // A list of headers to add to the request when forwarded
  repeated config.core.v3.HeaderValue request_headers_to_add = 4;

  // A response body to send to the downstream client when the response code is not OK.
  bytes raw_body = 5;

  // Optional response metadata that will be emitted as dynamic metadata to be consumed by the next
  // filter. This metadata lives in a namespace specified by the canonical name of extension filter
  // that requires it:
  //
  // - :ref:`envoy.filters.http.ratelimit <config_http_filters_ratelimit_dynamic_metadata>` for HTTP filter.
  // - :ref:`envoy.filters.network.ratelimit <config_network_filters_ratelimit_dynamic_metadata>` for network filter.
  // - :ref:`envoy.filters.thrift.rate_limit <config_thrift_filters_rate_limit_dynamic_metadata>` for Thrift filter.
  google.protobuf.Struct dynamic_metadata = 6;

  // Quota is available for a request if its entire descriptor set has cached quota available.
  // This is a union of all descriptors in the descriptor set. Clients can use the quota for future matches if and only if the descriptor set matches what was sent in the request that originated this response.
  //
  // If quota is available, a RLS request will not be made and the quota will be reduced by 1.
  // If quota is not available (i.e., a cached entry doesn't exist for a RLS descriptor set), a RLS request will be triggered.
  // If the server did not provide a quota, such as the quota message is empty then the request admission is determined by the
  // :ref:`overall_code <envoy_v3_api_field_service.ratelimit.v3.RateLimitResponse.overall_code>`.
  //
  // If there is not sufficient quota and the cached entry exists for a RLS descriptor set is out-of-quota but not expired,
  // the request will be treated as OVER_LIMIT.
  // [#not-implemented-hide:]
  Quota quota = 7;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "//envoy/extensions/common/ratelimit/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.endpoint.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.service.endpoint.v3";
option java_outer_classname = "LedsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/endpoint/v3;endpointv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#not-implemented-hide:]
// [#protodoc-title: LEDS]
// Locality-Endpoint discovery
// [#comment:TODO(adisuissa): Link to unified matching docs:
// :ref:`architecture overview<arch_overview_service_discovery_types_leds>`]

service LocalityEndpointDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.endpoint.v3.LbEndpoint";

  // State-of-the-World (DiscoveryRequest) and REST are not supported.

  // The resource_names_subscribe resource_names_unsubscribe fields in DeltaDiscoveryRequest
  // specify a list of glob collections to subscribe to updates for.
  rpc DeltaLocalityEndpoints(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message LedsDummy {
}
syntax = "proto3";

package envoy.service.endpoint.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.endpoint.v3";
option java_outer_classname = "EdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/endpoint/v3;endpointv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: EDS]
// Endpoint discovery :ref:`architecture overview <arch_overview_service_discovery_types_eds>`

service EndpointDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.config.endpoint.v3.ClusterLoadAssignment";

  // The resource_names field in DiscoveryRequest specifies a list of clusters
  // to subscribe to updates for.
  rpc StreamEndpoints(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc DeltaEndpoints(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc FetchEndpoints(discovery.v3.DiscoveryRequest) returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:endpoints";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221 and protoxform to upgrade the file.
message EdsDummy {
  option (udpa.annotations.versioning).previous_message_type = "envoy.api.v2.EdsDummy";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/service/discovery/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.accesslog.v2;

import "envoy/api/v2/core/base.proto";
import "envoy/data/accesslog/v2/accesslog.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.accesslog.v2";
option java_outer_classname = "AlsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/accesslog/v2;accesslogv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: gRPC Access Log Service (ALS)]

// Service for streaming access logs from Envoy to an access log server.
service AccessLogService {
  // Envoy will connect and send StreamAccessLogsMessage messages forever. It does not expect any
  // response to be sent as nothing would be done in the case of failure. The server should
  // disconnect if it expects Envoy to reconnect. In the future we may decide to add a different
  // API for "critical" access logs in which Envoy will buffer access logs for some period of time
  // until it gets an ACK so it could then retry. This API is designed for high throughput with the
  // expectation that it might be lossy.
  rpc StreamAccessLogs(stream StreamAccessLogsMessage) returns (StreamAccessLogsResponse) {
  }
}

// Empty response for the StreamAccessLogs API. Will never be sent. See below.
message StreamAccessLogsResponse {
}

// Stream message for the StreamAccessLogs API. Envoy will open a stream to the server and stream
// access logs without ever expecting a response.
message StreamAccessLogsMessage {
  message Identifier {
    // The node sending the access log messages over the stream.
    api.v2.core.Node node = 1 [(validate.rules).message = {required: true}];

    // The friendly name of the log configured in :ref:`CommonGrpcAccessLogConfig
    // <envoy_api_msg_config.accesslog.v2.CommonGrpcAccessLogConfig>`.
    string log_name = 2 [(validate.rules).string = {min_bytes: 1}];
  }

  // Wrapper for batches of HTTP access log entries.
  message HTTPAccessLogEntries {
    repeated data.accesslog.v2.HTTPAccessLogEntry log_entry = 1
        [(validate.rules).repeated = {min_items: 1}];
  }

  // Wrapper for batches of TCP access log entries.
  message TCPAccessLogEntries {
    repeated data.accesslog.v2.TCPAccessLogEntry log_entry = 1
        [(validate.rules).repeated = {min_items: 1}];
  }

  // Identifier data that will only be sent in the first message on the stream. This is effectively
  // structured metadata and is a performance optimization.
  Identifier identifier = 1;

  // Batches of log entries of a single type. Generally speaking, a given stream should only
  // ever include one type of log entry.
  oneof log_entries {
    option (validate.required) = true;

    HTTPAccessLogEntries http_logs = 2;

    TCPAccessLogEntries tcp_logs = 3;
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "//envoy/data/accesslog/v2:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.accesslog.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/data/accesslog/v3/accesslog.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.accesslog.v3";
option java_outer_classname = "AlsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/accesslog/v3;accesslogv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: gRPC access log service (ALS)]

// Service for streaming access logs from Envoy to an access log server.
service AccessLogService {
  // Envoy will connect and send StreamAccessLogsMessage messages forever. It does not expect any
  // response to be sent as nothing would be done in the case of failure. The server should
  // disconnect if it expects Envoy to reconnect. In the future we may decide to add a different
  // API for "critical" access logs in which Envoy will buffer access logs for some period of time
  // until it gets an ACK so it could then retry. This API is designed for high throughput with the
  // expectation that it might be lossy.
  rpc StreamAccessLogs(stream StreamAccessLogsMessage) returns (StreamAccessLogsResponse) {
  }
}

// Empty response for the StreamAccessLogs API. Will never be sent. See below.
message StreamAccessLogsResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.accesslog.v2.StreamAccessLogsResponse";
}

// Stream message for the StreamAccessLogs API. Envoy will open a stream to the server and stream
// access logs without ever expecting a response.
message StreamAccessLogsMessage {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.accesslog.v2.StreamAccessLogsMessage";

  message Identifier {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.accesslog.v2.StreamAccessLogsMessage.Identifier";

    // The node sending the access log messages over the stream.
    config.core.v3.Node node = 1 [(validate.rules).message = {required: true}];

    // The friendly name of the log configured in :ref:`CommonGrpcAccessLogConfig
    // <envoy_v3_api_msg_extensions.access_loggers.grpc.v3.CommonGrpcAccessLogConfig>`.
    string log_name = 2 [(validate.rules).string = {min_len: 1}];
  }

  // Wrapper for batches of HTTP access log entries.
  message HTTPAccessLogEntries {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.accesslog.v2.StreamAccessLogsMessage.HTTPAccessLogEntries";

    repeated data.accesslog.v3.HTTPAccessLogEntry log_entry = 1
        [(validate.rules).repeated = {min_items: 1}];
  }

  // Wrapper for batches of TCP access log entries.
  message TCPAccessLogEntries {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.accesslog.v2.StreamAccessLogsMessage.TCPAccessLogEntries";

    repeated data.accesslog.v3.TCPAccessLogEntry log_entry = 1
        [(validate.rules).repeated = {min_items: 1}];
  }

  // Identifier data that will only be sent in the first message on the stream. This is effectively
  // structured metadata and is a performance optimization.
  Identifier identifier = 1;

  // Batches of log entries of a single type. Generally speaking, a given stream should only
  // ever include one type of log entry.
  oneof log_entries {
    option (validate.required) = true;

    HTTPAccessLogEntries http_logs = 2;

    TCPAccessLogEntries tcp_logs = 3;
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "//envoy/data/accesslog/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.secret.v3;

import "envoy/service/discovery/v3/discovery.proto";

import "google/api/annotations.proto";

import "envoy/annotations/resource.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.service.secret.v3";
option java_outer_classname = "SdsProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/secret/v3;secretv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Secret Discovery Service (SDS)]

service SecretDiscoveryService {
  option (envoy.annotations.resource).type = "envoy.extensions.transport_sockets.tls.v3.Secret";

  rpc DeltaSecrets(stream discovery.v3.DeltaDiscoveryRequest)
      returns (stream discovery.v3.DeltaDiscoveryResponse) {
  }

  rpc StreamSecrets(stream discovery.v3.DiscoveryRequest)
      returns (stream discovery.v3.DiscoveryResponse) {
  }

  rpc FetchSecrets(discovery.v3.DiscoveryRequest) returns (discovery.v3.DiscoveryResponse) {
    option (google.api.http).post = "/v3/discovery:secrets";
    option (google.api.http).body = "*";
  }
}

// [#not-implemented-hide:] Not configuration. Workaround c++ protobuf issue with importing
// services: https://github.com/google/protobuf/issues/4221
message SdsDummy {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.discovery.v2.SdsDummy";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/service/discovery/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.ext_proc.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/extensions/filters/http/ext_proc/v3/processing_mode.proto";
import "envoy/type/v3/http_status.proto";

import "google/protobuf/duration.proto";
import "google/protobuf/struct.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.ext_proc.v3";
option java_outer_classname = "ExternalProcessorProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/ext_proc/v3;ext_procv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: External processing service]

// A service that can access and modify HTTP requests and responses
// as part of a filter chain.
// The overall external processing protocol works like this:
//
// 1. Envoy sends to the service information about the HTTP request.
// 2. The service sends back a ProcessingResponse message that directs Envoy
//    to either stop processing, continue without it, or send it the
//    next chunk of the message body.
// 3. If so requested, Envoy sends the server chunks of the message body,
//    or the entire body at once. In either case, the server sends back
//    a ProcessingResponse after each message it receives.
// 4. If so requested, Envoy sends the server the HTTP trailers,
//    and the server sends back a ProcessingResponse.
// 5. At this point, request processing is done, and we pick up again
//    at step 1 when Envoy receives a response from the upstream server.
// 6. At any point above, if the server closes the gRPC stream cleanly,
//    then Envoy proceeds without consulting the server.
// 7. At any point above, if the server closes the gRPC stream with an error,
//    then Envoy returns a 500 error to the client, unless the filter
//    was configured to ignore errors.
//
// In other words, the process is a request/response conversation, but
// using a gRPC stream to make it easier for the server to
// maintain state.

service ExternalProcessor {
  // This begins the bidirectional stream that Envoy will use to
  // give the server control over what the filter does. The actual
  // protocol is described by the ProcessingRequest and ProcessingResponse
  // messages below.
  rpc Process(stream ProcessingRequest) returns (stream ProcessingResponse) {
  }
}

// This represents the different types of messages that Envoy can send
// to an external processing server.
// [#next-free-field: 8]
message ProcessingRequest {
  // Specify whether the filter that sent this request is running in synchronous
  // or asynchronous mode. The choice of synchronous or asynchronous mode
  // can be set in the filter configuration, and defaults to false.
  //
  // * A value of ``false`` indicates that the server must respond
  //   to this message by either sending back a matching ProcessingResponse message,
  //   or by closing the stream.
  // * A value of ``true`` indicates that the server must not respond to this
  //   message, although it may still close the stream to indicate that no more messages
  //   are needed.
  //
  bool async_mode = 1;

  // Each request message will include one of the following sub-messages. Which
  // ones are set for a particular HTTP request/response depend on the
  // processing mode.
  oneof request {
    option (validate.required) = true;

    // Information about the HTTP request headers, as well as peer info and additional
    // properties. Unless ``async_mode`` is ``true``, the server must send back a
    // HeaderResponse message, an ImmediateResponse message, or close the stream.
    HttpHeaders request_headers = 2;

    // Information about the HTTP response headers, as well as peer info and additional
    // properties. Unless ``async_mode`` is ``true``, the server must send back a
    // HeaderResponse message or close the stream.
    HttpHeaders response_headers = 3;

    // A chunk of the HTTP request body. Unless ``async_mode`` is true, the server must send back
    // a BodyResponse message, an ImmediateResponse message, or close the stream.
    HttpBody request_body = 4;

    // A chunk of the HTTP request body. Unless ``async_mode`` is ``true``, the server must send back
    // a BodyResponse message or close the stream.
    HttpBody response_body = 5;

    // The HTTP trailers for the request path. Unless ``async_mode`` is ``true``, the server
    // must send back a TrailerResponse message or close the stream.
    //
    // This message is only sent if the trailers processing mode is set to ``SEND``.
    // If there are no trailers on the original downstream request, then this message
    // will only be sent (with empty trailers waiting to be populated) if the
    // processing mode is set before the request headers are sent, such as
    // in the filter configuration.
    HttpTrailers request_trailers = 6;

    // The HTTP trailers for the response path. Unless ``async_mode`` is ``true``, the server
    // must send back a TrailerResponse message or close the stream.
    //
    // This message is only sent if the trailers processing mode is set to ``SEND``.
    // If there are no trailers on the original downstream request, then this message
    // will only be sent (with empty trailers waiting to be populated) if the
    // processing mode is set before the request headers are sent, such as
    // in the filter configuration.
    HttpTrailers response_trailers = 7;
  }
}

// For every ProcessingRequest received by the server with the ``async_mode`` field
// set to false, the server must send back exactly one ProcessingResponse message.
// [#next-free-field: 11]
message ProcessingResponse {
  oneof response {
    option (validate.required) = true;

    // The server must send back this message in response to a message with the
    // ``request_headers`` field set.
    HeadersResponse request_headers = 1;

    // The server must send back this message in response to a message with the
    // ``response_headers`` field set.
    HeadersResponse response_headers = 2;

    // The server must send back this message in response to a message with
    // the ``request_body`` field set.
    BodyResponse request_body = 3;

    // The server must send back this message in response to a message with
    // the ``response_body`` field set.
    BodyResponse response_body = 4;

    // The server must send back this message in response to a message with
    // the ``request_trailers`` field set.
    TrailersResponse request_trailers = 5;

    // The server must send back this message in response to a message with
    // the ``response_trailers`` field set.
    TrailersResponse response_trailers = 6;

    // If specified, attempt to create a locally generated response, send it
    // downstream, and stop processing additional filters and ignore any
    // additional messages received from the remote server for this request or
    // response. If a response has already started -- for example, if this
    // message is sent response to a ``response_body`` message -- then
    // this will either ship the reply directly to the downstream codec,
    // or reset the stream.
    ImmediateResponse immediate_response = 7;
  }

  // [#not-implemented-hide:]
  // Optional metadata that will be emitted as dynamic metadata to be consumed by the next
  // filter. This metadata will be placed in the namespace ``envoy.filters.http.ext_proc``.
  google.protobuf.Struct dynamic_metadata = 8;

  // Override how parts of the HTTP request and response are processed
  // for the duration of this particular request/response only. Servers
  // may use this to intelligently control how requests are processed
  // based on the headers and other metadata that they see.
  // This field is only applicable when servers responding to the header requests.
  // If it is set in the response to the body or trailer requests, it will be ignored by Envoy.
  // It is also ignored by Envoy when the ext_proc filter config
  // :ref:`allow_mode_override
  // <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.allow_mode_override>`
  // is set to false.
  envoy.extensions.filters.http.ext_proc.v3.ProcessingMode mode_override = 9;

  // When ext_proc server receives a request message, in case it needs more
  // time to process the message, it sends back a ProcessingResponse message
  // with a new timeout value. When Envoy receives this response message,
  // it ignores other fields in the response, just stop the original timer,
  // which has the timeout value specified in
  // :ref:`message_timeout
  // <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.message_timeout>`
  // and start a new timer with this ``override_message_timeout`` value and keep the
  // Envoy ext_proc filter state machine intact.
  // Has to be >= 1ms and <=
  // :ref:`max_message_timeout <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.max_message_timeout>`
  // Such message can be sent at most once in a particular Envoy ext_proc filter processing state.
  // To enable this API, one has to set ``max_message_timeout`` to a number >= 1ms.
  google.protobuf.Duration override_message_timeout = 10;
}

// The following are messages that are sent to the server.

// This message is sent to the external server when the HTTP request and responses
// are first received.
message HttpHeaders {
  // The HTTP request headers. All header keys will be
  // lower-cased, because HTTP header keys are case-insensitive.
  // The ``headers`` encoding is based on the runtime guard
  // envoy_reloadable_features_send_header_raw_value setting.
  // When it is true, the header value is encoded in the
  // :ref:`raw_value <envoy_v3_api_field_config.core.v3.HeaderValue.raw_value>` field.
  // When it is false, the header value is encoded in the
  // :ref:`value <envoy_v3_api_field_config.core.v3.HeaderValue.value>` field.
  config.core.v3.HeaderMap headers = 1;

  // [#not-implemented-hide:]
  // The values of properties selected by the ``request_attributes``
  // or ``response_attributes`` list in the configuration. Each entry
  // in the list is populated
  // from the standard :ref:`attributes <arch_overview_attributes>`
  // supported across Envoy.
  map<string, google.protobuf.Struct> attributes = 2;

  // If true, then there is no message body associated with this
  // request or response.
  bool end_of_stream = 3;
}

// This message contains the message body that Envoy sends to the external server.
message HttpBody {
  bytes body = 1;

  bool end_of_stream = 2;
}

// This message contains the trailers.
message HttpTrailers {
  // The ``trailers`` encoding is based on the runtime guard
  // envoy_reloadable_features_send_header_raw_value setting.
  // When it is true, the header value is encoded in the
  // :ref:`raw_value <envoy_v3_api_field_config.core.v3.HeaderValue.raw_value>` field.
  // When it is false, the header value is encoded in the
  // :ref:`value <envoy_v3_api_field_config.core.v3.HeaderValue.value>` field.
  config.core.v3.HeaderMap trailers = 1;
}

// The following are messages that may be sent back by the server.

// This message must be sent in response to an HttpHeaders message.
message HeadersResponse {
  CommonResponse response = 1;
}

// This message must be sent in response to an HttpTrailers message.
message TrailersResponse {
  // Instructions on how to manipulate the trailers
  HeaderMutation header_mutation = 1;
}

// This message must be sent in response to an HttpBody message.
message BodyResponse {
  CommonResponse response = 1;
}

// This message contains common fields between header and body responses.
// [#next-free-field: 6]
message CommonResponse {
  enum ResponseStatus {
    // Apply the mutation instructions in this message to the
    // request or response, and then continue processing the filter
    // stream as normal. This is the default.
    CONTINUE = 0;

    // Apply the specified header mutation, replace the body with the body
    // specified in the body mutation (if present), and do not send any
    // further messages for this request or response even if the processing
    // mode is configured to do so.
    //
    // When used in response to a request_headers or response_headers message,
    // this status makes it possible to either completely replace the body
    // while discarding the original body, or to add a body to a message that
    // formerly did not have one.
    //
    // In other words, this response makes it possible to turn an HTTP GET
    // into a POST, PUT, or PATCH.
    CONTINUE_AND_REPLACE = 1;
  }

  // If set, provide additional direction on how the Envoy proxy should
  // handle the rest of the HTTP filter chain.
  ResponseStatus status = 1 [(validate.rules).enum = {defined_only: true}];

  // Instructions on how to manipulate the headers. When responding to an
  // HttpBody request, header mutations will only take effect if
  // the current processing mode for the body is BUFFERED.
  HeaderMutation header_mutation = 2;

  // Replace the body of the last message sent to the remote server on this
  // stream. If responding to an HttpBody request, simply replace or clear
  // the body chunk that was sent with that request. Body mutations may take
  // effect in response either to ``header`` or ``body`` messages. When it is
  // in response to ``header`` messages, it only take effect if the
  // :ref:`status <envoy_v3_api_field_service.ext_proc.v3.CommonResponse.status>`
  // is set to CONTINUE_AND_REPLACE.
  BodyMutation body_mutation = 3;

  // [#not-implemented-hide:]
  // Add new trailers to the message. This may be used when responding to either a
  // HttpHeaders or HttpBody message, but only if this message is returned
  // along with the CONTINUE_AND_REPLACE status.
  // The ``trailers`` encoding is based on the runtime guard
  // envoy_reloadable_features_send_header_raw_value setting.
  // When it is true, the header value is encoded in the
  // :ref:`raw_value <envoy_v3_api_field_config.core.v3.HeaderValue.raw_value>` field.
  // When it is false, the header value is encoded in the
  // :ref:`value <envoy_v3_api_field_config.core.v3.HeaderValue.value>` field.
  config.core.v3.HeaderMap trailers = 4;

  // Clear the route cache for the current client request. This is necessary
  // if the remote server modified headers that are used to calculate the route.
  // This field is ignored in the response direction.
  bool clear_route_cache = 5;
}

// This message causes the filter to attempt to create a locally
// generated response, send it  downstream, stop processing
// additional filters, and ignore any additional messages received
// from the remote server for this request or response. If a response
// has already started, then  this will either ship the reply directly
// to the downstream codec, or reset the stream.
// [#next-free-field: 6]
message ImmediateResponse {
  // The response code to return
  type.v3.HttpStatus status = 1 [(validate.rules).message = {required: true}];

  // Apply changes to the default headers, which will include content-type.
  HeaderMutation headers = 2;

  // The message body to return with the response which is sent using the
  // text/plain content type, or encoded in the grpc-message header.
  string body = 3;

  // If set, then include a gRPC status trailer.
  GrpcStatus grpc_status = 4;

  // A string detailing why this local reply was sent, which may be included
  // in log and debug output (e.g. this populates the %RESPONSE_CODE_DETAILS%
  // command operator field for use in access logging).
  string details = 5;
}

// This message specifies a gRPC status for an ImmediateResponse message.
message GrpcStatus {
  // The actual gRPC status
  uint32 status = 1;
}

// Change HTTP headers or trailers by appending, replacing, or removing
// headers.
message HeaderMutation {
  // Add or replace HTTP headers. Attempts to set the value of
  // any ``x-envoy`` header, and attempts to set the ``:method``,
  // ``:authority``, ``:scheme``, or ``host`` headers will be ignored.
  // The ``set_headers`` encoding is based on the runtime guard
  // envoy_reloadable_features_send_header_raw_value setting.
  // When it is true, the header value is encoded in the
  // :ref:`raw_value <envoy_v3_api_field_config.core.v3.HeaderValue.raw_value>` field.
  // When it is false, the header value is encoded in the
  // :ref:`value <envoy_v3_api_field_config.core.v3.HeaderValue.value>` field.
  repeated config.core.v3.HeaderValueOption set_headers = 1;

  // Remove these HTTP headers. Attempts to remove system headers --
  // any header starting with ``:``, plus ``host`` -- will be ignored.
  repeated string remove_headers = 2;
}

// Replace the entire message body chunk received in the corresponding
// HttpBody message with this new body, or clear the body.
message BodyMutation {
  oneof mutation {
    // The entire body to replace
    bytes body = 1;

    // Clear the corresponding body chunk
    bool clear_body = 2;
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "//envoy/extensions/filters/http/ext_proc/v3:pkg",
        "//envoy/type/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.event_reporting.v3;

import "envoy/config/core/v3/base.proto";

import "google/protobuf/any.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.event_reporting.v3";
option java_outer_classname = "EventReportingServiceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/event_reporting/v3;event_reportingv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: gRPC Event Reporting Service]

// [#not-implemented-hide:]
// Service for streaming different types of events from Envoy to a server. The examples of
// such events may be health check or outlier detection events.
service EventReportingService {
  // Envoy will connect and send StreamEventsRequest messages forever.
  // The management server may send StreamEventsResponse to configure event stream. See below.
  // This API is designed for high throughput with the expectation that it might be lossy.
  rpc StreamEvents(stream StreamEventsRequest) returns (stream StreamEventsResponse) {
  }
}

// [#not-implemented-hide:]
// An events envoy sends to the management server.
message StreamEventsRequest {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.event_reporting.v2alpha.StreamEventsRequest";

  message Identifier {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.service.event_reporting.v2alpha.StreamEventsRequest.Identifier";

    // The node sending the event messages over the stream.
    config.core.v3.Node node = 1 [(validate.rules).message = {required: true}];
  }

  // Identifier data that will only be sent in the first message on the stream. This is effectively
  // structured metadata and is a performance optimization.
  Identifier identifier = 1;

  // Batch of events. When the stream is already active, it will be the events occurred
  // since the last message had been sent. If the server receives unknown event type, it should
  // silently ignore it.
  //
  // The following events are supported:
  //
  // * :ref:`HealthCheckEvent <envoy_v3_api_msg_data.core.v3.HealthCheckEvent>`
  // * :ref:`OutlierDetectionEvent <envoy_v3_api_msg_data.cluster.v3.OutlierDetectionEvent>`
  repeated google.protobuf.Any events = 2 [(validate.rules).repeated = {min_items: 1}];
}

// [#not-implemented-hide:]
// The management server may send envoy a StreamEventsResponse to tell which events the server
// is interested in. In future, with aggregated event reporting service, this message will
// contain, for example, clusters the envoy should send events for, or event types the server
// wants to process.
message StreamEventsResponse {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.service.event_reporting.v2alpha.StreamEventsResponse";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.service.event_reporting.v2alpha;

import "envoy/api/v2/core/base.proto";

import "google/protobuf/any.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.service.event_reporting.v2alpha";
option java_outer_classname = "EventReportingServiceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/service/event_reporting/v2alpha";
option (udpa.annotations.file_migrate).move_to_package = "envoy.service.event_reporting.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: gRPC Event Reporting Service]

// [#not-implemented-hide:]
// Service for streaming different types of events from Envoy to a server. The examples of
// such events may be health check or outlier detection events.
service EventReportingService {
  // Envoy will connect and send StreamEventsRequest messages forever.
  // The management server may send StreamEventsResponse to configure event stream. See below.
  // This API is designed for high throughput with the expectation that it might be lossy.
  rpc StreamEvents(stream StreamEventsRequest) returns (stream StreamEventsResponse) {
  }
}

// [#not-implemented-hide:]
// An events envoy sends to the management server.
message StreamEventsRequest {
  message Identifier {
    // The node sending the event messages over the stream.
    api.v2.core.Node node = 1 [(validate.rules).message = {required: true}];
  }

  // Identifier data that will only be sent in the first message on the stream. This is effectively
  // structured metadata and is a performance optimization.
  Identifier identifier = 1;

  // Batch of events. When the stream is already active, it will be the events occurred
  // since the last message had been sent. If the server receives unknown event type, it should
  // silently ignore it.
  //
  // The following events are supported:
  //
  // * :ref:`HealthCheckEvent <envoy_api_msg_data.core.v2alpha.HealthCheckEvent>`
  // * :ref:`OutlierDetectionEvent <envoy_api_msg_data.cluster.v2alpha.OutlierDetectionEvent>`
  repeated google.protobuf.Any events = 2 [(validate.rules).repeated = {min_items: 1}];
}

// [#not-implemented-hide:]
// The management server may send envoy a StreamEventsResponse to tell which events the server
// is interested in. In future, with aggregated event reporting service, this message will
// contain, for example, clusters the envoy should send events for, or event types the server
// wants to process.
message StreamEventsResponse {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/api/v2/core:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.data.cluster.v3;

import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.cluster.v3";
option java_outer_classname = "OutlierDetectionEventProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/cluster/v3;clusterv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Outlier detection logging events]
// :ref:`Outlier detection logging <arch_overview_outlier_detection_logging>`.

// Type of ejection that took place
enum OutlierEjectionType {
  // In case upstream host returns certain number of consecutive 5xx.
  // If
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_v3_api_field_config.cluster.v3.OutlierDetection.split_external_local_origin_errors>`
  // is ``false``, all type of errors are treated as HTTP 5xx errors.
  // See :ref:`Cluster outlier detection <arch_overview_outlier_detection>` documentation for
  // details.
  CONSECUTIVE_5XX = 0;

  // In case upstream host returns certain number of consecutive gateway errors
  CONSECUTIVE_GATEWAY_FAILURE = 1;

  // Runs over aggregated success rate statistics from every host in cluster
  // and selects hosts for which ratio of successful replies deviates from other hosts
  // in the cluster.
  // If
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_v3_api_field_config.cluster.v3.OutlierDetection.split_external_local_origin_errors>`
  // is ``false``, all errors (externally and locally generated) are used to calculate success rate
  // statistics. See :ref:`Cluster outlier detection <arch_overview_outlier_detection>`
  // documentation for details.
  SUCCESS_RATE = 2;

  // Consecutive local origin failures: Connection failures, resets, timeouts, etc
  // This type of ejection happens only when
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_v3_api_field_config.cluster.v3.OutlierDetection.split_external_local_origin_errors>`
  // is set to ``true``.
  // See :ref:`Cluster outlier detection <arch_overview_outlier_detection>` documentation for
  CONSECUTIVE_LOCAL_ORIGIN_FAILURE = 3;

  // Runs over aggregated success rate statistics for local origin failures
  // for all hosts in the cluster and selects hosts for which success rate deviates from other
  // hosts in the cluster. This type of ejection happens only when
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_v3_api_field_config.cluster.v3.OutlierDetection.split_external_local_origin_errors>`
  // is set to ``true``.
  // See :ref:`Cluster outlier detection <arch_overview_outlier_detection>` documentation for
  SUCCESS_RATE_LOCAL_ORIGIN = 4;

  // Runs over aggregated success rate statistics from every host in cluster and selects hosts for
  // which ratio of failed replies is above configured value.
  FAILURE_PERCENTAGE = 5;

  // Runs over aggregated success rate statistics for local origin failures from every host in
  // cluster and selects hosts for which ratio of failed replies is above configured value.
  FAILURE_PERCENTAGE_LOCAL_ORIGIN = 6;
}

// Represents possible action applied to upstream host
enum Action {
  // In case host was excluded from service
  EJECT = 0;

  // In case host was brought back into service
  UNEJECT = 1;
}

// [#next-free-field: 12]
message OutlierDetectionEvent {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.cluster.v2alpha.OutlierDetectionEvent";

  // In case of eject represents type of ejection that took place.
  OutlierEjectionType type = 1 [(validate.rules).enum = {defined_only: true}];

  // Timestamp for event.
  google.protobuf.Timestamp timestamp = 2;

  // The time in seconds since the last action (either an ejection or unejection) took place.
  google.protobuf.UInt64Value secs_since_last_action = 3;

  // The :ref:`cluster <envoy_v3_api_msg_config.cluster.v3.Cluster>` that owns the ejected host.
  string cluster_name = 4 [(validate.rules).string = {min_len: 1}];

  // The URL of the ejected host. E.g., ``tcp://1.2.3.4:80``.
  string upstream_url = 5 [(validate.rules).string = {min_len: 1}];

  // The action that took place.
  Action action = 6 [(validate.rules).enum = {defined_only: true}];

  // If ``action`` is ``eject``, specifies the number of times the host has been ejected (local to
  // that Envoy and gets reset if the host gets removed from the upstream cluster for any reason and
  // then re-added).
  uint32 num_ejections = 7;

  // If ``action`` is ``eject``, specifies if the ejection was enforced. ``true`` means the host was
  // ejected. ``false`` means the event was logged but the host was not actually ejected.
  bool enforced = 8;

  oneof event {
    option (validate.required) = true;

    OutlierEjectSuccessRate eject_success_rate_event = 9;

    OutlierEjectConsecutive eject_consecutive_event = 10;

    OutlierEjectFailurePercentage eject_failure_percentage_event = 11;
  }
}

message OutlierEjectSuccessRate {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.cluster.v2alpha.OutlierEjectSuccessRate";

  // Host’s success rate at the time of the ejection event on a 0-100 range.
  uint32 host_success_rate = 1 [(validate.rules).uint32 = {lte: 100}];

  // Average success rate of the hosts in the cluster at the time of the ejection event on a 0-100
  // range.
  uint32 cluster_average_success_rate = 2 [(validate.rules).uint32 = {lte: 100}];

  // Success rate ejection threshold at the time of the ejection event.
  uint32 cluster_success_rate_ejection_threshold = 3 [(validate.rules).uint32 = {lte: 100}];
}

message OutlierEjectConsecutive {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.cluster.v2alpha.OutlierEjectConsecutive";
}

message OutlierEjectFailurePercentage {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.cluster.v2alpha.OutlierEjectFailurePercentage";

  // Host's success rate at the time of the ejection event on a 0-100 range.
  uint32 host_success_rate = 1 [(validate.rules).uint32 = {lte: 100}];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.data.cluster.v2alpha;

import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.cluster.v2alpha";
option java_outer_classname = "OutlierDetectionEventProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/cluster/v2alpha";
option (udpa.annotations.file_migrate).move_to_package = "envoy.data.cluster.v3";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Outlier detection logging events]
// :ref:`Outlier detection logging <arch_overview_outlier_detection_logging>`.

// Type of ejection that took place
enum OutlierEjectionType {
  // In case upstream host returns certain number of consecutive 5xx.
  // If
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_api_field_cluster.OutlierDetection.split_external_local_origin_errors>`
  // is *false*, all type of errors are treated as HTTP 5xx errors.
  // See :ref:`Cluster outlier detection <arch_overview_outlier_detection>` documentation for
  // details.
  CONSECUTIVE_5XX = 0;

  // In case upstream host returns certain number of consecutive gateway errors
  CONSECUTIVE_GATEWAY_FAILURE = 1;

  // Runs over aggregated success rate statistics from every host in cluster
  // and selects hosts for which ratio of successful replies deviates from other hosts
  // in the cluster.
  // If
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_api_field_cluster.OutlierDetection.split_external_local_origin_errors>`
  // is *false*, all errors (externally and locally generated) are used to calculate success rate
  // statistics. See :ref:`Cluster outlier detection <arch_overview_outlier_detection>`
  // documentation for details.
  SUCCESS_RATE = 2;

  // Consecutive local origin failures: Connection failures, resets, timeouts, etc
  // This type of ejection happens only when
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_api_field_cluster.OutlierDetection.split_external_local_origin_errors>`
  // is set to *true*.
  // See :ref:`Cluster outlier detection <arch_overview_outlier_detection>` documentation for
  CONSECUTIVE_LOCAL_ORIGIN_FAILURE = 3;

  // Runs over aggregated success rate statistics for local origin failures
  // for all hosts in the cluster and selects hosts for which success rate deviates from other
  // hosts in the cluster. This type of ejection happens only when
  // :ref:`outlier_detection.split_external_local_origin_errors<envoy_api_field_cluster.OutlierDetection.split_external_local_origin_errors>`
  // is set to *true*.
  // See :ref:`Cluster outlier detection <arch_overview_outlier_detection>` documentation for
  SUCCESS_RATE_LOCAL_ORIGIN = 4;

  // Runs over aggregated success rate statistics from every host in cluster and selects hosts for
  // which ratio of failed replies is above configured value.
  FAILURE_PERCENTAGE = 5;

  // Runs over aggregated success rate statistics for local origin failures from every host in
  // cluster and selects hosts for which ratio of failed replies is above configured value.
  FAILURE_PERCENTAGE_LOCAL_ORIGIN = 6;
}

// Represents possible action applied to upstream host
enum Action {
  // In case host was excluded from service
  EJECT = 0;

  // In case host was brought back into service
  UNEJECT = 1;
}

// [#next-free-field: 12]
message OutlierDetectionEvent {
  // In case of eject represents type of ejection that took place.
  OutlierEjectionType type = 1 [(validate.rules).enum = {defined_only: true}];

  // Timestamp for event.
  google.protobuf.Timestamp timestamp = 2;

  // The time in seconds since the last action (either an ejection or unejection) took place.
  google.protobuf.UInt64Value secs_since_last_action = 3;

  // The :ref:`cluster <envoy_api_msg_Cluster>` that owns the ejected host.
  string cluster_name = 4 [(validate.rules).string = {min_bytes: 1}];

  // The URL of the ejected host. E.g., ``tcp://1.2.3.4:80``.
  string upstream_url = 5 [(validate.rules).string = {min_bytes: 1}];

  // The action that took place.
  Action action = 6 [(validate.rules).enum = {defined_only: true}];

  // If ``action`` is ``eject``, specifies the number of times the host has been ejected (local to
  // that Envoy and gets reset if the host gets removed from the upstream cluster for any reason and
  // then re-added).
  uint32 num_ejections = 7;

  // If ``action`` is ``eject``, specifies if the ejection was enforced. ``true`` means the host was
  // ejected. ``false`` means the event was logged but the host was not actually ejected.
  bool enforced = 8;

  oneof event {
    option (validate.required) = true;

    OutlierEjectSuccessRate eject_success_rate_event = 9;

    OutlierEjectConsecutive eject_consecutive_event = 10;

    OutlierEjectFailurePercentage eject_failure_percentage_event = 11;
  }
}

message OutlierEjectSuccessRate {
  // Host’s success rate at the time of the ejection event on a 0-100 range.
  uint32 host_success_rate = 1 [(validate.rules).uint32 = {lte: 100}];

  // Average success rate of the hosts in the cluster at the time of the ejection event on a 0-100
  // range.
  uint32 cluster_average_success_rate = 2 [(validate.rules).uint32 = {lte: 100}];

  // Success rate ejection threshold at the time of the ejection event.
  uint32 cluster_success_rate_ejection_threshold = 3 [(validate.rules).uint32 = {lte: 100}];
}

message OutlierEjectConsecutive {
}

message OutlierEjectFailurePercentage {
  // Host's success rate at the time of the ejection event on a 0-100 range.
  uint32 host_success_rate = 1 [(validate.rules).uint32 = {lte: 100}];
}
syntax = "proto3";

package envoy.data.core.v3;

import "envoy/config/core/v3/address.proto";
import "envoy/config/core/v3/base.proto";

import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.core.v3";
option java_outer_classname = "HealthCheckEventProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/core/v3;corev3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Health check logging events]
// :ref:`Health check logging <arch_overview_health_check_logging>`.

enum HealthCheckFailureType {
  ACTIVE = 0;
  PASSIVE = 1;
  NETWORK = 2;
  NETWORK_TIMEOUT = 3;
}

enum HealthCheckerType {
  HTTP = 0;
  TCP = 1;
  GRPC = 2;
  REDIS = 3;
  THRIFT = 4;
}

// [#next-free-field: 12]
message HealthCheckEvent {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.core.v2alpha.HealthCheckEvent";

  HealthCheckerType health_checker_type = 1 [(validate.rules).enum = {defined_only: true}];

  config.core.v3.Address host = 2;

  string cluster_name = 3 [(validate.rules).string = {min_len: 1}];

  oneof event {
    option (validate.required) = true;

    // Host ejection.
    HealthCheckEjectUnhealthy eject_unhealthy_event = 4;

    // Host addition.
    HealthCheckAddHealthy add_healthy_event = 5;

    // Host failure.
    HealthCheckFailure health_check_failure_event = 7;

    // Healthy host became degraded.
    DegradedHealthyHost degraded_healthy_host = 8;

    // A degraded host returned to being healthy.
    NoLongerDegradedHost no_longer_degraded_host = 9;
  }

  // Timestamp for event.
  google.protobuf.Timestamp timestamp = 6;

  // Host metadata
  config.core.v3.Metadata metadata = 10;

  // Host locality
  config.core.v3.Locality locality = 11;
}

message HealthCheckEjectUnhealthy {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.core.v2alpha.HealthCheckEjectUnhealthy";

  // The type of failure that caused this ejection.
  HealthCheckFailureType failure_type = 1 [(validate.rules).enum = {defined_only: true}];
}

message HealthCheckAddHealthy {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.core.v2alpha.HealthCheckAddHealthy";

  // Whether this addition is the result of the first ever health check on a host, in which case
  // the configured :ref:`healthy threshold <envoy_v3_api_field_config.core.v3.HealthCheck.healthy_threshold>`
  // is bypassed and the host is immediately added.
  bool first_check = 1;
}

message HealthCheckFailure {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.core.v2alpha.HealthCheckFailure";

  // The type of failure that caused this event.
  HealthCheckFailureType failure_type = 1 [(validate.rules).enum = {defined_only: true}];

  // Whether this event is the result of the first ever health check on a host.
  bool first_check = 2;
}

message DegradedHealthyHost {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.core.v2alpha.DegradedHealthyHost";
}

message NoLongerDegradedHost {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.core.v2alpha.NoLongerDegradedHost";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.core.v2alpha;

import "envoy/api/v2/core/address.proto";

import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.core.v2alpha";
option java_outer_classname = "HealthCheckEventProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/core/v2alpha";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Health check logging events]
// :ref:`Health check logging <arch_overview_health_check_logging>`.

enum HealthCheckFailureType {
  ACTIVE = 0;
  PASSIVE = 1;
  NETWORK = 2;
}

enum HealthCheckerType {
  HTTP = 0;
  TCP = 1;
  GRPC = 2;
  REDIS = 3;
}

// [#next-free-field: 10]
message HealthCheckEvent {
  HealthCheckerType health_checker_type = 1 [(validate.rules).enum = {defined_only: true}];

  api.v2.core.Address host = 2;

  string cluster_name = 3 [(validate.rules).string = {min_bytes: 1}];

  oneof event {
    option (validate.required) = true;

    // Host ejection.
    HealthCheckEjectUnhealthy eject_unhealthy_event = 4;

    // Host addition.
    HealthCheckAddHealthy add_healthy_event = 5;

    // Host failure.
    HealthCheckFailure health_check_failure_event = 7;

    // Healthy host became degraded.
    DegradedHealthyHost degraded_healthy_host = 8;

    // A degraded host returned to being healthy.
    NoLongerDegradedHost no_longer_degraded_host = 9;
  }

  // Timestamp for event.
  google.protobuf.Timestamp timestamp = 6;
}

message HealthCheckEjectUnhealthy {
  // The type of failure that caused this ejection.
  HealthCheckFailureType failure_type = 1 [(validate.rules).enum = {defined_only: true}];
}

message HealthCheckAddHealthy {
  // Whether this addition is the result of the first ever health check on a host, in which case
  // the configured :ref:`healthy threshold <envoy_api_field_core.HealthCheck.healthy_threshold>`
  // is bypassed and the host is immediately added.
  bool first_check = 1;
}

message HealthCheckFailure {
  // The type of failure that caused this event.
  HealthCheckFailureType failure_type = 1 [(validate.rules).enum = {defined_only: true}];

  // Whether this event is the result of the first ever health check on a host.
  bool first_check = 2;
}

message DegradedHealthyHost {
}

message NoLongerDegradedHost {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/api/v2/core:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.tap.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/data/tap/v3/common.proto";

import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v3";
option java_outer_classname = "HttpProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v3;tapv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: HTTP tap data]

// A fully buffered HTTP trace message.
message HttpBufferedTrace {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.tap.v2alpha.HttpBufferedTrace";

  // HTTP message wrapper.
  message Message {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.tap.v2alpha.HttpBufferedTrace.Message";

    // Message headers.
    repeated config.core.v3.HeaderValue headers = 1;

    // Message body.
    Body body = 2;

    // Message trailers.
    repeated config.core.v3.HeaderValue trailers = 3;

    // The timestamp after receiving the message headers.
    google.protobuf.Timestamp headers_received_time = 4;
  }

  // Request message.
  Message request = 1;

  // Response message.
  Message response = 2;

  // downstream connection
  Connection downstream_connection = 3;
}

// A streamed HTTP trace segment. Multiple segments make up a full trace.
// [#next-free-field: 8]
message HttpStreamedTraceSegment {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.tap.v2alpha.HttpStreamedTraceSegment";

  // Trace ID unique to the originating Envoy only. Trace IDs can repeat and should not be used
  // for long term stable uniqueness.
  uint64 trace_id = 1;

  oneof message_piece {
    // Request headers.
    config.core.v3.HeaderMap request_headers = 2;

    // Request body chunk.
    Body request_body_chunk = 3;

    // Request trailers.
    config.core.v3.HeaderMap request_trailers = 4;

    // Response headers.
    config.core.v3.HeaderMap response_headers = 5;

    // Response body chunk.
    Body response_body_chunk = 6;

    // Response trailers.
    config.core.v3.HeaderMap response_trailers = 7;
  }
}
syntax = "proto3";

package envoy.data.tap.v3;

import "envoy/data/tap/v3/common.proto";

import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v3";
option java_outer_classname = "TransportProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v3;tapv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Transport tap data]
// Trace format for the tap transport socket extension. This dumps plain text read/write
// sequences on a socket.

// Event in a socket trace.
message SocketEvent {
  option (udpa.annotations.versioning).previous_message_type = "envoy.data.tap.v2alpha.SocketEvent";

  // Data read by Envoy from the transport socket.
  message Read {
    // TODO(htuch): Half-close for reads.

    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.tap.v2alpha.SocketEvent.Read";

    // Binary data read.
    Body data = 1;
  }

  // Data written by Envoy to the transport socket.
  message Write {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.tap.v2alpha.SocketEvent.Write";

    // Binary data written.
    Body data = 1;

    // Stream was half closed after this write.
    bool end_stream = 2;
  }

  // The connection was closed.
  message Closed {
    // TODO(mattklein123): Close event type.

    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.tap.v2alpha.SocketEvent.Closed";
  }

  // Timestamp for event.
  google.protobuf.Timestamp timestamp = 1;

  // Read or write with content as bytes string.
  oneof event_selector {
    Read read = 2;

    Write write = 3;

    Closed closed = 4;
  }
}

// Sequence of read/write events that constitute a buffered trace on a socket.
// [#next-free-field: 6]
message SocketBufferedTrace {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.tap.v2alpha.SocketBufferedTrace";

  // Trace ID unique to the originating Envoy only. Trace IDs can repeat and should not be used
  // for long term stable uniqueness. Matches connection IDs used in Envoy logs.
  uint64 trace_id = 1;

  // Connection properties.
  Connection connection = 2;

  // Sequence of observed events.
  repeated SocketEvent events = 3;

  // Set to true if read events were truncated due to the :ref:`max_buffered_rx_bytes
  // <envoy_v3_api_field_config.tap.v3.OutputConfig.max_buffered_rx_bytes>` setting.
  bool read_truncated = 4;

  // Set to true if write events were truncated due to the :ref:`max_buffered_tx_bytes
  // <envoy_v3_api_field_config.tap.v3.OutputConfig.max_buffered_tx_bytes>` setting.
  bool write_truncated = 5;
}

// A streamed socket trace segment. Multiple segments make up a full trace.
message SocketStreamedTraceSegment {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.tap.v2alpha.SocketStreamedTraceSegment";

  // Trace ID unique to the originating Envoy only. Trace IDs can repeat and should not be used
  // for long term stable uniqueness. Matches connection IDs used in Envoy logs.
  uint64 trace_id = 1;

  oneof message_piece {
    // Connection properties.
    Connection connection = 2;

    // Socket event.
    SocketEvent event = 3;
  }
}
syntax = "proto3";

package envoy.data.tap.v3;

import "envoy/data/tap/v3/http.proto";
import "envoy/data/tap/v3/transport.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v3";
option java_outer_classname = "WrapperProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v3;tapv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Tap data wrappers]

// Wrapper for all fully buffered and streamed tap traces that Envoy emits. This is required for
// sending traces over gRPC APIs or more easily persisting binary messages to files.
message TraceWrapper {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.tap.v2alpha.TraceWrapper";

  oneof trace {
    option (validate.required) = true;

    // An HTTP buffered tap trace.
    HttpBufferedTrace http_buffered_trace = 1;

    // An HTTP streamed tap trace segment.
    HttpStreamedTraceSegment http_streamed_trace_segment = 2;

    // A socket buffered tap trace.
    SocketBufferedTrace socket_buffered_trace = 3;

    // A socket streamed tap trace segment.
    SocketStreamedTraceSegment socket_streamed_trace_segment = 4;
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.tap.v3;

import "envoy/config/core/v3/address.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v3";
option java_outer_classname = "CommonProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v3;tapv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Tap common data]

// Wrapper for tapped body data. This includes HTTP request/response body, transport socket received
// and transmitted data, etc.
message Body {
  option (udpa.annotations.versioning).previous_message_type = "envoy.data.tap.v2alpha.Body";

  oneof body_type {
    // Body data as bytes. By default, tap body data will be present in this field, as the proto
    // ``bytes`` type can contain any valid byte.
    bytes as_bytes = 1;

    // Body data as string. This field is only used when the :ref:`JSON_BODY_AS_STRING
    // <envoy_v3_api_enum_value_config.tap.v3.OutputSink.Format.JSON_BODY_AS_STRING>` sink
    // format type is selected. See the documentation for that option for why this is useful.
    string as_string = 2;
  }

  // Specifies whether body data has been truncated to fit within the specified
  // :ref:`max_buffered_rx_bytes
  // <envoy_v3_api_field_config.tap.v3.OutputConfig.max_buffered_rx_bytes>` and
  // :ref:`max_buffered_tx_bytes
  // <envoy_v3_api_field_config.tap.v3.OutputConfig.max_buffered_tx_bytes>` settings.
  bool truncated = 3;
}

// Connection properties.
message Connection {
  option (udpa.annotations.versioning).previous_message_type = "envoy.data.tap.v2alpha.Connection";

  // Local address.
  config.core.v3.Address local_address = 1;

  // Remote address.
  config.core.v3.Address remote_address = 2;
}
syntax = "proto3";

package envoy.data.tap.v2alpha;

import "envoy/api/v2/core/base.proto";
import "envoy/data/tap/v2alpha/common.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v2alpha";
option java_outer_classname = "HttpProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v2alpha";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: HTTP tap data]

// A fully buffered HTTP trace message.
message HttpBufferedTrace {
  // HTTP message wrapper.
  message Message {
    // Message headers.
    repeated api.v2.core.HeaderValue headers = 1;

    // Message body.
    Body body = 2;

    // Message trailers.
    repeated api.v2.core.HeaderValue trailers = 3;
  }

  // Request message.
  Message request = 1;

  // Response message.
  Message response = 2;
}

// A streamed HTTP trace segment. Multiple segments make up a full trace.
// [#next-free-field: 8]
message HttpStreamedTraceSegment {
  // Trace ID unique to the originating Envoy only. Trace IDs can repeat and should not be used
  // for long term stable uniqueness.
  uint64 trace_id = 1;

  oneof message_piece {
    // Request headers.
    api.v2.core.HeaderMap request_headers = 2;

    // Request body chunk.
    Body request_body_chunk = 3;

    // Request trailers.
    api.v2.core.HeaderMap request_trailers = 4;

    // Response headers.
    api.v2.core.HeaderMap response_headers = 5;

    // Response body chunk.
    Body response_body_chunk = 6;

    // Response trailers.
    api.v2.core.HeaderMap response_trailers = 7;
  }
}
syntax = "proto3";

package envoy.data.tap.v2alpha;

import "envoy/api/v2/core/address.proto";
import "envoy/data/tap/v2alpha/common.proto";

import "google/protobuf/timestamp.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v2alpha";
option java_outer_classname = "TransportProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v2alpha";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Transport tap data]
// Trace format for the tap transport socket extension. This dumps plain text read/write
// sequences on a socket.

// Connection properties.
message Connection {
  // Local address.
  api.v2.core.Address local_address = 2;

  // Remote address.
  api.v2.core.Address remote_address = 3;
}

// Event in a socket trace.
message SocketEvent {
  // Data read by Envoy from the transport socket.
  message Read {
    // TODO(htuch): Half-close for reads.

    // Binary data read.
    Body data = 1;
  }

  // Data written by Envoy to the transport socket.
  message Write {
    // Binary data written.
    Body data = 1;

    // Stream was half closed after this write.
    bool end_stream = 2;
  }

  // The connection was closed.
  message Closed {
    // TODO(mattklein123): Close event type.
  }

  // Timestamp for event.
  google.protobuf.Timestamp timestamp = 1;

  // Read or write with content as bytes string.
  oneof event_selector {
    Read read = 2;

    Write write = 3;

    Closed closed = 4;
  }
}

// Sequence of read/write events that constitute a buffered trace on a socket.
// [#next-free-field: 6]
message SocketBufferedTrace {
  // Trace ID unique to the originating Envoy only. Trace IDs can repeat and should not be used
  // for long term stable uniqueness. Matches connection IDs used in Envoy logs.
  uint64 trace_id = 1;

  // Connection properties.
  Connection connection = 2;

  // Sequence of observed events.
  repeated SocketEvent events = 3;

  // Set to true if read events were truncated due to the :ref:`max_buffered_rx_bytes
  // <envoy_api_field_service.tap.v2alpha.OutputConfig.max_buffered_rx_bytes>` setting.
  bool read_truncated = 4;

  // Set to true if write events were truncated due to the :ref:`max_buffered_tx_bytes
  // <envoy_api_field_service.tap.v2alpha.OutputConfig.max_buffered_tx_bytes>` setting.
  bool write_truncated = 5;
}

// A streamed socket trace segment. Multiple segments make up a full trace.
message SocketStreamedTraceSegment {
  // Trace ID unique to the originating Envoy only. Trace IDs can repeat and should not be used
  // for long term stable uniqueness. Matches connection IDs used in Envoy logs.
  uint64 trace_id = 1;

  oneof message_piece {
    // Connection properties.
    Connection connection = 2;

    // Socket event.
    SocketEvent event = 3;
  }
}
syntax = "proto3";

package envoy.data.tap.v2alpha;

import "envoy/data/tap/v2alpha/http.proto";
import "envoy/data/tap/v2alpha/transport.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v2alpha";
option java_outer_classname = "WrapperProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v2alpha";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Tap data wrappers]

// Wrapper for all fully buffered and streamed tap traces that Envoy emits. This is required for
// sending traces over gRPC APIs or more easily persisting binary messages to files.
message TraceWrapper {
  oneof trace {
    option (validate.required) = true;

    // An HTTP buffered tap trace.
    HttpBufferedTrace http_buffered_trace = 1;

    // An HTTP streamed tap trace segment.
    HttpStreamedTraceSegment http_streamed_trace_segment = 2;

    // A socket buffered tap trace.
    SocketBufferedTrace socket_buffered_trace = 3;

    // A socket streamed tap trace segment.
    SocketStreamedTraceSegment socket_streamed_trace_segment = 4;
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/api/v2/core:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.tap.v2alpha;

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.data.tap.v2alpha";
option java_outer_classname = "CommonProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/tap/v2alpha";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: Tap common data]

// Wrapper for tapped body data. This includes HTTP request/response body, transport socket received
// and transmitted data, etc.
message Body {
  oneof body_type {
    // Body data as bytes. By default, tap body data will be present in this field, as the proto
    // `bytes` type can contain any valid byte.
    bytes as_bytes = 1;

    // Body data as string. This field is only used when the :ref:`JSON_BODY_AS_STRING
    // <envoy_api_enum_value_service.tap.v2alpha.OutputSink.Format.JSON_BODY_AS_STRING>` sink
    // format type is selected. See the documentation for that option for why this is useful.
    string as_string = 2;
  }

  // Specifies whether body data has been truncated to fit within the specified
  // :ref:`max_buffered_rx_bytes
  // <envoy_api_field_service.tap.v2alpha.OutputConfig.max_buffered_rx_bytes>` and
  // :ref:`max_buffered_tx_bytes
  // <envoy_api_field_service.tap.v2alpha.OutputConfig.max_buffered_tx_bytes>` settings.
  bool truncated = 3;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/api/v2/core:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.accesslog.v2;

import "envoy/api/v2/core/address.proto";
import "envoy/api/v2/core/base.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.accesslog.v2";
option java_outer_classname = "AccesslogProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/accesslog/v2;accesslogv2";
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: gRPC access logs]
// Envoy access logs describe incoming interaction with Envoy over a fixed
// period of time, and typically cover a single request/response exchange,
// (e.g. HTTP), stream (e.g. over HTTP/gRPC), or proxied connection (e.g. TCP).
// Access logs contain fields defined in protocol-specific protobuf messages.
//
// Except where explicitly declared otherwise, all fields describe
// *downstream* interaction between Envoy and a connected client.
// Fields describing *upstream* interaction will explicitly include ``upstream``
// in their name.

message TCPAccessLogEntry {
  // Common properties shared by all Envoy access logs.
  AccessLogCommon common_properties = 1;

  // Properties of the TCP connection.
  ConnectionProperties connection_properties = 2;
}

message HTTPAccessLogEntry {
  // HTTP version
  enum HTTPVersion {
    PROTOCOL_UNSPECIFIED = 0;
    HTTP10 = 1;
    HTTP11 = 2;
    HTTP2 = 3;
    HTTP3 = 4;
  }

  // Common properties shared by all Envoy access logs.
  AccessLogCommon common_properties = 1;

  HTTPVersion protocol_version = 2;

  // Description of the incoming HTTP request.
  HTTPRequestProperties request = 3;

  // Description of the outgoing HTTP response.
  HTTPResponseProperties response = 4;
}

// Defines fields for a connection
message ConnectionProperties {
  // Number of bytes received from downstream.
  uint64 received_bytes = 1;

  // Number of bytes sent to downstream.
  uint64 sent_bytes = 2;
}

// Defines fields that are shared by all Envoy access logs.
// [#next-free-field: 22]
message AccessLogCommon {
  // [#not-implemented-hide:]
  // This field indicates the rate at which this log entry was sampled.
  // Valid range is (0.0, 1.0].
  double sample_rate = 1 [(validate.rules).double = {lte: 1.0 gt: 0.0}];

  // This field is the remote/origin address on which the request from the user was received.
  // Note: This may not be the physical peer. E.g, if the remote address is inferred from for
  // example the x-forwarder-for header, proxy protocol, etc.
  api.v2.core.Address downstream_remote_address = 2;

  // This field is the local/destination address on which the request from the user was received.
  api.v2.core.Address downstream_local_address = 3;

  // If the connection is secure,S this field will contain TLS properties.
  TLSProperties tls_properties = 4;

  // The time that Envoy started servicing this request. This is effectively the time that the first
  // downstream byte is received.
  google.protobuf.Timestamp start_time = 5;

  // Interval between the first downstream byte received and the last
  // downstream byte received (i.e. time it takes to receive a request).
  google.protobuf.Duration time_to_last_rx_byte = 6;

  // Interval between the first downstream byte received and the first upstream byte sent. There may
  // by considerable delta between *time_to_last_rx_byte* and this value due to filters.
  // Additionally, the same caveats apply as documented in *time_to_last_downstream_tx_byte* about
  // not accounting for kernel socket buffer time, etc.
  google.protobuf.Duration time_to_first_upstream_tx_byte = 7;

  // Interval between the first downstream byte received and the last upstream byte sent. There may
  // by considerable delta between *time_to_last_rx_byte* and this value due to filters.
  // Additionally, the same caveats apply as documented in *time_to_last_downstream_tx_byte* about
  // not accounting for kernel socket buffer time, etc.
  google.protobuf.Duration time_to_last_upstream_tx_byte = 8;

  // Interval between the first downstream byte received and the first upstream
  // byte received (i.e. time it takes to start receiving a response).
  google.protobuf.Duration time_to_first_upstream_rx_byte = 9;

  // Interval between the first downstream byte received and the last upstream
  // byte received (i.e. time it takes to receive a complete response).
  google.protobuf.Duration time_to_last_upstream_rx_byte = 10;

  // Interval between the first downstream byte received and the first downstream byte sent.
  // There may be a considerable delta between the *time_to_first_upstream_rx_byte* and this field
  // due to filters. Additionally, the same caveats apply as documented in
  // *time_to_last_downstream_tx_byte* about not accounting for kernel socket buffer time, etc.
  google.protobuf.Duration time_to_first_downstream_tx_byte = 11;

  // Interval between the first downstream byte received and the last downstream byte sent.
  // Depending on protocol, buffering, windowing, filters, etc. there may be a considerable delta
  // between *time_to_last_upstream_rx_byte* and this field. Note also that this is an approximate
  // time. In the current implementation it does not include kernel socket buffer time. In the
  // current implementation it also does not include send window buffering inside the HTTP/2 codec.
  // In the future it is likely that work will be done to make this duration more accurate.
  google.protobuf.Duration time_to_last_downstream_tx_byte = 12;

  // The upstream remote/destination address that handles this exchange. This does not include
  // retries.
  api.v2.core.Address upstream_remote_address = 13;

  // The upstream local/origin address that handles this exchange. This does not include retries.
  api.v2.core.Address upstream_local_address = 14;

  // The upstream cluster that *upstream_remote_address* belongs to.
  string upstream_cluster = 15;

  // Flags indicating occurrences during request/response processing.
  ResponseFlags response_flags = 16;

  // All metadata encountered during request processing, including endpoint
  // selection.
  //
  // This can be used to associate IDs attached to the various configurations
  // used to process this request with the access log entry. For example, a
  // route created from a higher level forwarding rule with some ID can place
  // that ID in this field and cross reference later. It can also be used to
  // determine if a canary endpoint was used or not.
  api.v2.core.Metadata metadata = 17;

  // If upstream connection failed due to transport socket (e.g. TLS handshake), provides the
  // failure reason from the transport socket. The format of this field depends on the configured
  // upstream transport socket. Common TLS failures are in
  // :ref:`TLS trouble shooting <arch_overview_ssl_trouble_shooting>`.
  string upstream_transport_failure_reason = 18;

  // The name of the route
  string route_name = 19;

  // This field is the downstream direct remote address on which the request from the user was
  // received. Note: This is always the physical peer, even if the remote address is inferred from
  // for example the x-forwarder-for header, proxy protocol, etc.
  api.v2.core.Address downstream_direct_remote_address = 20;

  // Map of filter state in stream info that have been configured to be logged. If the filter
  // state serialized to any message other than `google.protobuf.Any` it will be packed into
  // `google.protobuf.Any`.
  map<string, google.protobuf.Any> filter_state_objects = 21;
}

// Flags indicating occurrences during request/response processing.
// [#next-free-field: 20]
message ResponseFlags {
  message Unauthorized {
    // Reasons why the request was unauthorized
    enum Reason {
      REASON_UNSPECIFIED = 0;

      // The request was denied by the external authorization service.
      EXTERNAL_SERVICE = 1;
    }

    Reason reason = 1;
  }

  // Indicates local server healthcheck failed.
  bool failed_local_healthcheck = 1;

  // Indicates there was no healthy upstream.
  bool no_healthy_upstream = 2;

  // Indicates an there was an upstream request timeout.
  bool upstream_request_timeout = 3;

  // Indicates local codec level reset was sent on the stream.
  bool local_reset = 4;

  // Indicates remote codec level reset was received on the stream.
  bool upstream_remote_reset = 5;

  // Indicates there was a local reset by a connection pool due to an initial connection failure.
  bool upstream_connection_failure = 6;

  // Indicates the stream was reset due to an upstream connection termination.
  bool upstream_connection_termination = 7;

  // Indicates the stream was reset because of a resource overflow.
  bool upstream_overflow = 8;

  // Indicates no route was found for the request.
  bool no_route_found = 9;

  // Indicates that the request was delayed before proxying.
  bool delay_injected = 10;

  // Indicates that the request was aborted with an injected error code.
  bool fault_injected = 11;

  // Indicates that the request was rate-limited locally.
  bool rate_limited = 12;

  // Indicates if the request was deemed unauthorized and the reason for it.
  Unauthorized unauthorized_details = 13;

  // Indicates that the request was rejected because there was an error in rate limit service.
  bool rate_limit_service_error = 14;

  // Indicates the stream was reset due to a downstream connection termination.
  bool downstream_connection_termination = 15;

  // Indicates that the upstream retry limit was exceeded, resulting in a downstream error.
  bool upstream_retry_limit_exceeded = 16;

  // Indicates that the stream idle timeout was hit, resulting in a downstream 408.
  bool stream_idle_timeout = 17;

  // Indicates that the request was rejected because an envoy request header failed strict
  // validation.
  bool invalid_envoy_request_headers = 18;

  // Indicates there was an HTTP protocol error on the downstream request.
  bool downstream_protocol_error = 19;
}

// Properties of a negotiated TLS connection.
// [#next-free-field: 7]
message TLSProperties {
  enum TLSVersion {
    VERSION_UNSPECIFIED = 0;
    TLSv1 = 1;
    TLSv1_1 = 2;
    TLSv1_2 = 3;
    TLSv1_3 = 4;
  }

  message CertificateProperties {
    message SubjectAltName {
      oneof san {
        string uri = 1;

        // [#not-implemented-hide:]
        string dns = 2;
      }
    }

    // SANs present in the certificate.
    repeated SubjectAltName subject_alt_name = 1;

    // The subject field of the certificate.
    string subject = 2;
  }

  // Version of TLS that was negotiated.
  TLSVersion tls_version = 1;

  // TLS cipher suite negotiated during handshake. The value is a
  // four-digit hex code defined by the IANA TLS Cipher Suite Registry
  // (e.g. ``009C`` for ``TLS_RSA_WITH_AES_128_GCM_SHA256``).
  //
  // Here it is expressed as an integer.
  google.protobuf.UInt32Value tls_cipher_suite = 2;

  // SNI hostname from handshake.
  string tls_sni_hostname = 3;

  // Properties of the local certificate used to negotiate TLS.
  CertificateProperties local_certificate_properties = 4;

  // Properties of the peer certificate used to negotiate TLS.
  CertificateProperties peer_certificate_properties = 5;

  // The TLS session ID.
  string tls_session_id = 6;
}

// [#next-free-field: 14]
message HTTPRequestProperties {
  // The request method (RFC 7231/2616).
  api.v2.core.RequestMethod request_method = 1 [(validate.rules).enum = {defined_only: true}];

  // The scheme portion of the incoming request URI.
  string scheme = 2;

  // HTTP/2 ``:authority`` or HTTP/1.1 ``Host`` header value.
  string authority = 3;

  // The port of the incoming request URI
  // (unused currently, as port is composed onto authority).
  google.protobuf.UInt32Value port = 4;

  // The path portion from the incoming request URI.
  string path = 5;

  // Value of the ``User-Agent`` request header.
  string user_agent = 6;

  // Value of the ``Referer`` request header.
  string referer = 7;

  // Value of the ``X-Forwarded-For`` request header.
  string forwarded_for = 8;

  // Value of the ``X-Request-Id`` request header
  //
  // This header is used by Envoy to uniquely identify a request.
  // It will be generated for all external requests and internal requests that
  // do not already have a request ID.
  string request_id = 9;

  // Value of the ``X-Envoy-Original-Path`` request header.
  string original_path = 10;

  // Size of the HTTP request headers in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include overhead from framing or encoding at other networking layers.
  uint64 request_headers_bytes = 11;

  // Size of the HTTP request body in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include overhead from framing or encoding at other networking layers.
  uint64 request_body_bytes = 12;

  // Map of additional headers that have been configured to be logged.
  map<string, string> request_headers = 13;
}

// [#next-free-field: 7]
message HTTPResponseProperties {
  // The HTTP response code returned by Envoy.
  google.protobuf.UInt32Value response_code = 1;

  // Size of the HTTP response headers in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include overhead from framing or encoding at other networking layers.
  uint64 response_headers_bytes = 2;

  // Size of the HTTP response body in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include overhead from framing or encoding at other networking layers.
  uint64 response_body_bytes = 3;

  // Map of additional headers configured to be logged.
  map<string, string> response_headers = 4;

  // Map of trailers configured to be logged.
  map<string, string> response_trailers = 5;

  // The HTTP response code details.
  string response_code_details = 6;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.accesslog.v3;

import "envoy/config/core/v3/address.proto";
import "envoy/config/core/v3/base.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.accesslog.v3";
option java_outer_classname = "AccesslogProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/accesslog/v3;accesslogv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: gRPC access logs]
// Envoy access logs describe incoming interaction with Envoy over a fixed
// period of time, and typically cover a single request/response exchange,
// (e.g. HTTP), stream (e.g. over HTTP/gRPC), or proxied connection (e.g. TCP).
// Access logs contain fields defined in protocol-specific protobuf messages.
//
// Except where explicitly declared otherwise, all fields describe
// *downstream* interaction between Envoy and a connected client.
// Fields describing *upstream* interaction will explicitly include ``upstream``
// in their name.

enum AccessLogType {
  NotSet = 0;
  TcpUpstreamConnected = 1;
  TcpPeriodic = 2;
  TcpConnectionEnd = 3;
  DownstreamStart = 4;
  DownstreamPeriodic = 5;
  DownstreamEnd = 6;
  UpstreamPoolReady = 7;
  UpstreamPeriodic = 8;
  UpstreamEnd = 9;
  DownstreamTunnelSuccessfullyEstablished = 10;
  UdpTunnelUpstreamConnected = 11;
  UdpPeriodic = 12;
  UdpSessionEnd = 13;
}

message TCPAccessLogEntry {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.TCPAccessLogEntry";

  // Common properties shared by all Envoy access logs.
  AccessLogCommon common_properties = 1;

  // Properties of the TCP connection.
  ConnectionProperties connection_properties = 2;
}

message HTTPAccessLogEntry {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.HTTPAccessLogEntry";

  // HTTP version
  enum HTTPVersion {
    PROTOCOL_UNSPECIFIED = 0;
    HTTP10 = 1;
    HTTP11 = 2;
    HTTP2 = 3;
    HTTP3 = 4;
  }

  // Common properties shared by all Envoy access logs.
  AccessLogCommon common_properties = 1;

  HTTPVersion protocol_version = 2;

  // Description of the incoming HTTP request.
  HTTPRequestProperties request = 3;

  // Description of the outgoing HTTP response.
  HTTPResponseProperties response = 4;
}

// Defines fields for a connection
message ConnectionProperties {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.ConnectionProperties";

  // Number of bytes received from downstream.
  uint64 received_bytes = 1;

  // Number of bytes sent to downstream.
  uint64 sent_bytes = 2;
}

// Defines fields that are shared by all Envoy access logs.
// [#next-free-field: 34]
message AccessLogCommon {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.AccessLogCommon";

  // [#not-implemented-hide:]
  // This field indicates the rate at which this log entry was sampled.
  // Valid range is (0.0, 1.0].
  double sample_rate = 1 [(validate.rules).double = {lte: 1.0 gt: 0.0}];

  // This field is the remote/origin address on which the request from the user was received.
  // Note: This may not be the physical peer. E.g, if the remote address is inferred from for
  // example the x-forwarder-for header, proxy protocol, etc.
  config.core.v3.Address downstream_remote_address = 2;

  // This field is the local/destination address on which the request from the user was received.
  config.core.v3.Address downstream_local_address = 3;

  // If the connection is secure,S this field will contain TLS properties.
  TLSProperties tls_properties = 4;

  // The time that Envoy started servicing this request. This is effectively the time that the first
  // downstream byte is received.
  google.protobuf.Timestamp start_time = 5;

  // Interval between the first downstream byte received and the last
  // downstream byte received (i.e. time it takes to receive a request).
  google.protobuf.Duration time_to_last_rx_byte = 6;

  // Interval between the first downstream byte received and the first upstream byte sent. There may
  // by considerable delta between ``time_to_last_rx_byte`` and this value due to filters.
  // Additionally, the same caveats apply as documented in ``time_to_last_downstream_tx_byte`` about
  // not accounting for kernel socket buffer time, etc.
  google.protobuf.Duration time_to_first_upstream_tx_byte = 7;

  // Interval between the first downstream byte received and the last upstream byte sent. There may
  // by considerable delta between ``time_to_last_rx_byte`` and this value due to filters.
  // Additionally, the same caveats apply as documented in ``time_to_last_downstream_tx_byte`` about
  // not accounting for kernel socket buffer time, etc.
  google.protobuf.Duration time_to_last_upstream_tx_byte = 8;

  // Interval between the first downstream byte received and the first upstream
  // byte received (i.e. time it takes to start receiving a response).
  google.protobuf.Duration time_to_first_upstream_rx_byte = 9;

  // Interval between the first downstream byte received and the last upstream
  // byte received (i.e. time it takes to receive a complete response).
  google.protobuf.Duration time_to_last_upstream_rx_byte = 10;

  // Interval between the first downstream byte received and the first downstream byte sent.
  // There may be a considerable delta between the ``time_to_first_upstream_rx_byte`` and this field
  // due to filters. Additionally, the same caveats apply as documented in
  // ``time_to_last_downstream_tx_byte`` about not accounting for kernel socket buffer time, etc.
  google.protobuf.Duration time_to_first_downstream_tx_byte = 11;

  // Interval between the first downstream byte received and the last downstream byte sent.
  // Depending on protocol, buffering, windowing, filters, etc. there may be a considerable delta
  // between ``time_to_last_upstream_rx_byte`` and this field. Note also that this is an approximate
  // time. In the current implementation it does not include kernel socket buffer time. In the
  // current implementation it also does not include send window buffering inside the HTTP/2 codec.
  // In the future it is likely that work will be done to make this duration more accurate.
  google.protobuf.Duration time_to_last_downstream_tx_byte = 12;

  // The upstream remote/destination address that handles this exchange. This does not include
  // retries.
  config.core.v3.Address upstream_remote_address = 13;

  // The upstream local/origin address that handles this exchange. This does not include retries.
  config.core.v3.Address upstream_local_address = 14;

  // The upstream cluster that ``upstream_remote_address`` belongs to.
  string upstream_cluster = 15;

  // Flags indicating occurrences during request/response processing.
  ResponseFlags response_flags = 16;

  // All metadata encountered during request processing, including endpoint
  // selection.
  //
  // This can be used to associate IDs attached to the various configurations
  // used to process this request with the access log entry. For example, a
  // route created from a higher level forwarding rule with some ID can place
  // that ID in this field and cross reference later. It can also be used to
  // determine if a canary endpoint was used or not.
  config.core.v3.Metadata metadata = 17;

  // If upstream connection failed due to transport socket (e.g. TLS handshake), provides the
  // failure reason from the transport socket. The format of this field depends on the configured
  // upstream transport socket. Common TLS failures are in
  // :ref:`TLS trouble shooting <arch_overview_ssl_trouble_shooting>`.
  string upstream_transport_failure_reason = 18;

  // The name of the route
  string route_name = 19;

  // This field is the downstream direct remote address on which the request from the user was
  // received. Note: This is always the physical peer, even if the remote address is inferred from
  // for example the x-forwarder-for header, proxy protocol, etc.
  config.core.v3.Address downstream_direct_remote_address = 20;

  // Map of filter state in stream info that have been configured to be logged. If the filter
  // state serialized to any message other than ``google.protobuf.Any`` it will be packed into
  // ``google.protobuf.Any``.
  map<string, google.protobuf.Any> filter_state_objects = 21;

  // A list of custom tags, which annotate logs with additional information.
  // To configure this value, users should configure
  // :ref:`custom_tags <envoy_v3_api_field_extensions.access_loggers.grpc.v3.CommonGrpcAccessLogConfig.custom_tags>`.
  map<string, string> custom_tags = 22;

  // For HTTP: Total duration in milliseconds of the request from the start time to the last byte out.
  // For TCP: Total duration in milliseconds of the downstream connection.
  // This is the total duration of the request (i.e., when the request's ActiveStream is destroyed)
  // and may be longer than ``time_to_last_downstream_tx_byte``.
  google.protobuf.Duration duration = 23;

  // For HTTP: Number of times the request is attempted upstream. Note that the field is omitted when the request was never attempted upstream.
  // For TCP: Number of times the connection request is attempted upstream. Note that the field is omitted when the connect request was never attempted upstream.
  uint32 upstream_request_attempt_count = 24;

  // Connection termination details may provide additional information about why the connection was terminated by Envoy for L4 reasons.
  string connection_termination_details = 25;

  // Optional unique id of stream (TCP connection, long-live HTTP2 stream, HTTP request) for logging and tracing.
  // This could be any format string that could be used to identify one stream.
  string stream_id = 26;

  // If this log entry is final log entry that flushed after the stream completed or
  // intermediate log entry that flushed periodically during the stream.
  // There may be multiple intermediate log entries and only one final log entry for each
  // long-live stream (TCP connection, long-live HTTP2 stream).
  // And if it is necessary, unique ID or identifier can be added to the log entry
  // :ref:`stream_id <envoy_v3_api_field_data.accesslog.v3.AccessLogCommon.stream_id>` to
  // correlate all these intermediate log entries and final log entry.
  //
  // .. attention::
  //
  //   This field is deprecated in favor of ``access_log_type`` for better indication of the
  //   type of the access log record.
  bool intermediate_log_entry = 27
      [deprecated = true, (envoy.annotations.deprecated_at_minor_version) = "3.0"];

  // If downstream connection in listener failed due to transport socket (e.g. TLS handshake), provides the
  // failure reason from the transport socket. The format of this field depends on the configured downstream
  // transport socket. Common TLS failures are in :ref:`TLS trouble shooting <arch_overview_ssl_trouble_shooting>`.
  string downstream_transport_failure_reason = 28;

  // For HTTP: Total number of bytes sent to the downstream by the http stream.
  // For TCP: Total number of bytes sent to the downstream by the tcp proxy.
  uint64 downstream_wire_bytes_sent = 29;

  // For HTTP: Total number of bytes received from the downstream by the http stream. Envoy over counts sizes of received HTTP/1.1 pipelined requests by adding up bytes of requests in the pipeline to the one currently being processed.
  // For TCP: Total number of bytes received from the downstream by the tcp proxy.
  uint64 downstream_wire_bytes_received = 30;

  // For HTTP: Total number of bytes sent to the upstream by the http stream. This value accumulates during upstream retries.
  // For TCP: Total number of bytes sent to the upstream by the tcp proxy.
  uint64 upstream_wire_bytes_sent = 31;

  // For HTTP: Total number of bytes received from the upstream by the http stream.
  // For TCP: Total number of bytes sent to the upstream by the tcp proxy.
  uint64 upstream_wire_bytes_received = 32;

  // The type of the access log, which indicates when the log was recorded.
  // See :ref:`ACCESS_LOG_TYPE <config_access_log_format_access_log_type>` for the available values.
  // In case the access log was recorded by a flow which does not correspond to one of the supported
  // values, then the default value will be ``NotSet``.
  // For more information about how access log behaves and when it is being recorded,
  // please refer to :ref:`access logging <arch_overview_access_logs>`.
  AccessLogType access_log_type = 33;
}

// Flags indicating occurrences during request/response processing.
// [#next-free-field: 28]
message ResponseFlags {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.ResponseFlags";

  message Unauthorized {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.accesslog.v2.ResponseFlags.Unauthorized";

    // Reasons why the request was unauthorized
    enum Reason {
      REASON_UNSPECIFIED = 0;

      // The request was denied by the external authorization service.
      EXTERNAL_SERVICE = 1;
    }

    Reason reason = 1;
  }

  // Indicates local server healthcheck failed.
  bool failed_local_healthcheck = 1;

  // Indicates there was no healthy upstream.
  bool no_healthy_upstream = 2;

  // Indicates an there was an upstream request timeout.
  bool upstream_request_timeout = 3;

  // Indicates local codec level reset was sent on the stream.
  bool local_reset = 4;

  // Indicates remote codec level reset was received on the stream.
  bool upstream_remote_reset = 5;

  // Indicates there was a local reset by a connection pool due to an initial connection failure.
  bool upstream_connection_failure = 6;

  // Indicates the stream was reset due to an upstream connection termination.
  bool upstream_connection_termination = 7;

  // Indicates the stream was reset because of a resource overflow.
  bool upstream_overflow = 8;

  // Indicates no route was found for the request.
  bool no_route_found = 9;

  // Indicates that the request was delayed before proxying.
  bool delay_injected = 10;

  // Indicates that the request was aborted with an injected error code.
  bool fault_injected = 11;

  // Indicates that the request was rate-limited locally.
  bool rate_limited = 12;

  // Indicates if the request was deemed unauthorized and the reason for it.
  Unauthorized unauthorized_details = 13;

  // Indicates that the request was rejected because there was an error in rate limit service.
  bool rate_limit_service_error = 14;

  // Indicates the stream was reset due to a downstream connection termination.
  bool downstream_connection_termination = 15;

  // Indicates that the upstream retry limit was exceeded, resulting in a downstream error.
  bool upstream_retry_limit_exceeded = 16;

  // Indicates that the stream idle timeout was hit, resulting in a downstream 408.
  bool stream_idle_timeout = 17;

  // Indicates that the request was rejected because an envoy request header failed strict
  // validation.
  bool invalid_envoy_request_headers = 18;

  // Indicates there was an HTTP protocol error on the downstream request.
  bool downstream_protocol_error = 19;

  // Indicates there was a max stream duration reached on the upstream request.
  bool upstream_max_stream_duration_reached = 20;

  // Indicates the response was served from a cache filter.
  bool response_from_cache_filter = 21;

  // Indicates that a filter configuration is not available.
  bool no_filter_config_found = 22;

  // Indicates that request or connection exceeded the downstream connection duration.
  bool duration_timeout = 23;

  // Indicates there was an HTTP protocol error in the upstream response.
  bool upstream_protocol_error = 24;

  // Indicates no cluster was found for the request.
  bool no_cluster_found = 25;

  // Indicates overload manager terminated the request.
  bool overload_manager = 26;

  // Indicates a DNS resolution failed.
  bool dns_resolution_failure = 27;
}

// Properties of a negotiated TLS connection.
// [#next-free-field: 8]
message TLSProperties {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.TLSProperties";

  enum TLSVersion {
    VERSION_UNSPECIFIED = 0;
    TLSv1 = 1;
    TLSv1_1 = 2;
    TLSv1_2 = 3;
    TLSv1_3 = 4;
  }

  message CertificateProperties {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.accesslog.v2.TLSProperties.CertificateProperties";

    message SubjectAltName {
      option (udpa.annotations.versioning).previous_message_type =
          "envoy.data.accesslog.v2.TLSProperties.CertificateProperties.SubjectAltName";

      oneof san {
        string uri = 1;

        // [#not-implemented-hide:]
        string dns = 2;
      }
    }

    // SANs present in the certificate.
    repeated SubjectAltName subject_alt_name = 1;

    // The subject field of the certificate.
    string subject = 2;

    // The issuer field of the certificate.
    string issuer = 3;
  }

  // Version of TLS that was negotiated.
  TLSVersion tls_version = 1;

  // TLS cipher suite negotiated during handshake. The value is a
  // four-digit hex code defined by the IANA TLS Cipher Suite Registry
  // (e.g. ``009C`` for ``TLS_RSA_WITH_AES_128_GCM_SHA256``).
  //
  // Here it is expressed as an integer.
  google.protobuf.UInt32Value tls_cipher_suite = 2;

  // SNI hostname from handshake.
  string tls_sni_hostname = 3;

  // Properties of the local certificate used to negotiate TLS.
  CertificateProperties local_certificate_properties = 4;

  // Properties of the peer certificate used to negotiate TLS.
  CertificateProperties peer_certificate_properties = 5;

  // The TLS session ID.
  string tls_session_id = 6;

  // The ``JA3`` fingerprint when ``JA3`` fingerprinting is enabled.
  string ja3_fingerprint = 7;
}

// [#next-free-field: 16]
message HTTPRequestProperties {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.HTTPRequestProperties";

  // The request method (RFC 7231/2616).
  config.core.v3.RequestMethod request_method = 1 [(validate.rules).enum = {defined_only: true}];

  // The scheme portion of the incoming request URI.
  string scheme = 2;

  // HTTP/2 ``:authority`` or HTTP/1.1 ``Host`` header value.
  string authority = 3;

  // The port of the incoming request URI
  // (unused currently, as port is composed onto authority).
  google.protobuf.UInt32Value port = 4;

  // The path portion from the incoming request URI.
  string path = 5;

  // Value of the ``User-Agent`` request header.
  string user_agent = 6;

  // Value of the ``Referer`` request header.
  string referer = 7;

  // Value of the ``X-Forwarded-For`` request header.
  string forwarded_for = 8;

  // Value of the ``X-Request-Id`` request header
  //
  // This header is used by Envoy to uniquely identify a request.
  // It will be generated for all external requests and internal requests that
  // do not already have a request ID.
  string request_id = 9;

  // Value of the ``X-Envoy-Original-Path`` request header.
  string original_path = 10;

  // Size of the HTTP request headers in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include overhead from framing or encoding at other networking layers.
  uint64 request_headers_bytes = 11;

  // Size of the HTTP request body in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include overhead from framing or encoding at other networking layers.
  uint64 request_body_bytes = 12;

  // Map of additional headers that have been configured to be logged.
  map<string, string> request_headers = 13;

  // Number of header bytes sent to the upstream by the http stream, including protocol overhead.
  //
  // This value accumulates during upstream retries.
  uint64 upstream_header_bytes_sent = 14;

  // Number of header bytes received from the downstream by the http stream, including protocol overhead.
  uint64 downstream_header_bytes_received = 15;
}

// [#next-free-field: 9]
message HTTPResponseProperties {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.data.accesslog.v2.HTTPResponseProperties";

  // The HTTP response code returned by Envoy.
  google.protobuf.UInt32Value response_code = 1;

  // Size of the HTTP response headers in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include protocol overhead or overhead from framing or encoding at other networking layers.
  uint64 response_headers_bytes = 2;

  // Size of the HTTP response body in bytes.
  //
  // This value is captured from the OSI layer 7 perspective, i.e. it does not
  // include overhead from framing or encoding at other networking layers.
  uint64 response_body_bytes = 3;

  // Map of additional headers configured to be logged.
  map<string, string> response_headers = 4;

  // Map of trailers configured to be logged.
  map<string, string> response_trailers = 5;

  // The HTTP response code details.
  string response_code_details = 6;

  // Number of header bytes received from the upstream by the http stream, including protocol overhead.
  uint64 upstream_header_bytes_received = 7;

  // Number of header bytes sent to the downstream by the http stream, including protocol overhead.
  uint64 downstream_header_bytes_sent = 8;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/annotations:pkg",
        "//envoy/type/matcher/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.dns.v3;

import "envoy/type/matcher/v3/string.proto";

import "google/protobuf/duration.proto";

import "envoy/annotations/deprecation.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.dns.v3";
option java_outer_classname = "DnsTableProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/dns/v3;dnsv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: DNS Filter Table Data]
// :ref:`DNS Filter config overview <config_udp_listener_filters_dns_filter>`.

// This message contains the configuration for the DNS Filter if populated
// from the control plane
message DnsTable {
  option (udpa.annotations.versioning).previous_message_type = "envoy.data.dns.v2alpha.DnsTable";

  // This message contains a list of IP addresses returned for a query for a known name
  message AddressList {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.dns.v2alpha.DnsTable.AddressList";

    // This field contains a well formed IP address that is returned in the answer for a
    // name query. The address field can be an IPv4 or IPv6 address. Address family
    // detection is done automatically when Envoy parses the string. Since this field is
    // repeated, Envoy will return as many entries from this list in the DNS response while
    // keeping the response under 512 bytes
    repeated string address = 1 [(validate.rules).repeated = {
      min_items: 1
      items {string {min_len: 3}}
    }];
  }

  // Specify the service protocol using a numeric or string value
  message DnsServiceProtocol {
    oneof protocol_config {
      option (validate.required) = true;

      // Specify the protocol number for the service. Envoy will try to resolve the number to
      // the protocol name. For example, 6 will resolve to "tcp". Refer to:
      // https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml
      // for protocol names and numbers
      uint32 number = 1 [(validate.rules).uint32 = {lt: 255}];

      // Specify the protocol name for the service.
      string name = 2 [(validate.rules).string = {min_len: 1 well_known_regex: HTTP_HEADER_NAME}];
    }
  }

  // Specify the target for a given DNS service
  // [#next-free-field: 6]
  message DnsServiceTarget {
    // Specify the name of the endpoint for the Service. The name is a hostname or a cluster
    oneof endpoint_type {
      option (validate.required) = true;

      // Use a resolvable hostname as the endpoint for a service.
      string host_name = 1
          [(validate.rules).string = {min_len: 1 well_known_regex: HTTP_HEADER_NAME}];

      // Use a cluster name as the endpoint for a service.
      string cluster_name = 2
          [(validate.rules).string = {min_len: 1 well_known_regex: HTTP_HEADER_NAME}];
    }

    // The priority of the service record target
    uint32 priority = 3 [(validate.rules).uint32 = {lt: 65536}];

    // The weight of the service record target
    uint32 weight = 4 [(validate.rules).uint32 = {lt: 65536}];

    // The port to which the service is bound. This value is optional if the target is a
    // cluster. Setting port to zero in this case makes the filter use the port value
    // from the cluster host
    uint32 port = 5 [(validate.rules).uint32 = {lt: 65536}];
  }

  // This message defines a service selection record returned for a service query in a domain
  message DnsService {
    // The name of the service without the protocol or domain name
    string service_name = 1
        [(validate.rules).string = {min_len: 1 well_known_regex: HTTP_HEADER_NAME}];

    // The service protocol. This can be specified as a string or the numeric value of the protocol
    DnsServiceProtocol protocol = 2;

    // The service entry time to live. This is independent from the DNS Answer record TTL
    google.protobuf.Duration ttl = 3 [(validate.rules).duration = {gte {seconds: 1}}];

    // The list of targets hosting the service
    repeated DnsServiceTarget targets = 4 [(validate.rules).repeated = {min_items: 1}];
  }

  // Define a list of service records for a given service
  message DnsServiceList {
    repeated DnsService services = 1 [(validate.rules).repeated = {min_items: 1}];
  }

  message DnsEndpoint {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.dns.v2alpha.DnsTable.DnsEndpoint";

    oneof endpoint_config {
      option (validate.required) = true;

      // Define a list of addresses to return for the specified endpoint
      AddressList address_list = 1;

      // Define a cluster whose addresses are returned for the specified endpoint
      string cluster_name = 2;

      // Define a DNS Service List for the specified endpoint
      DnsServiceList service_list = 3;
    }
  }

  message DnsVirtualDomain {
    option (udpa.annotations.versioning).previous_message_type =
        "envoy.data.dns.v2alpha.DnsTable.DnsVirtualDomain";

    // A domain name for which Envoy will respond to query requests
    string name = 1 [(validate.rules).string = {min_len: 1 well_known_regex: HTTP_HEADER_NAME}];

    // The configuration containing the method to determine the address of this endpoint
    DnsEndpoint endpoint = 2;

    // Sets the TTL in DNS answers from Envoy returned to the client. The default TTL is 300s
    google.protobuf.Duration answer_ttl = 3 [(validate.rules).duration = {gte {seconds: 30}}];
  }

  // Control how many times Envoy makes an attempt to forward a query to an external DNS server
  uint32 external_retry_count = 1 [(validate.rules).uint32 = {lte: 3}];

  // Fully qualified domain names for which Envoy will respond to DNS queries. By leaving this
  // list empty, Envoy will forward all queries to external resolvers
  repeated DnsVirtualDomain virtual_domains = 2;

  // This field is deprecated and no longer used in Envoy. The filter's behavior has changed
  // internally to use a different data structure allowing the filter to determine whether a
  // query is for known domain without the use of this field.
  //
  // This field serves to help Envoy determine whether it can authoritatively answer a query
  // for a name matching a suffix in this list. If the query name does not match a suffix in
  // this list, Envoy will forward the query to an upstream DNS server
  repeated type.matcher.v3.StringMatcher known_suffixes = 3
      [deprecated = true, (envoy.annotations.deprecated_at_minor_version) = "3.0"];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/type/matcher:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.data.dns.v2alpha;

import "envoy/type/matcher/string.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.data.dns.v2alpha";
option java_outer_classname = "DnsTableProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/envoy/data/dns/v2alpha";
option (udpa.annotations.file_status).work_in_progress = true;
option (udpa.annotations.file_status).package_version_status = FROZEN;

// [#protodoc-title: DNS Filter Table Data]
// :ref:`DNS Filter config overview <config_udp_listener_filters_dns_filter>`.

// This message contains the configuration for the DNS Filter if populated
// from the control plane
message DnsTable {
  // This message contains a list of IP addresses returned for a query for a known name
  message AddressList {
    // This field contains a well formed IP address that is returned
    // in the answer for a name query. The address field can be an
    // IPv4 or IPv6 address. Address family detection is done automatically
    // when Envoy parses the string. Since this field is repeated,
    // Envoy will return one randomly chosen entry from this list in the
    // DNS response. The random index will vary per query so that we prevent
    // clients pinning on a single address for a configured domain
    repeated string address = 1 [(validate.rules).repeated = {
      min_items: 1
      items {string {min_len: 3}}
    }];
  }

  // This message type is extensible and can contain a list of addresses
  // or dictate some other method for resolving the addresses for an
  // endpoint
  message DnsEndpoint {
    oneof endpoint_config {
      option (validate.required) = true;

      AddressList address_list = 1;
    }
  }

  message DnsVirtualDomain {
    // The domain name for which Envoy will respond to query requests
    string name = 1 [(validate.rules).string = {min_len: 2 well_known_regex: HTTP_HEADER_NAME}];

    // The configuration containing the method to determine the address
    // of this endpoint
    DnsEndpoint endpoint = 2;

    // Sets the TTL in dns answers from Envoy returned to the client
    google.protobuf.Duration answer_ttl = 3 [(validate.rules).duration = {gt {}}];
  }

  // Control how many times envoy makes an attempt to forward a query to
  // an external server
  uint32 external_retry_count = 1;

  // Fully qualified domain names for which Envoy will respond to queries
  repeated DnsVirtualDomain virtual_domains = 2 [(validate.rules).repeated = {min_items: 1}];

  // This field serves to help Envoy determine whether it can authoritatively
  // answer a query for a name matching a suffix in this list. If the query
  // name does not match a suffix in this list, Envoy will forward
  // the query to an upstream DNS server
  repeated type.matcher.StringMatcher known_suffixes = 3;
}
syntax = "proto3";

package envoy.extensions.matching.input_matchers.hyperscan.v3alpha;

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.matching.input_matchers.hyperscan.v3alpha";
option java_outer_classname = "HyperscanProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/matching/input_matchers/hyperscan/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Hyperscan matcher]
// Hyperscan :ref:`configuration overview <config_hyperscan>`.
// [#extension: envoy.matching.input_matchers.hyperscan]

// `Hyperscan <https://github.com/intel/hyperscan>`_ regex matcher. The matcher uses the Hyperscan
// engine which exploits x86 SIMD instructions to accelerate matching large numbers of regular
// expressions simultaneously across streams of data.
message Hyperscan {
  // [#next-free-field: 11]
  message Regex {
    // The regex expression.
    //
    // The expression must represent only the pattern to be matched, with no delimiters or flags.
    string regex = 1 [(validate.rules).string = {min_len: 1}];

    // The ID of the regex expression.
    //
    // This option is designed to be used on the sub-expressions in logical combinations.
    uint32 id = 2;

    // Matching will be performed case-insensitively.
    //
    // The expression may still use PCRE tokens (notably ``(?i)`` and ``(?-i)``) to switch
    // case-insensitive matching on and off.
    bool caseless = 3;

    // Matching a ``.`` will not exclude newlines.
    bool dot_all = 4;

    // ``^`` and ``$`` anchors match any newlines in data.
    bool multiline = 5;

    // Allow expressions which can match against an empty string.
    //
    // This option instructs the compiler to allow expressions that can match against empty buffers,
    // such as ``.?``, ``.*``, ``(a|)``. Since Hyperscan can return every possible match for an expression,
    // such expressions generally execute very slowly.
    bool allow_empty = 6;

    // Treat the pattern as a sequence of UTF-8 characters.
    bool utf8 = 7;

    // Use Unicode properties for character classes.
    //
    // This option instructs Hyperscan to use Unicode properties, rather than the default ASCII
    // interpretations, for character mnemonics like ``\w`` and ``\s`` as well as the POSIX character
    // classes. It is only meaningful in conjunction with ``utf8``.
    bool ucp = 8;

    // Logical combination.
    //
    // This option instructs Hyperscan to parse this expression as logical combination syntax.
    // Logical constraints consist of operands, operators and parentheses. The operands are
    // expression indices, and operators can be ``!``, ``&`` or ``|``.
    bool combination = 9;

    // Don’t do any match reporting.
    //
    // This option instructs Hyperscan to ignore match reporting for this expression. It is
    // designed to be used on the sub-expressions in logical combinations.
    bool quiet = 10;
  }

  // Specifies a set of regex expressions that the input should match on.
  repeated Regex regexes = 1 [(validate.rules).repeated = {min_items: 1}];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.network.golang.v3alpha;

import "google/protobuf/any.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.golang.v3alpha";
option java_outer_classname = "GolangProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/golang/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Golang network filter]
//
// Golang network filter :ref:`configuration overview <config_network_filters_golang>`.
// [#extension: envoy.filters.network.golang]

// [#next-free-field: 6]
message Config {
  // Bool ``true`` if this filter must be the last filter in a filter chain, ``false`` otherwise.
  bool is_terminal_filter = 1;

  // Globally unique ID for a dynamic library file.
  string library_id = 2 [(validate.rules).string = {min_len: 1}];

  // Path to a dynamic library implementing the
  // :repo:`DownstreamFilter API <contrib/golang/common/go/api.DownstreamFilter>`
  // interface.
  // [#comment:TODO(wangfakang): Support for downloading libraries from remote repositories.]
  string library_path = 3 [(validate.rules).string = {min_len: 1}];

  // Globally unique name of the Go plugin.
  //
  // This name **must** be consistent with the name registered in ``network::RegisterNetworkFilterConfigFactory``
  //
  string plugin_name = 4 [(validate.rules).string = {min_len: 1}];

  // Configuration for the Go plugin.
  //
  // .. note::
  //     This configuration is only parsed in the go plugin, and is therefore not validated
  //     by Envoy.
  //
  //     See the :repo:`DownstreamFilter API <contrib/golang/common/go/api/filter.go>`
  //     for more information about how the plugin's configuration data can be accessed.
  //
  google.protobuf.Any plugin_config = 5;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.kafka_broker.v3;

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.kafka_broker.v3";
option java_outer_classname = "KafkaBrokerProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/kafka_broker/v3;kafka_brokerv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Kafka Broker]
// Kafka Broker :ref:`configuration overview <config_network_filters_kafka_broker>`.
// [#extension: envoy.filters.network.kafka_broker]

message KafkaBroker {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.config.filter.network.kafka_broker.v2alpha1.KafkaBroker";

  // The prefix to use when emitting :ref:`statistics <config_network_filters_kafka_broker_stats>`.
  string stat_prefix = 1 [(validate.rules).string = {min_len: 1}];

  // Set to true if broker filter should attempt to serialize the received responses from the
  // upstream broker instead of passing received bytes as is.
  // Disabled by default.
  bool force_response_rewrite = 2;

  // Optional broker address rewrite specification.
  // Allows the broker filter to rewrite Kafka responses so that all connections established by
  // the Kafka clients point to Envoy.
  // This allows Kafka cluster not to configure its 'advertised.listeners' property
  // (as the necessary re-pointing will be done by this filter).
  // This collection of rules should cover all brokers in the cluster that is being proxied,
  // otherwise some nodes' addresses might leak to the downstream clients.
  oneof broker_address_rewrite_spec {
    // Broker address rewrite rules that match by broker ID.
    IdBasedBrokerRewriteSpec id_based_broker_address_rewrite_spec = 3;
  }
}

// Collection of rules matching by broker ID.
message IdBasedBrokerRewriteSpec {
  repeated IdBasedBrokerRewriteRule rules = 1;
}

// Defines a rule to rewrite broker address data.
message IdBasedBrokerRewriteRule {
  // Broker ID to match.
  uint32 id = 1 [(validate.rules).uint32 = {gte: 0}];

  // The host value to use (resembling the host part of Kafka's advertised.listeners).
  // The value should point to the Envoy (not Kafka) listener, so that all client traffic goes
  // through Envoy.
  string host = 2 [(validate.rules).string = {min_len: 1}];

  // The port value to use (resembling the port part of Kafka's advertised.listeners).
  // The value should point to the Envoy (not Kafka) listener, so that all client traffic goes
  // through Envoy.
  uint32 port = 3 [(validate.rules).uint32 = {lte: 65535}];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.network.client_ssl_auth.v3;

import "envoy/config/core/v3/address.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/migrate.proto";
import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.client_ssl_auth.v3";
option java_outer_classname = "ClientSslAuthProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/client_ssl_auth/v3;client_ssl_authv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Client TLS authentication]
// Client TLS authentication
// :ref:`configuration overview <config_network_filters_client_ssl_auth>`.
// [#extension: envoy.filters.network.client_ssl_auth]

message ClientSSLAuth {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.config.filter.network.client_ssl_auth.v2.ClientSSLAuth";

  // The :ref:`cluster manager <arch_overview_cluster_manager>` cluster that runs
  // the authentication service. The filter will connect to the service every 60s to fetch the list
  // of principals. The service must support the expected :ref:`REST API
  // <config_network_filters_client_ssl_auth_rest_api>`.
  string auth_api_cluster = 1
      [(validate.rules).string = {min_len: 1 well_known_regex: HTTP_HEADER_VALUE strict: false}];

  // The prefix to use when emitting :ref:`statistics
  // <config_network_filters_client_ssl_auth_stats>`.
  string stat_prefix = 2 [(validate.rules).string = {min_len: 1}];

  // Time in milliseconds between principal refreshes from the
  // authentication service. Default is 60000 (60s). The actual fetch time
  // will be this value plus a random jittered value between
  // 0-refresh_delay_ms milliseconds.
  google.protobuf.Duration refresh_delay = 3;

  // An optional list of IP address and subnet masks that should be white
  // listed for access by the filter. If no list is provided, there is no
  // IP allowlist.
  repeated config.core.v3.CidrRange ip_white_list = 4
      [(udpa.annotations.field_migrate).rename = "ip_allowlist"];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.rocketmq_proxy.v3;

import "contrib/envoy/extensions/filters/network/rocketmq_proxy/v3/route.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.rocketmq_proxy.v3";
option java_outer_classname = "RocketmqProxyProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/rocketmq_proxy/v3;rocketmq_proxyv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: RocketMQ Proxy]
// RocketMQ Proxy :ref:`configuration overview <config_network_filters_rocketmq_proxy>`.
// [#extension: envoy.filters.network.rocketmq_proxy]

message RocketmqProxy {
  // The human readable prefix to use when emitting statistics.
  string stat_prefix = 1 [(validate.rules).string = {min_len: 1}];

  // The route table for the connection manager is specified in this property.
  RouteConfiguration route_config = 2;

  // The largest duration transient object expected to live, more than 10s is recommended.
  google.protobuf.Duration transient_object_life_span = 3;

  // If develop_mode is enabled, this proxy plugin may work without dedicated traffic intercepting
  // facility without considering backward compatibility of exiting RocketMQ client SDK.
  bool develop_mode = 4;
}
syntax = "proto3";

package envoy.extensions.filters.network.rocketmq_proxy.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/config/route/v3/route_components.proto";
import "envoy/type/matcher/v3/string.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.rocketmq_proxy.v3";
option java_outer_classname = "RouteProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/rocketmq_proxy/v3;rocketmq_proxyv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Rocketmq Proxy Route Configuration]
// Rocketmq Proxy :ref:`configuration overview <config_network_filters_rocketmq_proxy>`.

message RouteConfiguration {
  // The name of the route configuration.
  string name = 1;

  // The list of routes that will be matched, in order, against incoming requests. The first route
  // that matches will be used.
  repeated Route routes = 2;
}

message Route {
  // Route matching parameters.
  RouteMatch match = 1 [(validate.rules).message = {required: true}];

  // Route request to some upstream cluster.
  RouteAction route = 2 [(validate.rules).message = {required: true}];
}

message RouteMatch {
  // The name of the topic.
  type.matcher.v3.StringMatcher topic = 1 [(validate.rules).message = {required: true}];

  // Specifies a set of headers that the route should match on. The router will check the request’s
  // headers against all the specified headers in the route config. A match will happen if all the
  // headers in the route are present in the request with the same values (or based on presence if
  // the value field is not in the config).
  repeated config.route.v3.HeaderMatcher headers = 2;
}

message RouteAction {
  // Indicates the upstream cluster to which the request should be routed.
  string cluster = 1 [(validate.rules).string = {min_len: 1}];

  // Optional endpoint metadata match criteria used by the subset load balancer.
  config.core.v3.Metadata metadata_match = 2;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/core/v3:pkg",
        "//envoy/config/route/v3:pkg",
        "//envoy/type/matcher/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.postgres_proxy.v3alpha;

import "google/protobuf/wrappers.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.postgres_proxy.v3alpha";
option java_outer_classname = "PostgresProxyProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/postgres_proxy/v3alpha";
option (udpa.annotations.file_status).work_in_progress = true;
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Postgres proxy]
// Postgres Proxy :ref:`configuration overview
// <config_network_filters_postgres_proxy>`.
// [#extension: envoy.filters.network.postgres_proxy]

message PostgresProxy {
  // Upstream SSL operational modes.
  enum SSLMode {
    // Do not encrypt upstream connection to the server.
    DISABLE = 0;

    // Establish upstream SSL connection to the server. If the server does not
    // accept the request for SSL connection, the session is terminated.
    REQUIRE = 1;
  }

  // The human readable prefix to use when emitting :ref:`statistics
  // <config_network_filters_postgres_proxy_stats>`.
  string stat_prefix = 1 [(validate.rules).string = {min_len: 1}];

  // Controls whether SQL statements received in Frontend Query messages
  // are parsed. Parsing is required to produce Postgres proxy filter
  // metadata. Defaults to true.
  google.protobuf.BoolValue enable_sql_parsing = 2;

  // Controls whether to terminate SSL session initiated by a client.
  // If the value is false, the Postgres proxy filter will not try to
  // terminate SSL session, but will pass all the packets to the upstream server.
  // If the value is true, the Postgres proxy filter will try to terminate SSL
  // session. In order to do that, the filter chain must use :ref:`starttls transport socket
  // <envoy_v3_api_msg_extensions.transport_sockets.starttls.v3.StartTlsConfig>`.
  // If the filter does not manage to terminate the SSL session, it will close the connection from the client.
  // Refer to official documentation for details
  // `SSL Session Encryption Message Flow <https://www.postgresql.org/docs/current/protocol-flow.html#id-1.10.5.7.11>`_.
  bool terminate_ssl = 3;

  // Controls whether to establish upstream SSL connection to the server.
  // Envoy will try to establish upstream SSL connection to the server only when
  // Postgres filter is able to read Postgres payload in clear-text. It happens when
  // a client established a clear-text connection to Envoy or when a client established
  // SSL connection to Envoy and Postgres filter is configured to terminate SSL.
  // In order for upstream encryption to work, the corresponding cluster must be configured to use
  // :ref:`starttls transport socket <envoy_v3_api_msg_extensions.transport_sockets.starttls.v3.UpstreamStartTlsConfig>`.
  // Defaults to ``SSL_DISABLE``.
  SSLMode upstream_ssl = 4;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.network.generic_proxy.codecs.kafka.v3;

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.generic_proxy.codecs.kafka.v3";
option java_outer_classname = "KafkaProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/generic_proxy/codecs/kafka/v3;kafkav3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Kafka codec configuration for Generic Proxy]
// [#extension: envoy.generic_proxy.codecs.kafka]

// Configuration for Kafka codec. This codec gives the generic proxy the ability to proxy
// Kafka traffic. But note any route configuration for Kafka traffic is not supported yet.
// The generic proxy can only used to generate logs or metrics for Kafka traffic but cannot
// do matching or routing.
//
// .. note::
//   The codec can currently only be used in the sidecar mode. And to ensure the codec works
//   properly, please make sure the following conditions are met:
//
//   1. The generic proxy must be configured with a wildcard route that matches all traffic.
//   2. The target cluster must be configured as a original destination cluster.
//   3. The :ref:`bind_upstream_connection
//      <envoy_v3_api_field_extensions.filters.network.generic_proxy.router.v3.Router.bind_upstream_connection>`
//      of generic proxy router must be set to true to ensure same upstream connection is used
//      for all traffic from same downstream connection.
message KafkaCodecConfig {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.generic_proxy.codecs.dubbo.v3;

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.generic_proxy.codecs.dubbo.v3";
option java_outer_classname = "DubboProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/generic_proxy/codecs/dubbo/v3;dubbov3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Dubbo codec configuration for Generic Proxy]
// [#extension: envoy.generic_proxy.codecs.dubbo]

message DubboCodecConfig {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.generic_proxy.matcher.v3;

import "envoy/type/matcher/v3/string.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.generic_proxy.matcher.v3";
option java_outer_classname = "MatcherProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/generic_proxy/matcher/v3;matcherv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Generic Proxy Route Matcher Configuration]

// Used to match request service of the downstream request. Only applicable if a service provided
// by the application protocol.
// This is deprecated and should be replaced by HostMatchInput. This is kept for backward compatibility.
message ServiceMatchInput {
}

// Used to match request host of the generic downstream request. Only applicable if a host provided
// by the application protocol.
// This is same with the ServiceMatchInput and this should be preferred over ServiceMatchInput.
message HostMatchInput {
}

// Used to match request path of the generic downstream request. Only applicable if a path provided
// by the application protocol.
message PathMatchInput {
}

// Used to match request method of the generic downstream request. Only applicable if a method provided
// by the application protocol.
message MethodMatchInput {
}

// Used to match an arbitrary property of the generic downstream request.
// These properties are populated by the codecs of application protocols.
message PropertyMatchInput {
  // The property name to match on.
  string property_name = 1 [(validate.rules).string = {min_len: 1}];
}

// Used to match an whole generic downstream request.
message RequestMatchInput {
}

// Used to match an arbitrary key-value pair for headers, trailers or properties.
message KeyValueMatchEntry {
  // The key name to match on.
  string name = 1 [(validate.rules).string = {min_len: 1}];

  // The key value pattern.
  type.matcher.v3.StringMatcher string_match = 2 [(validate.rules).message = {required: true}];
}

// Custom matcher to match on the generic downstream request. This is used to match
// multiple fields of the downstream request and avoid complex combinations of
// HostMatchInput, PathMatchInput, MethodMatchInput and PropertyMatchInput.
message RequestMatcher {
  // Optional host pattern to match on. If not specified, any host will match.
  type.matcher.v3.StringMatcher host = 1;

  // Optional path pattern to match on. If not specified, any path will match.
  type.matcher.v3.StringMatcher path = 2;

  // Optional method pattern to match on. If not specified, any method will match.
  type.matcher.v3.StringMatcher method = 3;

  // Optional arbitrary properties to match on. If not specified, any properties
  // will match. The key is the property name and the value is the property value
  // to match on.
  repeated KeyValueMatchEntry properties = 4;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/type/matcher/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.generic_proxy.action.v3;

import "envoy/config/core/v3/base.proto";
import "envoy/config/route/v3/route_components.proto";

import "google/protobuf/any.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.generic_proxy.action.v3";
option java_outer_classname = "ActionProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/generic_proxy/action/v3;actionv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Generic Proxy Route Action Configuration]

// Configuration for the route match action.
// [#next-free-field: 6]
message RouteAction {
  // The name of the route action. This should be unique across all route actions.
  string name = 5;

  oneof cluster_specifier {
    option (validate.required) = true;

    // Indicates the upstream cluster to which the request should be routed.
    string cluster = 1;

    // [#not-implemented-hide:]
    // Multiple upstream clusters can be specified for a given route. The request is routed to one
    // of the upstream clusters based on weights assigned to each cluster.
    // Currently ClusterWeight only supports the name and weight fields.
    config.route.v3.WeightedCluster weighted_clusters = 2;
  }

  // Route metadata.
  config.core.v3.Metadata metadata = 3;

  // Route level config for L7 generic filters. The key should be the related :ref:`extension name
  // <envoy_v3_api_field_config.core.v3.TypedExtensionConfig.name>` in the :ref:`generic filters
  // <envoy_v3_api_field_extensions.filters.network.generic_proxy.v3.GenericProxy.filters>`.
  map<string, google.protobuf.Any> per_filter_config = 4;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/core/v3:pkg",
        "//envoy/config/route/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.generic_proxy.v3;

import "xds/annotations/v3/status.proto";
import "xds/type/matcher/v3/matcher.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.generic_proxy.v3";
option java_outer_classname = "RouteProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/generic_proxy/v3;generic_proxyv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Generic Proxy Route Configuration]

message VirtualHost {
  // The name of the virtual host.
  string name = 1 [(validate.rules).string = {min_len: 1}];

  // A list of hosts that will be matched to this virtual host. Wildcard hosts are supported in
  // the suffix or prefix form.
  //
  // Host search order:
  //  1. Exact names: ``www.foo.com``.
  //  2. Suffix wildcards: ``*.foo.com`` or ``*-bar.foo.com``.
  //  3. Prefix wildcards: ``foo.*`` or ``foo-*``.
  //  4. Special wildcard ``*`` matching any host and will be the default virtual host.
  //
  // .. note::
  //
  //   The wildcard will not match the empty string.
  //   e.g. ``*-bar.foo.com`` will match ``baz-bar.foo.com`` but not ``-bar.foo.com``.
  //   The longest wildcards match first.
  //   Only a single virtual host in the entire route configuration can match on ``*``. A domain
  //   must be unique across all virtual hosts or the config will fail to load.
  repeated string hosts = 2 [(validate.rules).repeated = {min_items: 1}];

  // The match tree to use when resolving route actions for incoming requests.
  xds.type.matcher.v3.Matcher routes = 3 [(validate.rules).message = {required: true}];
}

// The generic proxy makes use of the xDS matching API for routing configurations.
//
// In the below example, we combine a top level tree matcher with a linear matcher to match
// the incoming requests, and send the matching requests to v1 of the upstream service.
//
// .. code-block:: yaml
//
//   name: example
//   routes:
//     matcher_tree:
//       input:
//         name: request-service
//         typed_config:
//           "@type": type.googleapis.com/envoy.extensions.filters.network.generic_proxy.matcher.v3.ServiceMatchInput
//       exact_match_map:
//         map:
//           service_name_0:
//             matcher:
//               matcher_list:
//                 matchers:
//                 - predicate:
//                     and_matcher:
//                       predicate:
//                       - single_predicate:
//                           input:
//                             name: request-properties
//                             typed_config:
//                               "@type": type.googleapis.com/envoy.extensions.filters.network.generic_proxy.matcher.v3.PropertyMatchInput
//                               property_name: version
//                           value_match:
//                             exact: v1
//                       - single_predicate:
//                           input:
//                             name: request-properties
//                             typed_config:
//                               "@type": type.googleapis.com/envoy.extensions.filters.network.generic_proxy.matcher.v3.PropertyMatchInput
//                               property_name: user
//                           value_match:
//                             exact: john
//                   on_match:
//                     action:
//                       name: route
//                       typed_config:
//                         "@type": type.googleapis.com/envoy.extensions.filters.network.generic_proxy.action.v3.routeAction
//                         cluster: cluster_0
message RouteConfiguration {
  // The name of the route configuration. For example, it might match route_config_name in
  // envoy.extensions.filters.network.generic_proxy.v3.Rds.
  string name = 1 [(validate.rules).string = {min_len: 1}];

  // The match tree to use when resolving route actions for incoming requests.
  // If no any virtual host is configured in the ``virtual_hosts`` field or no special wildcard
  // virtual host is configured, the ``routes`` field will be used as the default route table.
  // If both the wildcard virtual host and ``routes`` are configured, the configuration will fail
  // to load.
  xds.type.matcher.v3.Matcher routes = 2;

  // An array of virtual hosts that make up the route table.
  repeated VirtualHost virtual_hosts = 3;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/accesslog/v3:pkg",
        "//envoy/config/core/v3:pkg",
        "//envoy/extensions/filters/network/http_connection_manager/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
        "@com_github_cncf_xds//xds/type/matcher/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.generic_proxy.v3;

import "contrib/envoy/extensions/filters/network/generic_proxy/v3/route.proto";
import "envoy/config/accesslog/v3/accesslog.proto";
import "envoy/config/core/v3/config_source.proto";
import "envoy/config/core/v3/extension.proto";
import "envoy/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.generic_proxy.v3";
option java_outer_classname = "GenericProxyProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/generic_proxy/v3;generic_proxyv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Generic Proxy]
// Generic proxy.
// [#extension: envoy.filters.network.generic_proxy]

// [#next-free-field: 8]
message GenericProxy {
  // The human readable prefix to use when emitting statistics.
  string stat_prefix = 1 [(validate.rules).string = {min_len: 1}];

  // The codec which encodes and decodes the application protocol.
  // [#extension-category: envoy.generic_proxy.codecs]
  config.core.v3.TypedExtensionConfig codec_config = 2
      [(validate.rules).message = {required: true}];

  oneof route_specifier {
    option (validate.required) = true;

    // The generic proxies route table will be dynamically loaded via the meta RDS API.
    GenericRds generic_rds = 3;

    // The route table for the generic proxy is static and is specified in this property.
    RouteConfiguration route_config = 4;
  }

  // A list of individual Layer-7 filters that make up the filter chain for requests made to the
  // proxy. Order matters as the filters are processed sequentially as request events
  // happen.
  // [#extension-category: envoy.generic_proxy.filters]
  repeated config.core.v3.TypedExtensionConfig filters = 5;

  // Tracing configuration for the generic proxy.
  http_connection_manager.v3.HttpConnectionManager.Tracing tracing = 6;

  // Configuration for :ref:`access logs <arch_overview_access_logs>` emitted by generic proxy.
  repeated config.accesslog.v3.AccessLog access_log = 7;
}

message GenericRds {
  // Configuration source specifier for RDS.
  config.core.v3.ConfigSource config_source = 1 [(validate.rules).message = {required: true}];

  // The name of the route configuration. This name will be passed to the RDS API. This allows an
  // Envoy configuration with multiple generic proxies to use different route configurations.
  string route_config_name = 2 [(validate.rules).string = {min_len: 1}];
}
syntax = "proto3";

package envoy.extensions.filters.network.generic_proxy.router.v3;

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.generic_proxy.router.v3";
option java_outer_classname = "RouterProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/generic_proxy/router/v3;routerv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Router for generic proxy]
// [#extension: envoy.filters.generic.router]

message Router {
  // Set to true if the upstream connection should be bound to the downstream connection, false
  // otherwise.
  //
  // By default, one random upstream connection will be selected from the upstream connection pool
  // and used for every request. And after the request is finished, the upstream connection will be
  // released back to the upstream connection pool.
  //
  // If this option is true, the upstream connection will be bound to the downstream connection and
  // have same lifetime as the downstream connection. The same upstream connection will be used for
  // all requests from the same downstream connection.
  //
  // And if this options is true, one of the following requirements must be met:
  //
  // 1. The request must be handled one by one. That is, the next request can not be sent to the
  //    upstream until the previous request is finished.
  // 2. Unique request id must be provided for each request and response. The request id must be
  //    unique for each request and response pair in same connection pair. And the request id must
  //    be the same for the corresponding request and response.
  //
  // This could be useful for some protocols that require the same upstream connection to be used
  // for all requests from the same downstream connection. For example, the protocol using stateful
  // connection.
  bool bind_upstream_connection = 1;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.sip_proxy.tra.v3alpha;

import "envoy/config/core/v3/config_source.proto";
import "envoy/config/core/v3/grpc_service.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.sip_proxy.tra.v3alpha";
option java_outer_classname = "TraProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/sip_proxy/tra/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: TRA]

service TraService {
  rpc Create(TraServiceRequest) returns (TraServiceResponse) {
  }

  rpc Update(TraServiceRequest) returns (TraServiceResponse) {
  }

  rpc Retrieve(TraServiceRequest) returns (TraServiceResponse) {
  }

  rpc Delete(TraServiceRequest) returns (TraServiceResponse) {
  }

  rpc Subscribe(TraServiceRequest) returns (stream TraServiceResponse) {
  }
}

message TraServiceConfig {
  // Specifies the gRPC service that hosts the rate limit service. The client
  // will connect to this cluster when it needs to make rate limit service
  // requests.
  config.core.v3.GrpcService grpc_service = 1 [(validate.rules).message = {required: true}];

  // API version for rate limit transport protocol. This describes the rate limit gRPC endpoint and
  // version of messages used on the wire.
  config.core.v3.ApiVersion transport_api_version = 2
      [(validate.rules).enum = {defined_only: true}];

  google.protobuf.Duration timeout = 3;
}

// [#next-free-field: 7]
message TraServiceRequest {
  string type = 1;

  oneof request {
    CreateRequest create_request = 2;

    UpdateRequest update_request = 3;

    RetrieveRequest retrieve_request = 4;

    DeleteRequest delete_request = 5;

    SubscribeRequest subscribe_request = 6;
  }
}

// [#next-free-field: 9]
message TraServiceResponse {
  string type = 1;

  int32 ret = 2;

  string reason = 3;

  oneof response {
    CreateResponse create_response = 4;

    UpdateResponse update_response = 5;

    RetrieveResponse retrieve_response = 6;

    DeleteResponse delete_response = 7;

    SubscribeResponse subscribe_response = 8;
  }
}

message CreateRequest {
  map<string, string> data = 1;

  map<string, string> context = 2;
}

message CreateResponse {
}

message UpdateRequest {
  map<string, string> data = 1;

  map<string, string> context = 2;
}

message UpdateResponse {
}

message RetrieveRequest {
  string key = 1;

  map<string, string> context = 2;
}

message RetrieveResponse {
  map<string, string> data = 1;
}

message DeleteRequest {
  string key = 1;

  map<string, string> context = 2;
}

message DeleteResponse {
}

message SubscribeRequest {
}

message SubscribeResponse {
  map<string, string> data = 1;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    has_services = True,
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.sip_proxy.v3alpha;

import "contrib/envoy/extensions/filters/network/sip_proxy/tra/v3alpha/tra.proto";
import "contrib/envoy/extensions/filters/network/sip_proxy/v3alpha/route.proto";

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.sip_proxy.v3alpha";
option java_outer_classname = "SipProxyProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/sip_proxy/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Sip Proxy]
// [#extension: envoy.filters.network.sip_proxy]

message SipProxy {
  message SipSettings {
    // transaction timeout timer [Timer B] unit is milliseconds, default value 64*T1.
    //
    // Session Initiation Protocol (SIP) timer summary
    //
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer   | Default value           | Section  | Meaning                                                                      |
    // +=========+=========================+==========+==============================================================================+
    // | T1      | 500 ms                  | 17.1.1.1 | Round-trip time (RTT) estimate                                               |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | T2      | 4 sec                   | 17.1.2.2 | Maximum re-transmission interval for non-INVITE requests and INVITE responses|
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | T4      | 5 sec                   | 17.1.2.2 | Maximum duration that a message can remain in the network                    |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer A | initially T1            | 17.1.1.2 | INVITE request re-transmission interval, for UDP only                        |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer B | 64*T1                   | 17.1.1.2 | INVITE transaction timeout timer                                             |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer D | > 32 sec. for UDP       | 17.1.1.2 | Wait time for response re-transmissions                                      |
    // |         | 0 sec. for TCP and SCTP |          |                                                                              |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer E | initially T1            | 17.1.2.2 | Non-INVITE request re-transmission interval, UDP only                        |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer F | 64*T1                   | 17.1.2.2 | Non-INVITE transaction timeout timer                                         |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer G | initially T1            | 17.2.1   | INVITE response re-transmission interval                                     |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer H | 64*T1                   | 17.2.1   | Wait time for ACK receipt                                                    |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer I | T4 for UDP              | 17.2.1   | Wait time for ACK re-transmissions                                           |
    // |         | 0 sec. for TCP and SCTP |          |                                                                              |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer J | 64*T1 for UDP           | 17.2.2   | Wait time for re-transmissions of non-INVITE requests                        |
    // |         | 0 sec. for TCP and SCTP |          |                                                                              |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    // | Timer K | T4 for UDP              | 17.1.2.2 | Wait time for response re-transmissions                                      |
    // |         | 0 sec. for TCP and SCTP |          |                                                                              |
    // +---------+-------------------------+----------+------------------------------------------------------------------------------+
    google.protobuf.Duration transaction_timeout = 1;

    // The service to match for ep insert
    repeated LocalService local_services = 2;

    tra.v3alpha.TraServiceConfig tra_service_config = 3;

    // Whether via header is operated, including add via for request and pop via for response
    // False: sip service proxy
    // True:  sip load balancer
    bool operate_via = 4;
  }

  // The human readable prefix to use when emitting statistics.
  string stat_prefix = 1 [(validate.rules).string = {min_len: 1}];

  // The route table for the connection manager is static and is specified in this property.
  RouteConfiguration route_config = 2;

  // A list of individual Sip filters that make up the filter chain for requests made to the
  // Sip proxy. Order matters as the filters are processed sequentially. For backwards
  // compatibility, if no sip_filters are specified, a default Sip router filter
  // (``envoy.filters.sip.router``) is used.
  // [#extension-category: envoy.sip_proxy.filters]
  repeated SipFilter sip_filters = 3;

  SipSettings settings = 4;
}

// SipFilter configures a Sip filter.
message SipFilter {
  // The name of the filter to instantiate. The name must match a supported
  // filter. The built-in filters are:
  //
  string name = 1 [(validate.rules).string = {min_len: 1}];

  // Filter specific configuration which depends on the filter being instantiated. See the supported
  // filters for further documentation.
  oneof config_type {
    google.protobuf.Any typed_config = 3;
  }
}

// SipProtocolOptions specifies Sip upstream protocol options. This object is used in
// :ref:`typed_extension_protocol_options<envoy_v3_api_field_config.cluster.v3.Cluster.typed_extension_protocol_options>`,
// keyed by the name ``envoy.filters.network.sip_proxy``.
message SipProtocolOptions {
  // All sip messages in one dialog should go to the same endpoint.
  bool session_affinity = 1;

  // The Register with Authorization header should go to the same endpoint which send out the 401 Unauthorized.
  bool registration_affinity = 2;

  // Customized affinity
  CustomizedAffinity customized_affinity = 3;
}

// For affinity
message CustomizedAffinity {
  // Affinity rules to conclude the upstream endpoint
  repeated CustomizedAffinityEntry entries = 1;

  // Configures whether load balance should be stopped or continued after affinity handling.
  bool stop_load_balance = 2;
}

// [#next-free-field: 6]
message CustomizedAffinityEntry {
  // The header name to match, e.g. "From", if not specified, default is "Route"
  string header = 1;

  // Affinity key for TRA query/subscribe, e.g. "lskpmc", if key_name is "text" means use the header content as key.
  string key_name = 2;

  // Whether subscribe to TRA is required
  bool subscribe = 3;

  // Whether query to TRA is required
  bool query = 4;

  // Local cache
  Cache cache = 5;
}

message Cache {
  // Affinity local cache item max number
  int32 max_cache_item = 1;

  // Whether query result can be added to local cache
  bool add_query_to_cache = 2;
}

// Local Service
message LocalService {
  // The domain need to matched
  string domain = 1;

  // The parameter to get domain
  string parameter = 2;
}
syntax = "proto3";

package envoy.extensions.filters.network.sip_proxy.v3alpha;

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.sip_proxy.v3alpha";
option java_outer_classname = "RouteProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/sip_proxy/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Sip Proxy Route Configuration]

message RouteConfiguration {
  // The name of the route configuration. Reserved for future use in asynchronous route discovery.
  string name = 1;

  // The list of routes that will be matched, in order, against incoming requests. The first route
  // that matches will be used.
  repeated Route routes = 2;
}

message Route {
  // Route matching parameters.
  RouteMatch match = 1 [(validate.rules).message = {required: true}];

  // Route request to some upstream cluster.
  RouteAction route = 2 [(validate.rules).message = {required: true}];
}

message RouteMatch {
  oneof match_specifier {
    option (validate.required) = true;

    // The domain from Request URI or Route Header.
    string domain = 1;
  }

  // The header to get match parameter, default is "Route".
  string header = 2;

  // The parameter to get domain, default is "host".
  string parameter = 3;
}

message RouteAction {
  oneof cluster_specifier {
    option (validate.required) = true;

    // Indicates a single upstream cluster to which the request should be routed
    // to.
    string cluster = 1 [(validate.rules).string = {min_len: 1}];
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//contrib/envoy/extensions/filters/network/sip_proxy/tra/v3alpha:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.network.sip_proxy.router.v3alpha;

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.sip_proxy.router.v3alpha";
option java_outer_classname = "RouterProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/sip_proxy/router/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Router]
// [#extension: envoy.filters.sip.router]

message Router {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.network.mysql_proxy.v3;

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.mysql_proxy.v3";
option java_outer_classname = "MysqlProxyProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/mysql_proxy/v3;mysql_proxyv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: MySQL proxy]
// MySQL Proxy :ref:`configuration overview <config_network_filters_mysql_proxy>`.
// [#extension: envoy.filters.network.mysql_proxy]

message MySQLProxy {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.config.filter.network.mysql_proxy.v1alpha1.MySQLProxy";

  // The human readable prefix to use when emitting :ref:`statistics
  // <config_network_filters_mysql_proxy_stats>`.
  string stat_prefix = 1 [(validate.rules).string = {min_len: 1}];

  // [#not-implemented-hide:] The optional path to use for writing MySQL access logs.
  // If the access log field is empty, access logs will not be written.
  string access_log = 2;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.network.kafka_mesh.v3alpha;

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.network.kafka_mesh.v3alpha";
option java_outer_classname = "KafkaMeshProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Kafka Mesh]
// Kafka Mesh :ref:`configuration overview <config_network_filters_kafka_mesh>`.
// [#extension: envoy.filters.network.kafka_mesh]

// [#next-free-field: 6]
message KafkaMesh {
  enum ConsumerProxyMode {
    // Records received are going to be distributed amongst downstream consumer connections.
    // In this mode Envoy uses librdkafka consumers pointing at upstream Kafka clusters, what means that these
    // consumers' position is meaningful and affects what records are received from upstream.
    // Users might want to take a look into these consumers' custom configuration to manage their auto-committing
    // capabilities, as it will impact Envoy's behaviour in case of restarts.
    StatefulConsumerProxy = 0;
  }

  // Envoy's host that's advertised to clients.
  // Has the same meaning as corresponding Kafka broker properties.
  // Usually equal to filter chain's listener config, but needs to be reachable by clients
  // (so 0.0.0.0 will not work).
  string advertised_host = 1 [(validate.rules).string = {min_len: 1}];

  // Envoy's port that's advertised to clients.
  int32 advertised_port = 2 [(validate.rules).int32 = {gt: 0}];

  // Upstream clusters this filter will connect to.
  repeated KafkaClusterDefinition upstream_clusters = 3;

  // Rules that will decide which cluster gets which request.
  repeated ForwardingRule forwarding_rules = 4;

  // How the consumer proxying should behave - this relates mostly to Fetch request handling.
  ConsumerProxyMode consumer_proxy_mode = 5;
}

// [#next-free-field: 6]
message KafkaClusterDefinition {
  // Cluster name.
  string cluster_name = 1 [(validate.rules).string = {min_len: 1}];

  // Kafka cluster address.
  string bootstrap_servers = 2 [(validate.rules).string = {min_len: 1}];

  // Default number of partitions present in this cluster.
  // This is especially important for clients that do not specify partition in their payloads and depend on this value for hashing.
  // The same number of partitions is going to be used by upstream-pointing Kafka consumers for consumer proxying scenarios.
  int32 partition_count = 3 [(validate.rules).int32 = {gt: 0}];

  // Custom configuration passed to Kafka producer.
  map<string, string> producer_config = 4;

  // Custom configuration passed to Kafka consumer.
  map<string, string> consumer_config = 5;
}

message ForwardingRule {
  // Cluster name.
  string target_cluster = 1;

  oneof trigger {
    // Intended place for future types of forwarding rules.
    string topic_prefix = 2;
  }
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.http.squash.v3;

import "google/protobuf/duration.proto";
import "google/protobuf/struct.proto";

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.http.squash.v3";
option java_outer_classname = "SquashProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/http/squash/v3;squashv3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Squash]
// Squash :ref:`configuration overview <config_http_filters_squash>`.
// [#extension: envoy.filters.http.squash]

// [#next-free-field: 6]
message Squash {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.config.filter.http.squash.v2.Squash";

  // The name of the cluster that hosts the Squash server.
  string cluster = 1 [(validate.rules).string = {min_len: 1}];

  // When the filter requests the Squash server to create a DebugAttachment, it will use this
  // structure as template for the body of the request. It can contain reference to environment
  // variables in the form of '{{ ENV_VAR_NAME }}'. These can be used to provide the Squash server
  // with more information to find the process to attach the debugger to. For example, in a
  // Istio/k8s environment, this will contain information on the pod:
  //
  // .. code-block:: json
  //
  //  {
  //    "spec": {
  //      "attachment": {
  //        "pod": "{{ POD_NAME }}",
  //        "namespace": "{{ POD_NAMESPACE }}"
  //      },
  //      "match_request": true
  //    }
  //  }
  //
  // (where POD_NAME, POD_NAMESPACE are configured in the pod via the Downward API)
  google.protobuf.Struct attachment_template = 2;

  // The timeout for individual requests sent to the Squash cluster. Defaults to 1 second.
  google.protobuf.Duration request_timeout = 3;

  // The total timeout Squash will delay a request and wait for it to be attached. Defaults to 60
  // seconds.
  google.protobuf.Duration attachment_timeout = 4;

  // Amount of time to poll for the status of the attachment object in the Squash server
  // (to check if has been attached). Defaults to 1 second.
  google.protobuf.Duration attachment_poll_period = 5;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.http.golang.v3alpha;

import "google/protobuf/any.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.http.golang.v3alpha";
option java_outer_classname = "GolangProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/http/golang/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Golang HTTP filter]
//
// For an overview of the Golang HTTP filter please see the :ref:`configuration reference documentation <config_http_filters_golang>`.
// [#extension: envoy.filters.http.golang]

// [#next-free-field: 6]
message Config {
  // The meanings are as follows:
  //
  // :``MERGE_VIRTUALHOST_ROUTER_FILTER``: Pass all configuration into Go plugin.
  // :``MERGE_VIRTUALHOST_ROUTER``: Pass merged Virtual host and Router configuration into Go plugin.
  // :``OVERRIDE``: Pass merged Virtual host, Router, and plugin configuration into Go plugin.
  //
  // [#not-implemented-hide:]
  enum MergePolicy {
    MERGE_VIRTUALHOST_ROUTER_FILTER = 0;
    MERGE_VIRTUALHOST_ROUTER = 1;
    OVERRIDE = 3;
  }

  // Globally unique ID for a dynamic library file.
  string library_id = 1 [(validate.rules).string = {min_len: 1}];

  // Path to a dynamic library implementing the
  // :repo:`StreamFilter API <contrib/golang/common/go/api.StreamFilter>`
  // interface.
  // [#comment:TODO(wangfakang): Support for downloading libraries from remote repositories.]
  string library_path = 2 [(validate.rules).string = {min_len: 1}];

  // Globally unique name of the Go plugin.
  //
  // This name **must** be consistent with the name registered in ``http::RegisterHttpFilterConfigFactory``,
  // and can be used to associate :ref:`route and virtualHost plugin configuration
  // <envoy_v3_api_field_extensions.filters.http.golang.v3alpha.ConfigsPerRoute.plugins_config>`.
  //
  string plugin_name = 3 [(validate.rules).string = {min_len: 1}];

  // Configuration for the Go plugin.
  //
  // .. note::
  //     This configuration is only parsed in the go plugin, and is therefore not validated
  //     by Envoy.
  //
  //     See the :repo:`StreamFilter API <contrib/golang/common/go/api/filter.go>`
  //     for more information about how the plugin's configuration data can be accessed.
  //
  google.protobuf.Any plugin_config = 4;

  // Merge policy for plugin configuration.
  //
  // The Go plugin configuration supports three dimensions:
  //
  // * Virtual host’s :ref:`typed_per_filter_config <envoy_v3_api_field_config.route.v3.VirtualHost.typed_per_filter_config>`
  // * Route’s :ref:`typed_per_filter_config <envoy_v3_api_field_config.route.v3.Route.typed_per_filter_config>`
  // * The filter's :ref:`plugin_config <envoy_v3_api_field_extensions.filters.http.golang.v3alpha.Config.plugin_config>`
  //
  // [#not-implemented-hide:]
  MergePolicy merge_policy = 5 [(validate.rules).enum = {defined_only: true}];
}

message RouterPlugin {
  oneof override {
    option (validate.required) = true;

    // [#not-implemented-hide:]
    // Disable the filter for this particular vhost or route.
    // If disabled is specified in multiple per-filter-configs, the most specific one will be used.
    bool disabled = 1 [(validate.rules).bool = {const: true}];

    // The config field is used for setting per-route and per-virtualhost plugin config.
    google.protobuf.Any config = 2;
  }
}

message ConfigsPerRoute {
  // Configuration of the Go plugin at the per-router or per-virtualhost level,
  // keyed on the :ref:`plugin_name <envoy_v3_api_field_extensions.filters.http.golang.v3alpha.Config.plugin_name>`
  // of the Go plugin.
  //
  map<string, RouterPlugin> plugins_config = 1;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.http.dynamo.v3;

import "udpa/annotations/status.proto";
import "udpa/annotations/versioning.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.http.dynamo.v3";
option java_outer_classname = "DynamoProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/http/dynamo/v3;dynamov3";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Dynamo]
// Dynamo :ref:`configuration overview <config_http_filters_dynamo>`.
// [#extension: envoy.filters.http.dynamo]

// Dynamo filter config.
message Dynamo {
  option (udpa.annotations.versioning).previous_message_type =
      "envoy.config.filter.http.dynamo.v2.Dynamo";
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.http.language.v3alpha;

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.http.language.v3alpha";
option java_outer_classname = "LanguageProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/http/language/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Language]
// Language :ref:`configuration overview <config_http_filters_language>`.
// [#extension: envoy.filters.http.language]

// Language detection filter config.
message Language {
  // The default language to be used as a fallback.
  // The value will be included in the list of the supported languages.
  //
  // See https://unicode-org.github.io/icu/userguide/locale/
  string default_language = 1 [(validate.rules).string = {min_len: 2}];

  // The set of supported languages. There is no order priority.
  // The order will be determined by the Accept-Language header priority list
  // of the client.
  //
  // See https://unicode-org.github.io/icu/userguide/locale/
  repeated string supported_languages = 2 [(validate.rules).repeated = {
    min_items: 1
    unique: true
    items {string {min_len: 2}}
  }];

  // If the x-language header is altered, clear the route cache for the current request.
  // This should be set if the route configuration may depend on the x-language header.
  // Otherwise it should be unset to avoid the performance cost of route recalculation.
  bool clear_route_cache = 3;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.filters.http.sxg.v3alpha;

import "envoy/extensions/transport_sockets/tls/v3/secret.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.http.sxg.v3alpha";
option java_outer_classname = "SxgProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/http/sxg/v3alpha";
option (udpa.annotations.file_status).work_in_progress = true;
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Signed HTTP Exchange Filter]
// SXG :ref:`configuration overview <config_http_filters_sxg>`.
// [#extension: envoy.filters.http.sxg]

// [#next-free-field: 10]
message SXG {
  // The SDS configuration for the public key data for the SSL certificate that will be used to sign the
  // SXG response.
  transport_sockets.tls.v3.SdsSecretConfig certificate = 1;

  // The SDS configuration for the private key data for the SSL certificate that will be used to sign the
  // SXG response.
  transport_sockets.tls.v3.SdsSecretConfig private_key = 2;

  // The duration for which the generated SXG package will be valid. Default is 604800s (7 days in seconds).
  // Note that in order to account for clock skew, the timestamp will be backdated by a day. So, if duration
  // is set to 7 days, that will be 7 days from 24 hours ago (6 days from now). Also note that while 6/7 days
  // is appropriate for most content, if the downstream service is serving Javascript, or HTML with inline
  // Javascript, 1 day (so, with backdated expiry, 2 days, or 172800 seconds) is more appropriate.
  google.protobuf.Duration duration = 3;

  // The SXG response payload is Merkle Integrity Content Encoding (MICE) encoded (specification is [here](https://datatracker.ietf.org/doc/html/draft-thomson-http-mice-03))
  // This value indicates the record size in the encoded payload. The default value is 4096.
  uint64 mi_record_size = 4;

  // The URI of certificate CBOR file published. Since it is required that the certificate CBOR file
  // be served from the same domain as the SXG document, this should be a relative URI.
  string cbor_url = 5 [(validate.rules).string = {min_len: 1 prefix: "/"}];

  // URL to retrieve validity data for signature, a CBOR map. See specification [here](https://tools.ietf.org/html/draft-yasskin-httpbis-origin-signed-exchanges-impl-00#section-3.6)
  string validity_url = 6 [(validate.rules).string = {min_len: 1 prefix: "/"}];

  // Header that will be set if it is determined that the client can accept SXG (typically ``accept: application/signed-exchange;v=b3``)
  // If not set, filter will default to: ``x-client-can-accept-sxg``
  string client_can_accept_sxg_header = 7 [
    (validate.rules).string = {well_known_regex: HTTP_HEADER_NAME strict: false ignore_empty: true}
  ];

  // Header set by downstream service to signal that the response should be transformed to SXG If not set,
  // filter will default to: ``x-should-encode-sxg``
  string should_encode_sxg_header = 8 [
    (validate.rules).string = {well_known_regex: HTTP_HEADER_NAME strict: false ignore_empty: true}
  ];

  // Headers that will be stripped from the SXG document, by listing a prefix (i.e. ``x-custom-`` will cause
  // all headers prefixed by ``x-custom-`` to be omitted from the SXG document)
  repeated string header_prefix_filters = 9 [
    (validate.rules).repeated = {items {string {well_known_regex: HTTP_HEADER_NAME strict: false}}}
  ];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/extensions/transport_sockets/tls/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.filters.http.checksum.v3alpha;

import "envoy/type/matcher/v3/string.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.filters.http.checksum.v3alpha";
option java_outer_classname = "ChecksumProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/filters/http/checksum/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Checksum HTTP filter]
//
// Filter to reject responses that don't match a specified checksum.
// To avoid holding the entire response in memory, the rejection occurs at the end of the stream.
// [#extension: envoy.filters.http.checksum]

message ChecksumConfig {
  message Checksum {
    oneof matcher {
      // A matcher for a path that is expected to have a specific checksum, as specified
      // in the ``sha256`` field.
      type.matcher.v3.StringMatcher path_matcher = 1 [(validate.rules).message = {required: true}];
    }

    // A hex-encoded sha256 string required to match the sha256sum of the response body
    // of the path specified in the ``path_matcher`` field.
    string sha256 = 2 [(validate.rules).string = {pattern: "^[a-fA-F0-9]{64}"}];
  }

  // A set of matcher and checksum pairs for which, if a path matching ``path_matcher``
  // is requested and the checksum of the response body does not match the ``sha256``, the
  // response will be replaced with a 403 Forbidden status.
  //
  // If multiple matchers match the same path, the first to match takes precedence.
  repeated Checksum checksums = 1;

  // If a request doesn't match any of the specified checksum paths and reject_unmatched is
  // true, the request is rejected immediately with 403 Forbidden.
  bool reject_unmatched = 2;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/type/matcher/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.vcl.v3alpha;

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.vcl.v3alpha";
option java_outer_classname = "VclSocketInterfaceProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/vcl/v3alpha";
option (udpa.annotations.file_status).work_in_progress = true;
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: VCL socket interface configuration]
// VCL :ref:`configuration overview <config_sock_interface_vcl>`.
// [#extension: envoy.bootstrap.vcl]

// Configuration for vcl socket interface that relies on ``vpp`` ``comms`` library (VCL)
message VclSocketInterface {
}
syntax = "proto3";

package envoy.extensions.config.v3alpha;

import "envoy/config/common/key_value/v3/config.proto";

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.config.v3alpha";
option java_outer_classname = "KvStoreXdsDelegateConfigProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/config/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#extension: envoy.xds_delegates.kv_store]
//
// Configuration for a KeyValueStore-based XdsResourcesDelegate implementation. This implementation
// updates the underlying KV store with xDS resources received from the configured management
// servers, enabling configuration to be persisted locally and used on startup in case connectivity
// with the xDS management servers could not be established.
//
// The KV Store based delegate's handling of wildcard resources (empty resource list or "*") is
// designed for use with O(100) resources or fewer, so it's not currently advised to use this
// feature for large configurations with heavy use of wildcard resources.
message KeyValueStoreXdsDelegateConfig {
  // Configuration for the KeyValueStore that holds the xDS resources.
  // [#allow-fully-qualified-name:]
  .envoy.config.common.key_value.v3.KeyValueStoreConfig key_value_store_config = 1;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/common/key_value/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.private_key_providers.qat.v3alpha;

import "envoy/config/core/v3/base.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/sensitive.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.private_key_providers.qat.v3alpha";
option java_outer_classname = "QatProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/private_key_providers/qat/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: `QAT` private key provider]
// [#extension: envoy.tls.key_providers.qat]

// This message specifies how the private key provider is configured.
// The private key provider provides RSA sign and decrypt operation
// hardware acceleration.

message QatPrivateKeyMethodConfig {
  // Private key to use in the private key provider. If set to inline_bytes or
  // inline_string, the value needs to be the private key in PEM format.
  config.core.v3.DataSource private_key = 1 [(udpa.annotations.sensitive) = true];

  // How long to wait before polling the hardware accelerator after a
  // request has been submitted there. Having a small value leads to
  // quicker answers from the hardware but causes more polling loop
  // spins, leading to potentially larger CPU usage. The duration needs
  // to be set to a value greater than or equal to 1 millisecond.
  google.protobuf.Duration poll_delay = 2 [(validate.rules).duration = {
    required: true
    gte {nanos: 1000000}
  }];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.private_key_providers.cryptomb.v3alpha;

import "envoy/config/core/v3/base.proto";

import "google/protobuf/duration.proto";

import "udpa/annotations/sensitive.proto";
import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.private_key_providers.cryptomb.v3alpha";
option java_outer_classname = "CryptombProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/private_key_providers/cryptomb/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: CryptoMb private key provider]
// [#extension: envoy.tls.key_providers.cryptomb]

// A CryptoMbPrivateKeyMethodConfig message specifies how the CryptoMb private
// key provider is configured. The private key provider provides ``SIMD``
// processing for ECDSA sign operations and RSA sign and decrypt operations.
// The provider works by gathering the operations into a worker-thread specific
// queue, and processing the queue using ``ipp-crypto`` library when the queue
// is full or when a timer expires.
// [#extension-category: envoy.tls.key_providers]
message CryptoMbPrivateKeyMethodConfig {
  // Private key to use in the private key provider. If set to inline_bytes or
  // inline_string, the value needs to be the private key in PEM format.
  config.core.v3.DataSource private_key = 1 [(udpa.annotations.sensitive) = true];

  // How long to wait until the per-thread processing queue should be
  // processed. If the processing queue gets full (eight sign or decrypt
  // requests are received) it is processed immediately. However, if the
  // queue is not filled before the delay has expired, the requests
  // already in the queue are processed, even if the queue is not full.
  // In effect, this value controls the balance between latency and
  // throughput. The duration needs to be set to a value greater than or equal to 1 millisecond.
  google.protobuf.Duration poll_delay = 2 [(validate.rules).duration = {
    required: true
    gte {nanos: 1000000}
  }];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "//envoy/config/core/v3:pkg",
        "@com_github_cncf_xds//udpa/annotations:pkg",
    ],
)
syntax = "proto3";

package envoy.extensions.regex_engines.hyperscan.v3alpha;

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.regex_engines.hyperscan.v3alpha";
option java_outer_classname = "HyperscanProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/regex_engines/hyperscan/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Hyperscan]
// Hyperscan :ref:`configuration overview <config_hyperscan>`.
// [#extension: envoy.regex_engines.hyperscan]

// `Hyperscan <https://github.com/intel/hyperscan>`_ regex engine. The engine uses hybrid automata
// techniques to allow simultaneous matching of large numbers of regular expressions across streams
// of data.
//
// The engine follows PCRE pattern syntax, and the regex string must adhere to the documented
// `pattern support <https://intel.github.io/hyperscan/dev-reference/compilation.html#pattern-support>`_.
// The syntax is not compatible with the default RE2 regex engine. Depending on configured
// expressions, swapping regex engine may cause match rules to no longer be valid.
message Hyperscan {
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.network.connection_balance.dlb.v3alpha;

import "udpa/annotations/status.proto";

option java_package = "io.envoyproxy.envoy.extensions.network.connection_balance.dlb.v3alpha";
option java_outer_classname = "DlbProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/network/connection_balance/dlb/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Dlb connection balancer configuration]
// DLB :ref:`configuration overview <config_connection_balance_dlb>`.
// [#extension: envoy.network.connection_balance.dlb]

// The Dlb is a hardware managed system of queues and arbiters connecting producers and consumers. It is a PCIE device
// in the CPU package. It interacts with software running on cores and potentially other devices. The Dlb implements the
// following balancing features:
//
// -  Lock-free multi-producer/multi-consumer operation.
// -  Multiple priorities for varying traffic types.
// -  Various distribution schemes.
//
// Dlb connection balancer uses Dlb hardware to balance connections, and can significantly reduce latency.
//
// As the Dlb connection balancer provides assistance from dedicated Dlb hardware, it can be used for a proxy with a large number of connections
// (e.g., a gateway).
message Dlb {
  // The fallback policy if any error occurs.
  // The default policy is None.
  enum FallbackPolicy {
    // No fallback policy.
    None = 0;

    // Fall back to Nop Connection Balance.
    NopConnectionBalance = 1;

    // Fall back to Exact Connection Balance.
    ExactConnectionBalance = 2;
  }

  // The ID of the Dlb hardware, start from 0.
  // If not specified, use the first available device as default.
  uint32 id = 1;

  // Maximum number of retries when sending to DLB device fails.
  // No retry as default.
  uint32 max_retries = 2;

  FallbackPolicy fallback_policy = 3;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.compression.qatzip.compressor.v3alpha;

import "google/protobuf/wrappers.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.compression.qatzip.compressor.v3alpha";
option java_outer_classname = "QatzipProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/compression/qatzip/compressor/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;

// [#protodoc-title: Qatzip Compressor]
// Qatzip :ref:`configuration overview <config_qatzip>`.
// [#extension: envoy.compression.qatzip.compressor]

// [#next-free-field: 6]
message Qatzip {
  enum HardwareBufferSize {
    DEFAULT = 0;
    SZ_4K = 1;
    SZ_8K = 2;
    SZ_32K = 3;
    SZ_64K = 4;
    SZ_128K = 5;
    SZ_512K = 6;
  }

  // Value from 1 to 9 that controls the main compression speed-density lever.
  // The higher quality, the slower compression. The default value is 1.
  google.protobuf.UInt32Value compression_level = 1 [(validate.rules).uint32 = {lte: 9 gte: 1}];

  // A size of qat hardware buffer. This field will be set to "DEFAULT" if not specified.
  HardwareBufferSize hardware_buffer_size = 2 [(validate.rules).enum = {defined_only: true}];

  // Threshold of compression service’s input size for software failover.
  // If the size of input request less than the threshold, qatzip will route the request to software
  // compressor. The default value is 1024. The maximum value is 512*1024.
  google.protobuf.UInt32Value input_size_threshold = 3
      [(validate.rules).uint32 = {lte: 524288 gte: 128}];

  // A size of stream buffer. The default value is 128 * 1024. The maximum value is 2*1024*1024 -
  // 5*1024
  google.protobuf.UInt32Value stream_buffer_size = 4
      [(validate.rules).uint32 = {lte: 2092032 gte: 1024}];

  // Value for compressor's next output buffer. If not set, defaults to 4096.
  google.protobuf.UInt32Value chunk_size = 5 [(validate.rules).uint32 = {lte: 65536 gte: 4096}];
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
)
syntax = "proto3";

package envoy.extensions.router.cluster_specifier.golang.v3alpha;

import "google/protobuf/any.proto";

import "xds/annotations/v3/status.proto";

import "udpa/annotations/status.proto";
import "validate/validate.proto";

option java_package = "io.envoyproxy.envoy.extensions.router.cluster_specifier.golang.v3alpha";
option java_outer_classname = "GolangProto";
option java_multiple_files = true;
option go_package = "github.com/envoyproxy/go-control-plane/contrib/envoy/extensions/router/cluster_specifier/golang/v3alpha";
option (udpa.annotations.file_status).package_version_status = ACTIVE;
option (xds.annotations.v3.file_status).work_in_progress = true;

// [#protodoc-title: Golang]
//
// For an overview of the Golang cluster specifier please see the :ref:`configuration reference documentation <config_http_cluster_specifier_golang>`.
// [#extension: envoy.router.cluster_specifier_plugin.golang]

// [#extension-category: envoy.router.cluster_specifier_plugin]
message Config {
  // Globally unique ID for a dynamic library file.
  string library_id = 1 [(validate.rules).string = {min_len: 1}];

  // Path to a dynamic library implementing the
  // :repo:`ClusterSpecifier API <contrib/golang/router/cluster_specifier/source/go/pkg/api.ClusterSpecifier>`
  // interface.
  // [#comment:TODO(wangfakang): Support for downloading libraries from remote repositories.]
  string library_path = 2 [(validate.rules).string = {min_len: 1}];

  // Default cluster.
  //
  // It will be used when the specifier interface return empty string or panic.
  //
  string default_cluster = 3 [(validate.rules).string = {min_len: 1}];

  // Configuration for the Go cluster specifier plugin.
  //
  // .. note::
  //     This configuration is only parsed in the go cluster specifier, and is therefore not validated
  //     by Envoy.
  //
  //     See the :repo:`StreamFilter API <contrib/golang/router/cluster_specifier/source/go/pkg/cluster_specifier/config.go>`
  //     for more information about how the plugin's configuration data can be accessed.
  //
  google.protobuf.Any config = 4;
}
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

licenses(["notice"])  # Apache 2

api_proto_package(
    deps = [
        "@com_github_cncf_xds//udpa/annotations:pkg",
        "@com_github_cncf_xds//xds/annotations/v3:pkg",
    ],
)
# Data plane API

This tree hosts the configuration and APIs that drive [Envoy](https://www.envoyproxy.io/). The
APIs are also in some cases used by other proxy solutions that aim to interoperate with management
systems and configuration generators that are built against this standard. Thus, we consider these a
set of *universal data plane* APIs. See [this](https://medium.com/@mattklein123/the-universal-data-plane-api-d15cec7a)
blog post for more information on the universal data plane concept.

# Repository structure

The API tree can be found at two locations:
* https://github.com/envoyproxy/envoy/tree/main/api - canonical read/write home for the APIs.
* https://github.com/envoyproxy/data-plane-api - read-only mirror of
  https://github.com/envoyproxy/envoy/tree/main/api, providing the ability to consume the data
  plane APIs without the Envoy implementation.

# Further API reading

* [API style guide](STYLE.md)
* [API versioning guide](API_VERSIONING.md)
* [API overview for users](https://www.envoyproxy.io/docs/envoy/latest/configuration/overview/overview)
* [xDS protocol overview](https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol)
* [Contributing guide](CONTRIBUTING.md)
# API style guidelines

Generally follow guidance at https://cloud.google.com/apis/design/, in
particular for proto3 as described at:

* https://cloud.google.com/apis/design/proto3
* https://cloud.google.com/apis/design/naming_convention
* https://developers.google.com/protocol-buffers/docs/style

A key aspect of our API style is maintaining stability by following the [API versioning
guidelines](API_VERSIONING.md). All developers must familiarize themselves with these guidelines,
any PR which makes breaking changes to the API will not be merged.

In addition, the following conventions should be followed:

* Every proto directory should have a `README.md` describing its content. See
  for example [envoy.service](envoy/service/README.md).

* The data plane APIs are primarily intended for machine generation and consumption.
  It is expected that the management server is responsible for mapping higher
  level configuration concepts to concrete API concepts. Similarly, static configuration
  fragments may be generated by tools and UIs, etc. The APIs and tools used
  to generate xDS configuration are beyond the scope of the definitions in this
  repository.

* Use [wrapped scalar
  types](https://github.com/google/protobuf/blob/master/src/google/protobuf/wrappers.proto)
  if there is any potential need for a field to have a default value that does not
  match the proto3 defaults (0/false/""). For example, new features whose
  default value may change in the future or security mitigations that should be
  default safe in the future but are temporarily not enabled.

* Use a `[#not-implemented-hide:]` `protodoc` annotation in comments for fields that lack Envoy
  implementation. These indicate that the entity is not implemented in Envoy and the entity
  should be hidden from the Envoy documentation.

* For extensions that are a work-in-progress or the config proto documentation is hidden with
  `[#not-implemented-hide:]`, set its `status` field to `wip` in `extensions_metadata.yaml`.

* Use a `(xds.annotations.v3.file_status).work_in_progress`,
  `(xds.annotations.v3.message_status).work_in_progress`, or
  `(xds.annotations.v3.field_status).work_in_progress` option annotation for files,
  messages, or fields, respectively, that are considered work in progress and are not subject to the
  threat model or the breaking change policy. This is similar to the work-in-progress/alpha tagging
  of extensions described below, but allows tagging protos that are used as part of the core API
  as work in progress without having to break them into their own file.

* Always use plural field names for `repeated` fields, such as `filters`.

* Due to the fact that we consider JSON/YAML to be first class inputs, we cannot easily change a
  a singular field to a repeated field (both due to JSON/YAML array structural differences as well
  as singular vs. plural field naming). If there is a reasonable expectation that a field may need
  to be repeated in the future, but we don't need it to be repeated right away, consider making it
  repeated now but using constraints to enforce a maximum repeated size of 1. E.g.:

  ```proto
  repeated OutputSink sinks = 1 [(validate.rules).repeated = {min_items: 1, max_items: 1}];
  ```

* Always use upper camel case names for message types and enum types without embedded
  acronyms, such as `HttpRequest`.

* Prefer multiple fields with defined precedence over boolean overloads of fields or
  `oneof`. For example, prefer:

  ```proto
  // Simple path matcher. If regex_path is set, this field is not used.
  string simple_path = 1;
  // Regex path matcher. If set, takes precedence over simple_path.
  string regex_path = 2;
  ```

  to

  ```proto
  string path = 1;
  bool path_is_regex = 2;
  ```

  or

  ```
  oneof path_specifier {
    string simple_path = 1;
    string regex_path = 2;
  }
  ```

  This is more efficient on the wire. It also allows new alternatives to be
  added later in a way that allows control planes to be backward-compatible.

* The API includes two types for representing [percents](envoy/type/percent.proto). `Percent` is
  effectively a double value in the range 0.0-100.0. `FractionalPercent` is an integral fraction
  that can be used to create a truncated percentage also in the range 0.0-100.0. In high performance
  paths, `FractionalPercent` is preferred as randomness calculations can be performed using integral
  modulo and comparison operations only without any floating point conversions. Typically, most
  users do not need infinite precision in these paths.

* For enum types, if one of the enum values is used for most cases, make it the
  first enum value with `0` numeric value. Otherwise, define the first enum
  value like `TYPE_NAME_UNSPECIFIED = 0`, and treat it as an error. This design
  pattern forces developers to explicitly choose the correct enum value for
  their use case, and avoid misunderstanding of the default behavior.

* For time-related fields, prefer using the well-known types `google.protobuf.Duration` or
  `google.protobuf.Timestamp` instead of raw integers for seconds.

* If a field is going to contain raw bytes rather than a human-readable string, the field should
  be of type `bytes` instead of `string`.

* Proto fields should be sorted logically, not by field number.

## Package organization

API definitions are layered hierarchically in packages from top-to-bottom as following:
- `envoy.extensions` contains all definitions for the extensions, the package should match the structure of the `source` directory.
- `envoy.service` contains gRPC definitions of supporting services and top-level messages for the services.
e.g. `envoy.service.route.v3` contains RDS, `envoy.service.listener.v3` contains LDS.
- `envoy.config` contains other definitions for service configuration, bootstrap and some legacy core types.
- `envoy.data` contains data format declaration for data types that Envoy produces.
- `envoy.type` contains common protobuf types such as percent, range and matchers.

Extensions should use the regular hierarchy. For example, configuration for network filters belongs
in a package under `envoy.extensions.filter.network`.

## Adding an extension configuration to the API

Extensions must currently be added as v3 APIs following the [package
organization](#package-organization) above.
To add an extension config to the API, the steps below should be followed:

1. Place the v3 extension configuration `.proto` in `api/envoy/extensions` or `api/contrib/envoy/extensions`, e.g.
   `api/envoy/extensions/filters/http/foobar/v3/foobar.proto` together with an initial BUILD file:
   ```bazel
   load("@envoy_api//bazel:api_build_system.bzl", "api_proto_package")

   licenses(["notice"])  # Apache 2

   api_proto_package(
       deps = ["@com_github_cncf_xds//udpa/annotations:pkg"],
   )
   ```
1. If this is still WiP and subject to breaking changes, please tag it
   `option (xds.annotations.v3.file_status).work_in_progress = true;` and
   optionally hide it from the docs (`[#not-implemented-hide:]`).
1. Make sure your proto imports the v3 extension config proto (`import "udpa/annotations/status.proto";`)
1. Make sure your proto is tracked as ready to be used
   (`option (udpa.annotations.file_status).package_version_status = ACTIVE;`).
   This is required to automatically include the config proto in [api/versioning/BUILD](versioning/BUILD) under `active_protos`.
1. Add a reference to the v3 extension config in [api/BUILD](BUILD) under `v3_protos`.
1. Update [source/extensions/extensions_metadata.yaml](../source/extensions/extensions_metadata.yaml) or [contrib/extensions_metadata.yaml](../contrib/extensions_metadata.yaml)
   with the category, [security posture, and status](../EXTENSION_POLICY.md#extension-stability-and-security-posture).
   * Any extension category added to `extensions_metadata.yaml` should be annotated in precisely one proto file, associated with a field of a proto message. e.g.
     ```proto
     message SomeMessage {
       // An ordered list of http filters
       // [#extension-category: envoy.http.filters]
       repeated core.v3.TypedExtensionConfig http_filter_extensions = 1;
     }
     ```
   * Each extension added to `extensions_metadata.yaml` should have precisely one proto file annotated with the extension name. e.g.
     ```proto
     // [#protodoc-title: Your New Filter]
     // [#extension: envoy.http.filters.your_new_filter]

     // YourFilterConfig is the configuration for a YourFilter (write real documentation here).
     message YourFilterConfig {
     }
     ```
1. If you introduce a new extension category, you'll also need to add its name
   under `categories` in: [tools/extensions/extensions_schema.yaml](../tools/extensions/extensions_schema.yaml).
1. Update
   [source/extensions/extensions_build_config.bzl](../source/extensions/extensions_build_config.bzl) or [contrib/contrib_build_config.bzl](../contrib/contrib_build_config.bzl)
   to include the new extension.
1. If the extension is not hidden, find or create a docs file with a toctree
   and reference your proto to make sure users can navigate to it from the API docs
   (and to not break the docs build), like [docs/root/api-v3/admin/admin.rst](../docs/root/api-v3/admin/admin.rst).
1. Run `./tools/proto_format/proto_format.sh fix`. **Before running the script**, you will need to **commit your local changes**. By adding the commit, the tool will recognize the change, and will regenerate the `BUILD` file and reformat `foobar.proto` as needed. If you have not followed any of the above steps correctly `proto_format.sh` may remove some of the files that you added. If that is the case you can revert to the committed state, and try again once any issues are resolved.
1. See the [key-value-store PR](https://github.com/envoyproxy/envoy/pull/17745/files) for an example of adding a new extension point to common.

## API annotations

A number of annotations are used in the Envoy APIs to provide additional API
metadata. We describe these annotations below by category.

### Field level
* `[deprecated = true]` to denote fields that are deprecated in a major version.
  These fields are slated for removal at the next major cycle and follow the
  [breaking change policy](../CONTRIBUTING.md#breaking-change-policy).
* `[envoy.annotations.disallowed_by_default = true]` to denote fields that have
  been disallowed by default as per the [breaking change policy](../CONTRIBUTING.md#breaking-change-policy).
* `[(udpa.annotations.field_migrate).rename = "<new field name>"]` to denote that
  the field will be renamed to a given name in the next API major version.
* `[(udpa.annotations.sensitive) = true]` to denote sensitive fields that
  should be redacted in output such as logging or configuration dumps.
* [PGV annotations](https://github.com/bufbuild/protoc-gen-validate) to denote field
  value constraints.

### Enum value level
* `[(udpa.annotations.enum_value_migrate).rename = "new enum value name"]` to denote that
  the enum value will be renamed to a given name in the next API major version.

### Message level
* `option (udpa.annotations.versioning).previous_message_type = "<message type
  name>";` to denote the previous type name for an upgraded message. You should
  never have to write these manually, they are generated by `protoxform`.

### Service level
* `option (envoy.annotations.resource).type = "<resource type name>";` to denote
  the resource type for an xDS service definition.

### File level
* `option (udpa.annotations.file_migrate).move_to_package = "<package name>";`
  to denote that in the next major version of the API, the file will be moved to
  the given package. This is consumed by `protoxform`.
* `option (xds.annotations.v3.file_status).work_in_progress = true;` to denote a
  file that is still work-in-progress and subject to breaking changes.

## Principles

The following principles should be adhered to when extending or modifying the
xDS APIs:

* The xDS APIs have a logical distinction between transport and data model:
  - The xDS transport protocol describes the network transport on which xDS
    configuration resources are delivered to clients. A versioned gRPC streaming
    protocol with support for ACK/NACK is provided by xDS; this is known as the
    xDS transport protocol (xDS-TP). xDS configuration resources can also be
    delivered on other transports, e.g. HTTP or filesystem, with some
    limitations (e.g. no version feedback).
  - The xDS data model describes the xDS configuration resources themselves,
    e.g. listeners, route configurations, clusters, endpoints, secrets.

* The xDS APIs are directionally client and server neutral. While many aspects
  of the APIs reflect the history of their origin as Envoy's control plane APIs,
  API decisions going forward should reflect the principle of client neutrality.

* The xDS APIs are expressed canonically as [Proto3](https://developers.google.com/protocol-buffers/docs/proto3).
  Both JSON and YAML are also supported formats, with the standard JSON-proto3
  conversion used during client configuration ingestion.

* xDS APIs are eventual consistency first. For example, if RDS references a
  cluster that has not yet been supplied by CDS, it should be silently ignored
  and traffic not forwarded until the CDS update occurs. Stronger consistency
  guarantees are possible if the management server is able to sequence the xDS
  APIs carefully (for example by using the ADS API below). By following the
  `[CDS, EDS, LDS, RDS]` sequence for all pertinent resources, it will be
  possible to avoid traffic outages during configuration update.

* The API is primarily intended for machine generation and consumption. It is
  expected that the management server is responsible for mapping higher level
  configuration concepts to API responses. Similarly, static configuration
  fragments may be generated by templating tools, etc. With that consideration,
  we also aim to have API artifacts readable by humans for debugging and
  understanding applied configuration. This implies that APIs do not have to
  have ergonomics as the main driver, but should still be reasonable to read by
  humans. The APIs and tools used to generate xDS configuration are beyond the
  scope of the definitions in this repository.

* All supported transports (xDS-TP, HTTP, filesystem) support basic singleton xDS
  subscription services CDS/EDS/LDS/RDS/SDS. Advanced APIs such as HDS, ADS and
  EDS multi-dimensional LB are xDS-TP only. This avoids having to map
  complicated bidirectional stream semantics onto REST, etc..

* Versioning follows the scheme described [here](API_VERSIONING.md). A key
  principle that we target is that API consumers should not be exposed to
  breaking changes where there is no substantial gain in functionality,
  performance, security or implementation simplification. We will tolerate
  technical debt in the API itself, e.g. in the form of vestigial deprecated
  fields or reduced ergonomics in order to meet this principle.

* Namespaces for extensions, metadata, etc. use a reverse DNS naming scheme,
  e.g. `com.google.widget`, `com.lyft.widget`. Client built-ins may be prefixed
  with client name, e.g. `envoy.foo`, `grpc.bar`.
# Contributing guide

## API changes

All API changes should follow the [style guide](STYLE.md).

API changes are regular PRs in https://github.com/envoyproxy/envoy for the API/configuration
changes. They may be as part of a larger implementation PR. Please follow the standard Bazel and CI
process for validating build/test sanity of `api/` before submitting a PR.

*Note: New .proto files should be added to
[BUILD](https://github.com/envoyproxy/envoy/blob/main/api/versioning/BUILD) in order to get the RSTs generated.*

## Documentation changes

The Envoy project takes documentation seriously. We view it as one of the reasons the project has
seen rapid adoption. As such, it is required that all features have complete documentation. This is
generally going to be a combination of API documentation as well as architecture/overview
documentation.

### Building documentation locally

The documentation can be built locally in the root of https://github.com/envoyproxy/envoy via:

```
ci/do_ci.sh docs
```

To skip configuration examples validation:

```
SPHINX_SKIP_CONFIG_VALIDATION=true ci/do_ci.sh docs
```

Or to use a hermetic Docker container:

```
./ci/run_envoy_docker.sh 'ci/do_ci.sh docs'
```

This process builds RST documentation directly from the proto files, merges it with the static RST
files, and then runs [Sphinx](https://www.sphinx-doc.org/en/stable/rest.html) over the entire tree to
produce the final documentation. The generated RST files are not committed as they are regenerated
every time the documentation is built.

### Viewing documentation

Once the documentation is built, it is available rooted at `generated/docs/index.html`. The
generated RST files are also viewable in `generated/rst`.

Note also that the generated documentation can be viewed in CI:

1. Open docs job in Azure Pipelines.
2. Navigate to "Upload Docs to GCS" log.
3. Click on the link there.

If you do not see "Upload Docs to GCS" or it is failing, that means the docs are not built correctly.

### Documentation guidelines

The following are some general guidelines around documentation.

* Cross link as much as possible. Sphinx is fantastic at this. Use it! See ample examples with the
  existing documentation as a guide.
* Please use a **single space** after a period in documentation so that all generated text is
  consistent.
* Comments can be left inside comments if needed (that's pretty deep, right?) via the `[#comment:]`
  special tag. E.g.,

  ```
  // This is a really cool field!
  // [#comment:TODO(mattklein123): Do something cooler]
  string foo_field = 3;
  ```
* Please use *italics* (enclosed in asterisks `*emphasized word*`) for emphasis and `inline literals` for code quotation (enclosed in *double* backticks ` ``code`` `).
* All documentation is expected to use proper English grammar with proper punctuation. If you are
  not a fluent English speaker please let us know and we will help out.
codec_type: AUTO

stat_prefix: "ingress_http"

route_config {
  virtual_hosts {
    name: "service"
    domains: "*"
    routes {
      match {
        prefix: "/service"
      }
      route {
        cluster: "local_service"
        timeout {
          seconds: 0
        }
      }
    }
  }
}

http_filters {
  name: "router"
}
address {
  socket_address {
    protocol: TCP
    port_value: 80
  }
}
filter_chains {
  filters {
    name: "http_connection_manager"
  }
}
licenses(["notice"])  # Apache 2

exports_files([
    "http_connection_manager.pb",
    "listeners.pb",
])
load("@envoy_api//bazel:repository_locations_utils.bzl", "load_repository_locations_spec")

# Envoy dependencies may be annotated with the following attributes:
DEPENDENCY_ANNOTATIONS = [
    # Attribute specifying CPE (Common Platform Enumeration, see https://nvd.nist.gov/products/cpe) ID
    # of the dependency. The ID may be in v2.3 or v2.2 format, although v2.3 is prefferred. See
    # https://nvd.nist.gov/products/cpe for CPE format. Use single wildcard '*' for version and vector elements
    # i.e. 'cpe:2.3:a:nghttp2:nghttp2:*'. Use "N/A" for dependencies without CPE assigned.
    # This attribute is optional for components with use categories listed in the
    # USE_CATEGORIES_WITH_CPE_OPTIONAL
    "cpe",

    # A list of extensions when 'use_category' contains 'dataplane_ext' or 'observability_ext'.
    "extensions",

    # Additional dependencies loaded transitively via this dependency that are not tracked in
    # Envoy (see the external dependency at the given version for information).
    "implied_untracked_deps",

    # Project metadata.
    "project_desc",
    "project_name",
    "project_url",

    # Reflects the UTC date (YYYY-MM-DD format) for the dependency release. This
    # is when the dependency was updated in its repository. For dependencies
    # that have releases, this is the date of the release. For dependencies
    # without releases or for scenarios where we temporarily need to use a
    # commit, this date should be the date of the commit in UTC.
    "release_date",

    # List of the categories describing how the dependency is being used. This attribute is used
    # for automatic tracking of security posture of Envoy's dependencies.
    # Possible values are documented in the USE_CATEGORIES list below.
    # This attribute is mandatory for each dependecy.
    "use_category",

    # The dependency version. This may be either a tagged release (preferred)
    # or git SHA (as an exception when no release tagged version is suitable).
    "version",
]

# NOTE: If a dependency use case is either dataplane or controlplane, the other uses are not needed
# to be declared.
USE_CATEGORIES = [
    # This dependency is used in API protos.
    "api",
    # This dependency is used in build process.
    "build",
    # This dependency is used to process xDS requests.
    "controlplane",
    # This dependency is used in processing downstream or upstream requests (core).
    "dataplane_core",
    # This dependency is used in processing downstream or upstream requests (extensions).
    "dataplane_ext",
    # This dependecy is used for logging, metrics or tracing (core). It may process unstrusted input.
    "observability_core",
    # This dependecy is used for logging, metrics or tracing (extensions). It may process unstrusted input.
    "observability_ext",
    # This dependency does not handle untrusted data and is used for various utility purposes.
    "other",
    # This dependency is used only in tests.
    "test_only",
    # Documentation generation
    "docs",
    # Developer tools (not used in build or docs)
    "devtools",
]

# Components with these use categories are not required to specify the 'cpe'.
USE_CATEGORIES_WITH_CPE_OPTIONAL = ["build", "other", "test_only", "api"]

def _fail_missing_attribute(attr, key):
    fail("The '%s' attribute must be defined for external dependency " % attr + key)

# Method for verifying content of the repository location specifications.
#
# We also remove repository metadata attributes so that further consumers, e.g.
# http_archive, are not confused by them.
def load_repository_locations(repository_locations_spec):
    locations = {}
    for key, location in load_repository_locations_spec(repository_locations_spec).items():
        mutable_location = dict(location)
        locations[key] = mutable_location

        if "sha256" not in location or len(location["sha256"]) == 0:
            _fail_missing_attribute("sha256", key)

        if "project_name" not in location:
            _fail_missing_attribute("project_name", key)

        if "project_desc" not in location:
            _fail_missing_attribute("project_desc", key)

        if "project_url" not in location:
            _fail_missing_attribute("project_url", key)
        project_url = location["project_url"]
        if not project_url.startswith("https://") and not project_url.startswith("http://"):
            fail("project_url must start with https:// or http://: " + project_url)

        if "version" not in location:
            _fail_missing_attribute("version", key)

        if "use_category" not in location:
            _fail_missing_attribute("use_category", key)
        use_category = location["use_category"]

        if "dataplane_ext" in use_category or "observability_ext" in use_category:
            if "extensions" not in location:
                _fail_missing_attribute("extensions", key)

        if "release_date" not in location:
            _fail_missing_attribute("release_date", key)
        release_date = location["release_date"]

        # Starlark doesn't have regexes.
        if len(release_date) != 10 or release_date[4] != "-" or release_date[7] != "-":
            fail("release_date must match YYYY-DD-MM: " + release_date)

        if "cpe" in location:
            cpe = location["cpe"]

            # Starlark doesn't have regexes.
            cpe_components = len(cpe.split(":"))

            # We allow cpe:2.3:a:foo:*:* and cpe:2.3.:a:foo:bar:* only.
            cpe_components_valid = (cpe_components == 6)
            cpe_matches = (cpe == "N/A" or (cpe.startswith("cpe:2.3:a:") and cpe.endswith(":*") and cpe_components_valid))
            if not cpe_matches:
                fail("CPE must match cpe:2.3:a:<facet>:<facet>:*: " + cpe)
        elif not [category for category in USE_CATEGORIES_WITH_CPE_OPTIONAL if category in location["use_category"]]:
            _fail_missing_attribute("cpe", key)

        for category in location["use_category"]:
            if category not in USE_CATEGORIES:
                fail("Unknown use_category value '" + category + "' for dependecy " + key)

        # Remove any extra annotations that we add, so that we don't confuse http_archive etc.
        for annotation in DEPENDENCY_ANNOTATIONS:
            if annotation in mutable_location:
                mutable_location.pop(annotation)

    return locations
# This should match the schema defined in external_deps.bzl.
REPOSITORY_LOCATIONS_SPEC = dict(
    bazel_skylib = dict(
        project_name = "bazel-skylib",
        project_desc = "Common useful functions and rules for Bazel",
        project_url = "https://github.com/bazelbuild/bazel-skylib",
        version = "1.5.0",
        sha256 = "cd55a062e763b9349921f0f5db8c3933288dc8ba4f76dd9416aac68acee3cb94",
        release_date = "2023-11-06",
        urls = ["https://github.com/bazelbuild/bazel-skylib/releases/download/{version}/bazel-skylib-{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/bazelbuild/bazel-skylib/blob/{version}/LICENSE",
    ),
    com_envoyproxy_protoc_gen_validate = dict(
        project_name = "protoc-gen-validate (PGV)",
        project_desc = "protoc plugin to generate polyglot message validators",
        project_url = "https://github.com/bufbuild/protoc-gen-validate",
        use_category = ["api"],
        sha256 = "9372f9ecde8fbadf83c8c7de3dbb49b11067aa26fb608c501106d0b4bf06c28f",
        version = "1.0.4",
        urls = ["https://github.com/bufbuild/protoc-gen-validate/archive/refs/tags/v{version}.zip"],
        strip_prefix = "protoc-gen-validate-{version}",
        release_date = "2024-01-17",
        implied_untracked_deps = [
            "com_github_iancoleman_strcase",
            "com_github_lyft_protoc_gen_star_v2",
            "com_github_spf13_afero",
            "org_golang_google_genproto",
            "org_golang_x_text",
            "org_golang_x_mod",
            "org_golang_x_sys",
        ],
        license = "Apache-2.0",
        license_url = "https://github.com/bufbuild/protoc-gen-validate/blob/v{version}/LICENSE",
    ),
    com_github_cncf_xds = dict(
        project_name = "xDS API",
        project_desc = "xDS API Working Group (xDS-WG)",
        project_url = "https://github.com/cncf/xds",
        # During the UDPA -> xDS migration, we aren't working with releases.
        version = "3a472e524827f72d1ad621c4983dd5af54c46776",
        sha256 = "dc305e20c9fa80822322271b50aa2ffa917bf4fd3973bcec52bfc28dc32c5927",
        release_date = "2023-11-16",
        strip_prefix = "xds-{version}",
        urls = ["https://github.com/cncf/xds/archive/{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/cncf/xds/blob/{version}/LICENSE",
    ),
    com_github_openzipkin_zipkinapi = dict(
        project_name = "Zipkin API",
        project_desc = "Zipkin's language independent model and HTTP Api Definitions",
        project_url = "https://github.com/openzipkin/zipkin-api",
        version = "1.0.0",
        sha256 = "6c8ee2014cf0746ba452e5f2c01f038df60e85eb2d910b226f9aa27ddc0e44cf",
        release_date = "2020-11-22",
        strip_prefix = "zipkin-api-{version}",
        urls = ["https://github.com/openzipkin/zipkin-api/archive/{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/openzipkin/zipkin-api/blob/{version}/LICENSE",
    ),
    com_google_googleapis = dict(
        # TODO(dio): Consider writing a Starlark macro for importing Google API proto.
        project_name = "Google APIs",
        project_desc = "Public interface definitions of Google APIs",
        project_url = "https://github.com/googleapis/googleapis",
        version = "114a745b2841a044e98cdbb19358ed29fcf4a5f1",
        sha256 = "9b4e0d0a04a217c06b426aefd03b82581a9510ca766d2d1c70e52bb2ad4a0703",
        release_date = "2023-01-10",
        strip_prefix = "googleapis-{version}",
        urls = ["https://github.com/googleapis/googleapis/archive/{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/googleapis/googleapis/blob/{version}/LICENSE",
    ),
    opencensus_proto = dict(
        project_name = "OpenCensus Proto",
        project_desc = "Language Independent Interface Types For OpenCensus",
        project_url = "https://github.com/census-instrumentation/opencensus-proto",
        version = "0.4.1",
        sha256 = "e3d89f7f9ed84c9b6eee818c2e9306950519402bf803698b15c310b77ca2f0f3",
        release_date = "2022-09-23",
        strip_prefix = "opencensus-proto-{version}/src",
        urls = ["https://github.com/census-instrumentation/opencensus-proto/archive/v{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/census-instrumentation/opencensus-proto/blob/v{version}/LICENSE",
    ),
    prometheus_metrics_model = dict(
        project_name = "Prometheus client model",
        project_desc = "Data model artifacts for Prometheus",
        project_url = "https://github.com/prometheus/client_model",
        version = "0.5.0",
        sha256 = "170873e0b91cab5da6634af1498b88876842ff3e01212e2dabf6b4e6512c948d",
        release_date = "2023-10-03",
        strip_prefix = "client_model-{version}",
        urls = ["https://github.com/prometheus/client_model/archive/v{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/prometheus/client_model/blob/v{version}/LICENSE",
    ),
    rules_proto = dict(
        project_name = "Protobuf Rules for Bazel",
        project_desc = "Protocol buffer rules for Bazel",
        project_url = "https://github.com/bazelbuild/rules_proto",
        version = "4.0.0",
        sha256 = "66bfdf8782796239d3875d37e7de19b1d94301e8972b3cbd2446b332429b4df1",
        release_date = "2021-09-15",
        strip_prefix = "rules_proto-{version}",
        urls = ["https://github.com/bazelbuild/rules_proto/archive/refs/tags/{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/bazelbuild/rules_proto/blob/{version}/LICENSE",
    ),
    opentelemetry_proto = dict(
        project_name = "OpenTelemetry Proto",
        project_desc = "Language Independent Interface Types For OpenTelemetry",
        project_url = "https://github.com/open-telemetry/opentelemetry-proto",
        version = "1.1.0",
        sha256 = "df491a05f3fcbf86cc5ba5c9de81f6a624d74d4773d7009d573e37d6e2b6af64",
        release_date = "2024-01-11",
        strip_prefix = "opentelemetry-proto-{version}",
        urls = ["https://github.com/open-telemetry/opentelemetry-proto/archive/v{version}.tar.gz"],
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/open-telemetry/opentelemetry-proto/blob/v{version}/LICENSE",
    ),
    com_github_bufbuild_buf = dict(
        project_name = "buf",
        project_desc = "A new way of working with Protocol Buffers.",  # Used for breaking change detection in API protobufs
        project_url = "https://buf.build",
        version = "1.29.0",
        sha256 = "1033f26361e6fc30ffcfab9d4e4274ffd4af88d9c97de63d2e1721c4a07c1380",
        strip_prefix = "buf",
        urls = ["https://github.com/bufbuild/buf/releases/download/v{version}/buf-Linux-x86_64.tar.gz"],
        release_date = "2024-01-24",
        use_category = ["api"],
        license = "Apache-2.0",
        license_url = "https://github.com/bufbuild/buf/blob/v{version}/LICENSE",
    ),
    com_github_chrusty_protoc_gen_jsonschema = dict(
        project_name = "protoc-gen-jsonschema",
        project_desc = "Protobuf to JSON-Schema compiler",
        project_url = "https://github.com/norbjd/protoc-gen-jsonschema",
        strip_prefix = "protoc-gen-jsonschema-{version}",
        sha256 = "ba3e313b10a1b50a6c1232d994c13f6e23d3669be4ae7fea13762f42bb3b2abc",
        version = "7680e4998426e62b6896995ff73d4d91cc5fb13c",
        urls = ["https://github.com/norbjd/protoc-gen-jsonschema/archive/{version}.zip"],
        use_category = ["build"],
        release_date = "2023-05-30",
    ),
    envoy_toolshed = dict(
        project_name = "envoy_toolshed",
        project_desc = "Tooling, libraries, runners and checkers for Envoy proxy's CI",
        project_url = "https://github.com/envoyproxy/toolshed",
        version = "0.1.1",
        sha256 = "ee759b57270a2747f3f2a3d6ecaad63b834dd9887505a9f1c919d72429dbeffd",
        strip_prefix = "toolshed-bazel-v{version}/bazel",
        urls = ["https://github.com/envoyproxy/toolshed/archive/bazel-v{version}.tar.gz"],
        use_category = ["build"],
        release_date = "2023-10-21",
        cpe = "N/A",
        license = "Apache-2.0",
        license_url = "https://github.com/envoyproxy/envoy/blob/bazel-v{version}/LICENSE",
    ),
)
load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

def envoy_http_archive(name, locations, location_name = None, **kwargs):
    # `existing_rule_keys` contains the names of repositories that have already
    # been defined in the Bazel workspace. By skipping repos with existing keys,
    # users can override dependency versions by using standard Bazel repository
    # rules in their WORKSPACE files.
    existing_rule_keys = native.existing_rules().keys()
    if name in existing_rule_keys:
        # This repository has already been defined, probably because the user
        # wants to override the version. Do nothing.
        return
    location = locations[location_name or name]

    # HTTP tarball at a given URL. Add a BUILD file if requested.
    http_archive(
        name = name,
        urls = location["urls"],
        sha256 = location["sha256"],
        strip_prefix = location.get("strip_prefix", ""),
        **kwargs
    )
load("@com_envoyproxy_protoc_gen_validate//bazel:pgv_proto_library.bzl", "pgv_cc_proto_library")
load("@com_github_grpc_grpc//bazel:cc_grpc_library.bzl", "cc_grpc_library")
load("@com_github_grpc_grpc//bazel:python_rules.bzl", _py_proto_library = "py_proto_library")
load("@io_bazel_rules_go//proto:def.bzl", "go_proto_library")
load("@io_bazel_rules_go//go:def.bzl", "go_test")
load("@rules_proto//proto:defs.bzl", "proto_library")
load(
    "//bazel:external_proto_deps.bzl",
    "EXTERNAL_PROTO_CC_BAZEL_DEP_MAP",
    "EXTERNAL_PROTO_GO_BAZEL_DEP_MAP",
)
load(
    "//bazel/cc_proto_descriptor_library:builddefs.bzl",
    "cc_proto_descriptor_library",
)

EnvoyProtoDepsInfo = provider(fields = ["deps"])

_PY_PROTO_SUFFIX = "_py_proto"
_CC_PROTO_SUFFIX = "_cc_proto"
_CC_PROTO_DESCRIPTOR_SUFFIX = "_cc_proto_descriptor"
_CC_GRPC_SUFFIX = "_cc_grpc"
_GO_PROTO_SUFFIX = "_go_proto"
_GO_IMPORTPATH_PREFIX = "github.com/envoyproxy/go-control-plane/"

_COMMON_PROTO_DEPS = [
    "@com_google_protobuf//:any_proto",
    "@com_google_protobuf//:descriptor_proto",
    "@com_google_protobuf//:duration_proto",
    "@com_google_protobuf//:empty_proto",
    "@com_google_protobuf//:struct_proto",
    "@com_google_protobuf//:timestamp_proto",
    "@com_google_protobuf//:wrappers_proto",
    "@com_google_googleapis//google/api:http_proto",
    "@com_google_googleapis//google/api:httpbody_proto",
    "@com_google_googleapis//google/api:annotations_proto",
    "@com_google_googleapis//google/rpc:status_proto",
    "@com_envoyproxy_protoc_gen_validate//validate:validate_proto",
]

def _proto_mapping(dep, proto_dep_map, proto_suffix):
    mapped = proto_dep_map.get(dep)
    if mapped == None:
        prefix = "@" + Label(dep).workspace_name if not dep.startswith("//") else ""
        return prefix + "//" + Label(dep).package + ":" + Label(dep).name + proto_suffix
    return mapped

def _go_proto_mapping(dep):
    return _proto_mapping(dep, EXTERNAL_PROTO_GO_BAZEL_DEP_MAP, _GO_PROTO_SUFFIX)

def _cc_proto_mapping(dep):
    return _proto_mapping(dep, EXTERNAL_PROTO_CC_BAZEL_DEP_MAP, _CC_PROTO_SUFFIX)

def _api_cc_grpc_library(name, proto, deps = []):
    cc_grpc_library(
        name = name,
        srcs = [proto],
        deps = deps,
        proto_only = False,
        grpc_only = True,
        visibility = ["//visibility:public"],
    )

def api_cc_py_proto_library(
        name,
        visibility = ["//visibility:private"],
        srcs = [],
        deps = [],
        linkstatic = 0,
        has_services = 0):
    relative_name = ":" + name
    proto_library(
        name = name,
        srcs = srcs,
        deps = deps + _COMMON_PROTO_DEPS,
        visibility = visibility,
    )

    # This is to support Envoy Mobile using Protobuf-Lite.
    # Protobuf-Lite generated C++ code does not include reflection
    # capabilities but analogous functionality can be provided by
    # cc_proto_descriptor_library.
    cc_proto_descriptor_library(
        name = name + _CC_PROTO_DESCRIPTOR_SUFFIX,
        visibility = visibility,
        deps = [name],
    )

    cc_proto_library_name = name + _CC_PROTO_SUFFIX
    pgv_cc_proto_library(
        name = cc_proto_library_name,
        linkstatic = linkstatic,
        cc_deps = [_cc_proto_mapping(dep) for dep in deps] + [
            "@com_google_googleapis//google/api:http_cc_proto",
            "@com_google_googleapis//google/api:httpbody_cc_proto",
            "@com_google_googleapis//google/api:annotations_cc_proto",
            "@com_google_googleapis//google/rpc:status_cc_proto",
        ],
        deps = [relative_name],
        visibility = ["//visibility:public"],
    )

    # Uses gRPC implementation of py_proto_library.
    # https://github.com/grpc/grpc/blob/v1.59.1/bazel/python_rules.bzl#L160
    _py_proto_library(
        name = name + _PY_PROTO_SUFFIX,
        # Actual dependencies are resolved automatically from the proto_library dep tree.
        deps = [relative_name],
        visibility = ["//visibility:public"],
    )

    # Optionally define gRPC services
    if has_services:
        # TODO: when Python services are required, add to the below stub generations.
        cc_grpc_name = name + _CC_GRPC_SUFFIX
        cc_proto_deps = [cc_proto_library_name] + [_cc_proto_mapping(dep) for dep in deps]
        _api_cc_grpc_library(name = cc_grpc_name, proto = relative_name, deps = cc_proto_deps)

def api_cc_test(name, **kwargs):
    native.cc_test(
        name = name,
        **kwargs
    )

def api_go_test(name, **kwargs):
    go_test(
        name = name,
        **kwargs
    )

def api_proto_package(
        name = "pkg",
        srcs = [],
        deps = [],
        has_services = False,
        visibility = ["//visibility:public"]):
    if srcs == []:
        srcs = native.glob(["*.proto"])

    name = "pkg"
    api_cc_py_proto_library(
        name = name,
        visibility = visibility,
        srcs = srcs,
        deps = deps,
        has_services = has_services,
    )

    compilers = ["@io_bazel_rules_go//proto:go_proto", "@envoy_api//bazel:pgv_plugin_go", "@envoy_api//bazel:vtprotobuf_plugin_go"]
    if has_services:
        compilers = ["@io_bazel_rules_go//proto:go_grpc", "@envoy_api//bazel:pgv_plugin_go", "@envoy_api//bazel:vtprotobuf_plugin_go"]

    deps = (
        [_go_proto_mapping(dep) for dep in deps] +
        [
            "@com_envoyproxy_protoc_gen_validate//validate:go_default_library",
            "@com_github_golang_protobuf//ptypes:go_default_library_gen",
            "@go_googleapis//google/api:annotations_go_proto",
            "@go_googleapis//google/rpc:status_go_proto",
            "@io_bazel_rules_go//proto/wkt:any_go_proto",
            "@io_bazel_rules_go//proto/wkt:duration_go_proto",
            "@io_bazel_rules_go//proto/wkt:struct_go_proto",
            "@io_bazel_rules_go//proto/wkt:timestamp_go_proto",
            "@io_bazel_rules_go//proto/wkt:wrappers_go_proto",
            "@com_github_planetscale_vtprotobuf//types/known/anypb",
            "@com_github_planetscale_vtprotobuf//types/known/durationpb",
            "@com_github_planetscale_vtprotobuf//types/known/emptypb",
            "@com_github_planetscale_vtprotobuf//types/known/fieldmaskpb",
            "@com_github_planetscale_vtprotobuf//types/known/structpb",
            "@com_github_planetscale_vtprotobuf//types/known/timestamppb",
            "@com_github_planetscale_vtprotobuf//types/known/wrapperspb",
        ]
    )
    go_proto_library(
        name = name + _GO_PROTO_SUFFIX,
        compilers = compilers,
        importpath = _GO_IMPORTPATH_PREFIX + native.package_name(),
        proto = name,
        visibility = ["//visibility:public"],
        deps = {dep: True for dep in deps}.keys(),
    )
def _format_version(s, version):
    return s.format(version = version, dash_version = version.replace(".", "-"), underscore_version = version.replace(".", "_"))

# Generate a "repository location specification" from raw repository
# specification. The information should match the format required by
# external_deps.bzl. This function mostly does interpolation of {version} in
# the repository info fields. This code should be capable of running in both
# Python and Starlark.
def load_repository_locations_spec(repository_locations_spec):
    locations = {}
    for key, location in repository_locations_spec.items():
        mutable_location = dict(location)
        locations[key] = mutable_location

        # Fixup with version information.
        if "version" in location:
            if "strip_prefix" in location:
                mutable_location["strip_prefix"] = _format_version(location["strip_prefix"], location["version"])
            mutable_location["urls"] = [_format_version(url, location["version"]) for url in location["urls"]]
            if "license_url" in location:
                mutable_location["license_url"] = _format_version(location["license_url"], location["version"])
    return locations

def merge_dicts(*dicts):
    result = {}
    for d in dicts:
        result.update(d)
    return result
# Any external dependency imported in the api/ .protos requires entries in
# the maps below, to allow the Bazel proto and language specific bindings to be
# inferred from the import directives.
#
# This file needs to be interpreted as both Python 3 and Starlark, so only the
# common subset of Python should be used.

# This maps from .proto import directive path to the Bazel dependency path for
# external dependencies. Since BUILD files are generated, this is the canonical
# place to define this mapping.
EXTERNAL_PROTO_IMPORT_BAZEL_DEP_MAP = {
    "google/api/expr/v1alpha1/checked.proto": "@com_google_googleapis//google/api/expr/v1alpha1:checked_proto",
    "google/api/expr/v1alpha1/syntax.proto": "@com_google_googleapis//google/api/expr/v1alpha1:syntax_proto",
    "io/prometheus/client/metrics.proto": "@prometheus_metrics_model//:client_model",
    "opencensus/proto/trace/v1/trace.proto": "@opencensus_proto//opencensus/proto/trace/v1:trace_proto",
    "opencensus/proto/trace/v1/trace_config.proto": "@opencensus_proto//opencensus/proto/trace/v1:trace_config_proto",
    "opentelemetry/proto/common/v1/common.proto": "@opentelemetry_proto//:common",
}

# This maps from the Bazel proto_library target to the Go language binding target for external dependencies.
EXTERNAL_PROTO_GO_BAZEL_DEP_MAP = {
    # Note @com_google_googleapis are point to @go_googleapis.
    #
    # It is aligned to xDS dependency to suppress the conflicting package heights error between
    # @com_github_cncf_xds//xds/type/matcher/v3:pkg_go_proto
    # @envoy_api//envoy/config/rbac/v3:pkg_go_proto
    #
    # TODO(https://github.com/bazelbuild/rules_go/issues/1986): update to
    #    @com_google_googleapis when the bug is resolved. Also see the note to
    #    go_googleapis in https://github.com/bazelbuild/rules_go/blob/master/go/dependencies.rst#overriding-dependencies
    "@com_google_googleapis//google/api/expr/v1alpha1:checked_proto": "@go_googleapis//google/api/expr/v1alpha1:expr_go_proto",
    "@com_google_googleapis//google/api/expr/v1alpha1:syntax_proto": "@go_googleapis//google/api/expr/v1alpha1:expr_go_proto",
    "@opencensus_proto//opencensus/proto/trace/v1:trace_proto": "@opencensus_proto//opencensus/proto/trace/v1:trace_proto_go",
    "@opencensus_proto//opencensus/proto/trace/v1:trace_config_proto": "@opencensus_proto//opencensus/proto/trace/v1:trace_and_config_proto_go",
    "@opentelemetry_proto//:trace": "@opentelemetry_proto//:trace_go_proto",
    "@opentelemetry_proto//:logs": "@opentelemetry_proto//:logs_go_proto",
    "@opentelemetry_proto//:metrics": "@opentelemetry_proto//:metrics_go_proto",
    "@opentelemetry_proto//:common": "@opentelemetry_proto//:common_go_proto",
}

# This maps from the Bazel proto_library target to the C++ language binding target for external dependencies.
EXTERNAL_PROTO_CC_BAZEL_DEP_MAP = {
    "@com_google_googleapis//google/api/expr/v1alpha1:checked_proto": "@com_google_googleapis//google/api/expr/v1alpha1:checked_cc_proto",
    "@com_google_googleapis//google/api/expr/v1alpha1:syntax_proto": "@com_google_googleapis//google/api/expr/v1alpha1:syntax_cc_proto",
    "@opencensus_proto//opencensus/proto/trace/v1:trace_proto": "@opencensus_proto//opencensus/proto/trace/v1:trace_proto_cc",
    "@opencensus_proto//opencensus/proto/trace/v1:trace_config_proto": "@opencensus_proto//opencensus/proto/trace/v1:trace_config_proto_cc",
    "@opentelemetry_proto//:trace": "@opentelemetry_proto//:trace_cc_proto",
    "@opentelemetry_proto//:logs": "@opentelemetry_proto//:logs_cc_proto",
    "@opentelemetry_proto//:metrics": "@opentelemetry_proto//:metrics_cc_proto",
    "@opentelemetry_proto//:common": "@opentelemetry_proto//:common_cc_proto",
}
load("@envoy_toolshed//:macros.bzl", "json_data")
load("@io_bazel_rules_go//proto:compiler.bzl", "go_proto_compiler")
load(":repository_locations.bzl", "REPOSITORY_LOCATIONS_SPEC")
load(":repository_locations_utils.bzl", "load_repository_locations_spec")
load(
    ":external_proto_deps.bzl",
    "EXTERNAL_PROTO_CC_BAZEL_DEP_MAP",
    "EXTERNAL_PROTO_GO_BAZEL_DEP_MAP",
    "EXTERNAL_PROTO_IMPORT_BAZEL_DEP_MAP",
)

licenses(["notice"])  # Apache 2

exports_files([
    "repository_locations.bzl",
    "repository_locations_utils.bzl",
    "utils.bzl",
])

go_proto_compiler(
    name = "pgv_plugin_go",
    options = ["lang=go"],
    plugin = "@com_envoyproxy_protoc_gen_validate//:protoc-gen-validate",
    suffix = ".pb.validate.go",
    valid_archive = False,
    visibility = ["//visibility:public"],
)

go_proto_compiler(
    name = "vtprotobuf_plugin_go",
    options = ["features=marshal_strict+size"],
    plugin = "@com_github_planetscale_vtprotobuf//cmd/protoc-gen-go-vtproto",
    suffix = "_vtproto.pb.go",
    valid_archive = False,
    visibility = ["//visibility:public"],
)

json_data(
    name = "repository_locations",
    data = load_repository_locations_spec(REPOSITORY_LOCATIONS_SPEC),
)

json_data(
    name = "external_proto_deps",
    data = dict(
        cc = EXTERNAL_PROTO_CC_BAZEL_DEP_MAP,
        go = EXTERNAL_PROTO_GO_BAZEL_DEP_MAP,
        imports = EXTERNAL_PROTO_IMPORT_BAZEL_DEP_MAP,
    ),
)
"""Public rules for using cc proto descriptor library with protos:
  - cc_proto_descriptor_library()
"""

load("@bazel_skylib//lib:paths.bzl", "paths")

# begin:google_only
# load("@bazel_tools//tools/cpp:toolchain_utils.bzl", "find_cpp_toolchain", "use_cpp_toolchain")
# end:google_only

# begin:github_only
# Compatibility code for Bazel 4.x. Remove this when we drop support for Bazel 4.x.
load("@bazel_tools//tools/cpp:toolchain_utils.bzl", "find_cpp_toolchain")

def use_cpp_toolchain():
    return ["@bazel_tools//tools/cpp:toolchain_type"]

# end:github_only

# Generic support code #########################################################

# begin:github_only
_is_google3 = False
# end:github_only

# begin:google_only
# _is_google3 = True
# end:google_only

def _get_real_short_path(file):
    # For some reason, files from other archives have short paths that look like:
    #   ../com_google_protobuf/google/protobuf/descriptor.proto
    short_path = file.short_path
    if short_path.startswith("../"):
        second_slash = short_path.index("/", 3)
        short_path = short_path[second_slash + 1:]

    # Sometimes it has another few prefixes like:
    #   _virtual_imports/any_proto/google/protobuf/any.proto
    #   benchmarks/_virtual_imports/100_msgs_proto/benchmarks/100_msgs.proto
    # We want just google/protobuf/any.proto.
    virtual_imports = "_virtual_imports/"
    if virtual_imports in short_path:
        short_path = short_path.split(virtual_imports)[1].split("/", 1)[1]
    return short_path

def _get_real_root(ctx, file):
    real_short_path = _get_real_short_path(file)
    root = file.path[:-len(real_short_path) - 1]

    if not _is_google3 and ctx.rule.attr.strip_import_prefix:
        root = paths.join(root, ctx.rule.attr.strip_import_prefix[1:])
    return root

def _generate_output_file(ctx, src, extension):
    package = ctx.label.package
    if not _is_google3:
        strip_import_prefix = ctx.rule.attr.strip_import_prefix
        if strip_import_prefix and strip_import_prefix != "/":
            if not package.startswith(strip_import_prefix[1:]):
                fail("%s does not begin with prefix %s" % (package, strip_import_prefix))
            package = package[len(strip_import_prefix):]

    real_short_path = _get_real_short_path(src)
    real_short_path = paths.relativize(real_short_path, package)
    output_filename = paths.replace_extension(real_short_path, extension)
    ret = ctx.actions.declare_file(output_filename)
    return ret

def _generate_include_path(src, out, extension):
    short_path = _get_real_short_path(src)
    short_path = paths.replace_extension(short_path, extension)
    if not out.path.endswith(short_path):
        fail("%s does not end with %s" % (out.path, short_path))

    return out.path[:-len(short_path)]

def _filter_none(elems):
    out = []
    for elem in elems:
        if elem:
            out.append(elem)
    return out

def _cc_library_func(ctx, name, hdrs, srcs, copts, includes, dep_ccinfos):
    """Like cc_library(), but callable from rules.

    Args:
      ctx: Rule context.
      name: Unique name used to generate output files.
      hdrs: Public headers that can be #included from other rules.
      srcs: C/C++ source files.
      copts: Additional options for cc compilation.
      includes: Additional include paths.
      dep_ccinfos: CcInfo providers of dependencies we should build/link against.

    Returns:
      CcInfo provider for this compilation.
    """

    compilation_contexts = [info.compilation_context for info in dep_ccinfos]
    linking_contexts = [info.linking_context for info in dep_ccinfos]
    toolchain = find_cpp_toolchain(ctx)
    feature_configuration = cc_common.configure_features(
        ctx = ctx,
        cc_toolchain = toolchain,
        requested_features = ctx.features,
        unsupported_features = ctx.disabled_features,
    )

    blaze_only_args = {}

    if _is_google3:
        blaze_only_args["grep_includes"] = ctx.file._grep_includes

    (compilation_context, compilation_outputs) = cc_common.compile(
        actions = ctx.actions,
        feature_configuration = feature_configuration,
        cc_toolchain = toolchain,
        name = name,
        srcs = srcs,
        includes = includes,
        public_hdrs = hdrs,
        user_compile_flags = copts,
        compilation_contexts = compilation_contexts,
        **blaze_only_args
    )

    # buildifier: disable=unused-variable
    (linking_context, linking_outputs) = cc_common.create_linking_context_from_compilation_outputs(
        actions = ctx.actions,
        name = name,
        feature_configuration = feature_configuration,
        cc_toolchain = toolchain,
        compilation_outputs = compilation_outputs,
        linking_contexts = linking_contexts,
        disallow_dynamic_library = cc_common.is_enabled(feature_configuration = feature_configuration, feature_name = "targets_windows"),
        **blaze_only_args
    )

    return CcInfo(
        compilation_context = compilation_context,
        linking_context = linking_context,
    )

# Dummy rule to expose select() copts to aspects  ##############################

CcProtoDescriptorLibraryCoptsInfo = provider(
    "Provides copts for cc proto descriptor library targets",
    fields = {
        "copts": "copts for cc_proto_descriptor_library()",
    },
)

def cc_proto_descriptor_library_copts_impl(ctx):
    return CcProtoDescriptorLibraryCoptsInfo(copts = ctx.attr.copts)

cc_proto_descriptor_library_copts = rule(
    implementation = cc_proto_descriptor_library_copts_impl,
    attrs = {"copts": attr.string_list(default = [])},
)

# cc_proto_descriptor_library shared code #################

GeneratedSrcsInfo = provider(
    "Provides generated headers and sources",
    fields = {
        "srcs": "list of srcs",
        "hdrs": "list of hdrs",
        "includes": "list of extra includes",
    },
)

CcProtoDescriptorWrappedCcInfo = provider("Provider for cc_info for protos", fields = ["cc_info"])
_CcProtoDescriptorWrappedGeneratedSrcsInfo = provider("Provider for generated sources", fields = ["srcs"])

def _compile_protos(ctx, generator, proto_info, proto_sources):
    if len(proto_sources) == 0:
        return GeneratedSrcsInfo(srcs = [], hdrs = [], includes = [])

    ext = "_" + generator
    tool = getattr(ctx.executable, "_gen_" + generator)
    srcs = [_generate_output_file(ctx, name, ext + ".pb.cc") for name in proto_sources]
    hdrs = [_generate_output_file(ctx, name, ext + ".pb.h") for name in proto_sources]
    transitive_sets = proto_info.transitive_descriptor_sets.to_list()

    args = ctx.actions.args()
    args.use_param_file(param_file_arg = "@%s")
    args.set_param_file_format("multiline")

    args.add("--" + generator + "_out=" + _get_real_root(ctx, srcs[0]))
    args.add("--plugin=protoc-gen-" + generator + "=" + tool.path)
    args.add("--descriptor_set_in=" + ctx.configuration.host_path_separator.join([f.path for f in transitive_sets]))
    args.add_all(proto_sources, map_each = _get_real_short_path)

    ctx.actions.run(
        inputs = depset(
            direct = [proto_info.direct_descriptor_set],
            transitive = [proto_info.transitive_descriptor_sets],
        ),
        tools = [tool],
        outputs = srcs + hdrs,
        executable = ctx.executable._protoc,
        arguments = [args],
        progress_message = "Generating descriptor protos for :" + ctx.label.name,
        mnemonic = "GenDescriptorProtos",
    )
    return GeneratedSrcsInfo(
        srcs = srcs,
        hdrs = hdrs,
        includes = [_generate_include_path(proto_sources[0], hdrs[0], "_descriptor.pb.h")],
    )

def _cc_proto_descriptor_rule_impl(ctx):
    if len(ctx.attr.deps) != 1:
        fail("only one deps dependency allowed.")
    dep = ctx.attr.deps[0]

    if _CcProtoDescriptorWrappedGeneratedSrcsInfo in dep:
        srcs = dep[_CcProtoDescriptorWrappedGeneratedSrcsInfo].srcs
    else:
        fail("proto_library rule must generate _CcProtoDescriptorWrappedGeneratedSrcsInfo " +
             "(aspect should have handled this).")

    if CcProtoDescriptorWrappedCcInfo in dep:
        cc_info = dep[CcProtoDescriptorWrappedCcInfo].cc_info
    else:
        fail("proto_library rule must generate CcProtoDescriptorWrappedCcInfo  " +
             "(aspect should have handled this).")

    lib = cc_info.linking_context.linker_inputs.to_list()[0].libraries[0]
    files = _filter_none([
        lib.static_library,
        lib.pic_static_library,
        lib.dynamic_library,
    ])
    return [
        DefaultInfo(files = depset(files + srcs.hdrs + srcs.srcs)),
        srcs,
        cc_info,
    ]

def _cc_proto_descriptor_aspect_impl(target, ctx, generator, cc_provider, file_provider, provide_cc_shared_library_hints = True):
    proto_info = target[ProtoInfo]
    files = _compile_protos(ctx, generator, proto_info, proto_info.direct_sources)
    deps = ctx.rule.attr.deps + getattr(ctx.attr, "_" + generator)
    dep_ccinfos = [dep[CcInfo] for dep in deps if CcInfo in dep]
    dep_ccinfos += [dep[CcProtoDescriptorWrappedCcInfo].cc_info for dep in deps if CcProtoDescriptorWrappedCcInfo in dep]
    name = ctx.rule.attr.name + "." + generator
    owners = [ctx.label.relative(name)]
    cc_info = _cc_library_func(
        ctx = ctx,
        name = name,
        hdrs = files.hdrs,
        srcs = files.srcs,
        includes = files.includes,
        #copts = ctx.attr._copts[CcProtoDescriptorLibraryCoptsInfo].copts,
        copts = [],
        dep_ccinfos = dep_ccinfos,
    )

    wrapped_cc_info = cc_provider(
        cc_info = cc_info,
    )
    providers = [
        wrapped_cc_info,
        file_provider(srcs = files),
    ]
    if provide_cc_shared_library_hints:
        if hasattr(cc_common, "CcSharedLibraryHintInfo"):
            providers.append(cc_common.CcSharedLibraryHintInfo(owners = owners))
        elif hasattr(cc_common, "CcSharedLibraryHintInfo_6_X_constructor_do_not_use"):
            # This branch can be deleted once 6.X is not supported by rules
            providers.append(cc_common.CcSharedLibraryHintInfo_6_X_constructor_do_not_use(owners = owners))
    return providers

def cc_proto_descriptor_library_aspect_impl(target, ctx):
    return _cc_proto_descriptor_aspect_impl(target, ctx, "descriptor", CcProtoDescriptorWrappedCcInfo, _CcProtoDescriptorWrappedGeneratedSrcsInfo)

def _maybe_add(d):
    if _is_google3:
        d["_grep_includes"] = attr.label(
            allow_single_file = True,
            cfg = "exec",
            default = "@bazel_tools//tools/cpp:grep-includes",
        )
    return d

# cc_proto_descriptor_library() ##########################################################

def _get_cc_proto_descriptor_library_aspect_provides():
    provides = [
        CcProtoDescriptorWrappedCcInfo,
        _CcProtoDescriptorWrappedGeneratedSrcsInfo,
    ]

    if hasattr(cc_common, "CcSharedLibraryHintInfo"):
        provides.append(cc_common.CcSharedLibraryHintInfo)
    elif hasattr(cc_common, "CcSharedLibraryHintInfo_6_X_getter_do_not_use"):
        # This branch can be deleted once 6.X is not supported by upb rules
        provides.append(cc_common.CcSharedLibraryHintInfo_6_X_getter_do_not_use)

    return provides

cc_proto_descriptor_library_aspect = aspect(
    attrs = _maybe_add({
        #"_copts": attr.label(
        #    default = "//:upb_proto_library_copts__for_generated_code_only_do_not_use",
        #),
        "_gen_descriptor": attr.label(
            executable = True,
            cfg = "exec",
            default = "//bazel/cc_proto_descriptor_library:file_descriptor_generator",
        ),
        "_protoc": attr.label(
            executable = True,
            cfg = "exec",
            default = "@com_google_protobuf//:protoc",
        ),
        "_cc_toolchain": attr.label(
            default = "@bazel_tools//tools/cpp:current_cc_toolchain",
        ),
        "_descriptor": attr.label_list(
            default = [
                Label("//bazel/cc_proto_descriptor_library:file_descriptor_info"),
                Label("@com_google_absl//absl/base:core_headers"),
            ],
        ),
    }),
    implementation = cc_proto_descriptor_library_aspect_impl,
    provides = _get_cc_proto_descriptor_library_aspect_provides(),
    attr_aspects = ["deps"],
    fragments = ["cpp"],
    toolchains = use_cpp_toolchain(),
)

cc_proto_descriptor_library = rule(
    output_to_genfiles = True,
    implementation = _cc_proto_descriptor_rule_impl,
    attrs = {
        "deps": attr.label_list(
            aspects = [cc_proto_descriptor_library_aspect],
            allow_rules = ["proto_library"],
            providers = [ProtoInfo],
        ),
    },
    provides = [CcInfo],
)
#include "bazel/cc_proto_descriptor_library/create_dynamic_message.h"
#include "bazel/cc_proto_descriptor_library/testdata/test.pb.h"
#include "bazel/cc_proto_descriptor_library/testdata/test_descriptor.pb.h"
#include "bazel/cc_proto_descriptor_library/text_format_transcoder.h"
#include "gmock/gmock.h"
#include "gtest/gtest.h"

// NOLINT(namespace-envoy)
namespace {

using ::testing::Eq;
using ::testing::NotNull;
using ::testing::Test;

TEST(TextFormatTranscoderTest, CreateDynamicMessage) {
  cc_proto_descriptor_library::TextFormatTranscoder reserializer;
  reserializer.loadFileDescriptors(
      protobuf::reflection::bazel_cc_proto_descriptor_library_testdata_test::kFileDescriptorInfo);

  testdata::dynamic_descriptors::Foo concrete_message;
  concrete_message.set_bar("hello world");
  auto dynamic_message =
      cc_proto_descriptor_library::createDynamicMessage(reserializer, concrete_message);
  ASSERT_THAT(dynamic_message, NotNull());

  // Access the descriptor.
  auto descriptor = dynamic_message->GetDescriptor();
  ASSERT_THAT(descriptor, NotNull());
  ASSERT_THAT(descriptor->full_name(), Eq(concrete_message.GetTypeName()));

  // Access reflection.
  auto reflection = dynamic_message->GetReflection();
  const auto bar_feld_descriptor = descriptor->FindFieldByName("bar");
  ASSERT_THAT(bar_feld_descriptor, NotNull());
  ASSERT_THAT(reflection->GetString(*dynamic_message, bar_feld_descriptor), Eq("hello world"));
}

} // namespace
// Copyright 2019 Google Inc. All Rights Reserved.
// Author: kmensah@google.com (Kwasi Mensah)

#include "bazel/cc_proto_descriptor_library/testdata/test.pb.h"
#include "bazel/cc_proto_descriptor_library/text_format_transcoder.h"
#include "gmock/gmock.h"
#include "google/protobuf/descriptor.h"
#include "google/protobuf/text_format.h"
#include "gtest/gtest.h"

// NOLINT(namespace-envoy)
namespace {

using ::testing::Eq;
using ::testing::Test;

TEST(TextFormatTranscoderTest, GlobalFallbackAllowed) {
  cc_proto_descriptor_library::TextFormatTranscoder reserializer(
      /*allow_global_fallback=*/true);

  testdata::dynamic_descriptors::Foo concrete_message;
  ASSERT_TRUE(reserializer.parseInto(R"text(
bar: "hello world"
)text",
                                     &concrete_message));

  ASSERT_THAT(concrete_message.bar(), Eq("hello world"));
}

TEST(TextFormatTranscoderTest, GlobalFallbackDisallowed) {
  cc_proto_descriptor_library::TextFormatTranscoder reserializer(
      /*allow_global_fallback=*/false);

  testdata::dynamic_descriptors::Foo concrete_message;
  ASSERT_FALSE(reserializer.parseInto(R"text(
bar: "hello world"
)text",
                                      &concrete_message));
}

} // namespace
syntax = "proto2";

package testdata.dynamic_descriptors;

import "bazel/cc_proto_descriptor_library/testdata/test.proto";

extend Foo {
  optional int64 number = 11;
}
syntax = "proto2";

package testdata.dynamic_descriptors;

message FooCopy {
  optional string bar = 1;

  extensions 10 to 20;
}
syntax = "proto2";

import "google/protobuf/any.proto";

package testdata.dynamic_descriptors;

message Foo {
  optional string bar = 1;
  optional google.protobuf.Any baz = 2;

  extensions 10 to 20;
}
load("//bazel/cc_proto_descriptor_library:builddefs.bzl", "cc_proto_descriptor_library")

licenses(["notice"])  # Apache 2

# Tests for go/cc_proto_descriptor_library.
# These need to be in a separate directory to test visibility restrictions.

proto_library(
    name = "test_proto",
    testonly = True,
    srcs = ["test.proto"],
    visibility = ["//visibility:private"],
    deps = [
        "@com_google_protobuf//:any_proto",
    ],
)

proto_library(
    name = "test1_proto",
    testonly = True,
    srcs = ["test1.proto"],
    visibility = ["//visibility:private"],
    deps = [
        ":test_proto",
        "@com_google_protobuf//:any_proto",
    ],
)

proto_library(
    name = "test_extension_proto",
    testonly = True,
    # Purprosely a dash to make sure they're handled properly in filenames
    srcs = ["test-extension.proto"],
    visibility = ["//visibility:private"],
    deps = [
        ":test_proto",
        "@com_google_protobuf//:any_proto",
    ],
)

cc_proto_library(
    name = "test_cc_proto",
    testonly = True,
    visibility = ["//visibility:private"],
    deps = ["test_proto"],
)

cc_proto_library(
    name = "test1_cc_proto",
    testonly = True,
    visibility = ["//visibility:private"],
    deps = ["test1_proto"],
)

cc_proto_library(
    name = "test_extension_cc_proto",
    testonly = True,
    visibility = ["//visibility:private"],
    deps = ["test_extension_proto"],
)

cc_proto_descriptor_library(
    name = "test_descriptors",
    testonly = True,
    visibility = ["//visibility:private"],
    deps = [
        ":test_proto",
    ],
)

cc_proto_descriptor_library(
    name = "test1_descriptors",
    testonly = True,
    visibility = ["//visibility:private"],
    deps = [
        ":test1_proto",
    ],
)

cc_proto_descriptor_library(
    name = "test_extension_descriptors",
    testonly = True,
    visibility = ["//visibility:private"],
    deps = [
        ":test_extension_proto",
    ],
)

cc_test(
    name = "text_format_transcoder_test",
    srcs = [
        "text_format_transcoder_test.cc",
    ],
    visibility = ["//visibility:private"],
    deps = [
        "test1_descriptors",
        "test_cc_proto",
        "test_descriptors",
        "test_extension_cc_proto",
        "test_extension_descriptors",
        "//bazel/cc_proto_descriptor_library:text_format_transcoder",
        "@com_google_googletest//:gtest_main",
        "@com_google_protobuf//:protobuf",
    ],
)

cc_test(
    name = "global_fallback_test",
    srcs = [
        "global_fallback_test.cc",
    ],
    visibility = ["//visibility:private"],
    deps = [
        "test_cc_proto",
        "//bazel/cc_proto_descriptor_library:text_format_transcoder",
        "@com_google_googletest//:gtest_main",
        "@com_google_protobuf//:protobuf",
    ],
)

cc_test(
    name = "create_dynamic_message_test",
    srcs = [
        "create_dynamic_message_test.cc",
    ],
    visibility = ["//visibility:private"],
    deps = [
        "test_cc_proto",
        "test_descriptors",
        "//bazel/cc_proto_descriptor_library:create_dynamic_message",
        "//bazel/cc_proto_descriptor_library:text_format_transcoder",
        "@com_google_googletest//:gtest_main",
        "@com_google_protobuf//:protobuf",
    ],
)
// Copyright 2019 Google Inc. All Rights Reserved.
// Author: kmensah@google.com (Kwasi Mensah)

#include <string>

#include "absl/strings/substitute.h"
#include "bazel/cc_proto_descriptor_library/testdata/test-extension.pb.h"
#include "bazel/cc_proto_descriptor_library/testdata/test-extension_descriptor.pb.h"
#include "bazel/cc_proto_descriptor_library/testdata/test.pb.h"
#include "bazel/cc_proto_descriptor_library/testdata/test1_descriptor.pb.h"
#include "bazel/cc_proto_descriptor_library/testdata/test_descriptor.pb.h"
#include "bazel/cc_proto_descriptor_library/text_format_transcoder.h"
#include "gmock/gmock.h"
#include "google/protobuf/descriptor.h"
#include "google/protobuf/dynamic_message.h"
#include "google/protobuf/text_format.h"
#include "gtest/gtest.h"

// NOLINT(namespace-envoy)
namespace {

using google::protobuf::io::ColumnNumber;

class StringErrorCollector : public google::protobuf::io::ErrorCollector {
public:
  // String error_text is unowned and must remain valid during the use of
  // StringErrorCollector.
  // If one_indexing is set to true, all line and column numbers will be
  // increased by one for cases when provided indices are 0-indexed and
  // 1-indexed error messages are desired.
  // If ignore_warnings is set to true, warnings are discarded.
  explicit StringErrorCollector(std::string& error_text, bool one_indexing = false,
                                bool ignore_warnings = false);

  // google::protobuf::io::ErrorCollector:
  void AddError(int line, ColumnNumber column, const std::string& message) override;
  void AddWarning(int line, ColumnNumber column, const std::string& message) override;

private:
  std::string& error_text_;
  const int index_offset_;
  const bool ignore_warnings_;
};

StringErrorCollector::StringErrorCollector(std::string& error_text, bool one_indexing,
                                           bool ignore_warnings)
    : error_text_(error_text), index_offset_(one_indexing ? 1 : 0),
      ignore_warnings_(ignore_warnings) {}

void StringErrorCollector::AddError(int line, ColumnNumber column, const std::string& message) {
  absl::SubstituteAndAppend(&error_text_, "$0($1): $2\n", line + index_offset_,
                            column + index_offset_, message);
}

void StringErrorCollector::AddWarning(int line, ColumnNumber column, const std::string& message) {
  if (ignore_warnings_) {
    return;
  }
  AddError(line, column, message);
}

using ::testing::Eq;
using ::testing::Test;

TEST(TextFormatTranscoderTest, TextFormatWorks) {
  cc_proto_descriptor_library::TextFormatTranscoder reserializer;
  reserializer.loadFileDescriptors(
      protobuf::reflection::bazel_cc_proto_descriptor_library_testdata_test::kFileDescriptorInfo);

  testdata::dynamic_descriptors::Foo concrete_message;
  ASSERT_TRUE(reserializer.parseInto(R"text(
bar: "hello world"
)text",
                                     &concrete_message));

  ASSERT_THAT(concrete_message.bar(), Eq("hello world"));
}

TEST(TextToBinaryReserializerTest, TextFormatWithExtensionWorks) {
  cc_proto_descriptor_library::TextFormatTranscoder reserializer;
  const auto& file_descriptor_info = protobuf::reflection::
      bazel_cc_proto_descriptor_library_testdata_test_extension::kFileDescriptorInfo;
  reserializer.loadFileDescriptors(file_descriptor_info);

  std::string error_text;
  StringErrorCollector error_collector(error_text);
  testdata::dynamic_descriptors::Foo concrete_message;
  ASSERT_TRUE(reserializer.parseInto(R"text(
bar: "hello world"
[testdata.dynamic_descriptors.number]: 20
)text",
                                     &concrete_message, &error_collector));

  ASSERT_THAT(error_text, Eq(""));

  ASSERT_THAT(concrete_message.bar(), Eq("hello world"));
  ASSERT_THAT(concrete_message.GetExtension(testdata::dynamic_descriptors::number), Eq(20));
}

} // namespace
#include <cstdio>

#include "bazel/cc_proto_descriptor_library/file_descriptor_generator.h"
#include "google/protobuf/compiler/plugin.h"

// NOLINT(namespace-envoy)
int main(int argc, char* argv[]) {
  cc_proto_descriptor_library::ProtoDescriptorGenerator generator;
  return google::protobuf::compiler::PluginMain(argc, argv, &generator);
}
#include "bazel/cc_proto_descriptor_library/text_format_transcoder.h"

#include <memory>
#include <string>

#include "absl/memory/memory.h"
#include "absl/strings/escaping.h"
#include "absl/strings/str_format.h"
#include "absl/strings/string_view.h"
#include "bazel/cc_proto_descriptor_library/file_descriptor_info.h"
#include "google/protobuf/descriptor.h"
#include "google/protobuf/descriptor.pb.h"
#include "google/protobuf/dynamic_message.h"
#include "google/protobuf/message.h"
#include "google/protobuf/text_format.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {

struct TextFormatTranscoder::InternalData {
  google::protobuf::DescriptorPool descriptor_pool;
  google::protobuf::DynamicMessageFactory message_factory;
};

TextFormatTranscoder::TextFormatTranscoder(bool allow_global_fallback /*= true*/)
    : internals_(std::make_unique<InternalData>()), allow_global_fallback_(allow_global_fallback) {}

// Needs to be defined here so std::unique_ptr doesn't complain about
// TextToBinaryProtoReserializerInternals being incomplete in it's
// destructor.
TextFormatTranscoder::~TextFormatTranscoder() = default;

bool TextFormatTranscoder::parseInto(absl::string_view text_format,
                                     google::protobuf::MessageLite* msg,
                                     google::protobuf::io::ErrorCollector* error_collector) const {
  google::protobuf::io::ArrayInputStream input(text_format.data(), text_format.size());
  return parseInto(&input, msg, error_collector);
}

bool TextFormatTranscoder::parseInto(google::protobuf::io::ZeroCopyInputStream* input_stream,
                                     google::protobuf::MessageLite* msg,
                                     google::protobuf::io::ErrorCollector* error_collector) const {
  std::string serialization;

  if (!toBinarySerializationInternal(msg->GetTypeName(), input_stream, &serialization,
                                     error_collector)) {
    return false;
  }

  return msg->ParseFromString(serialization);
}

void TextFormatTranscoder::loadFileDescriptors(
    const internal::FileDescriptorInfo& file_descriptor_info) {
  if (internals_->descriptor_pool.FindFileByName(std::string(file_descriptor_info.file_name))) {
    return;
  }

  for (int i = 0; file_descriptor_info.deps[i] != nullptr; ++i) {
    loadFileDescriptors(*file_descriptor_info.deps[i]);
  }

  google::protobuf::FileDescriptorProto file_descriptor_proto;
  std::string file_descriptor_bytes;
  absl::Base64Unescape(file_descriptor_info.file_descriptor_bytes_base64, &file_descriptor_bytes);
  file_descriptor_proto.ParseFromString(file_descriptor_bytes);
  internals_->descriptor_pool.BuildFile(file_descriptor_proto);
}

bool TextFormatTranscoder::toBinarySerializationInternal(
    absl::string_view type_name, google::protobuf::io::ZeroCopyInputStream* input_stream,
    std::string* binary_serializtion, google::protobuf::io::ErrorCollector* error_collector) const {
  auto dynamic_message = createEmptyDynamicMessage(type_name, error_collector);
  if (!dynamic_message) {
    // createEmptyDynamicMessage already would have written to the
    // error_collector.
    return false;
  }

  // TextFormat defaults to using the DescriptorPool of the passed in
  // Message*. So we don't have to do anything special with TextFormat::Finder
  // for it to use our DescriptorPool.
  google::protobuf::TextFormat::Parser parser;
  parser.RecordErrorsTo(error_collector);
  if (!parser.Parse(input_stream, dynamic_message.get())) {
    return false;
  }

  return dynamic_message->SerializePartialToString(binary_serializtion);
}

std::unique_ptr<google::protobuf::Message> TextFormatTranscoder::createEmptyDynamicMessage(
    absl::string_view type_name, google::protobuf::io::ErrorCollector* error_collector) const {
  const google::protobuf::Descriptor* descriptor =
      internals_->descriptor_pool.FindMessageTypeByName(std::string(type_name));
  // If you're built with the full runtime then embedding the descriptors and
  // loading them would be information duplicated by the global descriptor
  // pool which hurts builds like superroot that are near all the blaze/forge
  // size limits. Teams that care about not silently falling into this fallback
  // case should make sure their tests are also run in mobile configurations or
  // set allow_global_fallback to false in TextFormatTranscoder's constructor.
  if (allow_global_fallback_ && (descriptor == nullptr)) {
    const auto generated_pool = google::protobuf::DescriptorPool::generated_pool();
    if (generated_pool != nullptr) {
      descriptor = generated_pool->FindMessageTypeByName(std::string(type_name));
    }
  }
  if (descriptor == nullptr) {
    if (error_collector) {
      error_collector->RecordError(0, 0,
                                   absl::StrFormat("Could not find descriptor for: %s", type_name));
    }
    return nullptr;
  }

  auto dynamic_message =
      absl::WrapUnique(internals_->message_factory.GetPrototype(descriptor)->New());
  if (!dynamic_message) {
    if (error_collector) {
      error_collector->RecordError(
          0, 0, absl::StrFormat("Could not create dynamic message for: %s", type_name));
    }
    return nullptr;
  }
  return dynamic_message;
}

} // namespace cc_proto_descriptor_library
#pragma once

#include <memory>

#include "google/protobuf/io/tokenizer.h"
#include "google/protobuf/io/zero_copy_stream.h"
#include "google/protobuf/io/zero_copy_stream_impl_lite.h"
#include "google/protobuf/message.h"
#include "google/protobuf/message_lite.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {

// Forward declare to make the friendship handshake easier.
class TextFormatTranscoder;

// Creates a DynamicMessage based off the passed in MessageLite. DynamicMessage
// gives you access to the Descriptor for the message. The message must have
// been registered with the passed in TextFormatTranscoder. Returned messages
// cannot outlive transcoder.
std::unique_ptr<google::protobuf::Message>
createDynamicMessage(const TextFormatTranscoder& transcoder,
                     const google::protobuf::MessageLite& message,
                     google::protobuf::io::ErrorCollector* error_collector = nullptr);

} // namespace cc_proto_descriptor_library
#pragma once

#include "absl/strings/string_view.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {
namespace internal {

struct FileDescriptorInfo final {
  // Path of the file this object represents.
  const absl::string_view file_name;
  // The bytes of the FileDescriptorProto for this proto file.
  const absl::string_view file_descriptor_bytes_base64;
  // File descriptors that this file is dependent on.
  // TODO(b/143243097): absl::Span requires special casing to deal with zero
  // length arrays and constexpr.
  const FileDescriptorInfo** const deps;
};

} // namespace internal
} // namespace cc_proto_descriptor_library
#pragma once

#include <string>

#include "google/protobuf/compiler/code_generator.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {

class ProtoDescriptorGenerator : public google::protobuf::compiler::CodeGenerator {
public:
  bool Generate(const google::protobuf::FileDescriptor* file, const std::string& parameter,
                google::protobuf::compiler::GeneratorContext* generator_context,
                std::string* error) const override;
};

} // namespace cc_proto_descriptor_library
#include "bazel/cc_proto_descriptor_library/create_dynamic_message.h"

#include <memory>

#include "absl/memory/memory.h"
#include "absl/strings/escaping.h"
#include "absl/strings/str_format.h"
#include "bazel/cc_proto_descriptor_library/text_format_transcoder.h"
#include "google/protobuf/descriptor.h"
#include "google/protobuf/descriptor.pb.h"
#include "google/protobuf/dynamic_message.h"
#include "google/protobuf/message.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {

std::unique_ptr<google::protobuf::Message>
createDynamicMessage(const TextFormatTranscoder& transcoder,
                     const google::protobuf::MessageLite& message,
                     google::protobuf::io::ErrorCollector* error_collector /*= nullptr*/) {
  auto dynamic_message =
      transcoder.createEmptyDynamicMessage(message.GetTypeName(), error_collector);

  if (dynamic_message) {
    dynamic_message->ParsePartialFromString(message.SerializePartialAsString());
  }

  return dynamic_message;
}

} // namespace cc_proto_descriptor_library
#include "bazel/cc_proto_descriptor_library/file_descriptor_generator.h"

#include <memory>
#include <sstream>
#include <string>

#include "absl/strings/escaping.h"
#include "absl/strings/str_cat.h"
#include "absl/strings/str_format.h"
#include "absl/strings/str_replace.h"
#include "absl/strings/str_split.h"
#include "absl/strings/string_view.h"
#include "absl/strings/strip.h"
#include "google/protobuf/compiler/retention.h"
#include "google/protobuf/descriptor.pb.h"
#include "google/protobuf/io/coded_stream.h"
#include "google/protobuf/io/zero_copy_stream.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {

namespace {

absl::string_view getFileBaseName(const google::protobuf::FileDescriptor* file) {
  absl::string_view stripped_name = file->name();
  if (absl::ConsumeSuffix(&stripped_name, ".proto")) {
    return stripped_name;
  } else if (absl::ConsumeSuffix(&stripped_name, ".protodevel")) {
    return stripped_name;
  }
  return stripped_name;
}

std::string getDescriptorHeaderName(const google::protobuf::FileDescriptor* file) {
  return absl::StrCat(getFileBaseName(file), "_descriptor.pb.h");
}

std::string getDescriptorSourceName(const google::protobuf::FileDescriptor* file) {
  return absl::StrCat(getFileBaseName(file), "_descriptor.pb.cc");
}

std::string getDescriptorNamespace(const google::protobuf::FileDescriptor* file) {
  return absl::AsciiStrToLower(
      absl::StrReplaceAll(getFileBaseName(file), {{"/", "_"}, {"-", "_"}}));
}

std::string getDependencyFileDescriptorInfoSymbol(const google::protobuf::FileDescriptor* file) {
  return absl::StrFormat("%s::kFileDescriptorInfo", getDescriptorNamespace(file));
}

std::string getDependencyFileDescriptorHeaderGuard(const google::protobuf::FileDescriptor* file) {
  std::string header_path = getDescriptorHeaderName(file);
  return absl::AsciiStrToUpper(
      absl::StrReplaceAll(header_path, {{"/", "_"}, {".", "_"}, {"-", "_"}}));
}

bool generateHeader(const google::protobuf::FileDescriptor* file,
                    google::protobuf::io::ZeroCopyOutputStream* output_stream) {
  auto header_guard = getDependencyFileDescriptorHeaderGuard(file);
  auto unique_namespace = getDescriptorNamespace(file);

  std::stringstream contents;
  contents << absl::StrFormat(R"text(
#ifndef %s
#define %s

#include "absl/base/attributes.h"

namespace cc_proto_descriptor_library {
namespace internal {

struct FileDescriptorInfo;

} // namespace internal
} // namespace cc_proto_descriptor_library

namespace protobuf {
namespace reflection {
namespace %s {

extern const ::cc_proto_descriptor_library::internal::FileDescriptorInfo kFileDescriptorInfo;
} // namespace %s
} // namespace reflection
} // namespace protobuf

#endif // %s

)text",
                              header_guard, header_guard, unique_namespace, unique_namespace,
                              header_guard);

  google::protobuf::io::CodedOutputStream output(output_stream);
  output.WriteString(contents.str());
  output.Trim();
  return !output.HadError();
}

bool generateSource(const google::protobuf::FileDescriptor* file,
                    google::protobuf::io::ZeroCopyOutputStream* output_stream) {
  auto unique_namespace = getDescriptorNamespace(file);
  std::stringstream contents;

  contents << absl::StrFormat("#include \"%s\"\n", getDescriptorHeaderName(file));
  for (int i = 0; i < file->dependency_count(); ++i) {
    contents << absl::StrFormat("#include \"%s\"\n", getDescriptorHeaderName(file->dependency(i)));
  }
  contents << R"text(#include "bazel/cc_proto_descriptor_library/file_descriptor_info.h"
)text";

  google::protobuf::FileDescriptorProto file_descriptor_proto =
      google::protobuf::compiler::StripSourceRetentionOptions(*file);

  contents << absl::StrFormat(
      R"text(
namespace protobuf {
namespace reflection {
namespace %s {

)text",
      unique_namespace);

  contents << "static const"
              "::cc_proto_descriptor_library::internal::"
              "FileDescriptorInfo* kDeps[] = {\n";
  for (int i = 0; i < file->dependency_count(); ++i) {
    contents << absl::StrFormat("&%s,\n",
                                getDependencyFileDescriptorInfoSymbol(file->dependency(i)));
  }
  contents << "nullptr};\n";

  contents << absl::StrFormat(
      R"text(

const ::cc_proto_descriptor_library::internal::FileDescriptorInfo kFileDescriptorInfo{
    "%s",
    "%s",
    kDeps
  };

} // namespace %s
} // namespace reflection
} // namespace protobuf

)text",
      file->name(), absl::Base64Escape(file_descriptor_proto.SerializeAsString()),
      unique_namespace);

  google::protobuf::io::CodedOutputStream output(output_stream);
  output.WriteString(contents.str());
  output.Trim();
  return !output.HadError();
}
} // namespace

bool ProtoDescriptorGenerator::Generate( // NOLINT(readability-identifier-naming)
    const google::protobuf::FileDescriptor* file, const std::string& parameter,
    google::protobuf::compiler::GeneratorContext* generator_context, std::string* error) const {
  std::string header_path = getDescriptorHeaderName(file);
  std::string source_path = getDescriptorSourceName(file);

  std::unique_ptr<google::protobuf::io::ZeroCopyOutputStream> header_output(
      generator_context->Open(header_path));
  std::unique_ptr<google::protobuf::io::ZeroCopyOutputStream> source_output(
      generator_context->Open(source_path));

  if (!generateHeader(file, header_output.get())) {
    return false;
  }
  if (!generateSource(file, source_output.get())) {
    return false;
  }

  return true;
}

} // namespace cc_proto_descriptor_library
licenses(["notice"])  # Apache 2

cc_library(
    name = "file_descriptor_info",
    hdrs = ["file_descriptor_info.h"],
    visibility = ["//visibility:public"],
    deps = [
        "@com_google_absl//absl/strings",
    ],
)

cc_library(
    name = "text_format_transcoder",
    srcs = [
        "create_dynamic_message.h",
        "text_format_transcoder.cc",
    ],
    hdrs = ["text_format_transcoder.h"],
    visibility = ["//visibility:public"],
    deps = [
        ":file_descriptor_info",
        "@com_google_absl//absl/memory",
        "@com_google_absl//absl/strings",
        "@com_google_absl//absl/strings:str_format",
        "@com_google_protobuf//:protobuf",
    ],
)

cc_library(
    name = "file_descriptor_generator_lib",
    srcs = ["file_descriptor_generator.cc"],
    hdrs = ["file_descriptor_generator.h"],
    deps = [
        "@com_google_absl//absl/strings",
        "@com_google_absl//absl/strings:str_format",
        "@com_google_protobuf//:protobuf",
        "@com_google_protobuf//src/google/protobuf/compiler:code_generator",
        "@com_google_protobuf//src/google/protobuf/compiler:retention",
    ],
)

cc_binary(
    name = "file_descriptor_generator",
    srcs = ["file_descriptor_generator_main.cc"],
    visibility = ["//visibility:public"],
    deps = [
        ":file_descriptor_generator_lib",
        "@com_google_protobuf//:protobuf",
        "@com_google_protobuf//:protoc_lib",
        "@com_google_protobuf//src/google/protobuf/compiler:code_generator",
    ],
)

cc_library(
    name = "create_dynamic_message",
    srcs = ["create_dynamic_message.cc"],
    hdrs = ["create_dynamic_message.h"],
    visibility = ["//visibility:public"],
    deps = [
        ":text_format_transcoder",
        "@com_google_absl//absl/memory",
        "@com_google_absl//absl/strings",
        "@com_google_absl//absl/strings:str_format",
        "@com_google_protobuf//:protobuf",
    ],
)
#include "bazel/cc_proto_descriptor_library/file_descriptor_info.h"

#include "absl/strings/escaping.h"
#include "google/protobuf/descriptor.h"
#include "net/proto2/proto/descriptor.proto.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {
namespace internal {

void loadFileDescriptors(const FileDescriptorInfo& descriptor_info,
                         google::protobuf::DescriptorPool* descriptor_pool) {
  if (descriptor_pool->FindFileByName(descriptor_info.file_name)) {
    return;
  }

  for (int i = 0; descriptor_info.deps[i] != nullptr; ++i) {
    loadFileDescriptors(*descriptor_info.deps[i], descriptor_pool);
  }

  google::protobuf::FileDescriptorProto file_descriptor_proto;
  std::string file_descriptor_bytes;
  absl::Base64Unescape(descriptor_info.file_descriptor_bytes_base64, &file_descriptor_bytes);
  file_descriptor_proto.ParseFromString(file_descriptor_bytes);
  descriptor_pool->BuildFile(file_descriptor_proto);
}

} // namespace internal
} // namespace cc_proto_descriptor_library
#pragma once

#include <memory>
#include <string>

#include "absl/strings/string_view.h"
#include "bazel/cc_proto_descriptor_library/create_dynamic_message.h"
#include "google/protobuf/io/tokenizer.h"
#include "google/protobuf/io/zero_copy_stream.h"
#include "google/protobuf/io/zero_copy_stream_impl_lite.h"
#include "google/protobuf/message.h"

// NOLINT(namespace-envoy)
namespace cc_proto_descriptor_library {
namespace internal {

struct FileDescriptorInfo;

} // namespace internal

// Takes a text formatted proto and translates it to a MessageLite proto so it
// can be used by the lite runtime. Requires the that text formatted protos and
// all of its extensions are registered with this object.
class TextFormatTranscoder final {
public:
  explicit TextFormatTranscoder(
      // Whether to allow using the global descriptor pool as a fallback. See
      // text_format_transcoder.cc for more information.
      bool allow_global_fallback = true);
  ~TextFormatTranscoder();

  bool parseInto(absl::string_view text_format, google::protobuf::MessageLite* msg,
                 google::protobuf::io::ErrorCollector* error_collector = nullptr) const;

  bool parseInto(google::protobuf::io::ZeroCopyInputStream* input_stream,
                 google::protobuf::MessageLite* msg,
                 google::protobuf::io::ErrorCollector* error_collector = nullptr) const;

  void loadFileDescriptors(const internal::FileDescriptorInfo& file_descriptor_info);

private:
  // This function needs the member function CreateEmptyDynamicMessage. The
  // target the defines the function has its own visibility whitelist which
  // requires a dance with these declarations.
  friend std::unique_ptr<google::protobuf::Message>
  createDynamicMessage(const TextFormatTranscoder& transcoder,
                       const google::protobuf::MessageLite& message,
                       google::protobuf::io::ErrorCollector* error_collector);

  std::unique_ptr<google::protobuf::Message>
  createEmptyDynamicMessage(absl::string_view type_name,
                            google::protobuf::io::ErrorCollector* error_collector) const;

  bool toBinarySerializationInternal(absl::string_view type_name,
                                     google::protobuf::io::ZeroCopyInputStream* input_stream,
                                     std::string* binary_serializtion,
                                     google::protobuf::io::ErrorCollector* error_collector) const;

  struct InternalData;

  std::unique_ptr<InternalData> internals_;
  const bool allow_global_fallback_;
};

} // namespace cc_proto_descriptor_library
load(":envoy_http_archive.bzl", "envoy_http_archive")
load(":external_deps.bzl", "load_repository_locations")
load(":repository_locations.bzl", "REPOSITORY_LOCATIONS_SPEC")

REPOSITORY_LOCATIONS = load_repository_locations(REPOSITORY_LOCATIONS_SPEC)

# Use this macro to reference any HTTP archive from bazel/repository_locations.bzl.
def external_http_archive(name, **kwargs):
    envoy_http_archive(
        name,
        locations = REPOSITORY_LOCATIONS,
        **kwargs
    )

def api_dependencies():
    external_http_archive(
        name = "bazel_skylib",
    )
    external_http_archive(
        name = "com_envoyproxy_protoc_gen_validate",
        patch_args = ["-p1"],
        patches = ["@envoy//bazel:pgv.patch"],
    )
    external_http_archive(
        name = "com_google_googleapis",
    )

    external_http_archive(
        name = "com_github_cncf_xds",
    )

    # Needed until @com_github_grpc_grpc renames @com_github_cncf_udpa
    # to @com_github_cncf_xds as well.
    external_http_archive(
        name = "com_github_cncf_udpa",
        location_name = "com_github_cncf_xds",
    )

    external_http_archive(
        name = "prometheus_metrics_model",
        build_file_content = PROMETHEUSMETRICS_BUILD_CONTENT,
    )
    external_http_archive(
        name = "opencensus_proto",
    )
    external_http_archive(
        name = "rules_proto",
    )
    external_http_archive(
        name = "com_github_openzipkin_zipkinapi",
        build_file_content = ZIPKINAPI_BUILD_CONTENT,
    )
    external_http_archive(
        name = "opentelemetry_proto",
        build_file_content = OPENTELEMETRY_BUILD_CONTENT,
    )
    external_http_archive(
        name = "com_github_bufbuild_buf",
        build_file_content = BUF_BUILD_CONTENT,
    )

    external_http_archive(
        name = "com_github_chrusty_protoc_gen_jsonschema",
    )

    external_http_archive(
        name = "envoy_toolshed",
    )

PROMETHEUSMETRICS_BUILD_CONTENT = """
load("@envoy_api//bazel:api_build_system.bzl", "api_cc_py_proto_library")
load("@io_bazel_rules_go//proto:def.bzl", "go_proto_library")

api_cc_py_proto_library(
    name = "client_model",
    srcs = [
        "io/prometheus/client/metrics.proto",
    ],
    visibility = ["//visibility:public"],
)

go_proto_library(
    name = "client_model_go_proto",
    importpath = "github.com/prometheus/client_model/go",
    proto = ":client_model",
    visibility = ["//visibility:public"],
)
"""

OPENCENSUSTRACE_BUILD_CONTENT = """
load("@envoy_api//bazel:api_build_system.bzl", "api_cc_py_proto_library")
load("@io_bazel_rules_go//proto:def.bzl", "go_proto_library")

api_cc_py_proto_library(
    name = "trace_model",
    srcs = [
        "trace.proto",
    ],
    visibility = ["//visibility:public"],
)

go_proto_library(
    name = "trace_model_go_proto",
    importpath = "trace_model",
    proto = ":trace_model",
    visibility = ["//visibility:public"],
)
"""

ZIPKINAPI_BUILD_CONTENT = """

load("@envoy_api//bazel:api_build_system.bzl", "api_cc_py_proto_library")
load("@io_bazel_rules_go//proto:def.bzl", "go_proto_library")

api_cc_py_proto_library(
    name = "zipkin",
    srcs = [
        "zipkin-jsonv2.proto",
        "zipkin.proto",
    ],
    visibility = ["//visibility:public"],
)

go_proto_library(
    name = "zipkin_go_proto",
    proto = ":zipkin",
    visibility = ["//visibility:public"],
)
"""

OPENTELEMETRY_BUILD_CONTENT = """
load("@envoy_api//bazel:api_build_system.bzl", "api_cc_py_proto_library")
load("@io_bazel_rules_go//proto:def.bzl", "go_proto_library")

api_cc_py_proto_library(
    name = "common",
    srcs = [
        "opentelemetry/proto/common/v1/common.proto",
    ],
    visibility = ["//visibility:public"],
)

api_cc_py_proto_library(
    name = "resource",
    srcs = [
        "opentelemetry/proto/resource/v1/resource.proto",
    ],
    deps = [
        "//:common",
    ],
    visibility = ["//visibility:public"],
)

go_proto_library(
    name = "common_go_proto",
    importpath = "go.opentelemetry.io/proto/otlp/common/v1",
    proto = ":common",
    visibility = ["//visibility:public"],
)

# TODO(snowp): Generating one Go package from all of these protos could cause problems in the future,
# but nothing references symbols from collector or resource so we're fine for now.
api_cc_py_proto_library(
    name = "logs",
    srcs = [
        "opentelemetry/proto/collector/logs/v1/logs_service.proto",
        "opentelemetry/proto/logs/v1/logs.proto",
    ],
    deps = [
        "//:common",
        "//:resource",
    ],
    visibility = ["//visibility:public"],
)

go_proto_library(
    name = "logs_go_proto",
    importpath = "go.opentelemetry.io/proto/otlp/logs/v1",
    proto = ":logs",
    visibility = ["//visibility:public"],
)

api_cc_py_proto_library(
    name = "metrics",
    srcs = [
        "opentelemetry/proto/collector/metrics/v1/metrics_service.proto",
        "opentelemetry/proto/metrics/v1/metrics.proto",
    ],
    deps = [
        "//:common",
        "//:resource",
    ],
    visibility = ["//visibility:public"],
)

go_proto_library(
    name = "metrics_go_proto",
    importpath = "go.opentelemetry.io/proto/otlp/metrics/v1",
    proto = ":metrics",
    visibility = ["//visibility:public"],
)

api_cc_py_proto_library(
    name = "trace",
    srcs = [
        "opentelemetry/proto/collector/trace/v1/trace_service.proto",
        "opentelemetry/proto/trace/v1/trace.proto",
    ],
    deps = [
        "//:common",
        "//:resource",
    ],
    visibility = ["//visibility:public"],
)
"""

BUF_BUILD_CONTENT = """
package(
    default_visibility = ["//visibility:public"],
)

filegroup(
    name = "buf",
    srcs = [
        "@com_github_bufbuild_buf//:bin/buf",
    ],
    tags = ["manual"], # buf is downloaded as a linux binary; tagged manual to prevent build for non-linux users
)
"""
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@rules_proto//proto:defs.bzl", "proto_descriptor_set", "proto_library")

licenses(["notice"])  # Apache 2

proto_library(
    name = "v2_protos",
    visibility = ["//visibility:public"],
    deps = [
        "//envoy/admin/v2alpha:pkg",
        "//envoy/api/v2:pkg",
        "//envoy/api/v2/auth:pkg",
        "//envoy/api/v2/cluster:pkg",
        "//envoy/api/v2/core:pkg",
        "//envoy/api/v2/endpoint:pkg",
        "//envoy/api/v2/listener:pkg",
        "//envoy/api/v2/ratelimit:pkg",
        "//envoy/api/v2/route:pkg",
        "//envoy/config/bootstrap/v2:pkg",
        "//envoy/config/cluster/dynamic_forward_proxy/v2alpha:pkg",
        "//envoy/config/common/dynamic_forward_proxy/v2alpha:pkg",
        "//envoy/config/common/tap/v2alpha:pkg",
        "//envoy/config/filter/accesslog/v2:pkg",
        "//envoy/config/filter/fault/v2:pkg",
        "//envoy/config/filter/http/compressor/v2:pkg",
        "//envoy/config/filter/http/dynamic_forward_proxy/v2alpha:pkg",
        "//envoy/config/filter/http/gzip/v2:pkg",
        "//envoy/config/filter/http/rate_limit/v2:pkg",
        "//envoy/config/filter/http/rbac/v2:pkg",
        "//envoy/config/filter/network/dubbo_proxy/v2alpha1:pkg",
        "//envoy/config/filter/network/http_connection_manager/v2:pkg",
        "//envoy/config/filter/network/rate_limit/v2:pkg",
        "//envoy/config/filter/network/rbac/v2:pkg",
        "//envoy/config/filter/network/redis_proxy/v2:pkg",
        "//envoy/config/filter/network/tcp_proxy/v2:pkg",
        "//envoy/config/filter/network/thrift_proxy/v2alpha1:pkg",
        "//envoy/config/filter/thrift/rate_limit/v2alpha1:pkg",
        "//envoy/config/filter/thrift/router/v2alpha1:pkg",
        "//envoy/config/health_checker/redis/v2:pkg",
        "//envoy/config/listener/v2:pkg",
        "//envoy/config/metrics/v2:pkg",
        "//envoy/config/overload/v2alpha:pkg",
        "//envoy/config/ratelimit/v2:pkg",
        "//envoy/config/rbac/v2:pkg",
        "//envoy/config/resource_monitor/fixed_heap/v2alpha:pkg",
        "//envoy/config/resource_monitor/injected_resource/v2alpha:pkg",
        "//envoy/config/retry/omit_canary_hosts/v2:pkg",
        "//envoy/config/retry/previous_hosts/v2:pkg",
        "//envoy/config/trace/v2:pkg",
        "//envoy/config/trace/v2alpha:pkg",
        "//envoy/config/transport_socket/alts/v2alpha:pkg",
        "//envoy/config/transport_socket/tap/v2alpha:pkg",
        "//envoy/data/accesslog/v2:pkg",
        "//envoy/data/tap/v2alpha:pkg",
        "//envoy/service/accesslog/v2:pkg",
        "//envoy/service/auth/v2:pkg",
        "//envoy/service/discovery/v2:pkg",
        "//envoy/service/load_stats/v2:pkg",
        "//envoy/service/metrics/v2:pkg",
        "//envoy/service/ratelimit/v2:pkg",
        "//envoy/service/status/v2:pkg",
        "//envoy/service/tap/v2alpha:pkg",
        "//envoy/type:pkg",
        "//envoy/type/matcher:pkg",
        "//envoy/type/metadata/v2:pkg",
        "//envoy/type/tracing/v2:pkg",
    ],
)

proto_library(
    name = "v3_protos",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/envoy/extensions/compression/qatzip/compressor/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/checksum/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/dynamo/v3:pkg",
        "//contrib/envoy/extensions/filters/http/golang/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/language/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/squash/v3:pkg",
        "//contrib/envoy/extensions/filters/http/sxg/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/client_ssl_auth/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/action/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/codecs/dubbo/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/codecs/kafka/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/matcher/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/router/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/v3:pkg",
        "//contrib/envoy/extensions/filters/network/golang/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/kafka_broker/v3:pkg",
        "//contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/mysql_proxy/v3:pkg",
        "//contrib/envoy/extensions/filters/network/postgres_proxy/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/rocketmq_proxy/v3:pkg",
        "//contrib/envoy/extensions/filters/network/sip_proxy/router/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/sip_proxy/tra/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/sip_proxy/v3alpha:pkg",
        "//contrib/envoy/extensions/matching/input_matchers/hyperscan/v3alpha:pkg",
        "//contrib/envoy/extensions/network/connection_balance/dlb/v3alpha:pkg",
        "//contrib/envoy/extensions/private_key_providers/cryptomb/v3alpha:pkg",
        "//contrib/envoy/extensions/private_key_providers/qat/v3alpha:pkg",
        "//contrib/envoy/extensions/regex_engines/hyperscan/v3alpha:pkg",
        "//contrib/envoy/extensions/router/cluster_specifier/golang/v3alpha:pkg",
        "//contrib/envoy/extensions/vcl/v3alpha:pkg",
        "//envoy/admin/v3:pkg",
        "//envoy/config/accesslog/v3:pkg",
        "//envoy/config/bootstrap/v3:pkg",
        "//envoy/config/cluster/v3:pkg",
        "//envoy/config/common/key_value/v3:pkg",
        "//envoy/config/common/matcher/v3:pkg",
        "//envoy/config/common/mutation_rules/v3:pkg",
        "//envoy/config/core/v3:pkg",
        "//envoy/config/endpoint/v3:pkg",
        "//envoy/config/filter/thrift/router/v2alpha1:pkg",
        "//envoy/config/grpc_credential/v3:pkg",
        "//envoy/config/health_checker/redis/v2:pkg",
        "//envoy/config/listener/v3:pkg",
        "//envoy/config/metrics/v3:pkg",
        "//envoy/config/overload/v3:pkg",
        "//envoy/config/ratelimit/v3:pkg",
        "//envoy/config/rbac/v3:pkg",
        "//envoy/config/resource_monitor/fixed_heap/v2alpha:pkg",
        "//envoy/config/resource_monitor/injected_resource/v2alpha:pkg",
        "//envoy/config/retry/omit_canary_hosts/v2:pkg",
        "//envoy/config/retry/previous_hosts/v2:pkg",
        "//envoy/config/route/v3:pkg",
        "//envoy/config/tap/v3:pkg",
        "//envoy/config/trace/v3:pkg",
        "//envoy/config/upstream/local_address_selector/v3:pkg",
        "//envoy/data/accesslog/v3:pkg",
        "//envoy/data/cluster/v3:pkg",
        "//envoy/data/core/v3:pkg",
        "//envoy/data/dns/v3:pkg",
        "//envoy/data/tap/v3:pkg",
        "//envoy/extensions/access_loggers/file/v3:pkg",
        "//envoy/extensions/access_loggers/filters/cel/v3:pkg",
        "//envoy/extensions/access_loggers/grpc/v3:pkg",
        "//envoy/extensions/access_loggers/open_telemetry/v3:pkg",
        "//envoy/extensions/access_loggers/stream/v3:pkg",
        "//envoy/extensions/access_loggers/wasm/v3:pkg",
        "//envoy/extensions/bootstrap/internal_listener/v3:pkg",
        "//envoy/extensions/clusters/aggregate/v3:pkg",
        "//envoy/extensions/clusters/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/clusters/redis/v3:pkg",
        "//envoy/extensions/common/async_files/v3:pkg",
        "//envoy/extensions/common/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/common/matching/v3:pkg",
        "//envoy/extensions/common/ratelimit/v3:pkg",
        "//envoy/extensions/common/tap/v3:pkg",
        "//envoy/extensions/compression/brotli/compressor/v3:pkg",
        "//envoy/extensions/compression/brotli/decompressor/v3:pkg",
        "//envoy/extensions/compression/gzip/compressor/v3:pkg",
        "//envoy/extensions/compression/gzip/decompressor/v3:pkg",
        "//envoy/extensions/compression/zstd/compressor/v3:pkg",
        "//envoy/extensions/compression/zstd/decompressor/v3:pkg",
        "//envoy/extensions/config/validators/minimum_clusters/v3:pkg",
        "//envoy/extensions/early_data/v3:pkg",
        "//envoy/extensions/filters/common/dependency/v3:pkg",
        "//envoy/extensions/filters/common/fault/v3:pkg",
        "//envoy/extensions/filters/common/matcher/action/v3:pkg",
        "//envoy/extensions/filters/common/set_filter_state/v3:pkg",
        "//envoy/extensions/filters/http/adaptive_concurrency/v3:pkg",
        "//envoy/extensions/filters/http/admission_control/v3:pkg",
        "//envoy/extensions/filters/http/alternate_protocols_cache/v3:pkg",
        "//envoy/extensions/filters/http/aws_lambda/v3:pkg",
        "//envoy/extensions/filters/http/aws_request_signing/v3:pkg",
        "//envoy/extensions/filters/http/bandwidth_limit/v3:pkg",
        "//envoy/extensions/filters/http/basic_auth/v3:pkg",
        "//envoy/extensions/filters/http/buffer/v3:pkg",
        "//envoy/extensions/filters/http/cache/v3:pkg",
        "//envoy/extensions/filters/http/cdn_loop/v3:pkg",
        "//envoy/extensions/filters/http/composite/v3:pkg",
        "//envoy/extensions/filters/http/compressor/v3:pkg",
        "//envoy/extensions/filters/http/connect_grpc_bridge/v3:pkg",
        "//envoy/extensions/filters/http/cors/v3:pkg",
        "//envoy/extensions/filters/http/credential_injector/v3:pkg",
        "//envoy/extensions/filters/http/csrf/v3:pkg",
        "//envoy/extensions/filters/http/custom_response/v3:pkg",
        "//envoy/extensions/filters/http/decompressor/v3:pkg",
        "//envoy/extensions/filters/http/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/filters/http/ext_authz/v3:pkg",
        "//envoy/extensions/filters/http/ext_proc/v3:pkg",
        "//envoy/extensions/filters/http/fault/v3:pkg",
        "//envoy/extensions/filters/http/file_system_buffer/v3:pkg",
        "//envoy/extensions/filters/http/gcp_authn/v3:pkg",
        "//envoy/extensions/filters/http/geoip/v3:pkg",
        "//envoy/extensions/filters/http/grpc_field_extraction/v3:pkg",
        "//envoy/extensions/filters/http/grpc_http1_bridge/v3:pkg",
        "//envoy/extensions/filters/http/grpc_http1_reverse_bridge/v3:pkg",
        "//envoy/extensions/filters/http/grpc_json_transcoder/v3:pkg",
        "//envoy/extensions/filters/http/grpc_stats/v3:pkg",
        "//envoy/extensions/filters/http/grpc_web/v3:pkg",
        "//envoy/extensions/filters/http/gzip/v3:pkg",
        "//envoy/extensions/filters/http/header_mutation/v3:pkg",
        "//envoy/extensions/filters/http/header_to_metadata/v3:pkg",
        "//envoy/extensions/filters/http/health_check/v3:pkg",
        "//envoy/extensions/filters/http/ip_tagging/v3:pkg",
        "//envoy/extensions/filters/http/json_to_metadata/v3:pkg",
        "//envoy/extensions/filters/http/jwt_authn/v3:pkg",
        "//envoy/extensions/filters/http/kill_request/v3:pkg",
        "//envoy/extensions/filters/http/local_ratelimit/v3:pkg",
        "//envoy/extensions/filters/http/lua/v3:pkg",
        "//envoy/extensions/filters/http/oauth2/v3:pkg",
        "//envoy/extensions/filters/http/on_demand/v3:pkg",
        "//envoy/extensions/filters/http/original_src/v3:pkg",
        "//envoy/extensions/filters/http/proto_message_logging/v3:pkg",
        "//envoy/extensions/filters/http/rate_limit_quota/v3:pkg",
        "//envoy/extensions/filters/http/ratelimit/v3:pkg",
        "//envoy/extensions/filters/http/rbac/v3:pkg",
        "//envoy/extensions/filters/http/router/v3:pkg",
        "//envoy/extensions/filters/http/set_filter_state/v3:pkg",
        "//envoy/extensions/filters/http/set_metadata/v3:pkg",
        "//envoy/extensions/filters/http/stateful_session/v3:pkg",
        "//envoy/extensions/filters/http/tap/v3:pkg",
        "//envoy/extensions/filters/http/upstream_codec/v3:pkg",
        "//envoy/extensions/filters/http/wasm/v3:pkg",
        "//envoy/extensions/filters/listener/http_inspector/v3:pkg",
        "//envoy/extensions/filters/listener/local_ratelimit/v3:pkg",
        "//envoy/extensions/filters/listener/original_dst/v3:pkg",
        "//envoy/extensions/filters/listener/original_src/v3:pkg",
        "//envoy/extensions/filters/listener/proxy_protocol/v3:pkg",
        "//envoy/extensions/filters/listener/tls_inspector/v3:pkg",
        "//envoy/extensions/filters/network/connection_limit/v3:pkg",
        "//envoy/extensions/filters/network/direct_response/v3:pkg",
        "//envoy/extensions/filters/network/dubbo_proxy/router/v3:pkg",
        "//envoy/extensions/filters/network/dubbo_proxy/v3:pkg",
        "//envoy/extensions/filters/network/echo/v3:pkg",
        "//envoy/extensions/filters/network/ext_authz/v3:pkg",
        "//envoy/extensions/filters/network/http_connection_manager/v3:pkg",
        "//envoy/extensions/filters/network/local_ratelimit/v3:pkg",
        "//envoy/extensions/filters/network/mongo_proxy/v3:pkg",
        "//envoy/extensions/filters/network/ratelimit/v3:pkg",
        "//envoy/extensions/filters/network/rbac/v3:pkg",
        "//envoy/extensions/filters/network/redis_proxy/v3:pkg",
        "//envoy/extensions/filters/network/set_filter_state/v3:pkg",
        "//envoy/extensions/filters/network/sni_cluster/v3:pkg",
        "//envoy/extensions/filters/network/sni_dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/filters/network/tcp_proxy/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/filters/header_to_metadata/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/filters/payload_to_metadata/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/filters/ratelimit/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/router/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/v3:pkg",
        "//envoy/extensions/filters/network/wasm/v3:pkg",
        "//envoy/extensions/filters/network/zookeeper_proxy/v3:pkg",
        "//envoy/extensions/filters/udp/dns_filter/v3:pkg",
        "//envoy/extensions/filters/udp/udp_proxy/session/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/filters/udp/udp_proxy/session/http_capsule/v3:pkg",
        "//envoy/extensions/filters/udp/udp_proxy/v3:pkg",
        "//envoy/extensions/formatter/cel/v3:pkg",
        "//envoy/extensions/formatter/metadata/v3:pkg",
        "//envoy/extensions/formatter/req_without_query/v3:pkg",
        "//envoy/extensions/geoip_providers/common/v3:pkg",
        "//envoy/extensions/geoip_providers/maxmind/v3:pkg",
        "//envoy/extensions/health_check/event_sinks/file/v3:pkg",
        "//envoy/extensions/health_checkers/redis/v3:pkg",
        "//envoy/extensions/health_checkers/thrift/v3:pkg",
        "//envoy/extensions/http/cache/file_system_http_cache/v3:pkg",
        "//envoy/extensions/http/cache/simple_http_cache/v3:pkg",
        "//envoy/extensions/http/custom_response/local_response_policy/v3:pkg",
        "//envoy/extensions/http/custom_response/redirect_policy/v3:pkg",
        "//envoy/extensions/http/early_header_mutation/header_mutation/v3:pkg",
        "//envoy/extensions/http/header_formatters/preserve_case/v3:pkg",
        "//envoy/extensions/http/header_validators/envoy_default/v3:pkg",
        "//envoy/extensions/http/original_ip_detection/custom_header/v3:pkg",
        "//envoy/extensions/http/original_ip_detection/xff/v3:pkg",
        "//envoy/extensions/http/stateful_session/cookie/v3:pkg",
        "//envoy/extensions/http/stateful_session/header/v3:pkg",
        "//envoy/extensions/injected_credentials/generic/v3:pkg",
        "//envoy/extensions/injected_credentials/oauth2/v3:pkg",
        "//envoy/extensions/internal_redirect/allow_listed_routes/v3:pkg",
        "//envoy/extensions/internal_redirect/previous_routes/v3:pkg",
        "//envoy/extensions/internal_redirect/safe_cross_scheme/v3:pkg",
        "//envoy/extensions/key_value/file_based/v3:pkg",
        "//envoy/extensions/load_balancing_policies/cluster_provided/v3:pkg",
        "//envoy/extensions/load_balancing_policies/common/v3:pkg",
        "//envoy/extensions/load_balancing_policies/least_request/v3:pkg",
        "//envoy/extensions/load_balancing_policies/maglev/v3:pkg",
        "//envoy/extensions/load_balancing_policies/pick_first/v3:pkg",
        "//envoy/extensions/load_balancing_policies/random/v3:pkg",
        "//envoy/extensions/load_balancing_policies/ring_hash/v3:pkg",
        "//envoy/extensions/load_balancing_policies/round_robin/v3:pkg",
        "//envoy/extensions/load_balancing_policies/subset/v3:pkg",
        "//envoy/extensions/load_balancing_policies/wrr_locality/v3:pkg",
        "//envoy/extensions/matching/common_inputs/environment_variable/v3:pkg",
        "//envoy/extensions/matching/common_inputs/network/v3:pkg",
        "//envoy/extensions/matching/common_inputs/ssl/v3:pkg",
        "//envoy/extensions/matching/input_matchers/consistent_hashing/v3:pkg",
        "//envoy/extensions/matching/input_matchers/ip/v3:pkg",
        "//envoy/extensions/matching/input_matchers/runtime_fraction/v3:pkg",
        "//envoy/extensions/network/dns_resolver/apple/v3:pkg",
        "//envoy/extensions/network/dns_resolver/cares/v3:pkg",
        "//envoy/extensions/network/dns_resolver/getaddrinfo/v3:pkg",
        "//envoy/extensions/network/socket_interface/v3:pkg",
        "//envoy/extensions/path/match/uri_template/v3:pkg",
        "//envoy/extensions/path/rewrite/uri_template/v3:pkg",
        "//envoy/extensions/quic/connection_id_generator/v3:pkg",
        "//envoy/extensions/quic/crypto_stream/v3:pkg",
        "//envoy/extensions/quic/proof_source/v3:pkg",
        "//envoy/extensions/quic/server_preferred_address/v3:pkg",
        "//envoy/extensions/rate_limit_descriptors/expr/v3:pkg",
        "//envoy/extensions/rbac/audit_loggers/stream/v3:pkg",
        "//envoy/extensions/rbac/matchers/upstream_ip_port/v3:pkg",
        "//envoy/extensions/regex_engines/v3:pkg",
        "//envoy/extensions/request_id/uuid/v3:pkg",
        "//envoy/extensions/resource_monitors/downstream_connections/v3:pkg",
        "//envoy/extensions/resource_monitors/fixed_heap/v3:pkg",
        "//envoy/extensions/resource_monitors/injected_resource/v3:pkg",
        "//envoy/extensions/retry/host/omit_canary_hosts/v3:pkg",
        "//envoy/extensions/retry/host/omit_host_metadata/v3:pkg",
        "//envoy/extensions/retry/host/previous_hosts/v3:pkg",
        "//envoy/extensions/retry/priority/previous_priorities/v3:pkg",
        "//envoy/extensions/router/cluster_specifiers/lua/v3:pkg",
        "//envoy/extensions/stat_sinks/graphite_statsd/v3:pkg",
        "//envoy/extensions/stat_sinks/open_telemetry/v3:pkg",
        "//envoy/extensions/stat_sinks/wasm/v3:pkg",
        "//envoy/extensions/tracers/opentelemetry/resource_detectors/v3:pkg",
        "//envoy/extensions/tracers/opentelemetry/samplers/v3:pkg",
        "//envoy/extensions/transport_sockets/alts/v3:pkg",
        "//envoy/extensions/transport_sockets/http_11_proxy/v3:pkg",
        "//envoy/extensions/transport_sockets/internal_upstream/v3:pkg",
        "//envoy/extensions/transport_sockets/proxy_protocol/v3:pkg",
        "//envoy/extensions/transport_sockets/quic/v3:pkg",
        "//envoy/extensions/transport_sockets/raw_buffer/v3:pkg",
        "//envoy/extensions/transport_sockets/s2a/v3:pkg",
        "//envoy/extensions/transport_sockets/starttls/v3:pkg",
        "//envoy/extensions/transport_sockets/tap/v3:pkg",
        "//envoy/extensions/transport_sockets/tcp_stats/v3:pkg",
        "//envoy/extensions/transport_sockets/tls/v3:pkg",
        "//envoy/extensions/udp_packet_writer/v3:pkg",
        "//envoy/extensions/upstreams/http/generic/v3:pkg",
        "//envoy/extensions/upstreams/http/http/v3:pkg",
        "//envoy/extensions/upstreams/http/tcp/v3:pkg",
        "//envoy/extensions/upstreams/http/udp/v3:pkg",
        "//envoy/extensions/upstreams/http/v3:pkg",
        "//envoy/extensions/upstreams/tcp/generic/v3:pkg",
        "//envoy/extensions/upstreams/tcp/v3:pkg",
        "//envoy/extensions/wasm/v3:pkg",
        "//envoy/extensions/watchdog/profile_action/v3:pkg",
        "//envoy/service/accesslog/v3:pkg",
        "//envoy/service/auth/v3:pkg",
        "//envoy/service/cluster/v3:pkg",
        "//envoy/service/discovery/v3:pkg",
        "//envoy/service/endpoint/v3:pkg",
        "//envoy/service/event_reporting/v3:pkg",
        "//envoy/service/ext_proc/v3:pkg",
        "//envoy/service/extension/v3:pkg",
        "//envoy/service/health/v3:pkg",
        "//envoy/service/listener/v3:pkg",
        "//envoy/service/load_stats/v3:pkg",
        "//envoy/service/metrics/v3:pkg",
        "//envoy/service/rate_limit_quota/v3:pkg",
        "//envoy/service/ratelimit/v3:pkg",
        "//envoy/service/route/v3:pkg",
        "//envoy/service/runtime/v3:pkg",
        "//envoy/service/secret/v3:pkg",
        "//envoy/service/status/v3:pkg",
        "//envoy/service/tap/v3:pkg",
        "//envoy/service/trace/v3:pkg",
        "//envoy/type/http/v3:pkg",
        "//envoy/type/matcher/v3:pkg",
        "//envoy/type/metadata/v3:pkg",
        "//envoy/type/tracing/v3:pkg",
        "//envoy/type/v3:pkg",
        "//envoy/watchdog/v3:pkg",
    ],
)

proto_library(
    name = "xds_protos",
    visibility = ["//visibility:public"],
    deps = [
        "@com_github_cncf_xds//xds/core/v3:pkg",
        "@com_github_cncf_xds//xds/type/matcher/v3:pkg",
        "@com_github_cncf_xds//xds/type/v3:pkg",
    ],
)

proto_library(
    name = "all_protos",
    visibility = ["//visibility:public"],
    deps = [
        ":v2_protos",
        ":v3_protos",
        ":xds_protos",
    ],
)

filegroup(
    name = "proto_breaking_change_detector_buf_config",
    srcs = [
        "buf.yaml",
    ],
    visibility = ["//visibility:public"],
)

proto_descriptor_set(
    name = "v3_proto_set",
    visibility = ["//visibility:public"],
    deps = [
        ":v3_protos",
        ":xds_protos",
    ],
)
# API Review Checklist

This checklist is intended to be used when reviewing xDS API changes.
Users who wish to contribute API changes should read this and proactively
consider the answers to these questions before sending a PR.

## Feature Enablement
- Are the default values going to cause behavior changes for existing users
  who do not know about the change and have not updated the resources being
  served by their control plane?
  - If yes, do we have some estimate of how many users will be affected?
  - Why is it justified to change the default behavior, rather than making
    this feature opt-in?
    - Some possible justifications include security concerns with existing
      behavior, or a desire to eliminate legacy behavior.
  - What is the plan to make this change in a safe way?  For example, is the
    transition going to be staged over the course of several minor xDS versions?
  - How will we warn users about this change?
    - Possible ways to do this include release notes, announcements, warnings
      from the code, etc.
- Will users have a way to disable this change if it causes problems?
  - If not, why do we think that's okay?  (It might be the case that we think
    it will not actually affect anyone, or no one will care.)
  - If so, is the mechanism to disable it part of the xDS API, or is it
    acceptable to have a separate knob for this in the client?  (See also
    "Genericness" below -- if this is not part of the API, will every xDS
    client need to add a different knob?  Is consistency across clients
    important for this?)
- If the feature is modeled as a proto3 scalar, is it plausible that its
  default value may change in the future? If so, it should be wrapped with
  a Well-Known Type (WKT), e.g. `bool` becomes `google.protobuf.BoolValue`.

## Style
- Is the PR aligned with the [API style guidelines](STYLE.md)?

## Validation Rules
- Does the field have protocgen-validate rules to enforce constraints on
  its value?
- Is the field mandatory? Does it have the required rule?
- Is the field numeric? Is it bounded correctly?
- Is the field repeated? Does it have the correct min/max items numbers? Are
  the values unique?
- If a field may eventually be repeated but initially there is a desire to
  cap it to a single value, consider using repeated with a max length of 1,
  which is easier to relax in the future.

## Deprecations
- When a field or enum value is deprecated, according to the minor/patch
  versioning policy this implies a minor version for support removal. Is the
  work necessary to add support for the newer replacement field acceptable to
  known xDS clients in this time frame?
- No deprecations are allowed when an alternative is "not ready yet" in any
  major known xDS client (Envoy or gRPC at this point), unless the
  maintainers of that xDS client have signed off on the change. If you are not
  sure about the current state of a feature in the major known xDS clients,
  ask one of the API shepherds.
- Is this deprecated field documented in a way that points to the preferred
  newer method of configuration?

## Extensibility
- Is this feature being directly added into the API itself, or is it being
  introduced via an extension point (i.e., using `TypedExtensionConfig`)?
- If not via an extension point, why not?
- If no appropriate extension point currently exists, is this a good
  opportunity to add one?  Can we move some existing "core" functionality
  into a set of standardized plugins for an extension point?
- Do we have good documentation showing what to plug into the extension point?
  (At minimum, it should have a comment pointing to the common protos to
  be plugged into the extension point.)
- If an enum is being introduced, should this be a oneof with empty messages
  for future API growth?
- When a new field is introduced for a distinct purpose, should this be a
  message to allow for future API growth?

## Consistency
- Can the proposed API reuse part or all of other APIs?
  - Can some other API be refactored to be part of it, or vice versa?
  - Example: Can it use common types such as matcher or number?
- Are there similar structures that already exist?
- Is the naming convention consistent with other APIs?
- If there are new proto messages being added, are they in the right
  place in the tree? Consider not just the current users of this proto
  but also other possible uses in the future. Would it make more sense
  to make the proto a generic type and put it in a common location, so
  that it can be used in other places in the future?

## Interactions With Other Features
- Will this feature interact in strange ways with other features, either
  existing or planned?
  - For example, if you are defining a new cluster type, how will the
    new type implement all of the features currently configured via CDS?
  - If this is a change in the upstream side of the API, will it work properly
    with LRS load reporting?
- Will there be combinations of features that won't work properly?  If so,
  please document each combination that won't work and justify why this is
  okay. Is there some other way to structure this feature that would not
  cause conflicts with other features?
- If this change involves extension configuration, how will it interact
  with ECDS?

## Genericness
- Is this an Envoy-specific or proxy-specific feature? How will it apply to
  xDS clients other than Envoy (e.g., gRPC)?

## Dependencies
- Does this feature pull in any new dependencies for clients?
- Are these dependencies optional or required?
- Will the dependencies cause problems for any known xDS clients (e.g.,
  [Envoy's dependency policy](https://github.com/envoyproxy/envoy/blob/main/DEPENDENCY_POLICY.md))?

## Failure Modes
- What is the failure mode if this feature is configured but is not working
  for some reason?
- Is the failure mode what users will expect/want?
- Is this failure mode specified in the API, or is each client expected to
  handle it on its own?  Consistency across clients is preferred; if there's
  a reason this isn't feasible, please explain.

## Scalability
- Does this feature add new per-request functionality?  How much overhead does
  it add on the per-request path?
- Are there ways that the API could be structured differently to make it
  possible to implement the feature in more efficient ways?  (Even if this
  efficiency is not needed now, it may be something we will need in the future,
  and we will save pain in the long run if we structure the API in a way that
  gives us the flexibility to change the implementation later.)
- How does this feature affect config size? For example, instead of
  adding a huge mandatory proto to every single route entry, consider
  ways of setting it at the virtual host level and then overriding only
  the parts that change on a per-route basis.
- Will the change require multiple round trips via the REST API?

## Monitoring
- Is there any behavior associated with this feature that will require
  monitoring?
- How will the data be exposed to monitoring?
- Is the monitoring configuration part of the xDS API, or is it client-specific?

## Documentation
- Can a user look at the docs and understand it without a bunch of extra
  context?
- Pay special attention to documentation around extensions and `typed_config`.
  Users generally find this extremely confusing. There should be examples
  showing how to configure extension points and optimally all known public
  extensions (there is tooling work in progress to automate this).
- Larger features should contain architecture overview documentation with
  relevant cross-linking.
- Relevant differences between clients need to be documented (in the future
  we will build tooling to allow for common documentation as well as per-client
  documentation).

## Where to Put New Protos
- The xDS API is currently partly in the Envoy repo and partly in the
  cncf/xds repo. We will move pieces of the API to the latter repo
  slowly over time, as they become less Envoy-specific, until eventually
  the whole API has been moved there. If your change involves adding
  new protos, they should generally go in the new repo.
# DO NOT EDIT. This file is generated by tools/proto_format/proto_sync.py.

load("@rules_proto//proto:defs.bzl", "proto_library")

licenses(["notice"])  # Apache 2

# This tracks active development versions of protos.
proto_library(
    name = "active_protos",
    visibility = ["//visibility:public"],
    deps = [
        "//contrib/envoy/extensions/compression/qatzip/compressor/v3alpha:pkg",
        "//contrib/envoy/extensions/config/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/checksum/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/dynamo/v3:pkg",
        "//contrib/envoy/extensions/filters/http/golang/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/language/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/http/squash/v3:pkg",
        "//contrib/envoy/extensions/filters/http/sxg/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/client_ssl_auth/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/action/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/codecs/dubbo/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/codecs/kafka/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/matcher/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/router/v3:pkg",
        "//contrib/envoy/extensions/filters/network/generic_proxy/v3:pkg",
        "//contrib/envoy/extensions/filters/network/golang/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/kafka_broker/v3:pkg",
        "//contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/mysql_proxy/v3:pkg",
        "//contrib/envoy/extensions/filters/network/postgres_proxy/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/rocketmq_proxy/v3:pkg",
        "//contrib/envoy/extensions/filters/network/sip_proxy/router/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/sip_proxy/tra/v3alpha:pkg",
        "//contrib/envoy/extensions/filters/network/sip_proxy/v3alpha:pkg",
        "//contrib/envoy/extensions/matching/input_matchers/hyperscan/v3alpha:pkg",
        "//contrib/envoy/extensions/network/connection_balance/dlb/v3alpha:pkg",
        "//contrib/envoy/extensions/private_key_providers/cryptomb/v3alpha:pkg",
        "//contrib/envoy/extensions/private_key_providers/qat/v3alpha:pkg",
        "//contrib/envoy/extensions/regex_engines/hyperscan/v3alpha:pkg",
        "//contrib/envoy/extensions/router/cluster_specifier/golang/v3alpha:pkg",
        "//contrib/envoy/extensions/vcl/v3alpha:pkg",
        "//envoy/admin/v3:pkg",
        "//envoy/config/accesslog/v3:pkg",
        "//envoy/config/bootstrap/v3:pkg",
        "//envoy/config/cluster/v3:pkg",
        "//envoy/config/common/key_value/v3:pkg",
        "//envoy/config/common/matcher/v3:pkg",
        "//envoy/config/common/mutation_rules/v3:pkg",
        "//envoy/config/core/v3:pkg",
        "//envoy/config/endpoint/v3:pkg",
        "//envoy/config/filter/thrift/router/v2alpha1:pkg",
        "//envoy/config/grpc_credential/v3:pkg",
        "//envoy/config/health_checker/redis/v2:pkg",
        "//envoy/config/listener/v3:pkg",
        "//envoy/config/metrics/v3:pkg",
        "//envoy/config/overload/v3:pkg",
        "//envoy/config/ratelimit/v3:pkg",
        "//envoy/config/rbac/v3:pkg",
        "//envoy/config/resource_monitor/fixed_heap/v2alpha:pkg",
        "//envoy/config/resource_monitor/injected_resource/v2alpha:pkg",
        "//envoy/config/retry/omit_canary_hosts/v2:pkg",
        "//envoy/config/retry/previous_hosts/v2:pkg",
        "//envoy/config/route/v3:pkg",
        "//envoy/config/tap/v3:pkg",
        "//envoy/config/trace/v3:pkg",
        "//envoy/config/upstream/local_address_selector/v3:pkg",
        "//envoy/data/accesslog/v3:pkg",
        "//envoy/data/cluster/v3:pkg",
        "//envoy/data/core/v3:pkg",
        "//envoy/data/dns/v3:pkg",
        "//envoy/data/tap/v3:pkg",
        "//envoy/extensions/access_loggers/file/v3:pkg",
        "//envoy/extensions/access_loggers/filters/cel/v3:pkg",
        "//envoy/extensions/access_loggers/grpc/v3:pkg",
        "//envoy/extensions/access_loggers/open_telemetry/v3:pkg",
        "//envoy/extensions/access_loggers/stream/v3:pkg",
        "//envoy/extensions/access_loggers/wasm/v3:pkg",
        "//envoy/extensions/bootstrap/internal_listener/v3:pkg",
        "//envoy/extensions/clusters/aggregate/v3:pkg",
        "//envoy/extensions/clusters/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/clusters/redis/v3:pkg",
        "//envoy/extensions/common/async_files/v3:pkg",
        "//envoy/extensions/common/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/common/matching/v3:pkg",
        "//envoy/extensions/common/ratelimit/v3:pkg",
        "//envoy/extensions/common/tap/v3:pkg",
        "//envoy/extensions/compression/brotli/compressor/v3:pkg",
        "//envoy/extensions/compression/brotli/decompressor/v3:pkg",
        "//envoy/extensions/compression/gzip/compressor/v3:pkg",
        "//envoy/extensions/compression/gzip/decompressor/v3:pkg",
        "//envoy/extensions/compression/zstd/compressor/v3:pkg",
        "//envoy/extensions/compression/zstd/decompressor/v3:pkg",
        "//envoy/extensions/config/validators/minimum_clusters/v3:pkg",
        "//envoy/extensions/early_data/v3:pkg",
        "//envoy/extensions/filters/common/dependency/v3:pkg",
        "//envoy/extensions/filters/common/fault/v3:pkg",
        "//envoy/extensions/filters/common/matcher/action/v3:pkg",
        "//envoy/extensions/filters/common/set_filter_state/v3:pkg",
        "//envoy/extensions/filters/http/adaptive_concurrency/v3:pkg",
        "//envoy/extensions/filters/http/admission_control/v3:pkg",
        "//envoy/extensions/filters/http/alternate_protocols_cache/v3:pkg",
        "//envoy/extensions/filters/http/aws_lambda/v3:pkg",
        "//envoy/extensions/filters/http/aws_request_signing/v3:pkg",
        "//envoy/extensions/filters/http/bandwidth_limit/v3:pkg",
        "//envoy/extensions/filters/http/basic_auth/v3:pkg",
        "//envoy/extensions/filters/http/buffer/v3:pkg",
        "//envoy/extensions/filters/http/cache/v3:pkg",
        "//envoy/extensions/filters/http/cdn_loop/v3:pkg",
        "//envoy/extensions/filters/http/composite/v3:pkg",
        "//envoy/extensions/filters/http/compressor/v3:pkg",
        "//envoy/extensions/filters/http/connect_grpc_bridge/v3:pkg",
        "//envoy/extensions/filters/http/cors/v3:pkg",
        "//envoy/extensions/filters/http/credential_injector/v3:pkg",
        "//envoy/extensions/filters/http/csrf/v3:pkg",
        "//envoy/extensions/filters/http/custom_response/v3:pkg",
        "//envoy/extensions/filters/http/decompressor/v3:pkg",
        "//envoy/extensions/filters/http/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/filters/http/ext_authz/v3:pkg",
        "//envoy/extensions/filters/http/ext_proc/v3:pkg",
        "//envoy/extensions/filters/http/fault/v3:pkg",
        "//envoy/extensions/filters/http/file_system_buffer/v3:pkg",
        "//envoy/extensions/filters/http/gcp_authn/v3:pkg",
        "//envoy/extensions/filters/http/geoip/v3:pkg",
        "//envoy/extensions/filters/http/grpc_field_extraction/v3:pkg",
        "//envoy/extensions/filters/http/grpc_http1_bridge/v3:pkg",
        "//envoy/extensions/filters/http/grpc_http1_reverse_bridge/v3:pkg",
        "//envoy/extensions/filters/http/grpc_json_transcoder/v3:pkg",
        "//envoy/extensions/filters/http/grpc_stats/v3:pkg",
        "//envoy/extensions/filters/http/grpc_web/v3:pkg",
        "//envoy/extensions/filters/http/gzip/v3:pkg",
        "//envoy/extensions/filters/http/header_mutation/v3:pkg",
        "//envoy/extensions/filters/http/header_to_metadata/v3:pkg",
        "//envoy/extensions/filters/http/health_check/v3:pkg",
        "//envoy/extensions/filters/http/ip_tagging/v3:pkg",
        "//envoy/extensions/filters/http/json_to_metadata/v3:pkg",
        "//envoy/extensions/filters/http/jwt_authn/v3:pkg",
        "//envoy/extensions/filters/http/kill_request/v3:pkg",
        "//envoy/extensions/filters/http/local_ratelimit/v3:pkg",
        "//envoy/extensions/filters/http/lua/v3:pkg",
        "//envoy/extensions/filters/http/oauth2/v3:pkg",
        "//envoy/extensions/filters/http/on_demand/v3:pkg",
        "//envoy/extensions/filters/http/original_src/v3:pkg",
        "//envoy/extensions/filters/http/proto_message_logging/v3:pkg",
        "//envoy/extensions/filters/http/rate_limit_quota/v3:pkg",
        "//envoy/extensions/filters/http/ratelimit/v3:pkg",
        "//envoy/extensions/filters/http/rbac/v3:pkg",
        "//envoy/extensions/filters/http/router/v3:pkg",
        "//envoy/extensions/filters/http/set_filter_state/v3:pkg",
        "//envoy/extensions/filters/http/set_metadata/v3:pkg",
        "//envoy/extensions/filters/http/stateful_session/v3:pkg",
        "//envoy/extensions/filters/http/tap/v3:pkg",
        "//envoy/extensions/filters/http/upstream_codec/v3:pkg",
        "//envoy/extensions/filters/http/wasm/v3:pkg",
        "//envoy/extensions/filters/listener/http_inspector/v3:pkg",
        "//envoy/extensions/filters/listener/local_ratelimit/v3:pkg",
        "//envoy/extensions/filters/listener/original_dst/v3:pkg",
        "//envoy/extensions/filters/listener/original_src/v3:pkg",
        "//envoy/extensions/filters/listener/proxy_protocol/v3:pkg",
        "//envoy/extensions/filters/listener/tls_inspector/v3:pkg",
        "//envoy/extensions/filters/network/connection_limit/v3:pkg",
        "//envoy/extensions/filters/network/direct_response/v3:pkg",
        "//envoy/extensions/filters/network/dubbo_proxy/router/v3:pkg",
        "//envoy/extensions/filters/network/dubbo_proxy/v3:pkg",
        "//envoy/extensions/filters/network/echo/v3:pkg",
        "//envoy/extensions/filters/network/ext_authz/v3:pkg",
        "//envoy/extensions/filters/network/http_connection_manager/v3:pkg",
        "//envoy/extensions/filters/network/local_ratelimit/v3:pkg",
        "//envoy/extensions/filters/network/mongo_proxy/v3:pkg",
        "//envoy/extensions/filters/network/ratelimit/v3:pkg",
        "//envoy/extensions/filters/network/rbac/v3:pkg",
        "//envoy/extensions/filters/network/redis_proxy/v3:pkg",
        "//envoy/extensions/filters/network/set_filter_state/v3:pkg",
        "//envoy/extensions/filters/network/sni_cluster/v3:pkg",
        "//envoy/extensions/filters/network/sni_dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/filters/network/tcp_proxy/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/filters/header_to_metadata/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/filters/payload_to_metadata/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/filters/ratelimit/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/router/v3:pkg",
        "//envoy/extensions/filters/network/thrift_proxy/v3:pkg",
        "//envoy/extensions/filters/network/wasm/v3:pkg",
        "//envoy/extensions/filters/network/zookeeper_proxy/v3:pkg",
        "//envoy/extensions/filters/udp/dns_filter/v3:pkg",
        "//envoy/extensions/filters/udp/udp_proxy/session/dynamic_forward_proxy/v3:pkg",
        "//envoy/extensions/filters/udp/udp_proxy/session/http_capsule/v3:pkg",
        "//envoy/extensions/filters/udp/udp_proxy/v3:pkg",
        "//envoy/extensions/formatter/cel/v3:pkg",
        "//envoy/extensions/formatter/metadata/v3:pkg",
        "//envoy/extensions/formatter/req_without_query/v3:pkg",
        "//envoy/extensions/geoip_providers/common/v3:pkg",
        "//envoy/extensions/geoip_providers/maxmind/v3:pkg",
        "//envoy/extensions/health_check/event_sinks/file/v3:pkg",
        "//envoy/extensions/health_checkers/redis/v3:pkg",
        "//envoy/extensions/health_checkers/thrift/v3:pkg",
        "//envoy/extensions/http/cache/file_system_http_cache/v3:pkg",
        "//envoy/extensions/http/cache/simple_http_cache/v3:pkg",
        "//envoy/extensions/http/custom_response/local_response_policy/v3:pkg",
        "//envoy/extensions/http/custom_response/redirect_policy/v3:pkg",
        "//envoy/extensions/http/early_header_mutation/header_mutation/v3:pkg",
        "//envoy/extensions/http/header_formatters/preserve_case/v3:pkg",
        "//envoy/extensions/http/header_validators/envoy_default/v3:pkg",
        "//envoy/extensions/http/original_ip_detection/custom_header/v3:pkg",
        "//envoy/extensions/http/original_ip_detection/xff/v3:pkg",
        "//envoy/extensions/http/stateful_session/cookie/v3:pkg",
        "//envoy/extensions/http/stateful_session/header/v3:pkg",
        "//envoy/extensions/injected_credentials/generic/v3:pkg",
        "//envoy/extensions/injected_credentials/oauth2/v3:pkg",
        "//envoy/extensions/internal_redirect/allow_listed_routes/v3:pkg",
        "//envoy/extensions/internal_redirect/previous_routes/v3:pkg",
        "//envoy/extensions/internal_redirect/safe_cross_scheme/v3:pkg",
        "//envoy/extensions/key_value/file_based/v3:pkg",
        "//envoy/extensions/load_balancing_policies/client_side_weighted_round_robin/v3:pkg",
        "//envoy/extensions/load_balancing_policies/cluster_provided/v3:pkg",
        "//envoy/extensions/load_balancing_policies/common/v3:pkg",
        "//envoy/extensions/load_balancing_policies/least_request/v3:pkg",
        "//envoy/extensions/load_balancing_policies/maglev/v3:pkg",
        "//envoy/extensions/load_balancing_policies/pick_first/v3:pkg",
        "//envoy/extensions/load_balancing_policies/random/v3:pkg",
        "//envoy/extensions/load_balancing_policies/ring_hash/v3:pkg",
        "//envoy/extensions/load_balancing_policies/round_robin/v3:pkg",
        "//envoy/extensions/load_balancing_policies/subset/v3:pkg",
        "//envoy/extensions/load_balancing_policies/wrr_locality/v3:pkg",
        "//envoy/extensions/matching/common_inputs/environment_variable/v3:pkg",
        "//envoy/extensions/matching/common_inputs/network/v3:pkg",
        "//envoy/extensions/matching/common_inputs/ssl/v3:pkg",
        "//envoy/extensions/matching/input_matchers/consistent_hashing/v3:pkg",
        "//envoy/extensions/matching/input_matchers/ip/v3:pkg",
        "//envoy/extensions/matching/input_matchers/runtime_fraction/v3:pkg",
        "//envoy/extensions/network/dns_resolver/apple/v3:pkg",
        "//envoy/extensions/network/dns_resolver/cares/v3:pkg",
        "//envoy/extensions/network/dns_resolver/getaddrinfo/v3:pkg",
        "//envoy/extensions/network/socket_interface/v3:pkg",
        "//envoy/extensions/path/match/uri_template/v3:pkg",
        "//envoy/extensions/path/rewrite/uri_template/v3:pkg",
        "//envoy/extensions/quic/connection_id_generator/v3:pkg",
        "//envoy/extensions/quic/crypto_stream/v3:pkg",
        "//envoy/extensions/quic/proof_source/v3:pkg",
        "//envoy/extensions/quic/server_preferred_address/v3:pkg",
        "//envoy/extensions/rate_limit_descriptors/expr/v3:pkg",
        "//envoy/extensions/rbac/audit_loggers/stream/v3:pkg",
        "//envoy/extensions/rbac/matchers/upstream_ip_port/v3:pkg",
        "//envoy/extensions/regex_engines/v3:pkg",
        "//envoy/extensions/request_id/uuid/v3:pkg",
        "//envoy/extensions/resource_monitors/downstream_connections/v3:pkg",
        "//envoy/extensions/resource_monitors/fixed_heap/v3:pkg",
        "//envoy/extensions/resource_monitors/injected_resource/v3:pkg",
        "//envoy/extensions/retry/host/omit_canary_hosts/v3:pkg",
        "//envoy/extensions/retry/host/omit_host_metadata/v3:pkg",
        "//envoy/extensions/retry/host/previous_hosts/v3:pkg",
        "//envoy/extensions/retry/priority/previous_priorities/v3:pkg",
        "//envoy/extensions/router/cluster_specifiers/lua/v3:pkg",
        "//envoy/extensions/stat_sinks/graphite_statsd/v3:pkg",
        "//envoy/extensions/stat_sinks/open_telemetry/v3:pkg",
        "//envoy/extensions/stat_sinks/wasm/v3:pkg",
        "//envoy/extensions/tracers/opentelemetry/resource_detectors/v3:pkg",
        "//envoy/extensions/tracers/opentelemetry/samplers/v3:pkg",
        "//envoy/extensions/transport_sockets/alts/v3:pkg",
        "//envoy/extensions/transport_sockets/http_11_proxy/v3:pkg",
        "//envoy/extensions/transport_sockets/internal_upstream/v3:pkg",
        "//envoy/extensions/transport_sockets/proxy_protocol/v3:pkg",
        "//envoy/extensions/transport_sockets/quic/v3:pkg",
        "//envoy/extensions/transport_sockets/raw_buffer/v3:pkg",
        "//envoy/extensions/transport_sockets/s2a/v3:pkg",
        "//envoy/extensions/transport_sockets/starttls/v3:pkg",
        "//envoy/extensions/transport_sockets/tap/v3:pkg",
        "//envoy/extensions/transport_sockets/tcp_stats/v3:pkg",
        "//envoy/extensions/transport_sockets/tls/v3:pkg",
        "//envoy/extensions/udp_packet_writer/v3:pkg",
        "//envoy/extensions/upstreams/http/generic/v3:pkg",
        "//envoy/extensions/upstreams/http/http/v3:pkg",
        "//envoy/extensions/upstreams/http/tcp/v3:pkg",
        "//envoy/extensions/upstreams/http/udp/v3:pkg",
        "//envoy/extensions/upstreams/http/v3:pkg",
        "//envoy/extensions/upstreams/tcp/generic/v3:pkg",
        "//envoy/extensions/upstreams/tcp/v3:pkg",
        "//envoy/extensions/wasm/v3:pkg",
        "//envoy/extensions/watchdog/profile_action/v3:pkg",
        "//envoy/service/accesslog/v3:pkg",
        "//envoy/service/auth/v3:pkg",
        "//envoy/service/cluster/v3:pkg",
        "//envoy/service/discovery/v3:pkg",
        "//envoy/service/endpoint/v3:pkg",
        "//envoy/service/event_reporting/v3:pkg",
        "//envoy/service/ext_proc/v3:pkg",
        "//envoy/service/extension/v3:pkg",
        "//envoy/service/health/v3:pkg",
        "//envoy/service/listener/v3:pkg",
        "//envoy/service/load_stats/v3:pkg",
        "//envoy/service/metrics/v3:pkg",
        "//envoy/service/rate_limit_quota/v3:pkg",
        "//envoy/service/ratelimit/v3:pkg",
        "//envoy/service/route/v3:pkg",
        "//envoy/service/runtime/v3:pkg",
        "//envoy/service/secret/v3:pkg",
        "//envoy/service/status/v3:pkg",
        "//envoy/service/tap/v3:pkg",
        "//envoy/service/trace/v3:pkg",
        "//envoy/type/http/v3:pkg",
        "//envoy/type/matcher/v3:pkg",
        "//envoy/type/metadata/v3:pkg",
        "//envoy/type/tracing/v3:pkg",
        "//envoy/type/v3:pkg",
        "//envoy/watchdog/v3:pkg",
    ],
)

# This tracks frozen versions of protos.
proto_library(
    name = "frozen_protos",
    visibility = ["//visibility:public"],
    deps = [
        "//envoy/admin/v2alpha:pkg",
        "//envoy/api/v2:pkg",
        "//envoy/api/v2/auth:pkg",
        "//envoy/api/v2/cluster:pkg",
        "//envoy/api/v2/core:pkg",
        "//envoy/api/v2/endpoint:pkg",
        "//envoy/api/v2/listener:pkg",
        "//envoy/api/v2/ratelimit:pkg",
        "//envoy/api/v2/route:pkg",
        "//envoy/config/accesslog/v2:pkg",
        "//envoy/config/bootstrap/v2:pkg",
        "//envoy/config/cluster/aggregate/v2alpha:pkg",
        "//envoy/config/cluster/dynamic_forward_proxy/v2alpha:pkg",
        "//envoy/config/cluster/redis:pkg",
        "//envoy/config/common/dynamic_forward_proxy/v2alpha:pkg",
        "//envoy/config/common/tap/v2alpha:pkg",
        "//envoy/config/filter/accesslog/v2:pkg",
        "//envoy/config/filter/dubbo/router/v2alpha1:pkg",
        "//envoy/config/filter/fault/v2:pkg",
        "//envoy/config/filter/http/adaptive_concurrency/v2alpha:pkg",
        "//envoy/config/filter/http/aws_lambda/v2alpha:pkg",
        "//envoy/config/filter/http/aws_request_signing/v2alpha:pkg",
        "//envoy/config/filter/http/buffer/v2:pkg",
        "//envoy/config/filter/http/cache/v2alpha:pkg",
        "//envoy/config/filter/http/compressor/v2:pkg",
        "//envoy/config/filter/http/cors/v2:pkg",
        "//envoy/config/filter/http/csrf/v2:pkg",
        "//envoy/config/filter/http/dynamic_forward_proxy/v2alpha:pkg",
        "//envoy/config/filter/http/dynamo/v2:pkg",
        "//envoy/config/filter/http/ext_authz/v2:pkg",
        "//envoy/config/filter/http/fault/v2:pkg",
        "//envoy/config/filter/http/grpc_http1_bridge/v2:pkg",
        "//envoy/config/filter/http/grpc_http1_reverse_bridge/v2alpha1:pkg",
        "//envoy/config/filter/http/grpc_stats/v2alpha:pkg",
        "//envoy/config/filter/http/grpc_web/v2:pkg",
        "//envoy/config/filter/http/gzip/v2:pkg",
        "//envoy/config/filter/http/header_to_metadata/v2:pkg",
        "//envoy/config/filter/http/health_check/v2:pkg",
        "//envoy/config/filter/http/ip_tagging/v2:pkg",
        "//envoy/config/filter/http/jwt_authn/v2alpha:pkg",
        "//envoy/config/filter/http/lua/v2:pkg",
        "//envoy/config/filter/http/on_demand/v2:pkg",
        "//envoy/config/filter/http/original_src/v2alpha1:pkg",
        "//envoy/config/filter/http/rate_limit/v2:pkg",
        "//envoy/config/filter/http/rbac/v2:pkg",
        "//envoy/config/filter/http/router/v2:pkg",
        "//envoy/config/filter/http/squash/v2:pkg",
        "//envoy/config/filter/http/tap/v2alpha:pkg",
        "//envoy/config/filter/http/transcoder/v2:pkg",
        "//envoy/config/filter/listener/http_inspector/v2:pkg",
        "//envoy/config/filter/listener/original_dst/v2:pkg",
        "//envoy/config/filter/listener/original_src/v2alpha1:pkg",
        "//envoy/config/filter/listener/proxy_protocol/v2:pkg",
        "//envoy/config/filter/listener/tls_inspector/v2:pkg",
        "//envoy/config/filter/network/client_ssl_auth/v2:pkg",
        "//envoy/config/filter/network/direct_response/v2:pkg",
        "//envoy/config/filter/network/dubbo_proxy/v2alpha1:pkg",
        "//envoy/config/filter/network/echo/v2:pkg",
        "//envoy/config/filter/network/ext_authz/v2:pkg",
        "//envoy/config/filter/network/http_connection_manager/v2:pkg",
        "//envoy/config/filter/network/kafka_broker/v2alpha1:pkg",
        "//envoy/config/filter/network/local_rate_limit/v2alpha:pkg",
        "//envoy/config/filter/network/mongo_proxy/v2:pkg",
        "//envoy/config/filter/network/mysql_proxy/v1alpha1:pkg",
        "//envoy/config/filter/network/rate_limit/v2:pkg",
        "//envoy/config/filter/network/rbac/v2:pkg",
        "//envoy/config/filter/network/redis_proxy/v2:pkg",
        "//envoy/config/filter/network/sni_cluster/v2:pkg",
        "//envoy/config/filter/network/tcp_proxy/v2:pkg",
        "//envoy/config/filter/network/thrift_proxy/v2alpha1:pkg",
        "//envoy/config/filter/network/zookeeper_proxy/v1alpha1:pkg",
        "//envoy/config/filter/thrift/rate_limit/v2alpha1:pkg",
        "//envoy/config/filter/udp/udp_proxy/v2alpha:pkg",
        "//envoy/config/grpc_credential/v2alpha:pkg",
        "//envoy/config/listener/v2:pkg",
        "//envoy/config/metrics/v2:pkg",
        "//envoy/config/overload/v2alpha:pkg",
        "//envoy/config/ratelimit/v2:pkg",
        "//envoy/config/rbac/v2:pkg",
        "//envoy/config/retry/omit_host_metadata/v2:pkg",
        "//envoy/config/retry/previous_priorities:pkg",
        "//envoy/config/trace/v2:pkg",
        "//envoy/config/trace/v2alpha:pkg",
        "//envoy/config/transport_socket/alts/v2alpha:pkg",
        "//envoy/config/transport_socket/raw_buffer/v2:pkg",
        "//envoy/config/transport_socket/tap/v2alpha:pkg",
        "//envoy/data/accesslog/v2:pkg",
        "//envoy/data/cluster/v2alpha:pkg",
        "//envoy/data/core/v2alpha:pkg",
        "//envoy/data/dns/v2alpha:pkg",
        "//envoy/data/tap/v2alpha:pkg",
        "//envoy/service/accesslog/v2:pkg",
        "//envoy/service/auth/v2:pkg",
        "//envoy/service/discovery/v2:pkg",
        "//envoy/service/event_reporting/v2alpha:pkg",
        "//envoy/service/load_stats/v2:pkg",
        "//envoy/service/metrics/v2:pkg",
        "//envoy/service/ratelimit/v2:pkg",
        "//envoy/service/status/v2:pkg",
        "//envoy/service/tap/v2alpha:pkg",
        "//envoy/service/trace/v2:pkg",
        "//envoy/type:pkg",
        "//envoy/type/matcher:pkg",
        "//envoy/type/metadata/v2:pkg",
        "//envoy/type/tracing/v2:pkg",
    ],
)
# Generated by buf. DO NOT EDIT.
version: v1
deps:
  - remote: buf.build
    owner: cncf
    repository: xds
    commit: c313df85559e44248d0115969f2c8c24
  - remote: buf.build
    owner: envoyproxy
    repository: protoc-gen-validate
    commit: 6607b10f00ed4a3d98f906807131c44a
  - remote: buf.build
    owner: gogo
    repository: protobuf
    commit: 5461a3dfa9d941da82028ab185dc2a0e
  - remote: buf.build
    owner: googleapis
    repository: googleapis
    commit: 62f35d8aed1149c291d606d958a7ce32
  - remote: buf.build
    owner: opencensus
    repository: opencensus
    commit: bc2645b085534e4f8ebae3ef20088165
  - remote: buf.build
    owner: opentelemetry
    repository: opentelemetry
    commit: 43554dfbbfbd4873bde8993d32ea8332
  - remote: buf.build
    owner: prometheus
    repository: client-model
    commit: 1d56a02d481a412a83b3c4984eb90c2e
# Envoy Extension Policy

## Quality requirements

All extensions contained in the main Envoy repository will be held to the same quality bar as the
core Envoy code. This includes coding style, code reviews, test coverage, etc. In the future we
may consider creating a sandbox repository for extensions that are not compiled/tested by default
and held to a lower quality standard, but that is out of scope currently.

## Adding new extensions

The following procedure will be used when proposing new extensions for inclusion in the repository:
  1. A GitHub issue should be opened describing the proposed extension as with any major feature
  proposal.
  2. All extensions must be sponsored by an existing maintainer. Sponsorship means that the
  maintainer will shepherd the extension through design/code reviews. Maintainers can self-sponsor
  extensions if they are going to write them, shepherd them, and maintain them.

     Sponsorship serves two purposes:
     * It ensures that the extension will ultimately meet the Envoy quality bar.
     * It makes sure that incentives are aligned and that extensions are not added to the repo without
     sufficient thought put into future maintenance.

     *If sponsorship cannot be found from an existing maintainer, an organization can consider
     [doing the work to become a maintainer](./GOVERNANCE.md#process-for-becoming-a-maintainer) in
     order to be able to self-sponsor extensions.*

  3. Each extension must have two reviewers proposed for reviewing PRs to the extension. Neither of
  the reviewers must be a senior maintainer. Existing maintainers (including the sponsor) and other
  contributors can count towards this number. The initial reviewers will be codified in the
  [CODEOWNERS](./CODEOWNERS) file for long term maintenance. These reviewers can be swapped out as
  needed.
  4. Any extension added via this process becomes a full part of the repository. This means that any
  API breaking changes in the core code will be automatically fixed as part of the normal PR process
  by other contributors.
  5. Any new dependencies added for this extension must comply with
  [DEPENDENCY_POLICY.md](DEPENDENCY_POLICY.md), please follow the steps detailed there.
  6. If an extension depends on platform specific functionality, be sure to guard it in the build
  system. See [platform specific features](./PULL_REQUESTS.md#platform-specific-features).
  Add the extension to the necessary `*_SKIP_TARGETS` in [bazel/repositories.bzl](bazel/repositories.bzl)
  and tag tests to be skipped/failed on the unsupported platform.

## Removing existing extensions

As stated in the previous section, once an extension becomes part of the repository it will be
maintained by the collective set of Envoy contributors as needed.

However, if an extension has known issues that are not being rectified by the original sponsor and
reviewers or new contributors that are willing to step into the role of extension owner, a
[vote of the maintainers](./GOVERNANCE.md#conflict-resolution-and-voting) can be called to remove the
extension from the repository.

Extension removal process:

  1. A GitHub Issue is opened listing the reason for extension removal and any available replacements.
  2. Extension factory is modified to emit a deprecation warning.
  3. This starts a 6 month deprecation interval, after which extension is decommissioned.
  4. An announcement about extension deprecation is sent to the
     [envoy-announce](https://groups.google.com/forum/#!forum/envoy-announce) email list, with the
     instruction to comment on the GitHub issue to extend the deprecation interval. Heavily used
     extensions may have their deprecation interval extended by 6 more months.
  5. After the deprecation interval has expired the extension source code is removed.

## Extension pull request reviews

Extension PRs must not modify core Envoy code. In the event that an extension requires changes to core
Envoy code, those changes should be submitted as a separate PR and will undergo the normal code review
process, as documented in the [contributor's guide](./CONTRIBUTING.md).

Extension PRs must be approved by at least one sponsoring maintainer and an extension reviewer. These
may be a single individual, but it is always preferred to have multiple reviewers when feasible.

In the event that the Extension PR author is a sponsoring maintainer and no other sponsoring maintainer
is available, another maintainer may be enlisted to perform a minimal review for style and common C++
anti-patterns. The Extension PR must still be approved by a non-maintainer reviewer.

## Wasm extensions

Wasm extensions are not allowed in the main envoyproxy/envoy repository unless
part of the Wasm implementation validation. The rationale for this policy:
* Wasm extensions should not depend upon Envoy implementation specifics as
  they exist behind a version independent ABI. Hence, there is little value in
  qualifying Wasm extensions in the main repository.
* Wasm extensions introduce extensive dependencies via crates, etc. We would
  prefer to keep the envoyproxy/envoy repository dependencies minimal, easy
  to reason about and maintain.
* We do not implement any core extensions in Wasm and do not plan to in the
  medium term.

## Extension stability and security posture

Every extension is expected to be tagged with a `status` and `security_posture` in its
`envoy_cc_extension` rule.

The `status` is one of:
* `stable`: The extension is stable and is expected to be production usable. This is the default if
  no `status` is specified.
* `alpha`: The extension is functional but has not had substantial production burn time, use only
  with this caveat.
* `wip`: The extension is work-in-progress. Functionality is incomplete and it is not intended for
  production use.

The extension status may be adjusted by the extension [CODEOWNERS](./CODEOWNERS) and/or Envoy
maintainers based on an assessment of the above criteria. Note that the status of the extension
reflects the implementation status. It is orthogonal to the API stability, for example, an extension
API marked with `(xds.annotations.v3.file_status).work_in_progress` might have a `stable` implementation and
and an extension with a stable config proto can have a `wip` implementation.

The `security_posture` is one of:
* `robust_to_untrusted_downstream`: The extension is hardened against untrusted downstream traffic. It
   assumes that the upstream is trusted.
* `robust_to_untrusted_downstream_and_upstream`: The extension is hardened against both untrusted
   downstream and upstream traffic.
* `requires_trusted_downstream_and_upstream`: The extension is not hardened and should only be used in deployments
   where both the downstream and upstream are trusted.
* `unknown`: This is functionally equivalent to `requires_trusted_downstream_and_upstream`, but acts
  as a placeholder to allow us to identify extensions that need classifying.
* `data_plane_agnostic`: Not relevant to data plane threats, e.g. stats sinks.

An assessment of a robust security posture for an extension is subject to the following guidelines:

* Does the extension have fuzz coverage? If it's only receiving fuzzing
  courtesy of the generic listener/network/HTTP filter fuzzers, does it have a
  dedicated fuzzer for any parts of the code that would benefit?
* Does the extension have unbounded internal buffering? Does it participate in
  flow control via watermarking as needed?
* Does the extension have at least one deployment with live untrusted traffic
  for a period of time, N months?
* Does the extension rely on dependencies that meet our [extension maturity
  model](https://github.com/envoyproxy/envoy/issues/10471)?
* Is the extension reasonable to audit by Envoy security team?
* Is the extension free of obvious scary things, e.g. `memcpy`, does it have gnarly parsing code, etc?
* Does the extension have active [CODEOWNERS](CODEOWNERS) who are willing to
  vouch for the robustness of the extension?
* Is the extension absent a [low coverage
  exception](https://github.com/envoyproxy/envoy/blob/main/test/per_file_coverage.sh#L5)?

The current stability and security posture of all extensions can be seen
[here](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/threat_model#core-and-extensions).

## Adding Extension Points

Envoy might lack the extension point necessary for an extension. In that
case we need to install an extension point, which can be done as follows:

  1. Open a GitHub issue describing the proposed extension point and use cases.
  2. Make changes in core Envoy for the extension point.
  3. Update [extending envoy](docs/root/extending/extending.rst) to list the new
     extension point and add any documentation explaining the extension point.
     At the very least this should link to the corresponding proto.

## Contrib extensions

As described in [this document](https://docs.google.com/document/d/1yl7GOZK1TDm_7vxQvt8UQEAu07UQFru1uEKXM6ZZg_g/edit#),
Envoy allows an alternate path to adding extensions called `contrib/`. The barrier to entry for a
contrib extension is lower than a core extension, with the tradeoff that contrib extensions are not
included by default in the main image builds. Consumers need to pull directly from the contrib
images described in the installation guide. Please read the linked document in detail to determine
whether contrib extensions are the right choice for a newly proposed extension.

**NOTE:** Contrib extensions **require** an end-user sponsor. The sponsor is someone who will run
the extension at sufficient scale as to make the build maintenance and other overhead worthwhile.
The definition of "sufficient scale" is up to the maintainers and can change at any time. The
end-user sponsor *does not* have to author the extension, but the end-user sponsor will need to make
an "on the record" attestation of their planned usage of the extension. This attestation should
occur in a GitHub issue opened to discuss the new extension. In this context "end user" has the
same definition as the one specified in the [security policy](SECURITY.md#membership-criteria)
membership criteria (point 1.3.5).

**NOTE:** Contrib extensions are not eligible for Envoy security team coverage.

**NOTE:** As per the linked Google Doc, contrib extensions generally should use `v3alpha` to avoid
requiring API shepherd reviews.
Developer Certificate of Origin
Version 1.1

Copyright (C) 2004, 2006 The Linux Foundation and its contributors.
1 Letterman Drive
Suite D4700
San Francisco, CA, 94129

Everyone is permitted to copy and distribute verbatim copies of this
license document, but changing it is not allowed.


Developer's Certificate of Origin 1.1

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I
    have the right to submit it under the open source license
    indicated in the file; or

(b) The contribution is based upon previous work that, to the best
    of my knowledge, is covered under an appropriate open source
    license and I have the right under that license to submit that
    work with modifications, whether created in whole or in part
    by me, under the same open source license (unless I am
    permitted to submit under a different license), as indicated
    in the file; or

(c) The contribution was provided directly to me by some other
    person who certified (a), (b) or (c) and I have not modified
    it.

(d) I understand and agree that this project and the contribution
    are public and that a record of the contribution (including all
    personal information I submit with it, including my sign-off) is
    maintained indefinitely and may be redistributed consistent with
    this project or the open source license(s) involved.
- project:
    name: envoyproxy/envoy
    check:
      jobs:
      - envoy-build-arm64

- job:
    name: envoy-build-arm64
    parent: init-test
    description: |
      Envoy build in openlab cluster.
    run: .zuul/playbooks/envoy-build/run.yaml
    nodeset: ubuntu-xenial-arm64
    voting: false
-----BEGIN PGP PUBLIC KEY BLOCK-----

mQINBGNidjsBEADJeNLHg1Jj6r+988j6IGpg55Gbw1QlNt0iDrJGHYuDONtti70E
F3tJZmdreuue89jQZgDc9cBMdAixHpxenuBwfuKCT99txYocQGwZKVCu9U3sbRA6
eDaD4adj+o05pznCzde+Evz4g0T/BIquj/EWuk6DH5BbOwI80XyOPaumef7hBqyU
goUPBIMkbh/JsnVAPtFsPDbHZsNO/lHDt5d7bbfWdtv58LIoGP/yNvufV7+vwymr
rWyuTBHQzanwDZwGGhJdaz6ubaiJ6eupheLS0d3xXJr4r2ZB3nGeCzW0g257Rd1Z
3FHhFmMNGROtSfja7Mk5Z2R/dp+dLbMHzFlo+GFkOUGNjXI3JbUCYlm+6+4nrwU6
o45LJv9TusMHi6ncfCziV0WCDUGlL7dzNqjT5Fux/P/InjvnH/Iz7e/awdUnim1P
nJsOrT4pwfPJWYBtkf2aRmu+NICs5sqy6HIZgs09yOTOVyb5eKy2SV0APrGWq0OH
MkWd2WIwWuBeowByzGUymsrq7G2aQwqyJe4JYxqq8KFbjRisqhZqZ+xHxGuIvwAI
cxs/EmQLKsk/b06j5uojucv6SPKWSGGf7cO7PDfDqZxkQTNmcqkYHkGJA5D+hLsW
kt1RKIRRFineRUANIvwaFP5Ce06CX3urvOi8ZY0OL0jOx6dp2ldVoYOdSwARAQAB
tDZFbnZveSBtYWludGFpbmVycyA8ZW52b3ktbWFpbnRhaW5lcnNAZ29vZ2xlZ3Jv
dXBzLmNvbT6JAk4EEwEKADgWIQQK/Og2uk0dNXY8hSPYzcN1AYHzHwUCY2J2OwIb
AwULCQgHAgYVCgkICwIEFgIDAQIeAQIXgAAKCRDYzcN1AYHzHwDsD/kBLu+5BDx9
2Cr0cHn6dENBVS3wL53RCKSG0Uzlh+SY90BN7WTI5KyACt3SZYZHbbZbAYvvXbr5
9ooh0U+eGi3HsIqrP7GdrDC+8gtJB+Pud9+ltMCgU92IW/YAUSI/IgCbCOru6czc
H2Lah8S272YOpds/wuaiRSiMCU8TROcFXMfriGvcmNKwrJsRhJVARDZC3DB6m/ve
jWHAIX2eY7WhXyS6DJhs2iVrwz6fDxAiji5zrEZo+4CEUIfBojb+KeaC7PpusHu1
Sleq+Ck6zekAoDX1MmbPOeBT8qCoTNQ1w30jj6CFW8T5ncVottPRIzQGe0S350HR
o8JLWdEAAds4QIHsvnaBAKfRHPRPo4Uw5n4/xklLJIXzbafhN/goLA/AC9tznQzq
A95SoMn7fMXMd/sLVOs4V1XHQsCaXt9gR+Qwd1tqBNC/mZBrCPkJiStnRrTufhRb
R97z1lCynJMJhdqeRX7VnwncNS62U30pfV0mtYpS3URKqc62Jm3+fJalnMPaeXfM
xgjlerxVNkYXFNb+5NEMQSKuGI4Uq+kLTmajk0aS4cJGM+JvjRx9UtW2rbfz9Svh
8dSKV89wVQ/Zu9R3dAontooo9eC5WTHYGoiPDjFqBGsaLePQyJzMwWJ/XYG8ClF1
zQnAJL32VY1ECG7GbXw4tWtYdIErBnIckrkCDQRjYnY7ARAAto3DSLlcFkzKLBZ8
9hmjgonSZlHyMMAP8Ys+js8jKic2aDp7IPorrQSD8autoahbnPBe/TMlkGf+pD7z
iqsIImClNAJrqArRifNEwHO96uMAFsa7UiyyUz+P0n7zR3oznasZo7r64oCE5f60
+xbaoruyGiffiBnJ8uVqxEKvijilj2vCdc7ON+KOv6E6/iiBeQTEvTuSTs5Gm8IF
hRG2Ia4SHOCjdpglPU1mgoTF9PzFBbsAILLJxGhM8LldIeELELrofu760IvRapC+
4baSW9eu4WuHraiD26ZpEZKAQgtO8YFPQyltBSbLB2CU7i6kBlXTUL34h8BOfzwM
lFdwz7aTQlY3ZxLtjh4FXdLlM0YqolmJjShIOyd865mlx2BqFnqQigNHXZdqQiEc
06+YN6CxichTiLCCN9+UwTjO7w/QTYOyb4vBs5ubMB54KE+peUor56i9/UdpYxmT
eaugXX9M74mNDpWcdfBwJZJHCjTYbX+NZnxNysyf5QWr3roSBPLHFMzL/fbBsbWe
kZ59p4LcBNLD6wIGtqbrX7fF5mH28B5ZFGV8q8yI3mpt13adIsuuw+S8dLfQrQwP
ipitYHsBQ0F6JtGFs8DzD7f2KoLlVbXgP40adoG1dPs3NmiGLVZkjmqDaHzL6Fwk
G41YcBDHyqSOpQMVS6pJiWJIvsMAEQEAAYkCNgQYAQoAIBYhBAr86Da6TR01djyF
I9jNw3UBgfMfBQJjYnY7AhsMAAoJENjNw3UBgfMffWwQALhSp2fedt/oRIEP8rdN
yFex6FSXlZQOMuGyeo6PeDob23O+WcTIcS5POM7UckERen+30ZPHsOET3jIPH4h5
wIFuhUe/ap2JT6+CmsW+2kPz7nHGFQwCdjRScFOmRpEiu8+7X2l5LqUaO0PiC88m
n4fCWH1mIDqdGLxjd2EoNBt8vlQUt58wQbRPTs7P79209o44zYz04TTNL8iXzr3B
ngK13Z8QwaR1H56Ba3DdyU2xG016W1pN4B4C7DgTNWZwzRJkd3vg5AMzV6ZnP5+D
U38oQElfpGBy6YQCtC/ZVPDFNL0JaevraIG1LdFBqpUCu2bz4FURd1KoUITFdxeY
6sqL018q8wtTb7nRKMqPXiGelAlLvSaML8cq8CefUAlhjihhpJs14B1M+KOuCOt9
r0VHdH1Zp5FOTutA+QgTwVDngNWacoxjMY/zaZxLPrB9SppPpR4UNUDvcXCBJzgr
34tnUdDvcb7fuysexVUiUCfs9dMuv8TxCYmaVjY2Hq7oBEN8pM1o05FfLqCXwdrB
lkCRorH68WyLJv/FKZTyfVyHn32WOjr5DKGI0rT/nwkRRaI0SQbKEejvUxlq1WdK
cn7oQEM2sZdH3v85GQU0fA/0V4UV12pha1b1p1rbRhdJh4h/iY+rUdRz/YyOMlz5
xn73SlvaQy7ttkC8vuPiVI6g
=YtjB
-----END PGP PUBLIC KEY BLOCK-----
load("//bazel:envoy_build_system.bzl", "envoy_package")
load("//tools/python:namespace.bzl", "envoy_py_namespace")

licenses(["notice"])  # Apache 2

envoy_package()

envoy_py_namespace()

exports_files([
    "VERSION.txt",
    "API_VERSION.txt",
    ".clang-format",
    "pytest.ini",
    ".coveragerc",
    "CODEOWNERS",
    "OWNERS.md",
    ".github/config.yml",
])

alias(
    name = "envoy",
    actual = "//source/exe:envoy",
)

alias(
    name = "envoy.stripped",
    actual = "//source/exe:envoy-static.stripped",
)

filegroup(
    name = "clang_tidy_config",
    srcs = [".clang-tidy"],
    visibility = ["//visibility:public"],
)

# These two definitions exist to help reduce Envoy upstream core code depending on extensions.
# To avoid visibility problems, see notes in source/extensions/extensions_build_config.bzl
#
# TODO(#9953) //test/config_test:__pkg__ should probably be split up and removed.
# TODO(#9953) the config fuzz tests should be moved somewhere local and //test/config_test and //test/server removed.
package_group(
    name = "extension_config",
    packages = [
        "//source/exe",
        "//source/extensions/...",
        "//test/config_test",
        "//test/extensions/...",
        "//test/server",
        "//test/server/config_validation",
        "//test/tools/...",
        "//tools/extensions/...",
    ],
)

package_group(
    name = "extension_library",
    packages = [
        "//source/extensions/...",
        "//test/extensions/...",
    ],
)

package_group(
    name = "contrib_library",
    packages = [
        "//contrib/...",
    ],
)

package_group(
    name = "examples_library",
    packages = [
        "//examples/...",
    ],
)

package_group(
    name = "mobile_library",
    packages = [
        "//mobile/...",
    ],
)
1.30.0-dev
settings.json
launch.json
{
  // See https://go.microsoft.com/fwlink/?LinkId=733558
  // for the documentation about the tasks.json format
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Build All Tests",
      "type": "shell",
      "command": "bazel build //test/...",
      "group": {
        "kind": "build",
        "isDefault": true
      }
    },
    {
      "label": "Run All Tests",
      "type": "shell",
      "command": "bazel test //test/...",
      "group": {
        "kind": "test",
        "isDefault": true
      }
    },
    {
      "label": "Refresh Compilation Database",
      "type": "shell",
      "command": "tools/vscode/refresh_compdb.sh",
      "problemMatcher": []
    },
    {
      "label": "Refresh Compilation Database Exclude Contrib",
      "type": "shell",
      "command": "EXCLUDE_CONTRIB=true tools/vscode/refresh_compdb.sh",
      "problemMatcher": []
    },
    {
      "label": "Local Fix Format",
      "type": "shell",
      "command": "tools/local_fix_format.sh -all",
      "problemMatcher": []
    }
  ]
}
version: 2.1

jobs:
  build:
    docker:
    - image: debian:bullseye-slim@sha256:d3d0d14f49b49a4dd98a436711f5646dc39e1c99203ef223d1b6620061e2c0e5
    steps:
    - run: echo "no circle on this branch"
# Envoy specific Bazel build/test options.

# Bazel doesn't need more than 200MB of memory for local build based on memory profiling:
# https://docs.bazel.build/versions/master/skylark/performance.html#memory-profiling
# The default JVM max heapsize is 1/4 of physical memory up to 32GB which could be large
# enough to consume all memory constrained by cgroup in large host.
# Limiting JVM heapsize here to let it do GC more when approaching the limit to
# leave room for compiler/linker.
# The number 3G is chosen heuristically to both support large VM and small VM with RBE.
# Startup options cannot be selected via config.
startup --host_jvm_args=-Xmx3g

fetch --color=yes
run --color=yes

build --color=yes
build --jobs=HOST_CPUS-1
build --workspace_status_command="bash bazel/get_workspace_status"
build --incompatible_strict_action_env
build --java_runtime_version=remotejdk_11
build --tool_java_runtime_version=remotejdk_11
build --platform_mappings=bazel/platform_mappings
# silence absl logspam.
build --copt=-DABSL_MIN_LOG_LEVEL=4
build --define envoy_mobile_listener=enabled
build --experimental_repository_downloader_retries=2
build --enable_platform_specific_config

# Pass CC, CXX and LLVM_CONFIG variables from the environment.
# We assume they have stable values, so this won't cause action cache misses.
build --action_env=CC --host_action_env=CC
build --action_env=CXX --host_action_env=CXX
build --action_env=LLVM_CONFIG --host_action_env=LLVM_CONFIG
# Do not pass through PATH however.
# It tends to have machine-specific values, such as dynamically created temp folders.
# This would make it impossible to share remote action cache hits among machines.
# build --action_env=PATH --host_action_env=PATH
# To make our own CI green, we do need that flag on Windows though.
build:windows --action_env=PATH --host_action_env=PATH

# Allow stamped caches to bust when local filesystem changes.
# Requires setting `BAZEL_VOLATILE_DIRTY` in the env.
build --action_env=BAZEL_VOLATILE_DIRTY --host_action_env=BAZEL_VOLATILE_DIRTY

# Prevent stamped caches from busting (eg in PRs)
# Requires setting `BAZEL_FAKE_SCM_REVISION` in the env.
build --action_env=BAZEL_FAKE_SCM_REVISION --host_action_env=BAZEL_FAKE_SCM_REVISION

build --test_summary=terse

build:docs-ci --action_env=DOCS_RST_CHECK=1 --host_action_env=DOCS_RST_CHECK=1

# TODO(keith): Remove once these 2 are the default
build --incompatible_config_setting_private_default_visibility
build --incompatible_enforce_config_setting_visibility

test --test_verbose_timeout_warnings

# Allow tags to influence execution requirements
common --experimental_allow_tags_propagation

# Enable position independent code (this is the default on macOS and Windows)
# (Workaround for https://github.com/bazelbuild/rules_foreign_cc/issues/421)
build:linux --copt=-fdebug-types-section
build:linux --copt=-fPIC
build:linux --copt=-Wno-deprecated-declarations
build:linux --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
build:linux --conlyopt=-fexceptions
build:linux --fission=dbg,opt
build:linux --features=per_object_debug_info
build:linux --action_env=BAZEL_LINKLIBS=-l%:libstdc++.a
build:linux --action_env=BAZEL_LINKOPTS=-lm

# We already have absl in the build, define absl=1 to tell googletest to use absl for backtrace.
build --define absl=1

# Disable ICU linking for googleurl.
build --@com_googlesource_googleurl//build_config:system_icu=0

# Common flags for sanitizers
build:sanitizer --define tcmalloc=disabled
build:sanitizer --linkopt -ldl

# Common flags for Clang
build:clang --action_env=BAZEL_COMPILER=clang
build:clang --action_env=CC=clang --action_env=CXX=clang++
build:clang --linkopt=-fuse-ld=lld

# Flags for Clang + PCH
build:clang-pch --spawn_strategy=local
build:clang-pch --define=ENVOY_CLANG_PCH=1

# Use gold linker for gcc compiler.
build:gcc --linkopt=-fuse-ld=gold

# Clang-tidy
# TODO(phlax): enable this, its throwing some errors as well as finding more issues
# build:clang-tidy --@envoy_toolshed//format/clang_tidy:executable=@envoy//tools/clang-tidy
build:clang-tidy --@envoy_toolshed//format/clang_tidy:config=//:clang_tidy_config
build:clang-tidy --aspects @envoy_toolshed//format/clang_tidy:clang_tidy.bzl%clang_tidy_aspect
build:clang-tidy --output_groups=report
build:clang-tidy --build_tag_filters=-notidy

# Basic ASAN/UBSAN that works for gcc
build:asan --action_env=ENVOY_ASAN=1
build:asan --config=sanitizer
# ASAN install its signal handler, disable ours so the stacktrace will be printed by ASAN
build:asan --define signal_trace=disabled
build:asan --define ENVOY_CONFIG_ASAN=1
build:asan --build_tag_filters=-no_san
build:asan --test_tag_filters=-no_san
build:asan --copt -fsanitize=address,undefined
build:asan --linkopt -fsanitize=address,undefined
# vptr and function sanitizer are enabled in clang-asan if it is set up via bazel/setup_clang.sh.
build:asan --copt -fno-sanitize=vptr,function
build:asan --linkopt -fno-sanitize=vptr,function
build:asan --copt -DADDRESS_SANITIZER=1
build:asan --copt -DUNDEFINED_SANITIZER=1
build:asan --copt -D__SANITIZE_ADDRESS__
build:asan --test_env=ASAN_OPTIONS=handle_abort=1:allow_addr2line=true:check_initialization_order=true:strict_init_order=true:detect_odr_violation=1
build:asan --test_env=UBSAN_OPTIONS=halt_on_error=true:print_stacktrace=1
build:asan --test_env=ASAN_SYMBOLIZER_PATH
# ASAN needs -O1 to get reasonable performance.
build:asan --copt -O1
build:asan --copt -fno-optimize-sibling-calls

# Clang ASAN/UBSAN
build:clang-asan --config=clang
build:clang-asan --config=asan
build:clang-asan --linkopt -fuse-ld=lld
build:clang-asan --linkopt --rtlib=compiler-rt
build:clang-asan --linkopt --unwindlib=libgcc

# macOS
build:macos --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
build:macos --action_env=PATH=/opt/homebrew/bin:/opt/local/bin:/usr/local/bin:/usr/bin:/bin
build:macos --host_action_env=PATH=/opt/homebrew/bin:/opt/local/bin:/usr/local/bin:/usr/bin:/bin
build:macos --define tcmalloc=disabled

# macOS ASAN/UBSAN
build:macos-asan --config=asan
# Workaround, see https://github.com/bazelbuild/bazel/issues/6932
build:macos-asan --copt -Wno-macro-redefined
build:macos-asan --copt -D_FORTIFY_SOURCE=0
# Workaround, see https://github.com/bazelbuild/bazel/issues/4341
build:macos-asan --copt -DGRPC_BAZEL_BUILD
# Dynamic link cause issues like: `dyld: malformed mach-o: load commands size (59272) > 32768`
build:macos-asan --dynamic_mode=off

# Clang TSAN
build:clang-tsan --action_env=ENVOY_TSAN=1
build:clang-tsan --config=sanitizer
build:clang-tsan --define ENVOY_CONFIG_TSAN=1
build:clang-tsan --copt -fsanitize=thread
build:clang-tsan --linkopt -fsanitize=thread
build:clang-tsan --linkopt -fuse-ld=lld
build:clang-tsan --copt -DTHREAD_SANITIZER=1
build:clang-tsan --build_tag_filters=-no_san,-no_tsan
build:clang-tsan --test_tag_filters=-no_san,-no_tsan
# Needed due to https://github.com/libevent/libevent/issues/777
build:clang-tsan --copt -DEVENT__DISABLE_DEBUG_MODE
# https://github.com/abseil/abseil-cpp/issues/760
# https://github.com/google/sanitizers/issues/953
build:clang-tsan --test_env="TSAN_OPTIONS=report_atomic_races=0"
build:clang-tsan --test_timeout=120,600,1500,4800

# Clang MSAN - this is the base config for remote-msan and docker-msan. To run this config without
# our build image, follow https://github.com/google/sanitizers/wiki/MemorySanitizerLibcxxHowTo
# with libc++ instruction and provide corresponding `--copt` and `--linkopt` as well.
build:clang-msan --action_env=ENVOY_MSAN=1
build:clang-msan --config=sanitizer
build:clang-msan --build_tag_filters=-no_san
build:clang-msan --test_tag_filters=-no_san
build:clang-msan --define ENVOY_CONFIG_MSAN=1
build:clang-msan --copt -fsanitize=memory
build:clang-msan --linkopt -fsanitize=memory
build:clang-msan --linkopt -fuse-ld=lld
build:clang-msan --copt -fsanitize-memory-track-origins=2
build:clang-msan --copt -DMEMORY_SANITIZER=1
build:clang-msan --test_env=MSAN_SYMBOLIZER_PATH
# MSAN needs -O1 to get reasonable performance.
build:clang-msan --copt -O1
build:clang-msan --copt -fno-optimize-sibling-calls

# Clang with libc++
build:libc++ --config=clang
build:libc++ --action_env=CXXFLAGS=-stdlib=libc++
build:libc++ --action_env=LDFLAGS=-stdlib=libc++
build:libc++ --action_env=BAZEL_CXXOPTS=-stdlib=libc++
build:libc++ --action_env=BAZEL_LINKLIBS=-l%:libc++.a:-l%:libc++abi.a
build:libc++ --action_env=BAZEL_LINKOPTS=-lm:-pthread
build:libc++ --define force_libcpp=enabled

build:libc++20 --config=libc++
# gRPC has a lot of deprecated-enum-enum-conversion warning. Remove once it is addressed
build:libc++20 --cxxopt=-std=c++20 --copt=-Wno-error=deprecated-enum-enum-conversion

# Optimize build for binary size reduction.
build:sizeopt -c opt --copt -Os

# Test options
build --test_env=HEAPCHECK=normal --test_env=PPROF_PATH

# Coverage options
coverage --config=coverage
coverage --build_tests_only

build:coverage --action_env=BAZEL_USE_LLVM_NATIVE_COVERAGE=1
build:coverage --action_env=GCOV=llvm-profdata
build:coverage --copt=-DNDEBUG
# 1.5x original timeout + 300s for trace merger in all categories
build:coverage --test_timeout=390,750,1500,5700
build:coverage --define=dynamic_link_tests=true
build:coverage --define=ENVOY_CONFIG_COVERAGE=1
build:coverage --cxxopt="-DENVOY_CONFIG_COVERAGE=1"
build:coverage --test_env=HEAPCHECK=
build:coverage --combined_report=lcov
build:coverage --strategy=TestRunner=remote,sandboxed,local
build:coverage --strategy=CoverageReport=sandboxed,local
build:coverage --experimental_use_llvm_covmap
build:coverage --experimental_generate_llvm_lcov
build:coverage --experimental_split_coverage_postprocessing
build:coverage --experimental_fetch_all_coverage_outputs
build:coverage --collect_code_coverage
build:coverage --instrumentation_filter="^//source(?!/common/quic/platform)[/:],^//envoy[/:],^//contrib(?!/.*/test)[/:]"
build:coverage --remote_download_minimal
build:coverage --define=tcmalloc=gperftools
build:coverage --define=no_debug_info=1
# `--no-relax` is required for coverage to not err with `relocation R_X86_64_REX_GOTPCRELX`
build:coverage --linkopt=-Wl,-s,--no-relax
build:coverage --test_env=ENVOY_IP_TEST_VERSIONS=v4only

build:test-coverage --test_arg="-l trace"
build:test-coverage --test_arg="--log-path /dev/null"
build:test-coverage --test_tag_filters=-nocoverage,-fuzz_target
build:fuzz-coverage --config=plain-fuzzer
build:fuzz-coverage --run_under=@envoy//bazel/coverage:fuzz_coverage_wrapper.sh
build:fuzz-coverage --test_tag_filters=-nocoverage

build:cache-local --remote_cache=grpc://localhost:9092

# Remote execution: https://docs.bazel.build/versions/master/remote-execution.html
build:rbe-toolchain --action_env=BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1

build:rbe-toolchain-clang --config=rbe-toolchain
build:rbe-toolchain-clang --platforms=@envoy_build_tools//toolchains:rbe_linux_clang_platform
build:rbe-toolchain-clang --host_platform=@envoy_build_tools//toolchains:rbe_linux_clang_platform
build:rbe-toolchain-clang --crosstool_top=@envoy_build_tools//toolchains/configs/linux/clang/cc:toolchain
build:rbe-toolchain-clang --extra_toolchains=@envoy_build_tools//toolchains/configs/linux/clang/config:cc-toolchain
build:rbe-toolchain-clang --action_env=CC=clang --action_env=CXX=clang++ --action_env=PATH=/usr/sbin:/usr/bin:/sbin:/bin:/opt/llvm/bin

build:rbe-toolchain-clang-libc++ --config=rbe-toolchain
build:rbe-toolchain-clang-libc++ --platforms=@envoy_build_tools//toolchains:rbe_linux_clang_libcxx_platform
build:rbe-toolchain-clang-libc++ --host_platform=@envoy_build_tools//toolchains:rbe_linux_clang_libcxx_platform
build:rbe-toolchain-clang-libc++ --crosstool_top=@envoy_build_tools//toolchains/configs/linux/clang_libcxx/cc:toolchain
build:rbe-toolchain-clang-libc++ --extra_toolchains=@envoy_build_tools//toolchains/configs/linux/clang_libcxx/config:cc-toolchain
build:rbe-toolchain-clang-libc++ --action_env=CC=clang --action_env=CXX=clang++ --action_env=PATH=/usr/sbin:/usr/bin:/sbin:/bin:/opt/llvm/bin
build:rbe-toolchain-clang-libc++ --action_env=CXXFLAGS=-stdlib=libc++
build:rbe-toolchain-clang-libc++ --action_env=LDFLAGS=-stdlib=libc++
build:rbe-toolchain-clang-libc++ --define force_libcpp=enabled

# Do not inherit from "clang-asan" to avoid picking up flags from local clang.bazelrc.
build:rbe-toolchain-asan --config=asan
build:rbe-toolchain-asan --linkopt -fuse-ld=lld
build:rbe-toolchain-asan --action_env=ENVOY_UBSAN_VPTR=1
build:rbe-toolchain-asan --copt=-fsanitize=vptr,function
build:rbe-toolchain-asan --linkopt=-fsanitize=vptr,function
build:rbe-toolchain-asan --linkopt='-L/opt/llvm/lib/clang/14.0.0/lib/x86_64-unknown-linux-gnu'
build:rbe-toolchain-asan --linkopt=-l:libclang_rt.ubsan_standalone.a
build:rbe-toolchain-asan --linkopt=-l:libclang_rt.ubsan_standalone_cxx.a

build:rbe-toolchain-msan --linkopt=-L/opt/libcxx_msan/lib
build:rbe-toolchain-msan --linkopt=-Wl,-rpath,/opt/libcxx_msan/lib
build:rbe-toolchain-msan --config=clang-msan

build:rbe-toolchain-tsan --linkopt=-L/opt/libcxx_tsan/lib
build:rbe-toolchain-tsan --linkopt=-Wl,-rpath,/opt/libcxx_tsan/lib
build:rbe-toolchain-tsan --config=clang-tsan

build:rbe-toolchain-gcc --config=rbe-toolchain
build:rbe-toolchain-gcc --platforms=@envoy_build_tools//toolchains:rbe_linux_gcc_platform
build:rbe-toolchain-gcc --host_platform=@envoy_build_tools//toolchains:rbe_linux_gcc_platform
build:rbe-toolchain-gcc --crosstool_top=@envoy_build_tools//toolchains/configs/linux/gcc/cc:toolchain
build:rbe-toolchain-gcc --extra_toolchains=@envoy_build_tools//toolchains/configs/linux/gcc/config:cc-toolchain

build:rbe-toolchain-msvc-cl --host_platform=@envoy_build_tools//toolchains:rbe_windows_msvc_cl_platform
build:rbe-toolchain-msvc-cl --platforms=@envoy_build_tools//toolchains:rbe_windows_msvc_cl_platform
build:rbe-toolchain-msvc-cl --crosstool_top=@envoy_build_tools//toolchains/configs/windows/msvc-cl/cc:toolchain
build:rbe-toolchain-msvc-cl --extra_toolchains=@envoy_build_tools//toolchains/configs/windows/msvc-cl/config:cc-toolchain

build:rbe-toolchain-clang-cl --host_platform=@envoy_build_tools//toolchains:rbe_windows_clang_cl_platform
build:rbe-toolchain-clang-cl --platforms=@envoy_build_tools//toolchains:rbe_windows_clang_cl_platform
build:rbe-toolchain-clang-cl --crosstool_top=@envoy_build_tools//toolchains/configs/windows/clang-cl/cc:toolchain
build:rbe-toolchain-clang-cl --extra_toolchains=@envoy_build_tools//toolchains/configs/windows/clang-cl/config:cc-toolchain

build:remote --spawn_strategy=remote,sandboxed,local
build:remote --strategy=Javac=remote,sandboxed,local
build:remote --strategy=Closure=remote,sandboxed,local
build:remote --strategy=Genrule=remote,sandboxed,local

# Windows bazel does not allow sandboxed as a spawn strategy
build:remote-windows --spawn_strategy=remote,local
build:remote-windows --strategy=Javac=remote,local
build:remote-windows --strategy=Closure=remote,local
build:remote-windows --strategy=Genrule=remote,local
build:remote-windows --strategy=CppLink=local
build:remote-windows --remote_timeout=7200
build:remote-windows --google_default_credentials=true
build:remote-windows --remote_download_toplevel

build:remote-clang --config=remote
build:remote-clang --config=rbe-toolchain-clang

build:remote-clang-libc++ --config=remote
build:remote-clang-libc++ --config=rbe-toolchain-clang-libc++

build:remote-gcc --config=remote
build:remote-gcc --config=rbe-toolchain-gcc

build:remote-asan --config=remote
build:remote-asan --config=rbe-toolchain-clang-libc++
build:remote-asan --config=rbe-toolchain-asan

build:remote-msan --config=remote
build:remote-msan --config=rbe-toolchain-clang-libc++
build:remote-msan --config=rbe-toolchain-msan

build:remote-tsan --config=remote
build:remote-tsan --config=rbe-toolchain-clang-libc++
build:remote-tsan --config=rbe-toolchain-tsan

build:remote-msvc-cl --config=remote-windows
build:remote-msvc-cl --config=msvc-cl
build:remote-msvc-cl --config=rbe-toolchain-msvc-cl

build:remote-clang-cl --config=remote-windows
build:remote-clang-cl --config=clang-cl
build:remote-clang-cl --config=rbe-toolchain-clang-cl

## Compile-time-options testing
# Right now, none of the available compile-time options conflict with each other. If this
# changes, this build type may need to be broken up.
build:compile-time-options --define=admin_html=disabled
build:compile-time-options --define=signal_trace=disabled
build:compile-time-options --define=hot_restart=disabled
build:compile-time-options --define=google_grpc=disabled
build:compile-time-options --define=boringssl=fips
build:compile-time-options --define=log_debug_assert_in_release=enabled
build:compile-time-options --define=path_normalization_by_default=true
build:compile-time-options --define=deprecated_features=disabled
build:compile-time-options --define=tcmalloc=gperftools
build:compile-time-options --define=zlib=ng
build:compile-time-options --define=uhv=enabled
build:compile-time-options --config=libc++20
build:compile-time-options --test_env=ENVOY_HAS_EXTRA_EXTENSIONS=true
build:compile-time-options --@envoy//bazel:http3=False
build:compile-time-options --@envoy//source/extensions/filters/http/kill_request:enabled

# Docker sandbox
# NOTE: Update this from https://github.com/envoyproxy/envoy-build-tools/blob/main/toolchains/rbe_toolchains_config.bzl#L8
build:docker-sandbox --experimental_docker_image=envoyproxy/envoy-build-ubuntu:0ca52447572ee105a4730da5e76fe47c9c5a7c64@sha256:d736c58f06f36848e7966752cc7e01519cc1b5101a178d5c6634807e8ac3deab
build:docker-sandbox --spawn_strategy=docker
build:docker-sandbox --strategy=Javac=docker
build:docker-sandbox --strategy=Closure=docker
build:docker-sandbox --strategy=Genrule=docker
build:docker-sandbox --define=EXECUTOR=remote
build:docker-sandbox --experimental_docker_verbose
build:docker-sandbox --experimental_enable_docker_sandbox

build:docker-clang --config=docker-sandbox
build:docker-clang --config=rbe-toolchain-clang

build:docker-clang-libc++ --config=docker-sandbox
build:docker-clang-libc++ --config=rbe-toolchain-clang-libc++

build:docker-gcc --config=docker-sandbox
build:docker-gcc --config=rbe-toolchain-gcc

build:docker-asan --config=docker-sandbox
build:docker-asan --config=rbe-toolchain-clang-libc++
build:docker-asan --config=rbe-toolchain-asan

build:docker-msan --config=docker-sandbox
build:docker-msan --config=rbe-toolchain-clang-libc++
build:docker-msan --config=rbe-toolchain-msan

build:docker-tsan --config=docker-sandbox
build:docker-tsan --config=rbe-toolchain-clang-libc++
build:docker-tsan --config=rbe-toolchain-tsan

# CI configurations
build:remote-ci --config=ci
build:remote-ci --remote_download_minimal

# Note this config is used by mobile CI also.
build:ci --noshow_progress
build:ci --noshow_loading_progress
build:ci --test_output=errors

# Fuzz builds

# Shared fuzzing configuration.
build:fuzzing --define=ENVOY_CONFIG_ASAN=1
build:fuzzing --copt=-DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION
build:fuzzing --config=libc++

# Fuzzing without ASAN. This is useful for profiling fuzzers without any ASAN artifacts.
build:plain-fuzzer --config=fuzzing
build:plain-fuzzer --define=FUZZING_ENGINE=libfuzzer
# The fuzzing rules provide their own instrumentation, but it is currently
# disabled due to bazelbuild/bazel#12888. Instead, we provide instrumentation at
# the top level through these options.
build:plain-fuzzer --copt=-fsanitize=fuzzer-no-link
build:plain-fuzzer --linkopt=-fsanitize=fuzzer-no-link

build:asan-fuzzer --config=plain-fuzzer
build:asan-fuzzer --config=clang-asan
build:asan-fuzzer --copt=-fno-omit-frame-pointer
# Remove UBSAN halt_on_error to avoid crashing on protobuf errors.
build:asan-fuzzer --test_env=UBSAN_OPTIONS=print_stacktrace=1

build:oss-fuzz --config=fuzzing
build:oss-fuzz --define=FUZZING_ENGINE=oss-fuzz
build:oss-fuzz --@rules_fuzzing//fuzzing:cc_engine_instrumentation=oss-fuzz
build:oss-fuzz --@rules_fuzzing//fuzzing:cc_engine_sanitizer=none
build:oss-fuzz --dynamic_mode=off
build:oss-fuzz --strip=never
build:oss-fuzz --copt=-fno-sanitize=vptr
build:oss-fuzz --linkopt=-fno-sanitize=vptr
build:oss-fuzz --define=tcmalloc=disabled
build:oss-fuzz --define=signal_trace=disabled
build:oss-fuzz --copt=-D_LIBCPP_DISABLE_DEPRECATION_WARNINGS
build:oss-fuzz --define=force_libcpp=enabled
build:oss-fuzz --linkopt=-lc++
build:oss-fuzz --linkopt=-pthread

# Compile database generation config
build:compdb --build_tag_filters=-nocompdb

# Windows build quirks
build:windows --action_env=TMPDIR
build:windows --define signal_trace=disabled
build:windows --define hot_restart=disabled
build:windows --define tcmalloc=disabled
build:windows --define wasm=disabled
build:windows --define manual_stamp=manual_stamp
build:windows --cxxopt="/std:c++20"
build:windows --output_groups=+pdb_file

# TODO(wrowe,sunjayBhatia): Resolve bugs upstream in curl and rules_foreign_cc
# See issue https://github.com/bazelbuild/rules_foreign_cc/issues/301
build:windows --copt="-DCARES_STATICLIB"
build:windows --copt="-DNGHTTP2_STATICLIB"
build:windows --copt="-DCURL_STATICLIB"

# Override any clang preference if building msvc-cl
# Drop the determinism feature (-DDATE etc are a no-op in msvc-cl)
build:msvc-cl --action_env=USE_CLANG_CL=""
build:msvc-cl --define clang_cl=0
build:msvc-cl --features=-determinism

# Windows build behaviors when using clang-cl
build:clang-cl --action_env=USE_CLANG_CL=1
build:clang-cl --define clang_cl=1

# Required to work around Windows clang-cl build defects
# Ignore conflicting definitions of _WIN32_WINNT
# Override determinism flags (DATE etc) is valid on clang-cl compiler
build:clang-cl --copt="-Wno-macro-redefined"
build:clang-cl --copt="-Wno-builtin-macro-redefined"
# Workaround problematic missing override declarations of mocks
# TODO: resolve this class of problematic mocks, e.g.
# ./test/mocks/http/stream.h(16,21): error: 'addCallbacks'
#     overrides a member function but is not marked 'override'
#   MOCK_METHOD(void, addCallbacks, (StreamCallbacks & callbacks));
build:clang-cl --copt="-Wno-inconsistent-missing-override"

# Defaults to 'auto' - Off for windows, so override to linux behavior
build:windows --enable_runfiles=yes

# This should become adopted by bazel as the default
build:windows --features=compiler_param_file

# These options attempt to force a monolithic binary including the CRT
build:windows --features=fully_static_link
build:windows --features=static_link_msvcrt
build:windows --dynamic_mode=off

# RBE (Google)
build:cache-google --google_default_credentials=true
build:cache-google --remote_cache=grpcs://remotebuildexecution.googleapis.com
build:cache-google --remote_instance_name=projects/envoy-ci/instances/default_instance
build:cache-google --remote_timeout=7200
build:rbe-google --remote_executor=grpcs://remotebuildexecution.googleapis.com
build:rbe-google --config=cache-google

build:rbe-google-bes --bes_backend=grpcs://buildeventservice.googleapis.com
build:rbe-google-bes --bes_results_url=https://source.cloud.google.com/results/invocations/

# RBE (Engflow mobile)
build:rbe-engflow --google_default_credentials=false
build:rbe-engflow --remote_cache=grpcs://envoy.cluster.engflow.com
build:rbe-engflow --remote_executor=grpcs://envoy.cluster.engflow.com
build:rbe-engflow --bes_backend=grpcs://envoy.cluster.engflow.com/
build:rbe-engflow --bes_results_url=https://envoy.cluster.engflow.com/invocation/
build:rbe-engflow --credential_helper=*.engflow.com=%workspace%/bazel/engflow-bazel-credential-helper.sh
build:rbe-engflow --grpc_keepalive_time=30s
build:rbe-engflow --remote_timeout=3600s
build:rbe-engflow --bes_timeout=3600s
build:rbe-engflow --bes_upload_mode=fully_async

build:rbe-envoy-engflow --google_default_credentials=false
build:rbe-envoy-engflow --remote_cache=grpcs://morganite.cluster.engflow.com
build:rbe-envoy-engflow --remote_executor=grpcs://morganite.cluster.engflow.com
build:rbe-envoy-engflow --bes_backend=grpcs://morganite.cluster.engflow.com/
build:rbe-envoy-engflow --bes_results_url=https://morganite.cluster.engflow.com/invocation/
build:rbe-envoy-engflow --credential_helper=*.engflow.com=%workspace%/bazel/engflow-bazel-credential-helper.sh
build:rbe-envoy-engflow --grpc_keepalive_time=30s
build:rbe-envoy-engflow --remote_timeout=3600s
build:rbe-envoy-engflow --bes_timeout=3600s
build:rbe-envoy-engflow --bes_upload_mode=fully_async
build:rbe-envoy-engflow --remote_default_exec_properties=container-image=docker://docker.io/envoyproxy/envoy-build-ubuntu:0ca52447572ee105a4730da5e76fe47c9c5a7c64@sha256:d736c58f06f36848e7966752cc7e01519cc1b5101a178d5c6634807e8ac3deab

#############################################################################
# debug: Various Bazel debugging flags
#############################################################################
# debug/bazel
common:debug-bazel --announce_rc
common:debug-bazel -s
# debug/sandbox
common:debug-sandbox --verbose_failures
common:debug-sandbox --sandbox_debug
# debug/coverage
common:debug-coverage --action_env=VERBOSE_COVERAGE=true
common:debug-coverage --test_env=VERBOSE_COVERAGE=true
common:debug-coverage --test_env=DISPLAY_LCOV_CMD=true
common:debug-coverage --config=debug-tests
# debug/tests
common:debug-tests --test_output=all
# debug/everything
common:debug --config=debug-bazel
common:debug --config=debug-sandbox
common:debug --config=debug-coverage
common:debug --config=debug-tests

try-import %workspace%/clang.bazelrc
try-import %workspace%/user.bazelrc
try-import %workspace%/local_tsan.bazelrc
* See [CONTRIBUTING.md](CONTRIBUTING.md) for general contribution guidelines.
* See [GOVERNANCE.md](GOVERNANCE.md) for governance guidelines and maintainer responsibilities.
* See [CODEOWNERS](CODEOWNERS) for a detailed list of owners for the various source directories.

This page lists all active maintainers and their areas of expertise. This can be used for
routing PRs, questions, etc. to the right place.

# Senior maintainers
<!--- If you modify senior maintainers list, please update the core-maintainers section of SECURITY-INSIGHTS.yml  -->

* Matt Klein ([mattklein123](https://github.com/mattklein123)) (mattklein123@gmail.com)
  * Catch-all, "all the things", and generally trying to make himself obsolete as fast as
    possible.
* Harvey Tuch ([htuch](https://github.com/htuch)) (htuch@google.com)
  * xDS APIs, configuration and control plane.
* Alyssa Wilk ([alyssawilk](https://github.com/alyssawilk)) (alyssar@google.com)
  * HTTP, flow control, cluster manager, load balancing, and core networking (listeners,
    connections, etc.), Envoy Mobile.
* Stephan Zuercher ([zuercher](https://github.com/zuercher)) (zuercher@gmail.com)
  * Load balancing, upstream clusters and cluster manager, logging, complex HTTP routing
    (metadata, etc.), and macOS build.
* Lizan Zhou ([lizan](https://github.com/lizan)) (lizan.j@gmail.com)
  * gRPC, gRPC/JSON transcoding, and core networking (transport socket abstractions), Bazel, build
    issues, and CI in general.
* Greg Greenway ([ggreenway](https://github.com/ggreenway)) (ggreenway@apple.com)
  * TLS, TCP proxy, listeners, and HTTP proxy/connection pooling.
* Yan Avlasov ([yanavlasov](https://github.com/yanavlasov)) (yavlasov@google.com)
  * Data plane, codecs, security, configuration.
* Ryan Northey ([phlax](https://github.com/phlax)) (ryan@synca.io)
  * Docs, tooling, CI, containers and sandbox examples
* Ryan Hamilton ([RyanTheOptimist](https://github.com/ryantheoptimist)) (rch@google.com)
  * HTTP/3, upstream connection management, Envoy Mobile.
* Baiping Wang ([wbpcode](https://github.com/wbpcode)) (wbphub@live.com)
  * Upstream, LB, tracing, logging, performance, and generic/dubbo proxy.

# Maintainers
<!--- If you modify maintainers list, please update the core-maintainers section of SECURITY-INSIGHTS.yml -->

* Joshua Marantz ([jmarantz](https://github.com/jmarantz)) (jmarantz@google.com)
  * Stats, abseil, scalability, and performance.
* Adi Peleg ([adisuissa](https://github.com/adisuissa)) (adip@google.com)
  * xDS APIs, configuration, control plane, fuzzing.
* Kevin Baichoo ([KBaichoo](https://github.com/KBaichoo)) (kbaichoo@google.com)
  * Data plane, overload management, flow control.
* Keith Smiley ([keith](https://github.com/keith)) (keithbsmiley@gmail.com)
  * Bazel, CI, compilers, linkers, general build issues, etc.
* Kuat Yessenov ([kyessenov](https://github.com/kyessenov)) (kuat@google.com)
  * Listeners, RBAC, CEL, matching, Istio.
* Raven Black ([ravenblackx](https://github.com/ravenblackx)) (ravenblack@dropbox.com)
  * Caches, file filters, and file I/O.
* Alex Xu ([soulxu](https://github.com/soulxu)) (hejie.xu@intel.com)
  * Listeners, iouring, data plane.
* Kateryna Nezdolii ([nezdolik](https://github.com/nezdolik)) (kateryna.nezdolii@gmail.com)
  * Load balancing, GeoIP, overload manager, security.

# Envoy mobile maintainers

The following Envoy maintainers have final say over any changes only affecting /mobile

* Ali Beyad ([abeyad](https://github.com/abeyad)) (abeyad@google.com)
  * xDS, C++ integration tests.

# Senior extension maintainers

The following extension maintainers have final say over the extensions mentioned below. Once they
approve an extension PR, it will be merged by the maintainer on-call (or any other maintainer)
without further review.

* Michael Warres ([mpwarres] (https://github.com/mpwarres)) (mpw@google.com)
  * Wasm
* doujiang24 ([doujiang24] https://github.com/doujiang24) (doujiang24@gmail.com)
  * Golang

# Envoy security team

* All senior maintainers
* Tony Allen ([tonya11en](https://github.com/tonya11en)) (tony@allen.gg)
* Otto van der Schaaf ([oschaaf](https://github.com/oschaaf)) (oschaaf@redhat.com)
* Tim Walsh ([twghu](https://github.com/twghu)) (twalsh@redhat.com)
* Pradeep Rao ([pradeepcrao](https://github.com/pradeepcrao)) (pcrao@google.com)
* Kateryna Nezdolii ([nezdolik](https://github.com/nezdolik)) (nezdolik@spotify.com)
* Boteng Yao ([botengyao](https://github.com/botengyao)) (boteng@google.com)
* Kevin Baichoo ([KBaichoo](https://github.com/KBaichoo)) (kbaichoo@google.com)
* Tianyu Xia ([tyxia](https://github.com/tyxia)) (tyxia@google.com)

# Emeritus maintainers

* Constance Caramanolis ([ccaraman](https://github.com/ccaraman)) (ccaramanolis@lyft.com)
* Roman Dzhabarov ([RomanDzhabarov](https://github.com/RomanDzhabarov)) (rdzhabarov@lyft.com)
* Bill Gallagher ([wgallagher](https://github.com/wgallagher)) (bgallagher@lyft.com)
* Dan Noé ([dnoe](https://github.com/dnoe)) (dpn@google.com)
* Sotiris Nanopoulos ([davinci26](https://github.com/davinci26)) (Sotiris.Nanopoulos@microsoft.com)
* Asra Ali ([asraa](https://github.com/asraa)) (asraa@google.com)
* Jose Nino ([junr03](https://github.com/junr03)) (recruiting@junr03.com)
* Dhi Aurrahman ([dio](https://github.com/dio)) (dio@rockybars.com)
* Dmitry Rozhkov ([rojkov](https://github.com/rojkov)) (dmitry.rozhkov@intel.com)
* Michael Rebello ([rebello95](https://github.com/rebello95)) (mrebello@lyft.com)
* Alan Chiu ([buildbreaker](https://github.com/buildbreaker)) (achiu@lyft.com)
* Charles Le Borgne ([carloseltuerto](https://github.com/carloseltuerto)) (cleborgne@google.com)
* William A Rowe Jr ([wrowe](https://github.com/wrowe)) (wrowe@rowe-clan.net)
* Antonio Vicente ([antoniovicente](https://github.com/antoniovicente)) (avd@google.com)
* JP Simard ([jpsim](https://github.com/jpsim)) (jp@lyft.com)
* Rafal Augustyniak ([Augustyniak](https://github.com/Augustyniak)) (raugustyniak@lyft.com)
* Snow Pettersen ([snowp](https://github.com/snowp)) (aickck@gmail.com)

# Friends of Envoy

This section lists a few people that are not maintainers but typically help out with subject
matter expert reviews. Feel free to loop them in as needed.

* Yuchen Dai ([lambdai](https://github.com/lambdai)) (lambdai@google.com)
  * v2 xDS, listeners, filter chain discovery service.
* Michael Payne ([moderation](https://github.com/moderation)) (m@m17e.org)
  * External dependencies, Envoy's supply chain and documentation.
* Cerek Hillen ([crockeo](https://github.com/crockeo)) (chillen@lyft.com)
  * Python and C++ platform bindings.
load("@build_bazel_apple_support//lib:repositories.bzl", "apple_support_dependencies")
load("@build_bazel_rules_apple//apple:repositories.bzl", "apple_rules_dependencies")
load("@build_bazel_rules_swift//swift:repositories.bzl", "swift_rules_dependencies")
load("@io_bazel_rules_kotlin//kotlin:repositories.bzl", "kotlin_repositories")
load("@robolectric//bazel:robolectric.bzl", "robolectric_repositories")
load("@rules_detekt//detekt:dependencies.bzl", "rules_detekt_dependencies")
load("@rules_java//java:repositories.bzl", "rules_java_dependencies")
load("@rules_jvm_external//:defs.bzl", "maven_install")
load("@rules_proto//proto:repositories.bzl", "rules_proto_dependencies", "rules_proto_toolchains")
load("@rules_proto_grpc//:repositories.bzl", "rules_proto_grpc_repos", "rules_proto_grpc_toolchains")

def _default_extra_swift_sources_impl(ctx):
    ctx.file("WORKSPACE", "")
    ctx.file("empty.swift", "")
    ctx.file("BUILD.bazel", """
filegroup(
    name = "extra_swift_srcs",
    srcs = ["empty.swift"],
    visibility = ["//visibility:public"],
)

objc_library(
    name = "extra_private_dep",
    module_name = "FakeDep",
    visibility = ["//visibility:public"],
)""")

_default_extra_swift_sources = repository_rule(
    implementation = _default_extra_swift_sources_impl,
)

def _default_extra_jni_deps_impl(ctx):
    ctx.file("WORKSPACE", "")
    ctx.file("BUILD.bazel", """
cc_library(
    name = "extra_jni_dep",
    visibility = ["//visibility:public"],
)""")

_default_extra_jni_deps = repository_rule(
    implementation = _default_extra_jni_deps_impl,
)

def envoy_mobile_dependencies(extra_maven_dependencies = []):
    if not native.existing_rule("envoy_mobile_extra_swift_sources"):
        _default_extra_swift_sources(name = "envoy_mobile_extra_swift_sources")
    if not native.existing_rule("envoy_mobile_extra_jni_deps"):
        _default_extra_jni_deps(name = "envoy_mobile_extra_jni_deps")

    swift_dependencies()
    kotlin_dependencies(extra_maven_dependencies)

def swift_dependencies():
    apple_support_dependencies()
    apple_rules_dependencies(ignore_version_differences = True)
    swift_rules_dependencies()

def kotlin_dependencies(extra_maven_dependencies = []):
    rules_java_dependencies()
    maven_install(
        artifacts = [
            "com.google.code.findbugs:jsr305:3.0.2",
            # Java Proto Lite
            "com.google.protobuf:protobuf-javalite:3.24.4",
            # Kotlin
            "org.jetbrains.kotlin:kotlin-stdlib-jdk8:1.6.21",
            "org.jetbrains.kotlin:kotlin-stdlib-common:1.6.21",
            "org.jetbrains.kotlin:kotlin-stdlib:1.6.21",
            "androidx.recyclerview:recyclerview:1.1.0",
            "androidx.core:core:1.3.2",
            # Dokka
            "org.jetbrains.dokka:dokka-cli:1.5.31",
            "org.jetbrains.dokka:javadoc-plugin:1.5.31",
            # Test artifacts
            "org.assertj:assertj-core:3.23.1",
            "junit:junit:4.13",
            "org.mockito:mockito-inline:4.8.0",
            "org.mockito:mockito-core:4.8.0",
            "com.squareup.okhttp3:okhttp:4.10.0",
            "com.squareup.okhttp3:mockwebserver:4.10.0",
            "io.github.classgraph:classgraph:4.8.149",
            "io.netty:netty-all:4.1.82.Final",
            # Android test artifacts
            "androidx.test:core:1.4.0",
            "androidx.test:rules:1.4.0",
            "androidx.test:runner:1.4.0",
            "androidx.test:monitor:1.5.0",
            "androidx.test.ext:junit:1.1.3",
            "org.robolectric:robolectric:4.8.2",
            "org.hamcrest:hamcrest:2.2",
            "com.google.truth:truth:1.1.3",
        ] + extra_maven_dependencies,
        version_conflict_policy = "pinned",
        repositories = [
            "https://repo1.maven.org/maven2",
            "https://maven.google.com",
        ],
    )
    kotlin_repositories()
    rules_detekt_dependencies()
    robolectric_repositories()

    rules_proto_grpc_toolchains()
    rules_proto_grpc_repos()
    rules_proto_dependencies()
    rules_proto_toolchains()
"""
Rule to create objdump debug info from a native dynamic library built for
Android.

This is a workaround for generally not being able to produce dwp files for
Android https://github.com/bazelbuild/bazel/pull/14765

But even if we could create those we'd need to get them out of the build
somehow, this rule provides a separate --output_group for this
"""

def _impl(ctx):
    library_outputs = []
    objdump_outputs = []
    for platform, dep in ctx.split_attr.dep.items():
        # When --fat_apk_cpu isn't set, the platform is None
        if len(dep.files.to_list()) != 1:
            fail("Expected exactly one file in the library")

        cc_toolchain = ctx.split_attr._cc_toolchain[platform][cc_common.CcToolchainInfo]
        lib = dep.files.to_list()[0]
        platform_name = platform or ctx.fragments.android.android_cpu
        objdump_output = ctx.actions.declare_file(platform_name + "/" + platform_name + ".objdump.gz")

        ctx.actions.run_shell(
            inputs = [lib],
            outputs = [objdump_output],
            command = cc_toolchain.objdump_executable + " --syms " + lib.path + "| gzip -c >" + objdump_output.path,
            tools = [cc_toolchain.all_files],
            progress_message = "Generating symbol map " + platform_name,
        )

        strip_output = ctx.actions.declare_file(platform_name + "/" + lib.basename)
        ctx.actions.run_shell(
            inputs = [lib],
            outputs = [strip_output],
            command = cc_toolchain.strip_executable + " --strip-all " + lib.path + " -o " + strip_output.path,
            tools = [cc_toolchain.all_files],
            progress_message = "Stripping library " + lib.path,
        )

        library_outputs.append(strip_output)
        objdump_outputs.append(objdump_output)

    return [
        DefaultInfo(files = depset(library_outputs)),
        OutputGroupInfo(objdump = objdump_outputs),
    ]

android_debug_info = rule(
    implementation = _impl,
    attrs = dict(
        dep = attr.label(
            providers = [CcInfo],
            cfg = android_common.multi_cpu_configuration,
        ),
        _cc_toolchain = attr.label(
            default = Label("@bazel_tools//tools/cpp:current_cc_toolchain"),
            cfg = android_common.multi_cpu_configuration,
        ),
    ),
    fragments = ["cpp", "android"],
)
load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive", "http_file")

def envoy_mobile_repositories():
    http_archive(
        name = "google_bazel_common",
        sha256 = "d8c9586b24ce4a5513d972668f94b62eb7d705b92405d4bc102131f294751f1d",
        strip_prefix = "bazel-common-413b433b91f26dbe39cdbc20f742ad6555dd1e27",
        urls = ["https://github.com/google/bazel-common/archive/413b433b91f26dbe39cdbc20f742ad6555dd1e27.zip"],
    )

    upstream_envoy_overrides()
    swift_repos()
    kotlin_repos()
    android_repos()
    python_repos()

def upstream_envoy_overrides():
    # Workaround old NDK version breakages https://github.com/envoyproxy/envoy-mobile/issues/934
    http_archive(
        name = "com_github_libevent_libevent",
        urls = ["https://github.com/libevent/libevent/archive/0d7d85c2083f7a4c9efe01c061486f332b576d28.tar.gz"],
        strip_prefix = "libevent-0d7d85c2083f7a4c9efe01c061486f332b576d28",
        sha256 = "549d34065eb2485dfad6c8de638caaa6616ed130eec36dd978f73b6bdd5af113",
        build_file_content = """filegroup(name = "all", srcs = glob(["**"]), visibility = ["//visibility:public"])""",
    )

def swift_repos():
    http_archive(
        name = "DrString",
        build_file_content = """exports_files(["drstring"])""",
        sha256 = "860788450cf9900613454a51276366ea324d5bfe71d1844106e9c1f1d7dfd82b",
        url = "https://github.com/dduan/DrString/releases/download/0.5.2/drstring-x86_64-apple-darwin.tar.gz",
    )

    http_archive(
        name = "SwiftLint",
        build_file_content = """exports_files(["swiftlint"])""",
        sha256 = "47078845857fa7cf8497f5861967c7ce67f91915e073fb3d3114b8b2486a9270",
        url = "https://github.com/realm/SwiftLint/releases/download/0.50.3/portable_swiftlint.zip",
    )

    http_archive(
        name = "com_github_buildbuddy_io_rules_xcodeproj",
        sha256 = "d02932255ba3ffaab1859e44528c69988e93fa353fa349243e1ef5054bd1ba80",
        url = "https://github.com/buildbuddy-io/rules_xcodeproj/releases/download/1.2.0/release.tar.gz",
    )

def kotlin_repos():
    http_archive(
        name = "rules_java",
        sha256 = "241822bf5fad614e3e1c42431002abd9af757136fa590a6a7870c6e0640a82e3",
        strip_prefix = "rules_java-6.4.0",
        url = "https://github.com/bazelbuild/rules_java/archive/6.4.0.tar.gz",
        patch_args = ["-p1"],
        patches = ["@envoy//bazel:rules_java.patch"],
    )

    http_archive(
        name = "rules_jvm_external",
        sha256 = "b17d7388feb9bfa7f2fa09031b32707df529f26c91ab9e5d909eb1676badd9a6",
        strip_prefix = "rules_jvm_external-4.5",
        url = "https://github.com/bazelbuild/rules_jvm_external/archive/4.5.zip",
    )

    http_archive(
        name = "io_bazel_rules_kotlin",
        sha256 = "01293740a16e474669aba5b5a1fe3d368de5832442f164e4fbfc566815a8bc3a",
        urls = ["https://github.com/bazelbuild/rules_kotlin/releases/download/v1.8/rules_kotlin_release.tgz"],
    )

    http_archive(
        name = "rules_detekt",
        sha256 = "44912c74dc2e164227b1102ef36227d0e78fdbd7c7359868ae13424eb4f0d5c2",
        strip_prefix = "bazel_rules_detekt-0.6.0",
        url = "https://github.com/buildfoundation/bazel_rules_detekt/archive/v0.6.0.tar.gz",
    )

    http_archive(
        name = "rules_proto_grpc",
        sha256 = "507e38c8d95c7efa4f3b1c0595a8e8f139c885cb41a76cab7e20e4e67ae87731",
        strip_prefix = "rules_proto_grpc-4.1.1",
        urls = ["https://github.com/rules-proto-grpc/rules_proto_grpc/archive/4.1.1.tar.gz"],
    )

    http_file(
        name = "kotlin_formatter",
        executable = 1,
        sha256 = "115d4c5cb3421eae732c42c137f5db8881ff9cc1ef180a01e638283f3ccbae44",
        urls = ["https://github.com/pinterest/ktlint/releases/download/0.37.1/ktlint"],
    )

    http_archive(
        name = "robolectric",
        sha256 = "5bcde5db598f6938c9887a140a0a1249f95d3c16274d40869503d0c322a20d5d",
        urls = ["https://github.com/robolectric/robolectric-bazel/archive/4.8.2.tar.gz"],
        strip_prefix = "robolectric-bazel-4.8.2",
    )

def android_repos():
    http_archive(
        name = "build_bazel_rules_android",
        urls = ["https://github.com/bazelbuild/rules_android/archive/refs/tags/v0.1.1.zip"],
        sha256 = "cd06d15dd8bb59926e4d65f9003bfc20f9da4b2519985c27e190cddc8b7a7806",
        strip_prefix = "rules_android-0.1.1",
    )

def python_repos():
    http_archive(
        name = "pybind11_bazel",
        strip_prefix = "pybind11_bazel-26973c0ff320cb4b39e45bc3e4297b82bc3a6c09",
        urls = ["https://github.com/pybind/pybind11_bazel/archive/26973c0ff320cb4b39e45bc3e4297b82bc3a6c09.zip"],
        sha256 = "a5666d950c3344a8b0d3892a88dc6b55c8e0c78764f9294e806d69213c03f19d",
    )
    http_archive(
        name = "pybind11",
        build_file = "@pybind11_bazel//:pybind11.BUILD",
        strip_prefix = "pybind11-2.6.1",
        urls = ["https://github.com/pybind/pybind11/archive/v2.6.1.tar.gz"],
        sha256 = "cdbe326d357f18b83d10322ba202d69f11b2f49e2d87ade0dc2be0c5c34f8e2a",
    )
abi_bzl_template = """\
def python_tag():
    return "{python_tag}"

def abi_tag():
    return "{abi_tag}"
"""

# we reuse the PYTHON_BIN_PATH environment variable from pybind11 so that the
# ABI tag we detect is always compatible with the version of python that was
# used for the build
_PYTHON_BIN_PATH_ENV = "PYTHON_BIN_PATH"

def _get_python_bin(rctx):
    python_version = rctx.attr.python_version
    if python_version != "2" and python_version != "3":
        fail("python_version must be one of: '2', '3'")

    python_bin = rctx.os.environ.get(_PYTHON_BIN_PATH_ENV)
    if python_bin != None:
        return python_bin

    python_bin = rctx.which("python" + python_version)
    if python_bin != None:
        return python_bin

    fail("cannot find python binary")

def _get_python_tag(rctx, python_bin):
    result = rctx.execute([
        python_bin,
        "-c",
        "import platform;" +
        "assert platform.python_implementation() == 'CPython';" +
        "version = platform.python_version_tuple();" +
        "print(f'cp{version[0]}{version[1]}')",
    ])
    return result.stdout.splitlines()[0]

def _get_abi_tag(rctx, python_bin):
    result = rctx.execute([
        python_bin,
        "-c",
        "import platform;" +
        "import sys;" +
        "assert platform.python_implementation() == 'CPython';" +
        "version = platform.python_version_tuple();" +
        "print(f'cp{version[0]}{version[1]}{sys.abiflags}')",
    ])
    return result.stdout.splitlines()[0]

def _declare_python_abi_impl(rctx):
    python_bin = _get_python_bin(rctx)
    python_tag = _get_python_tag(rctx, python_bin)
    abi_tag = _get_abi_tag(rctx, python_bin)
    rctx.file("BUILD")
    rctx.file(
        "abi.bzl",
        abi_bzl_template.format(
            python_tag = python_tag,
            abi_tag = abi_tag,
        ),
    )

declare_python_abi = repository_rule(
    implementation = _declare_python_abi_impl,
    attrs = {
        "python_version": attr.string(mandatory = True),
    },
    local = True,
)
load("@bazel_skylib//rules:copy_file.bzl", "copy_file")
load("@io_bazel_rules_kotlin//kotlin:jvm.bzl", "kt_jvm_library")

# This is the magic function which helps get the name of the native library
# from the native dependency. In general, the bazel cc_binary rules will
# output a binary based on the target name. This macro just infers the output
# so file name.
#
# The main functionality of this method is used for integration java/android
# testing. Bazel itself doesn't play well with different genrules outputting
# the same output. In this project there's 3 types of artifacts we end up
# using: envoy's aar, integration tests with just vanilla jvm, integration
# tests with android. Each of these require a different so file built which
# means that Bazel will have to output 3 types of so files with different names.
def native_lib_name(native_dep):
    lib_name = ""
    if ":" in native_dep:
        lib_name = native_dep.split(":")[1].split(".so")[0]
    else:
        lib_name = native_dep.split(".so")[0]
    return lib_name

def envoy_mobile_kt_library(name, visibility = None, srcs = [], deps = [], exports = []):
    # These source files must be re-exported to the kotlin custom library rule to ensure their
    # inclusion. This is used to work around testing visibility.
    native.filegroup(
        name = name + "_srcs",
        srcs = srcs,
        visibility = visibility,
    )

    kt_jvm_library(
        name = name,
        srcs = srcs,
        deps = deps,
        exports = exports,
        visibility = visibility,
    )

# Basic macro which uses a genrule to generate a jnilib file from an so file
#
# The native dep passed in should be the cc_binary target of the name
# lib{}.so since linux platforms expect the .so file to be named in that form.
# The resulting output will be lib{}.jnilib for the OS X platforms.
#
# Example usage:
# envoy_mobile_so_to_jni_lib(
#     name = "java_jni_lib.jnilib",
#     native_dep = "libjava_jni_lib.so",
# )
def envoy_mobile_so_to_jni_lib(name, native_dep, testonly = False):
    lib_name = native_lib_name(native_dep)
    copy_file(
        name = name,
        src = native_dep,
        out = "{}.jnilib".format(lib_name),
        testonly = testonly,
    )
"""
Propagate the generated Swift header from a swift_library target
This exists to work around https://github.com/bazelbuild/rules_swift/issues/291
"""

def _swift_header_collector(ctx):
    headers = [
        DefaultInfo(
            files = ctx.attr.library[CcInfo].compilation_context.headers,
        ),
    ]

    if len(headers[0].files.to_list()) != 1:
        header_names = [header.basename for header in headers[0].files.to_list()]
        fail("Expected exactly 1 '-Swift.h' header, got {}".format(header_names))

    return headers

swift_header_collector = rule(
    attrs = dict(
        library = attr.label(
            mandatory = True,
            providers = [CcInfo],
        ),
    ),
    implementation = _swift_header_collector,
)
package io.envoyproxy.envoymobile.bazel

import io.github.classgraph.ClassGraph
import junit.framework.JUnit4TestAdapter
import junit.framework.TestSuite
import org.junit.runner.RunWith

@RunWith(org.junit.runners.AllTests::class)
object EnvoyMobileTestSuite {
  private val junitTestAnnotation = org.junit.Test::class.java.name

  @JvmStatic
  fun suite(): TestSuite {
    val suite = TestSuite()

    val scan = ClassGraph()
      .disableModuleScanning()
      .enableAnnotationInfo()
      .enableMethodInfo()
      .ignoreClassVisibility()
      .acceptPackages("io.envoyproxy", "test.kotlin.integration", "org.chromium.net")
      .scan()
    scan.getClassesWithMethodAnnotation(junitTestAnnotation)
      .asSequence()
      .sortedByDescending { it.name }
      .map { JUnit4TestAdapter(it.loadClass()) }
      .forEach(suite::addTest)

    return suite
  }
}
"""Shared configuration for things we don't want to duplicate"""

MINIMUM_IOS_VERSION = "13.0"
load("@io_bazel_rules_kotlin//kotlin:core.bzl", "kt_register_toolchains")
load("@rules_detekt//detekt:toolchains.bzl", "rules_detekt_toolchains")
load("@rules_java//java:repositories.bzl", "rules_java_toolchains")
load("@rules_proto_grpc//:repositories.bzl", "rules_proto_grpc_toolchains")

def envoy_mobile_toolchains():
    rules_java_toolchains()
    kt_register_toolchains()
    rules_detekt_toolchains(detekt_version = "1.20.0")
    rules_proto_grpc_toolchains()
load("@build_bazel_rules_apple//apple:ios.bzl", "ios_unit_test")
load("@build_bazel_rules_swift//swift:swift.bzl", "swift_library")
load("@envoy//bazel:envoy_build_system.bzl", "envoy_mobile_defines")
load("//bazel:config.bzl", "MINIMUM_IOS_VERSION")

def envoy_objc_library(name, hdrs = [], visibility = [], data = [], deps = [], module_name = None, sdk_frameworks = [], srcs = [], testonly = False):
    native.objc_library(
        name = name,
        srcs = srcs,
        hdrs = hdrs,
        copts = ["-ObjC++", "-std=c++17", "-Wno-shorten-64-to-32"],
        defines = envoy_mobile_defines("@envoy"),
        module_name = module_name,
        sdk_frameworks = sdk_frameworks,
        visibility = visibility,
        data = data,
        deps = deps,
        testonly = testonly,
    )

# Macros providing a way to easily/consistently define Swift/ObjC unit test targets.
#
# - Prevents consumers from having to define both swift_library and ios_unit_test targets
# - Provides a set of linker options that is required to properly run tests
# - Sets default visibility and OS requirements
#
# Usage example:
# load("@envoy_mobile//bazel:apple.bzl", "envoy_mobile_swift_test")
#
# envoy_mobile_swift_test(
#     name = "sample_test",
#     srcs = [
#         "SampleTest.swift",
#     ],
# )
#
def envoy_mobile_swift_test(name, srcs, size = None, data = [], deps = [], tags = [], repository = "", visibility = [], flaky = False):
    test_lib_name = name + "_lib"
    swift_library(
        name = test_lib_name,
        srcs = srcs,
        data = data,
        deps = [
            repository + "//library/swift:ios_lib",
        ] + deps,
        linkopts = ["-lresolv.9"],
        testonly = True,
        visibility = ["//visibility:private"],
    )

    ios_unit_test(
        name = name,
        data = data,
        deps = [test_lib_name],
        minimum_os_version = MINIMUM_IOS_VERSION,
        size = size,
        tags = tags,
        visibility = visibility,
        flaky = flaky,
    )

def envoy_mobile_objc_test(name, srcs, data = [], deps = [], tags = [], visibility = [], flaky = False):
    test_lib_name = name + "_lib"
    envoy_objc_library(
        name = test_lib_name,
        srcs = srcs,
        data = data,
        deps = deps,
        visibility = ["//visibility:private"],
    )

    ios_unit_test(
        name = name,
        data = data,
        deps = [test_lib_name],
        minimum_os_version = MINIMUM_IOS_VERSION,
        tags = tags,
        visibility = visibility,
        flaky = flaky,
    )

def envoy_mobile_swift_copts(enable_cxx_interop):
    if enable_cxx_interop:
        return [
            "-enable-experimental-cxx-interop",
            "-Xcc",
            "-std=c++17",
            "-Xcc",
            "-Wno-deprecated-declarations",
        ]
    else:
        return []
"""Repository rule for Android SDK and NDK autoconfiguration.

This rule is a no-op unless the required android environment variables are set.
"""

# Based on https://github.com/tensorflow/tensorflow/tree/34c03ed67692eb76cb3399cebca50ea8bcde064c/third_party/android
# Workaround for https://github.com/bazelbuild/bazel/issues/14260

_ANDROID_NDK_HOME = "ANDROID_NDK_HOME"
_ANDROID_SDK_HOME = "ANDROID_HOME"

def _android_autoconf_impl(repository_ctx):
    sdk_home = repository_ctx.os.environ.get(_ANDROID_SDK_HOME)
    ndk_home = repository_ctx.os.environ.get(_ANDROID_NDK_HOME)

    sdk_api_level = repository_ctx.attr.sdk_api_level
    ndk_api_level = repository_ctx.attr.ndk_api_level
    build_tools_version = repository_ctx.attr.build_tools_version

    sdk_rule = ""
    if sdk_home:
        sdk_rule = """
    native.android_sdk_repository(
        name="androidsdk",
        path="{}",
        api_level={},
        build_tools_version="{}",
    )
""".format(sdk_home, sdk_api_level, build_tools_version)

    ndk_rule = ""
    if ndk_home:
        ndk_rule = """
    native.android_ndk_repository(
        name="androidndk",
        path="{}",
        api_level={},
    )
""".format(ndk_home, ndk_api_level)

    if ndk_rule == "" and sdk_rule == "":
        sdk_rule = "pass"

    repository_ctx.file("BUILD.bazel", "")
    repository_ctx.file("android_configure.bzl", """
def android_workspace():
    {}
    {}
    """.format(sdk_rule, ndk_rule))

android_configure = repository_rule(
    implementation = _android_autoconf_impl,
    environ = [
        _ANDROID_NDK_HOME,
        _ANDROID_SDK_HOME,
    ],
    attrs = {
        "sdk_api_level": attr.int(mandatory = True),
        "ndk_api_level": attr.int(mandatory = True),
        "build_tools_version": attr.string(mandatory = True),
    },
)
#!/usr/bin/env bash

# Bazel expects the helper to read stdin.
# See https://github.com/bazelbuild/bazel/pull/17666
cat /dev/stdin > /dev/null

# `GITHUB_TOKEN` is provided as a secret.
echo "{\"headers\":{\"Authorization\":[\"Bearer ${GITHUB_TOKEN}\"]}}"
load("@build_bazel_rules_android//android:rules.bzl", "android_library")
load("@io_bazel_rules_kotlin//kotlin:android.bzl", "kt_android_local_test")
load("@io_bazel_rules_kotlin//kotlin:jvm.bzl", "kt_jvm_test")
load("//bazel:kotlin_lib.bzl", "native_lib_name")

def _internal_kt_test(name, srcs, deps = [], data = [], jvm_flags = [], repository = "", exec_properties = {}):
    # This is to work around the issue where we have specific implementation functionality which
    # we want to avoid consumers to use but we want to unit test
    dep_srcs = []
    for dep in deps:
        # We'll resolve only the targets in `//library/kotlin/io/envoyproxy/envoymobile`
        if dep.startswith(repository + "//library/kotlin/io/envoyproxy/envoymobile"):
            dep_srcs.append(dep + "_srcs")
        elif dep.startswith(repository + "//library/java/io/envoyproxy/envoymobile"):
            dep_srcs.append(dep + "_srcs")

    kt_jvm_test(
        name = name,
        test_class = "io.envoyproxy.envoymobile.bazel.EnvoyMobileTestSuite",
        srcs = srcs + dep_srcs,
        deps = [
            repository + "//bazel:envoy_mobile_test_suite",
            "@maven//:org_assertj_assertj_core",
            "@maven//:junit_junit",
            "@maven//:org_mockito_mockito_inline",
            "@maven//:org_mockito_mockito_core",
        ] + deps,
        data = data,
        jvm_flags = jvm_flags,
        exec_properties = exec_properties,
    )

# A simple macro to define the JVM flags that are common for envoy_mobile_jni_kt_test and
# envoy_mobile_android_test.
def jvm_flags(lib_name):
    return [
        "-Djava.library.path=library/common/jni:test/common/jni",
        "-Denvoy_jni_library_name={}".format(lib_name),
        "-Xcheck:jni",
    ] + select({
        "@envoy//bazel:disable_google_grpc": ["-Denvoy_jni_google_grpc_disabled=true"],
        "//conditions:default": [],
    }) + select({
        "@envoy//bazel:disable_envoy_mobile_xds": ["-Denvoy_jni_envoy_mobile_xds_disabled=true"],
        "//conditions:default": [],
    })

# A basic macro to make it easier to declare and run kotlin tests which depend on a JNI lib
# This will create the native .so binary (for linux) and a .jnilib (for macOS) look up
def envoy_mobile_jni_kt_test(name, srcs, native_lib_name, native_deps = [], deps = [], repository = "", exec_properties = {}):
    _internal_kt_test(
        name,
        srcs,
        deps,
        data = native_deps,
        jvm_flags = jvm_flags(native_lib_name),
        repository = repository,
        exec_properties = exec_properties,
    )

# A basic macro to make it easier to declare and run kotlin tests
#
# Ergonomic improvements include:
# 1. Avoiding the need to declare the test_class which requires a fully qualified class name (example below)
# 2. Avoiding the need to redeclare common unit testing dependencies like JUnit
# 3. Ability to run more than one test file per target
# 4. Ability to test internal envoy mobile entities
# Usage example:
# load("@envoy_mobile//bazel:kotlin_test.bzl", "envoy_mobile_kt_test)
#
# envoy_mobile_kt_test(
#     name = "example_kotlin_test",
#     srcs = [
#         "ExampleTest.kt",
#     ],
# )
def envoy_mobile_kt_test(name, srcs, deps = [], repository = "", exec_properties = {}):
    _internal_kt_test(name, srcs, deps, repository = repository, exec_properties = exec_properties)

# A basic macro to run android based (robolectric) tests with native dependencies
def envoy_mobile_android_test(name, srcs, native_lib_name, deps = [], native_deps = [], repository = "", exec_properties = {}):
    android_library(
        name = name + "_test_lib",
        custom_package = "io.envoyproxy.envoymobile.test",
        manifest = repository + "//bazel:test_manifest.xml",
        visibility = ["//visibility:public"],
        data = native_deps,
        exports = deps,
        testonly = True,
    )
    kt_android_local_test(
        name = name,
        srcs = srcs,
        data = native_deps,
        deps = deps + [
            repository + "//bazel:envoy_mobile_test_suite",
            "@maven//:androidx_annotation_annotation",
            "@maven//:androidx_test_core",
            "@maven//:androidx_test_ext_junit",
            "@maven//:androidx_test_runner",
            "@maven//:androidx_test_monitor",
            "@maven//:androidx_test_rules",
            "@maven//:org_robolectric_robolectric",
            "@robolectric//bazel:android-all",
            "@maven//:org_assertj_assertj_core",
            "@maven//:junit_junit",
            "@maven//:org_mockito_mockito_inline",
            "@maven//:org_mockito_mockito_core",
            "@maven//:com_squareup_okhttp3_okhttp",
            "@maven//:com_squareup_okhttp3_mockwebserver",
            "@maven//:com_squareup_okio_okio_jvm",
            "@maven//:org_hamcrest_hamcrest",
            "@maven//:com_google_truth_truth",
        ],
        manifest = repository + "//bazel:test_manifest.xml",
        custom_package = "io.envoyproxy.envoymobile.tests",
        test_class = "io.envoyproxy.envoymobile.bazel.EnvoyMobileTestSuite",
        jvm_flags = jvm_flags(native_lib_name),
        exec_properties = exec_properties,
    )
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
          package="io.envoyproxy.envoymobile.test">

    <uses-permission android:name="android.permission.INTERNET"/>
    <!-- This permission is needed in order to allow the application
     to initiate DNS resolution via c-ares. -->
    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE"/>

    <uses-sdk
            android:minSdkVersion="21"
            android:targetSdkVersion="27"/>
</manifest>
<?xml version="1.0" encoding="UTF-8"?>
<project xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"
         xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <modelVersion>4.0.0</modelVersion>

    <groupId>io.envoyproxy.envoymobile</groupId>
    <artifactId>{pom_artifact_id}</artifactId>
    <version>{pom_version}</version>
    <packaging>aar</packaging>
    <dependencies>
        {generated_bzl_deps}
        {pom_extra_dependencies}
    </dependencies>

    <name>Envoy Mobile</name>
    <description>Client networking libraries based on the Envoy project.</description>
    <url>https://envoymobile.io</url>

    <licenses>
        <license>
            <name>The Apache Software License, Version 2.0</name>
            <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>
            <distribution>repo</distribution>
        </license>
    </licenses>
    <developers>
        <developer>
            <id>alyssawilk</id>
            <name>Alyssa Wilk</name>
            <email>alyssar@google.com</email>
        </developer>
        <developer>
            <id>ryantheoptimist</id>
            <name>Ryan Hamilton</name>
            <email>rch@google.com</email>
        </developer>
        <developer>
            <id>abeyad</id>
            <name>Ali Beyad</name>
            <email>abeyad@google.com</email>
        </developer>
    </developers>
    <scm>
        <url>https://github.com/envoyproxy/envoy</url>
        <connection>scm:git:git@github.com:envoyproxy/envoy.git</connection>
        <developerConnection>scm:git:git@github.com:envoyproxy/envoy.git</developerConnection>
        <tag>HEAD</tag>
    </scm>

</project>
load("@io_bazel_rules_kotlin//kotlin:jvm.bzl", "kt_jvm_library")
load("@envoy//bazel:envoy_build_system.bzl", "envoy_mobile_package")

licenses(["notice"])  # Apache 2

envoy_mobile_package()

config_setting(
    name = "envoy_mobile_swift_cxx_interop",
    values = {"define": "envoy_mobile_swift_cxx_interop=enabled"},
)

kt_jvm_library(
    name = "envoy_mobile_test_suite",
    srcs = [
        "EnvoyMobileTestSuite.kt",
    ],
    visibility = ["//visibility:public"],
    deps = [
        "@maven//:io_github_classgraph_classgraph",
        "@maven//:junit_junit",
    ],
)

config_setting(
    name = "exclude_certificates",
    values = {"define": "exclude_certificates=true"},
)

exports_files([
    "pom_template.xml",
    "test_manifest.xml",
])

alias(
    name = "zipper",
    actual = "@bazel_tools//tools/zip:zipper",
)

# Don't depend on these directly, use //library/common/jni:jni_import_lib.
alias(
    name = "jni",
    actual = "@bazel_tools//tools/jdk:jni",
    visibility = ["//library/common/jni/import:__pkg__"],
)

# autoformat requires that there be no @bazel_tools references outside of //bazel,
# so we alias all of the relevant platforms for determining wheel packaging here
alias(
    name = "darwin_arm64",
    actual = "@bazel_tools//src/conditions:darwin_arm64",
)

alias(
    name = "darwin_x86_64",
    actual = "@bazel_tools//src/conditions:darwin_x86_64",
)

alias(
    name = "darwin",
    actual = "@bazel_tools//src/conditions:darwin",
)

alias(
    name = "linux_aarch64",
    actual = "@bazel_tools//src/conditions:linux_aarch64",
)

alias(
    name = "linux_x86_64",
    actual = "@bazel_tools//src/conditions:linux_x86_64",
)
"""
This rule declares outputs for files from a distributable framework.
This allows us to reproduce what it's like to import our distribution artifact
within the same build. Ideally we could just propagate the directory so we
didn't have to enumerate the files in the framework zip, but that isn't
supported by 'apple_static_framework_import'.
"""

load("//bazel:config.bzl", "MINIMUM_IOS_VERSION")

def _framework_imports_extractor(ctx):
    outputs = [
        ctx.actions.declare_file("Envoy.framework/Envoy"),
        ctx.actions.declare_file("Envoy.framework/Headers/Envoy.h"),
        ctx.actions.declare_file("Envoy.framework/Modules/module.modulemap"),
    ]
    for arch in ctx.split_attr.framework.keys():
        if not arch.startswith("ios_"):
            fail("Unexpected arch: {}".format(arch))

        arch = arch[4:]

        # ios_sim_arm64 is a temporary special case for the M1.
        if arch.startswith("sim_"):
            arch = arch[4:]

        outputs.extend([
            ctx.actions.declare_file("Envoy.framework/Modules/Envoy.swiftmodule/{}.swiftdoc".format(arch)),
            ctx.actions.declare_file("Envoy.framework/Modules/Envoy.swiftmodule/{}.swiftinterface".format(arch)),
        ])

    if len(ctx.attr.framework[0].files.to_list()) != 1:
        fail("Expected exactly one framework zip, got {}".format(ctx.attr.framework[0].files))

    framework_zip = ctx.attr.framework[0].files.to_list()[0]
    ctx.actions.run_shell(
        inputs = [framework_zip],
        outputs = outputs,
        # Workaround for https://github.com/bazelbuild/rules_apple/issues/1489
        command = "unzip -qq {} -d {} || true".format(framework_zip.path, ctx.bin_dir.path),
        progress_message = "Extracting framework",
    )

    return [DefaultInfo(files = depset(outputs))]

framework_imports_extractor = rule(
    attrs = dict(
        framework = attr.label(
            mandatory = True,
            cfg = apple_common.multi_arch_split,
        ),
        platform_type = attr.string(default = "ios"),
        minimum_os_version = attr.string(default = MINIMUM_IOS_VERSION),
    ),
    # fragments = ["apple"],
    implementation = _framework_imports_extractor,
)
def _sources_javadocs_impl(ctx):
    javabase = ctx.attr._javabase[java_common.JavaRuntimeInfo]
    plugins_classpath = ";".join([
        ctx.file._dokka_base_jar.path,
        ctx.file._dokka_analysis_jar.path,
        ctx.file._dokka_kotlin_as_java_jar.path,
        ctx.file._dokka_analysis_intellij_jar.path,
        ctx.file._dokka_analysis_compiler_jar.path,
        ctx.file._korte_jvm_jar.path,
        ctx.file._dokka_javadoc_jar.path,
    ])
    output_jar = ctx.actions.declare_file("{}.jar".format(ctx.attr.name))

    ctx.actions.run_shell(
        command = """
        set -euo pipefail

        java=$1
        dokka_cli_jar=$2
        plugin_classpath=$3
        sources_jar=$4
        output_jar=$5

        sources_dir=$(mktemp -d)
        tmp_dir=$(mktemp -d)
        trap 'rm -rf "$sources_dir" "$tmp_dir"' EXIT

        unzip $sources_jar -d $sources_dir > /dev/null

        $java \
            --add-opens java.base/java.util=ALL-UNNAMED \
            -jar $dokka_cli_jar \
            -pluginsClasspath $plugin_classpath \
            -moduleName "Envoy Mobile" \
            -sourceSet "-src $sources_dir -noStdlibLink -noJdkLink" \
            -outputDir $tmp_dir > /dev/null

        original_directory=$PWD
        cd $tmp_dir
        zip -r $original_directory/$output_jar . > /dev/null
        """,
        arguments = [
            javabase.java_executable_exec_path,
            ctx.file._dokka_cli_jar.path,
            plugins_classpath,
            ctx.file.sources_jar.path,
            output_jar.path,
        ],
        inputs = [
            ctx.file._dokka_cli_jar,
            ctx.file._dokka_base_jar,
            ctx.file._dokka_analysis_jar,
            ctx.file._dokka_kotlin_as_java_jar,
            ctx.file._dokka_analysis_intellij_jar,
            ctx.file._dokka_analysis_compiler_jar,
            ctx.file._korte_jvm_jar,
            ctx.file._dokka_javadoc_jar,
            ctx.file.sources_jar,
        ] + ctx.files._javabase,
        outputs = [output_jar],
        mnemonic = "EnvoyMobileDokka",
        progress_message = "Generating javadocs...",
    )

    return [
        DefaultInfo(files = depset([output_jar])),
    ]

sources_javadocs = rule(
    implementation = _sources_javadocs_impl,
    attrs = {
        "sources_jar": attr.label(allow_single_file = True),
        # Dokka CLI
        "_dokka_cli_jar": attr.label(
            default = "@maven//:org_jetbrains_dokka_dokka_cli",
            allow_single_file = True,
        ),
        # Dokka Javadoc plugin and dependencies
        "_dokka_analysis_jar": attr.label(
            default = "@maven//:org_jetbrains_dokka_dokka_analysis",
            allow_single_file = True,
        ),
        "_dokka_analysis_compiler_jar": attr.label(
            default = "@maven//:org_jetbrains_dokka_kotlin_analysis_compiler",
            allow_single_file = True,
        ),
        "_dokka_analysis_intellij_jar": attr.label(
            default = "@maven//:org_jetbrains_dokka_kotlin_analysis_intellij",
            allow_single_file = True,
        ),
        "_dokka_base_jar": attr.label(
            default = "@maven//:org_jetbrains_dokka_dokka_base",
            allow_single_file = True,
        ),
        "_dokka_javadoc_jar": attr.label(
            default = "@maven//:org_jetbrains_dokka_javadoc_plugin",
            allow_single_file = True,
        ),
        "_dokka_kotlin_as_java_jar": attr.label(
            default = "@maven//:org_jetbrains_dokka_kotlin_as_java_plugin",
            allow_single_file = True,
        ),
        "_korte_jvm_jar": attr.label(
            default = "@maven//:com_soywiz_korlibs_korte_korte_jvm",
            allow_single_file = True,
        ),
        # Java Runtime
        "_javabase": attr.label(
            default = Label("@bazel_tools//tools/jdk:current_java_runtime"),
            allow_files = True,
            providers = [java_common.JavaRuntimeInfo],
        ),
    },
)
load("@build_bazel_rules_android//android:rules.bzl", "android_binary")
load("@envoy_mobile//bazel:dokka.bzl", "sources_javadocs")
load("@google_bazel_common//tools/maven:pom_file.bzl", "pom_file")
load("@rules_java//java:defs.bzl", "java_binary")

# This file is based on https://github.com/aj-michael/aar_with_jni which is
# subject to the following copyright and license:
#
# MIT License
#
# Copyright (c) 2019 Adam Michael
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
def android_artifacts(name, android_library, manifest, archive_name, native_deps = [], proguard_rules = "", visibility = [], substitutions = {}):
    """
    NOTE: The bazel android_library's implicit aar output doesn't flatten its transitive
    dependencies. Additionally, when using the kotlin rules, the kt_android_library rule
    creates a few underlying libraries which makes the declared sources and dependencies
    a transitive dependency on the resulting android_library. The result of this is that
    the classes.jar in the resulting aar will be empty. In order to workaround this issue,
    this rule manually constructs the aar.


    This macro exposes two gen rules:
    1. `{name}` which outputs the aar, pom, sources.jar, javadoc.jar.
    2. `{name}_aar_only` which outputs the aar.

    :param name The name of the underlying gen rule.
    :param android_library The android library target.
    :native_deps The native dependency targets.
    :proguard_rules The proguard rules used for the aar.
    :visibility The visibility of the underlying gen rule.
    """

    # Create the aar
    _classes_jar = _create_classes_jar(name, manifest, android_library)
    _jni_archive = _create_jni_library(name, native_deps)
    _aar_output = _create_aar(name, archive_name, _classes_jar, _jni_archive, proguard_rules, visibility)

    native.filegroup(
        name = name + "_objdump_collector",
        srcs = native_deps,
        output_group = "objdump",
        visibility = ["//visibility:public"],
    )

    # Generate other needed files for a maven publish
    _sources_name, _javadocs_name = _create_sources_javadocs(name, android_library)
    _pom_name = _create_pom_xml(name, android_library, visibility, substitutions)
    native.genrule(
        name = name + "_with_artifacts",
        srcs = [
            _aar_output,
            _pom_name,
            _sources_name + "_deploy-src.jar",
            _javadocs_name,
        ],
        outs = [
            archive_name + ".aar",
            archive_name + "-pom.xml",
            archive_name + "-sources.jar",
            archive_name + "-javadoc.jar",
        ],
        visibility = visibility,
        cmd = """
        # Set source variables
        set -- $(SRCS)
        src_aar=$$1
        src_pom_xml=$$2
        src_sources_jar=$$3
        src_javadocs=$$4

        # Set output variables
        set -- $(OUTS)
        out_aar=$$1
        out_pom_xml=$$2
        out_sources_jar=$$3
        out_javadocs=$$4

        echo "Outputting pom.xml, sources.jar, and javadocs.jar..."
        cp $$src_aar $$out_aar
        cp $$src_pom_xml $$out_pom_xml
        cp $$src_sources_jar $$out_sources_jar
        cp $$src_javadocs $$out_javadocs
        echo "Finished!"
        """,
    )

def _create_aar(name, archive_name, classes_jar, jni_archive, proguard_rules, visibility):
    """
    This macro rule manually creates an aar artifact.

    The underlying gen rule does the following:
    1. Create the final aar manifest file.
    2. Unzips the apk file generated by the `jni_archive_name` into a temporary directory.
    3. Renames the `lib` directory to `jni` directory since the aar requires the so files
       to be in the `jni` directory.
    4. Copy the android binary `jar` output from the `android_binary_name` as `classes.jar`.
    5. Copy the proguard rules specified in the macro parameters.
    6. Override the apk's aar with a generated one.
    7. Zip everything in the temporary directory into the output.


    :param name Name of the aar generation rule.
    :param archive_name Name of the resulting aar archive.
    :param classes_jar The classes.jar file which contains all the kotlin/java classes.
    :param jni_archive The apk with the desired jni libraries.
    :param proguard_rules The proguard.txt file.
    :param visibility The bazel visibility for the underlying rule.
    """
    _aar_output = name + "_local.aar"

    # This is to generate the envoy mobile aar AndroidManifest.xml
    _manifest_name = name + "_android_manifest"
    native.genrule(
        name = _manifest_name,
        outs = [_manifest_name + ".xml"],
        cmd = "cat > $(OUTS) <<EOF {}EOF".format(_manifest("io.envoyproxy.envoymobile")),
    )

    native.genrule(
        name = name,
        outs = [_aar_output],
        srcs = [
            classes_jar,
            jni_archive,
            _manifest_name,
            proguard_rules,
        ],
        cmd = """
        # Set source variables
        set -- $(SRCS)
        src_classes_jar=$$1
        src_jni_archive_apk=$$2
        src_manifest_xml=$$3
        src_proguard_txt=$$4

        original_directory=$$PWD

        echo "Constructing aar..."
        final_dir=$$(mktemp -d)
        cp $$src_classes_jar $$final_dir/classes.jar
        cd $$final_dir
        unzip $$original_directory/$$src_jni_archive_apk > /dev/null
        if [[ -d lib ]]; then
            mv lib jni
        else
            echo "No jni directory found"
        fi
        cp $$original_directory/$$src_proguard_txt ./proguard.txt
        cp $$original_directory/$$src_manifest_xml AndroidManifest.xml
        zip -r tmp.aar * > /dev/null
        cp tmp.aar $$original_directory/$@
        """,
        visibility = visibility,
    )

    return _aar_output

def _create_jni_library(name, native_deps = []):
    """
    Creates an apk containing the jni so files.

    :param name The name of the top level macro.
    :param native_deps The list of native dependency targets.
    """
    cc_lib_name = name + "_jni_interface_lib"
    jni_archive_name = name + "_jni"

    # Create a dummy manifest file for our android_binary
    native.genrule(
        name = name + "_binary_manifest_generator",
        outs = [name + "_generated_AndroidManifest.xml"],
        cmd = """cat > $(OUTS) <<EOF {}EOF""".format(_manifest("does.not.matter")),
    )

    # This outputs {jni_archive_name}_unsigned.apk which will contain the base files for our aar
    android_binary(
        name = jni_archive_name,
        manifest = name + "_generated_AndroidManifest.xml",
        custom_package = "does.not.matter",
        srcs = [],
        deps = [cc_lib_name],
    )

    # We wrap our native so dependencies in a cc_library because android_binaries
    # require a library target as dependencies in order to generate the appropriate
    # architectures in the directory `lib/`
    native.cc_library(
        name = cc_lib_name,
        srcs = native_deps,
    )

    return jni_archive_name + "_unsigned.apk"

def _create_classes_jar(name, manifest, android_library):
    """
    Creates the classes.jar which contains all the kotlin/java classes

    :param name The name of the top level macro
    :param manifest The manifest file used to create the initial apk
    :param android_library The android library target
    """
    android_binary_name = name + "_bin"

    # This creates bazel-bin/library/kotlin/io/envoyproxy/envoymobile/{name}_bin_deploy.jar
    # This jar has all the classes needed for our aar and will be our `classes.jar`
    android_binary(
        name = android_binary_name,
        manifest = manifest,
        custom_package = "does.not.matter",
        srcs = [],
        deps = [android_library],
    )

    native.genrule(
        name = name + "_classes_jar",
        outs = [name + "_classes.jar"],
        srcs = [android_binary_name + "_deploy.jar"],
        cmd = """
        original_directory=$$PWD
        classes_dir=$$(mktemp -d)
        echo "Creating classes.jar from $(SRCS)"
        pushd $$classes_dir
        unzip $$original_directory/$(SRCS) "io/envoyproxy/*" "META-INF/" > /dev/null
        find . -name "R.class" -type f -exec rm {} \\;
        find . -name "R\\$$*.class" -type f -exec rm {} \\;
        zip -r classes.jar * > /dev/null
        popd
        cp $$classes_dir/classes.jar $@
        """,
    )

    return name + "_classes.jar"

def _create_sources_javadocs(name, android_library):
    """
    Creates the sources.jar and javadocs.jar for the provided android library.

    This rule generates a sources jar first using a proxy java_binary's result and then uses
    kotlin/dokka's CLI tool to generate javadocs from the sources.jar.

    :param name The name of the top level macro.
    :param android_library The android library which to extract the sources and javadocs.
    """
    _sources_name = name + "_android_sources_jar"
    _javadocs_name = name + "_android_javadocs"

    # This implicitly outputs {name}_deploy-src.jar which is the sources jar
    java_binary(
        name = _sources_name,
        runtime_deps = [android_library],
        main_class = "EngineImpl",
    )

    # This takes all the source files from the source jar and creates a javadoc.jar from it
    sources_javadocs(
        name = _javadocs_name,
        sources_jar = _sources_name + "_deploy-src.jar",
    )

    return _sources_name, _javadocs_name

def _create_pom_xml(name, android_library, visibility, substitutions):
    """
    Creates a pom xml associated with the android_library target.

    :param name The name of the top level macro.
    :param android_library The android library to generate a pom xml for.
    """
    _pom_name = name + "_pom_xml"

    # This is for the pom xml. It has a public visibility since this can be accessed in the root BUILD file
    pom_file(
        name = _pom_name,
        targets = [android_library],
        visibility = visibility,
        substitutions = substitutions,
        template_file = "@envoy_mobile//bazel:pom_template.xml",
    )

    return _pom_name

def _manifest(package_name):
    """
    Helper function to create an appropriate manifest with a provided package name.

    :pram package_name The package name used in the manifest file.
    """
    return """
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
    package="{}" >

    <uses-sdk
            android:minSdkVersion="21"
            android:targetSdkVersion="29"/>
</manifest>
""".format(package_name)
# https://github.com/protocolbuffers/protobuf/pull/6720
diff --git a/third_party/BUILD b/third_party/BUILD
new file mode 100644
index 0000000000..b66101a39a
--- /dev/null
+++ b/third_party/BUILD
@@ -0,0 +1 @@
+exports_files(["six.BUILD", "zlib.BUILD"])

# patching for zlib binding
diff --git a/BUILD b/BUILD
index efc3d8e7f..746ad4851 100644
--- a/BUILD
+++ b/BUILD
@@ -24,7 +24,7 @@ config_setting(
 # ZLIB configuration
 ################################################################################

-ZLIB_DEPS = ["@zlib//:zlib"]
+ZLIB_DEPS = ["//external:zlib"]

 ################################################################################
 # Protobuf Runtime Library
diff --git a/python/google/protobuf/__init__.py b/python/google/protobuf/__init__.py
index 97ac28028..8b7585d9d 100644
--- a/python/google/protobuf/__init__.py
+++ b/python/google/protobuf/__init__.py
@@ -31,3 +31,9 @@
 # Copyright 2007 Google Inc. All Rights Reserved.

 __version__ = '3.16.0'
+
+if __name__ != '__main__':
+  try:
+    __import__('pkg_resources').declare_namespace(__name__)
+  except ImportError:
+    __path__ = __import__('pkgutil').extend_path(__path__, __name__)
# DEPRECATED

The [deprecated log](https://www.envoyproxy.io/docs/envoy/latest/version_history/version_history)
for each version can be found in the official Envoy developer documentation.
# Security Reporting Process

Please report any security issue or Envoy crash report to
envoy-security@googlegroups.com where the issue will be triaged appropriately.
Thank you in advance for helping to keep Envoy secure.

# Security Release Process

Envoy is a large growing community of volunteers, users, and vendors. The Envoy community has
adopted this security disclosure and response policy to ensure we responsibly handle critical
issues.

## Product Security Team (PST)

Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this
process is to reduce the total time users are vulnerable to publicly known exploits.

The Product Security Team (PST) is responsible for organizing the entire response including internal
communication and external disclosure but will need help from relevant developers to successfully
run this process.

The initial Product Security Team will consist of all [maintainers](OWNERS.md) in the private
[envoy-security](https://groups.google.com/forum/#!forum/envoy-security) list. In the future we may
decide to have a subset of maintainers work on security response given that this process is time
consuming.

## Disclosures

### Private Disclosure Processes

The Envoy community asks that all suspected vulnerabilities be privately and responsibly disclosed
via the [reporting policy](README.md#reporting-security-vulnerabilities).

### Public Disclosure Processes

If you know of a publicly disclosed security vulnerability please IMMEDIATELY email
[envoy-security](https://groups.google.com/forum/#!forum/envoy-security) to inform the Product
Security Team (PST) about the vulnerability so they may start the patch, release, and communication
process.

If possible the PST will ask the person making the public report if the issue can be handled via a
private disclosure process (for example if the full exploit details have not yet been published). If
the reporter denies the request for private disclosure, the PST will move swiftly with the fix and
release process. In extreme cases GitHub can be asked to delete the issue but this generally isn't
necessary and is unlikely to make a public disclosure less damaging.

## Patch, Release, and Public Communication

For each vulnerability a member of the PST will volunteer to lead coordination with the "Fix Team"
and is responsible for sending disclosure emails to the rest of the community. This lead will be
referred to as the "Fix Lead." The detailed list of responsibilities is outlined on the
[Fix Lead Checklist](https://docs.google.com/document/d/1cuU0m9hTQ73Te3i06-8LjQkFVn83IL22FbCoc_4IFEY/edit#heading=h.c6thx0zc0gtz)

The role of Fix Lead should rotate round-robin across the PST.

Note that given the current size of the Envoy community it is likely that the PST is the same as
the "Fix team." (I.e., all maintainers). The PST may decide to bring in additional contributors
for added expertise depending on the area of the code that contains the vulnerability.

All of the timelines below are suggestions and assume a private disclosure. The Fix Lead drives the
schedule using their best judgment based on severity and development time. If the Fix Lead is
dealing with a public disclosure all timelines become ASAP (assuming the vulnerability has a CVSS
score >= 4; see below). If the fix relies on another upstream project's disclosure timeline, that
will adjust the process as well. We will work with the upstream project to fit their timeline and
best protect our users.

### Released versions and main branch

If the vulnerability affects the last point release version, e.g. 1.10, then the full security
release process described in this document will be activated. A security point release will be
created for each currently supported Envoy version, as described in [stable releases](RELEASES.md#stable-releases),
together with a fix to main if necessary. Older point releases,
e.g. 1.5, are not supported by the Envoy project and will not have any security release created.

If a security vulnerability affects only these older versions but not main or the last supported
point release, the Envoy security team will share this information with the private distributor
list, following the standard embargo process, but not create a security release. After the embargo
expires, the vulnerability will be described as a GitHub issue. A CVE will be filed if warranted by
severity.

If a vulnerability does not affect any point release but only main, additional caveats apply:

* If the issue is detected and a fix is available within 7 days of the introduction of the
  vulnerability, or the issue is deemed a low severity vulnerability by the Envoy maintainer and
  security teams, the fix will be publicly reviewed and landed on main. If the severity is at least
  medium or at maintainer discretion a courtesy e-mail will be sent to envoy-users@googlegroups.com,
  envoy-dev@googlegroups.com, envoy-security-announce@googlegroups.com and
  cncf-envoy-distributors-announce@lists.cncf.io.
* If the vulnerability has been in existence for more than 7 days and is medium or higher, we will
  activate the security release process.

We advise distributors and operators working from the main branch to allow at least 5 days soak
time after cutting a binary release before distribution or rollout, to allow time for our fuzzers to
detect issues during their execution on ClusterFuzz. A soak period of 7 days provides an even stronger
guarantee, since we will invoke the security release process for medium or higher severity issues
for these older bugs.

**NOTE:** Contrib extensions are not eligible for Envoy security team coverage.

### Threat model

See https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/threat_model.
Vulnerabilities are evaluated against this threat model when deciding whether to activate the Envoy
security release process.

### Fix Team Organization

These steps should be completed within the first 24 hours of disclosure.

- The Fix Lead will work quickly to identify relevant engineers from the affected projects and
  packages and CC those engineers into the disclosure thread. These selected developers are the Fix
  Team.
- The Fix Lead will get the Fix Team access to private security repos to develop the fix.

### Fix Development Process

These steps should be completed within the 1-7 days of Disclosure.

- The Fix Lead and the Fix Team will create a
  [CVSS](https://www.first.org/cvss/specification-document) using the [CVSS
  Calculator](https://www.first.org/cvss/calculator/3.0). The Fix Lead makes the final call on the
  calculated CVSS; it is better to move quickly than making the CVSS perfect.
- The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs
  on all commits in the private repo from one or more maintainers.

If the CVSS score is under 4.0 ([a low severity
score](https://www.first.org/cvss/specification-document#i5)) the Fix Team can decide to slow the
release process down in the face of holidays, developer bandwidth, etc. These decisions must be
discussed on the envoy-security mailing list.

A three week window will be provided to members of the private distributor list from candidate patch
availability until the security release date. It is expected that distributors will normally be able
to perform a release within this time window. If there are exceptional circumstances, the Envoy
security team will raise this window to four weeks. The release window will be reduced if the
security issue is public or embargo is broken.

We will endeavor not to overlap this three week window with or place it adjacent to major corporate
holiday periods or end-of-quarter (e.g. impacting downstream Istio releases), where possible.

### Fix and disclosure SLOs

* All reports to envoy-security@googlegroups.com will be triaged and have an
  initial response within 1 business day.

* Privately disclosed issues will be fixed or publicly disclosed within 90 days
  by the Envoy security team. In exceptional circumstances we reserve the right
  to work with the discloser to coordinate on an extension, but this will be
  rarely used.

* Any issue discovered by the Envoy security team and raised in our private bug
  tracker will be converted to a public issue within 90 days. We will regularly
  audit these issues to ensure that no major vulnerability (from the perspective
  of the threat model) is accidentally leaked.

* Fuzz bugs are subject to a 90 day disclosure deadline.

* Three weeks notice will be provided to private distributors from patch
  availability until the embargo deadline.

* Public zero days which affect the optimized binary will be fixed ASAP, but there is
  no SLO for this, since this will depend on the severity and impact to the
  organizations backing the Envoy security team. After a zero day bug fix is in, the
  PST will kick off point releases unless the bug is deemed unlikely to be encountered
  in production (e.g. only triggered by trace logs) at which point there will instead be an email
  to envoy-announce and users can request point releases if they believe they will be affected.
  Publicly announced bugs which only affect debug binaries will neither trigger point
  releases nor announce emails.

### Fix Disclosure Process

With the fix development underway, the Fix Lead needs to come up with an overall communication plan
for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix
or mitigation so that a realistic timeline can be communicated to users.

**Disclosure of Forthcoming Fix to Users** (Completed within 1-7 days of Disclosure)

- The Fix Lead will email [envoy-security-announce@googlegroups.com](https://groups.google.com/forum/#!forum/envoy-security-announce)
  (CC [envoy-announce@googlegroups.com](https://groups.google.com/forum/#!forum/envoy-announce))
  informing users that a security vulnerability has been disclosed and that a fix will be made
  available at YYYY-MM-DD HH:MM UTC in the future via this list. This time is the Release Date.
- The Fix Lead will include any mitigating steps users can take until a fix is available.

The communication to users should be actionable. They should know when to block time to apply
patches, understand exact mitigation steps, etc.

**Optional Fix Disclosure to Private Distributors List** (Completed within 1-14 days of Disclosure):

- The Fix Lead will make a determination with the help of the Fix Team if an issue is critical enough
  to require early disclosure to distributors. Generally this Private Distributor Disclosure process
  should be reserved for remotely exploitable or privilege escalation issues. Otherwise, this
  process can be skipped.
- The Fix Lead will email the patches to cncf-envoy-distributors-announce@lists.cncf.io so
  distributors can prepare builds to be available to users on the day of the issue's announcement. Any
  patches against main will be updated and resent weekly.
  Distributors should read about the [Private Distributors List](#private-distributors-list) to find
  out the requirements for being added to this list.
- **What if a vendor breaks embargo?** The PST will assess the damage. The Fix Lead will make the
  call to release earlier or continue with the plan. When in doubt push forward and go public ASAP.

**Fix Release Day** (Completed within 1-21 days of Disclosure)

- The maintainers will create a new patch release branch from the latest patch release tag + the fix
  from the security branch. As a practical example if v1.5.3 is the latest patch release in Envoy.git
  a new branch will be created called v1.5.4 which includes only patches required to fix the issue.
- The Fix Lead will cherry-pick the patches onto the main branch and all relevant release branches.
  The Fix Team will LGTM and merge. Maintainers will merge these PRs as quickly as possible. Changes
  shouldn't be made to the commits even for a typo in the CHANGELOG as this will change the git sha
  of the commits leading to confusion and potentially conflicts as the fix is cherry-picked around
  branches.
- The Fix Lead will request a CVE from [DWF](https://github.com/distributedweaknessfiling/DWF-Documentation)
  and include the CVSS and release details.
- The Fix Lead will email envoy-{dev,users,announce}@googlegroups.com now that everything is public
  announcing the new releases, the CVE number, and the relevant merged PRs to get wide distribution
  and user action. As much as possible this email should be actionable and include links on how to apply
  the fix to user's environments; this can include links to external distributor documentation.
- The Fix Lead will remove the Fix Team from the private security repo.

The reporter of a vulnerability will be granted early access to fix patches upon
request, prior to the general disclosure of patches to the Private Distributors
List for the purpose of testing and internal vulnerability mitigation. They must
accept the embargo policy below in order for this to occur. If the vulnerability
reporter is also responsible for developing fix patches, they may make use of
the patches internally in their organization at any point in the fix cycle.

### Retrospective

These steps should be completed 1-3 days after the Release Date. The retrospective process
[should be blameless](https://landing.google.com/sre/book/chapters/postmortem-culture.html).

- The Fix Lead will send a retrospective of the process to envoy-dev@googlegroups.com including
  details on everyone involved, the timeline of the process, links to relevant PRs that introduced
  the issue, if relevant, and any critiques of the response and release process.
- Maintainers and Fix Team are also encouraged to send their own feedback on the process to
  envoy-dev@googlegroups.com. Honest critique is the only way we are going to get good at this as a
  community.

## Private Distributors List

This list is intended to be used primarily to provide actionable information to
multiple distribution vendors as well as a *limited* set of high impact end users at once. *This
list is not intended in the general case for end users to find out about security issues*.

### Embargo Policy

The information members receive on cncf-envoy-distributors-announce must not be made public, shared, nor
even hinted at anywhere beyond the need-to-know within your specific team except with the list's
explicit approval. This holds true until the public disclosure date/time that was agreed upon by the
list. Members of the list and others may not use the information for anything other than getting the
issue fixed for your respective users.

Before any information from the list is shared with respective members of your team required to fix
said issue, they must agree to the same terms and only find out information on a need-to-know basis.

We typically expect a single point-of-contact (PoC) at any given legal entity. Within the
organization, it is the responsibility of the PoC to share CVE and related patches internally. This
should be performed on a strictly need-to-know basis with affected groups to the extent that this is
technically plausible. All teams should be aware of the embargo conditions and accept them.
Ultimately, if an organization breaks embargo transitively through such sharing, they will lose
the early disclosure privilege, so it's in their best interest to carefully share information internally,
following best practices and use their judgement in balancing the tradeoff between protecting users
and maintaining confidentiality.

The embargo applies to information shared, source code and binary images. **It is a violation of the
embargo policy to share binary distributions of the security fixes before the public release date.**
This includes, but is not limited to, Envoy binaries and Docker images. It is expected that
distributors have a method to stage and validate new binaries without exposing them publicly.

If the information shared is under embargo from a third party, where Envoy is one of many projects
that a disclosure is shared with, it is critical to consider that the ramifications of any leak will
extend beyond the Envoy community and will leave us in a position in which we will be less likely to
receive embargoed reports in the future.

In the unfortunate event you share the information beyond what is allowed by this policy, you _must_
urgently inform the envoy-security@googlegroups.com mailing list of exactly what information leaked
and to whom. A retrospective will take place after the leak so we can assess how to prevent making the
same mistake in the future.

If you continue to leak information and break the policy outlined here, you will be removed from the
list.

### Contributing Back

This is a team effort. As a member of the list you must carry some water. This
could be in the form of the following:

**Technical**

- Review and/or test the proposed patches and point out potential issues with
  them (such as incomplete fixes for the originally reported issues, additional
  issues you might notice, and newly introduced bugs), and inform the list of the
  work done even if no issues were encountered.

**Administrative**

- Help draft emails to the public disclosure mailing list.
- Help with release notes.

### Membership Criteria

To be eligible for the cncf-envoy-distributors-announce mailing list, your
use of Envoy should:

1. Be either:
   1. An actively maintained distribution of Envoy components. An example is
      "SuperAwesomeLinuxDistro" which offers Envoy pre-built packages. Another
      example is "SuperAwesomeServiceMesh" which offers a service mesh product
      that includes Envoy as a component.

   OR

   2. Offer Envoy as a publicly available infrastructure or platform service, in
      which the product clearly states (e.g. public documentation, blog posts,
      marketing copy, etc.) that it is built on top of Envoy. E.g.,
      "SuperAwesomeCloudProvider's Envoy as a Service (EaaS)". An infrastructure
      service that uses Envoy for a product but does not publicly say they are
      using Envoy does not *generally* qualify (see option 3 that follows). This is essentially IaaS
      or PaaS. If you use Envoy to support a SaaS, e.g. "SuperAwesomeCatVideoService", this does not
      *generally* qualify.

   OR

   3. An end user of Envoy that satisfies the following requirements:
       1. Is "well known" to the Envoy community. Being "well known" is fully subjective and
          determined by the Envoy maintainers and security team. Becoming "well known" would
          generally be achieved by activities such as: PR contributions, either code or
          documentation; helping other end users on Slack, GitHub, and the mailing lists; speaking
          about use of Envoy at conferences; writing about use of Envoy in blog posts; sponsoring
          Envoy conferences, meetups, and other activities; etc. This is a more strict variant of
          item 5 below.
       2. Is of sufficient size, scale, and impact to make your inclusion on the list
          worthwhile. The definition of size, scale, and impact is fully subjective and
          determined by the Envoy maintainers and security team. The definition will not be
          discussed further in this document.
       3. You *must* smoke test and then widely deploy security patches promptly and report back
          success or failure ASAP. Furthermore, the Envoy maintainers may occasionally ask you to
          smoke test especially risky public PRs before they are merged. Not performing these tasks
          in a reasonably prompt timeframe will result in removal from the list. This is a more
          strict variant of item 7 below.
       4. In order to balance inclusion in the list versus a greater chance of accidental
          disclosure, end users added to the list via this option will be limited to a total of
          **10** slots. Periodic review (see below) may allow new slots to open, so please continue
          to apply if it seems your organization would otherwise qualify. The security team also
          reserves the right to change this limit in the future.
       5. Note that in this context "end user" is defined as an organization that *directly*
          operates Envoy in order to serve traffic for 1st party use cases. The 1st party use case
          can be either internal or external facing. Critically, vendors of cloud native software
          and solutions can *also* be end users. Being a vendor does not preclude an organization
          from being an end user as long as it satisfies the 1st party usage criteria.
2. Have a user or customer base not limited to your own organization (except for option 3 above).
   We will use the size of the user or customer base as part of the criteria to determine
   eligibility.
3. Have a publicly verifiable track record up to present day of fixing security
   issues.
4. Not be a downstream or rebuild of another distribution.
5. Be a participant and active contributor in the community.
6. Accept the [Embargo Policy](#embargo-policy) that is outlined above. You must
   have a way to privately stage and validate your updates that does not violate
   the embargo.
7. Be willing to [contribute back](#contributing-back) as outlined above.
8. Be able to perform a security release of your product within a three week window from candidate fix
   patch availability.
9. Have someone already on the list vouch for the person requesting membership
   on behalf of your distribution.
10. Nominate an e-mail alias or list for your organization to receive updates. This should not be
    an individual user address, but instead a list that can be maintained by your organization as
    individuals come and go. A good example is envoy-security@seven.com, a bad example is
    acidburn@seven.com. You must accept the invite sent to this address or you will not receive any
    e-mail updates. This e-mail address will be [shared with the Envoy community](#Members).

Note that Envoy maintainers are members of the Envoy security team. [Members of the Envoy security
team](OWNERS.md#envoy-security-team) and the organizations that they represent are implicitly
included in the private distributor list. These organizations do not need to meet the above list of
criteria with the exception of the acceptance of the embargo policy.

### Requesting to Join

New membership requests are sent to envoy-security@googlegroups.com.

In the body of your request please specify how you qualify and fulfill each
criterion listed in [Membership Criteria](#membership-criteria).

Here is a pseudo example:

```
To: envoy-security@googlegroups.com
Subject: Seven-Corp Membership to cncf-envoy-distributors-announce

Below are each criterion and why I think we, Seven-Corp, qualify.

> 1. Be an actively maintained distribution of Envoy components OR offer Envoy as a publicly
     available service in which the product clearly states that it is built on top of Envoy OR
     be a well known end user of sufficient size, scale, and impact to make your
     inclusion worthwhile.

We distribute the "Seven" distribution of Envoy [link]. We have been doing
this since 1999 before proxies were even cool.

OR

We use Envoy for our #1 rated cat video service and have 40 billion MAU, proxying 40 trillion^2 RPS
through Envoy at the edge. Secure cat videos are our top priority. We also contribute a lot to the Envoy
community by implementing features, not making Matt ask for documentation or tests, and writing blog
posts about efficient Envoy cat video serving.

> 2. Have a user or customer base not limited to your own organization. Please specify an
>    approximate size of your user or customer base, including the number of
>    production deployments.

Our user base spans of the extensive "Seven" community. We have a slack and
GitHub repos and mailing lists where the community hangs out. We have ~2000
customers, of which approximately 400 are using Seven in production. [links]

> 3. Have a publicly verifiable track record up to present day of fixing security
     issues.

We announce on our blog all upstream patches we apply to "Seven." [link to blog
posts]

> 4. Not be a downstream or rebuild of another distribution. If you offer Envoy as a publicly
>    available infrastructure or platform service, this condition does not need to apply.

This does not apply, "Seven" is a unique snowflake distribution.

> 5. Be a participant and active contributor in the community.

Our members, Acidburn, Cereal, and ZeroCool are outstanding members and are well
known throughout the Envoy community. Especially for their contributions
in hacking the Gibson.

> 6. Accept the Embargo Policy that is outlined above. You must
     have a way to privately stage and validate your updates that does not violate
     the embargo.

We accept.

> 7. Be willing to contribute back as outlined above.

We are definitely willing to help!

> 8. Be able to perform a security release of your product within a three week window from candidate fix
     patch availability.

We affirm we can spin out new security releases within a 2 week window.

> 9. Have someone already on the list vouch for the person requesting membership
>    on behalf of your distribution.

CrashOverride will vouch for the "Seven" distribution joining the distribution list.

> 10. Nominate an e-mail alias or list for your organization to receive updates. This should not be
      an individual user address, but instead a list that can be maintained by your organization as
      individuals come and go. A good example is envoy-security@seven.com, a bad example is
      acidburn@seven.com. You must accept the invite sent to this address or you will not receive any
      e-mail updates. This e-mail address will be shared with the Envoy community.

envoy-security@seven.com
```

### Review of membership criteria

In all cases, members of the distribution list will be reviewed on a yearly basis by the maintainers
and security team to ensure they still qualify for inclusion on the list.

### Members

| Organization  | End User | Last Review |
|:-------------:|:--------:|:-----------:|
| AWS           | No       | 06/21       |
| Cilium        | No       | 06/21       |
| Cloud Foundry | No       | 06/21       |
| Datawire      | No       | 06/21       |
| F5            | No       | 06/21       |
| Google        | No       | 06/21       |
| IBM           | No       | 06/21       |
| Istio         | No       | 06/21       |
| Microsoft     | No       | 2/21        |
| Red Hat       | No       | 06/21       |
| solo.io       | No       | 06/21       |
| Tetrate       | No       | 06/21       |
| VMware        | No       | 06/21       |
| Pinterest     | Yes      | 06/21       |
| Dropbox       | Yes      | 01/20       |
| Stripe        | Yes      | 01/20       |
| Square        | Yes      | 05/21       |
| Apple         | Yes      | 05/21       |
| Spotify       | Yes      | 06/21       |
| Netflix       | Yes      | 06/22       |
